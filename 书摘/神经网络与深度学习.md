### 概率图模型

概率图模型（Probabilistic Graphical Model，PGM），简称图模型（Graphical Model，GM），是指一种用图结构来描述多元随机变量之间条件独立关系的概率模型，从而给研究高维空间中的概率模型带来了很大的便捷性。

对于一个$K$维随机向量$\boldsymbol{X} = [X_1, X_2, \cdots, X_K]^\top$，其联合概率为高维空间中的分布，一般难以直接建模 (随机变量用斜体的大写字母表示，其取值用斜体的小写字母表示；随机向量用粗斜体的大写字母表示，其取值用粗斜体的小写字母表示。) 。一种有效减少参数量的方法是独立性假设。当概率模型中的变量数量比较
多时，其条件依赖关系也比较复杂。我们可以使用图结构的方式将概率模型可视化，以一种直观、简单的方式描述随机变量之间的条件独立性，并可以将一个复杂的联合概率模型分解为一些简单的条件概率模型的组合。


#### 模型表示

图由一组节点和节点之间的边组成。在概率图模型中，每个节点都表示一个（或一组）随机变量，边表示这些随机变量之间的概率依赖关系。常见的概率图模型可以分为两类：有向图模型和无向图模型。在有向图模型中，如果两个节点之间有连边，表示对应的两个变量为因果关系。常见的有向图模型如朴素贝叶斯模型、隐马尔可夫模型、深度信念
网络等。在无向图模型中，每条边代表两个变量之间有概率依赖关系，但是并不一定是因果关系。常见的无向图模型如条件随机场、玻尔兹曼机等。


#### 学习

图模型的学习可以分为两部分：一是网络结构学习，即寻找最优的网络结构；二是网络参数估计，即已知网络结构，估计每个条件概率分布的参数。网络结构学习比较困难，一般是由领域专家来构建。图模型的参数估计问题又分为不包含隐变量时的参数估计问题和包含隐变量时的参数估计问题。

含隐变量的参数估计 如果图模型中包含隐变量，即有部分变量是不可观测的，就需要用EM算法进行参数估计。

EM算法。在一个包含隐变量的图模型中，令$\boldsymbol{X}$定义可观测变量集合，$\boldsymbol{Z}$定义隐变量集合。


### 生成模型

在一个高维空间$\mathcal X$中，存在一个随机向量$\boldsymbol{X}$服从一个未知的数据分布$p_r(\boldsymbol{x}), \boldsymbol{x}\in\mathcal X$。生成模型是根据一些可观测的样本$\boldsymbol{x}^{(1)}, \boldsymbol{x}^{(2)}, \cdots, \boldsymbol{x}^{(N)}$来学习一个参数化的模型$p_\theta (\boldsymbol{x})$来近似未知分布$p_r(\boldsymbol{x})$，并可以用这个模型来生成一些样本，使得“生成”的样本和“真实”的样本尽可能地相似。生成模型通常包含两个基本功能：概率密度估计和生成样本（即采样）。深度生成模型就是利用深度神经网络可以近似任意函数的能力来建模一个复杂分布$p_r(\boldsymbol{x})$或直接生成符合分布$p_r(\boldsymbol{x})$的样本。        


#### 密度估计

给定一组数据$\mathcal{D} = \{\boldsymbol{x}^{(n)}\}_{n=1}^N$，假设它们都是独立地从相同的概率密度函数为$p_r(\boldsymbol{x})$的未知分布中产生的。密度估计（Density Estimation）是根据数据集$\mathcal{D}$来估计其概率密度函数 $p_\theta(\boldsymbol{x})$。

在机器学习中，密度估计是一类无监督学习问题。比如在手写体数字图像的密度估计问题中，我们将图像表示为一个随机向量 $\boldsymbol{X}$，其中每一维都表示一个像素值。假设手写体数字图像都服从一个未知的分布 $p_r(\boldsymbol{x})$，希望通过一些观测样本来估计其分布。由于所以直接建模$p_r(\boldsymbol{x})$比较困难，通常通 过引入隐变量$\boldsymbol{z}$来简化模型，这样密度估计问题可以转换为估计变量$(\boldsymbol{x}, \boldsymbol{z})$的两个局部条件概率$p_\theta(\boldsymbol{z})$和$p_\theta(\boldsymbol{x}|\boldsymbol{z})$。一般为了简化模型，假设隐变量$\boldsymbol{z}$的先验分布为标准高斯分布$\mathcal{N}(\boldsymbol{0}, \boldsymbol{I})$。隐变量$\boldsymbol{z}$的每一维之间都是独立的。在这个假设下，先验分布$p(\boldsymbol{z}; \theta)$中没有参数。因此，密度估计的重点是估计条件分布$p(\boldsymbol{x|z}; \theta)$。

如果要建模含隐变量的分布，就需要利用EM算法来进行密度估计。而在EM算法中，需要估计条件分布$p(\boldsymbol{x|z}; \theta)$以及近似后验分布$p(\boldsymbol{z|x}; \theta)$。当这两个分布比较复杂时，我们可以利用神经网络来进行建模，这就是\uwave{变分自编
码器}的思想。


### 附录  数学基础


#### B.4.2.2  Softmax函数

Softmax函数可以将多个标量映射为一个概率分布。对语$K$个标量$x_1, \cdots, x_K$，Softmax函数定义为
$$z_k = \text{softmax}(x_k) = \frac{\text{exp}(x_k)}{\sum_{i=1}^K \exp (x_i)}$$
这样，我们可以将$K$个标量$x_1, \cdots, x_K$转换为一个分布：$z_1, \cdots, z_K$，满足：
$$z_k\in (0, 1), \forall k$$
$$\sum_{k=1}^K z_k = 1$$