- 在使用基础模型之前应评估成本和收益：（1）部署基础模型的社会成本和环境成本是否大于模型的社会效益？ (2)另一种计算更简单、成本更低的方法是否能实现可比的社会效益？
- GPT-3 训练成本超过 460 万美元，研发成本在 1140 万美元到 2760 万美元之间，运行 GPT-3 所需的硬件成本在 10 万美元到 15 万美元之间，不考虑其他成本（电力、冷却、备份、等），每年的运行成本至少为 87,000 美元。 


# 摘要

随着模型（例如 BERT、DALL-）的兴起，人工智能正在经历范式转变E，GPT-3）接受了广泛的数据训练（通常使用大规模的自我监督），可以适应广泛的下游任务。我们将这些模型称为基础模型，以强调它们至关重要但不完整的特征。该报告全面介绍了基础模型的机遇和风险，从其能力（例如，语言、视觉、机器人操作、推理、人类交互）到技术原理（例如，模型架构、训练程序、数据、系统、安全、评估、理论）及其应用（例如法律、医疗保健、教育）和社会影响（例如不平等、滥用、经济和环境影响、法律和道德考虑）。尽管基础模型基于标准深度学习和迁移学习，但它们的规模会带来新的涌现能力，并且它们在如此多任务中的有效性会刺激同质化。同质化提供了强大的杠杆作用，但需要谨慎，因为基础模型的缺陷会被下游所有适应的模型继承。尽管基础模型即将得到广泛部署，但我们目前对它们如何工作、何时失败以及由于其新兴特性而能够做什么还缺乏清晰的了解。为了解决这些问题，我们相信对基础模型的许多关键研究将需要与其基本社会技术性质相称的深入的跨学科合作。

# 1 引言 

本报告研究了一种基于一般模型类别构建人工智能 (AI) 系统的新兴范式我们将其称为基础模型。2基础模型是在广泛数据（通常使用大规模自我监督）上进行训练的任何模型，可以适应（例如微调）广泛的下游任务；当前的例子包括 BERT [Devlin 等人。 2019]、GPT-3 [Brown 等人 2020] 和 CLIP [Radford 等人 2021]。从技术角度来看，基础模型并不新鲜——它们基于深度神经网络和自我监督学习，这两者都已经存在了几十年。然而，过去几年基础模型的规模和范围已经超出了我们对可能性的想象。例如，GPT-3 拥有 1750 亿个参数，可以通过自然语言提示进行调整，以在广泛的任务上完成尚可的工作，尽管没有经过明确的训练来完成其中许多任务 [Brown et al.2020]。与此同时，现有的基础模型有可能加剧危害，而且人们普遍对其特征知之甚少。鉴于它们即将广泛部署，它们已成为密切关注的话题 [Bender 等人。 2021]。 

## 1.1 涌现与同质化 

基础模型的意义可以用两个词来概括：涌现与同质化(homogenization)。涌现意味着系统的行为是隐式引发的，而不是显式构建的；它既是科学兴奋的根源，也是对意外后果的焦虑的根源。同质化意味着跨广泛应用构建机器学习系统的方法的整合；它为许多任务提供了强大的杠杆作用，但也造成了单点故障。为了更好地理解涌现和同质化，让我们回顾一下过去 30 年人工智能研究的崛起。

图 1. 人工智能的故事是不断涌现和同质化的故事。随着机器学习的引入，任务的执行方式从示例中浮现（自动推断）；通过深度学习，用于预测的高级特征出现；通过基础模型，甚至可以出现情境学习等高级功能。同时，机器学习使学习算法同质化（例如逻辑回归），深度学习使模型架构同质化（例如卷积神经网络），基础模型使模型本身同质化（例如GPT-3）。机器学习。如今，大多数人工智能系统都由机器学习提供支持，其中预测模型根据历史数据进行训练并用于做出未来预测。人工智能领域机器学习的兴起始于 20 世纪 90 年代，代表着人工智能系统构建方式的显着转变：学习算法不是指定如何解决任务，而是根据数据诱导任务，即如何产生任务学习的动力。机器学习还2我们选择了术语基础模型来捕捉这些模型未完成但重要的状态 - 请参阅§1.1.1：命名以进一步讨论该名称。4基础模型研究中心（CRFM）代表了迈向同质化的一步：现在，诸如逻辑回归之类的单一通用学习算法可以为广泛的应用程序提供支持。尽管机器学习在人工智能中无处不在，但自然语言处理（NLP）和计算机视觉中语义复杂的任务（例如问答或对象识别）（输入是句子或图像）仍然需要领域专家执行“特征编码”。 gineering”——即编写特定领域的逻辑将原始数据转换为更适合流行机器学习方法的更高级别特征（例如计算机视觉中的 SIFT [Lowe 1999]）。深度学习。 2010 年左右，深度神经网络以深度学习的名义复兴[LeCun et al.2015]，开始在机器学习领域获得关注。更大的数据集、更多的计算（特别是 GPU 的可用性）和更大的大胆性推动了深度学习。深度神经网络将在原始输入（例如像素）上进行训练，并且更高级别的特征将通过训练出现（称为“表示学习”的过程）。这导致了标准基准测试的巨大性能提升，例如 AlexNet [Krizhevsky et al.2012] 在 ImageNet 数据集 [Deng et al.2009] 上的开创性工作。深度学习还反映出向同质化的进一步转变：不再为每个应用程序提供定制的特征工程管道，而是可以将相同的深度神经网络架构用于许多应用程序。基础模型。基础模型在 NLP 中已经形成最为牢固，因此我们暂时将故事重点放在此处。也就是说，正如深度学习在计算机视觉中普及但也存在于计算机视觉之外一样，我们将基础模型理解为人工智能的一般范式，而不是特定于 NLP 的任何方式。

2018年底，NLP领域即将经历另一场巨变，标志着基础模型时代的开始。在技术层面上，基础模型是通过迁移学习 [Thrun 1998] 和规模来实现的。迁移学习的想法是将从一项任务（例如图像中的对象识别）中学到的“知识”应用于另一项任务（例如视频中的活动识别）。在深度学习中，预训练是迁移学习的主要方法：模型在代理任务上进行训练（通常只是作为达到目的的手段），然后通过微调适应感兴趣的下游任务。迁移学习使基础模型成为可能，但规模使它们变得强大。规模需要三个要素：(i) 计算机硬件的改进——例如，GPU 吞吐量和内存在过去四年中增加了 10 倍（§4.5：系统）； (ii) Transformer 模型架构的开发 [Vaswani et al.2017]，它利用硬件的并行性来训练比以前更具表现力的模型（第 4.1 节：建模）； (iii) 可以获得更多的训练数据。数据可用性和利用数据的能力的重要性不容低估。使用带注释的数据集进行迁移学习已经成为至少十年来的常见做法，例如，在计算机视觉社区中对 ImageNet 数据集 [Deng et al.2009] 进行图像分类的预训练。然而，注释的巨大成本对预训练的好处施加了实际限制。另一方面，在自监督学习中，预训练任务是从未注释的数据中自动导出的。3例如，用于训练 BERT 的掩码语言建模任务 [Devlin et al.2019] 是在给定其周围的情况下预测句子中缺失的单词上下文（例如，我喜欢豆芽）。自监督任务不仅更具可扩展性，仅依赖于未标记的数据，而且它们旨在迫使模型预测部分输入，使它们比在更有限的标签空间上训练的模型更丰富且可能更有用。

有趣的是，自监督学习在深度学习的早期占据主导地位 [Hinton et al.2006]，但十年来，随着标记数据集变得越来越大，自监督学习在很大程度上被纯监督学习所取代。关于基础模型的机遇和风险 5自词嵌入以来，自监督学习已经取得了相当大的进展 [Turian et al.2010;米科洛夫等人，2013； Pennington et al.2014]将每个单词与上下文无关的向量相关联，为广泛的 NLP 模型提供了基础。此后不久，基于自回归语言模型的自监督学习（根据前面的单词预测下一个单词）[Dai and Le 2015]开始流行。这产生了在上下文中表示单词的模型，例如 GPT [Radford et al.2018]、ELMo [Peters et al.2018] 和 ULMFiT [Howard and Ruder 2018]。4 自我监督学习的下一波发展 - BERT [Devlin et al.2019] GPT-2 [Radford et al.2019]、RoBERTa [Liu et al.2019]、T5 [Raffel et al.2019]、BART [Lewis et al.2020a] — 快速跟进，拥抱Transformer 架构，包含更强大的句子深度双向编码器，并扩展到更大的模型和数据集。虽然人们可以纯粹通过自我监督学习的视角来看待最后一波技术发展，但 BERT 的引入出现了一个社会学转折点。

2019 年之前，使用语言模型进行自我监督学习本质上是 NLP 的一个子领域，它与 NLP 的其他发展并行。 2019 年之后，语言模型的自监督学习更多地成为 NLP 的基础，因为使用 BERT 已成为常态。人们接受单一模型可用于如此广泛的任务，这标志着基础模型时代的开始。基础模型导致了前所未有的同质化水平：几乎所有最先进的 NLP 模型现在都改编自少数基础模型之一，例如 BERT、RoBERTa、BART、T5 等。高杠杆（基础模型的任何改进都可以为整个 NLP 带来立竿见影的好处），但它也是一种责任；所有人工智能系统都可能继承一些基础模型的相同问题偏差 [Bolukbasi 等人。 2016年； Caliskan 等人，2017； Abid 等人 .2021，除其他外]) — 请参阅第 5.1 节：公平，第 5.6 节：道德以供进一步讨论。我们也开始看到研究界的同质化。例如，类似的基于 Transformer 的序列建模方法现在应用于文本 [Devlin et al.2019;雷德福德等人，2019； Raffel et al.2019]，图像[Dosovitskiy et al.2020； Chen et al.2020d]、语音 [Liu et al.2020d]、表格数据 [Yin et al.2020]、蛋白质序列 [Rives et al.2021]、有机分子 [Rothchild et al.2021] 和强化学习 [ Chen 等人.2021b； Janner 等人，2021]。这些例子指出了一个可能的未来，即我们拥有一套统一的工具来开发各种模式的基础模型 [Tamkin 等人，2017]。 2021b]。除了方法的同质化之外，我们还看到跨研究社区以多模态模型的形式存在实际模型的同质化——例如，在语言和视觉数据上训练的基础模型[Luo et al.2020; Kim 等人，2021a； Cho 等人，2021；拉梅什等人，2021； Radford 等人，2021]。在某些领域，数据自然是多模态的，例如医学图像、结构化数据、医疗保健中的临床文本（§3.1：医疗保健）。因此，多模式基础模型是融合某个领域的所有相关信息并适应跨多种模式的任务的自然方式（图 2）。基础模型也因规模而导致令人惊讶的出现。例如，GPT-3 [Brown et al.2020] 具有 1750 亿个参数，而 GPT-2 有 15 亿个参数，允许上下文学习，其中语言模型只需提供一个参数即可适应下游任务。提示（任务的自然语言描述），一种既没有经过专门训练也没有预期会出现的突发属性。 4Collobert 和 Weston [2008] 的有先见之明的工作是相关的：他们对类似于屏蔽语言建模的可扩展任务与下游任务联合进行训练，而不是生成一个可以在事后适应下游任务的单一基础模型。

基础模型研究 (CRFM) 图 2。基础模型可以集中来自各种模式的所有数据的信息。然后，这一模型可以适应各种下游任务。同质化和涌现以一种潜在令人不安的方式相互作用。同质化可能会为许多特定任务数据相当有限的领域带来巨大收益——请参阅几个此类领域中提供的机会（例如，§3.1：医疗保健，§3.2：法律，§3.3：教育）；另一方面，模型中的任何缺陷都会被所有适应的模型盲目继承（§5.1：公平，§5.6：道德）。由于基础模型的力量来自于它们的涌现质量而不是它们的显式构造，因此现有的基础模型很难理解（§4.4：评估，§4.10：理论，§4.11：可解释性），并且它们具有意想不到的失败模式（§4.7：安全性，§4.8：稳健性）。由于涌现会对基础模型的能力和缺陷产生很大的不确定性，因此通过这些模型进行激进的同质化是有风险的。从道德（§5.6：道德）和人工智能安全（§4.9：人工智能安全）的角度来看，去风险是进一步开发基础模型的核心挑战。 1.1.1 命名。我们引入基础模型这个术语来填补描述我们正在见证的范式转变的空白；我们简要叙述了我们做出这一决定的一些理由。现有术语（例如，预训练模型、自监督模型）部分捕获了这些模型的技术维度，但未能以机器学习之外的人员可理解的方式捕获范式转变的重要性。特别是，基础模型指定了一个模型类别，该模型类别具有独特的社会学影响以及它们如何赋予人工智能研究和部署广泛的转变。相比之下，技术上预示的基础模型的预训练和自我监督形式未能阐明我们希望强调的实践转变。关于基础模型的机遇和风险 7 图 3。

在推理基础模型的社会影响之前，重要的是要了解它们是从数据创建到部署的更广泛生态系统的一部分。在两端，我们强调人作为基础模型训练的最终数据来源的作用，但也是任何利益和危害的下游接受者。深思熟虑的数据管理和适应应该成为任何人工智能系统负责任开发的一部分。最后，请注意，适应性基础模型的部署是与其构建分开的决定，可以用于研究。此外，虽然在撰写本文时许多标志性的基础模型都是语言模型，但术语“语言模型”对于我们的目的来说太狭窄了：正如我们所描述的，基础模型的范围远远超出了语言的范围。我们还考虑了通用模型和多用途模型等术语，这些术语抓住了这些模型可以服务于多个下游任务的重要方面，但两者都未能抓住其未完成的特征和适应的需要。诸如任务无关模型之类的术语可以捕捉训练的方式，但无法捕捉对下游应用程序的重大影响。我们选择新术语基础模型来识别作为本报告主题的模型和新兴范式。特别是，“基础”一词指定了这些模型所扮演的角色：基础模型本身并不完整，但可以作为通过适应构建许多特定任务模型的共同基础。我们还选择“基础”一词来暗示建筑稳定性、安全性和安全性的重要性：建造不良的基础是灾难的根源，而执行良好的基础是未来应用的可靠基石。目前，我们强调，我们不完全了解基础模型提供的基础的性质或质量；我们无法表征该基础是否值得信赖。因此，这对于研究人员、基础模型提供者、依赖基础模型的应用程序开发人员、政策制定者来说是一个关键问题

## 1.2 社会影响和基础模型生态系统

基础模型由于其令人印象深刻的性能和功能而在科学上很有趣，但研究它们的关键在于它们正在快速集成到现实世界的部署中人工智能系统对人类产生深远的影响。例如，拥有 40 亿用户的谷歌搜索现在依赖于像 BERT 这样的基础模型 [Devlin 等人。 2019] 作为其信号之一。5 5https://blog.google/products/search/search-language-understanding-bert/8 基础模型研究中心 (CRFM) 因此，我们必须停下来问一问：本质是什么这种社会影响？在本报告中，我们讨论了这个问题的许多方面：社会不平等的潜在加剧（第 5.1 节：公平）、由于能力增强而产生的经济影响（第 5.5 节：经济学）、由于计算需求增加而产生的环境影响（第 5.3 节） ：环境）、放大虚假信息的潜在担忧（§5.2：滥用）、强大生成能力带来的法律后果（§5.4：合法性）、同质化导致的伦理问题以及基础模型所在的更广泛的政治经济开发和部署（§5.6：道德）。鉴于基础模型千变万化的性质及其未映射的功能，我们如何负责任地预测和解决它们提出的道德和社会问题？一个反复出现的主题是，推理部署到特定用户的特定系统的社会影响比推理基础模型的社会影响更容易，因为基础模型可以适应任意数量的不可预见的下游系统。在尝试回答这些问题之前，我们需要打下一些基础。首先，我们区分基础模型的研究和基础模型的部署。大多数公众所知的是基础模型研究——通过学术论文、演示和排行榜上的进展。虽然知识的生产可以在塑造未来方面发挥至关重要的作用，但直接的社会影响是通过这些模型的实际部署来实现的，而这些模型通常是由私有数据的专有实践控制的。有时，部署是通过新产品进行的——例如，GitHub 的 Copilot6 基于 OpenAI 的 Codex 模型 [Chen 等人 .2021f]，但通常是通过升级现有产品（例如，使用 BERT 的 Google 搜索）。研究模型通常没有经过广泛的测试，并且可能具有未知的故障模式；应在不适合部署的研究模型上贴上警告标签。另一方面，实际影响人们生活的已部署基础模型应该接受更严格的测试和审核。为了进一步了解基础模型的研究和部署，我们必须把目光放远，考虑这些基础模型所处的完整生态系统，从数据创建到实际部署。值得注意的是，基础模型只是人工智能系统的一个组成部分（尽管这个组成部分越来越重要）。简单地说，我们可以按照阶段顺序来思考基础模型的生态系统，从而延长之前的训练和适应阶段。7适当地，由于我们对社会影响感兴趣，所以人们占据了管道的两端。这种生态系统观点让我们看到，关于基础模型的不同问题（例如，基础模型是否道德）实际上应该在不同阶段得到回答。 
- (1)数据创建：数据创建从根本上来说是一个以人为中心的过程：所有数据都是由人创建的，并且大多数数据至少隐含地与人有关。有时数据是由人们以电子邮件、文章、照片等形式为其他人创建的，有时它是对人的测量（例如基因组数据）或对人们居住环境的测量（例如卫星图像） ）。值得注意的是，所有数据都有一个所有者，并且是出于特定目的而创建的（该目的可能包括也可能不包括训练基础模型）。 
- (2)数据整理：然后将数据整理成数据集。数据不存在单一的自然分布；即使是最宽松的互联网抓取也需要一些选择和后过滤。确保数据的相关性和质量，同时尊重法律和道德约束至关重要，但也具有挑战性。虽然这一点在业界得到了认可，但在人工智能研究中却没有得到充分重视（§4.6：数据）。 6https://copilot.github.com/ 7实践中，管道结束后进行监控，并利用反馈重新调整前面的阶段。关于基础模型的机会和风险
- (3)训练：训练基础模型这些精心策划的数据集8是人工智能研究中著名的核心，尽管它只是众多阶段之一。
- (4)适应：在机器学习研究的背景下，适应是基于执行某些任务（例如文档摘要）的基础模型创建新模型。对于部署，适应是关于创建一个系统，该系统可能需要许多不同的模块、自定义规则（例如，对输出空间的限制）或分类器（例如，用于毒性分类），以及与其他补充信号的组合（例如，回答问题）模型生成的答案将根据相关文档进行验证）。例如，如果下游采取适当的预防措施，能够产生有毒内容的有问题的模型可能是可以容忍的。额外的特定于应用程序的逻辑对于减轻危害至关重要。
- (5)部署：人工智能系统的直接社会影响发生在将其部署到人们身上时。尽管我们不想部署在可疑数据上训练的潜在有害的基础模型，但允许它们参与研究以促进科学理解可能仍然有价值，尽管人们仍然必须谨慎行事。更一般地说，大规模部署中的标准做法是进行逐步发布，其中部署发生在越来越多的用户身上；这可以部分减轻任何潜在的危害。虽然本报告是关于基础模型的，但值得注意的是，许多影响来自管道中其他阶段做出的决策，每个阶段都需要深思熟虑的监控和干预。虽然大型组织可能拥有整个管道，但每个阶段都可以由不同的组织执行，例如专门为应用程序开发人员可以使用的各种领域创建自定义基础模型的公司。思考生态系统，行动模式。虽然社会影响取决于整个生态系统，但考虑到许多研究人员和从业者的权限仅限于训练阶段，能够推理基础模型的社会影响仍然很重要。这很困难，因为基础模型是未完成的中间对象，可以适应许多下游应用程序，有时可以通过完全不同的实体来实现不可预见的目的。

我们需要的是两件事：
- （i）一组具有代表性的潜在下游评估的替代指标（§4.4：评估），以及
- （ii）承诺记录这些指标[Mitchell et al.2019]，类似于材料的数据表例如金属和塑料，可以适应许多下游用例。描述基础模型潜在的下游社会影响具有挑战性，需要对技术生态系统和社会有深入的了解。如果不认识到如何部署基础模型，就无法充分评估基础模型的危害（第 5.1 节：公平性），也不能在不考虑丰富的社会和历史背景的情况下仅仅定义自动指标。 

## 1.3 基础模型的未来

基础模型已经展示了原始潜力，但我们仍处于早期阶段。尽管它们被部署到现实世界中，但这些模型在很大程度上只是研究原型，人们对其了解甚少。甚至围绕基础模型的专业规范——罗伯特·默顿所说的科学精神[Merton 1979]——也不够发达。例如，在一些基本问题上缺乏共识，例如模型何时可以“安全”发布，或者社区应如何应对方法不当行为。鉴于基础模型的未来充满了不确定性，一个大问题是：谁将决定这个未来？ 8 基础模型（例如 Codex）也可以用另一个模型（例如 GPT-3）作为起点进行训练。10 基础模型研究中心 (CRFM) 学科多样性。基础模型背后的技术基于机器学习、优化、NLP、计算机视觉和其他领域数十年的研究。这些技术贡献来自学术界和工业研究实验室。然而，构建基础模型本身的研究几乎完全发生在工业界——谷歌、Facebook、微软或华为等大型科技公司，或 OpenAI 或 AI21 Labs 等初创公司，尽管 AI2 是一个明显的例外。 2018；泽勒斯等人。 2019b]。技术进步的迅猛步伐和集中化带来的巩固引起了强烈的担忧，除了技术专家之外，还需要人文主义者和社会科学家的关注。我们不应该依赖于道德和社会后果的事后审计，这种审计仅在技术架构和部署决策做出之后进行。相反，我们需要从一开始就将社会考虑和伦理设计深深地融入到基础模型及其周围生态系统的技术开发中。学术机构的独特之处在于，它们在一个屋檐下拥有最广泛的学科，从而汇集了计算机科学家、社会科学家、经济学家、伦理学家、法律学者等。鉴于学科多样性在理解和解决技术、道德、法律、社会和政治层面 [Hong 和 Page 2004；所罗门 2006； Steel et al.2018]，因此，我们认为学术界在开发基础模型方面发挥着至关重要的作用，以促进其社会效益并减轻其社会危害，并确定每个阶段的行动背景。应严格禁止从数据管理到部署的生态系统（§1.2：生态系统）。激励措施。设计、开发和部署基础模型的政治经济学为每个阶段的决策提供了不可避免的激励结构。人们和机构如何对激励做出反应是经济学的基础课程。市场驱动的商业激励可以与社会效益很好地结合起来：使基础模型更加准确、可靠、安全和高效，同时寻找各种潜在的用例，可以产生大量的社会效用。然而，商业激励措施也可能导致市场失灵和股东无法获取创新价值的领域投资不足。正如制药业没有动力投入大量资源来研究和开发疟疾治疗方法，因为穷人买不起药物，9科技行业也没有动力投入大量资源来开发旨在改善穷人和边缘化群体状况的技术。人 [Reich et al.2021]。此外，商业激励可能导致公司忽视社会外部性[Acemoglu 2021；赖希等人。 2021]例如劳动力的技术取代、民主所需的信息生态系统的健康、计算资源的环境成本以及以利润为导向向非民主政权出售技术。最后，任何特定公司都没有动力去创建一个开放、去中心化的生态系统来开发鼓励广泛参与的基础模型。相比之下，大学长期而根深蒂固的研究使命是知识的生产和传播以及全球公共产品的创造[Kerr 2001；罗滕和卡尔霍恩 2011；努斯鲍姆 2010]。我们相信，学术界在塑造基础模型的发展方面具有独特的地位，以确保我们抓住具有潜在巨大社会效益的方向，而这些方向可能不会被工业界优先考虑。可访问性的损失。不幸的是，由于缺乏可及性，学术界未能以最充分的方式参与。深度学习革命经常被忽视的影响之一是可重复性和开放科学的增加：它日益成为常态 9参见 https://www.gatesfoundation.org/about/our-role.On the Opportunities and Risks of Foundation Models 11 公开发布代码和数据集，以及 TensorFlow [Abadi et al.2016] 和 PyTorch [Paszke et al.2019] 等软件包使人们更容易协作和构建彼此的工作。 ML 再现性挑战10 等举措以及主要会议采用的再现性检查表 [Pineau 等人 .2020]，以及 CodaLab Worksheets11 等平台帮助推进了再现性的社区标准。这导致了技术创新和进步的激增。基础模型开始扭转这一积极趋势。某些模型（例如 GPT-3）根本没有发布（仅向有限的人群提供 API 访问权限）。甚至数据集（例如，GPT-2）也没有发布。虽然经过训练的模型可能是可用的（例如 BERT），但由于计算成本更高且工程要求复杂，绝大多数人工智能研究人员无法进行基础模型的实际训练。一些有意义的研究仍然可以通过在学术预算范围内训练较小的模型来完成，并且确实可以通过缩放定律预测出令人惊讶的规律性[Kaplan 等人。 2020]使其成为一种可行的策略，适用于因规模而产生的差异是定量的（例如，准确性提高）的情况。然而，由于这些基础模型的新兴性质，一些功能（例如上下文学习）只能在足够规模的模型中得到证明，因此即使提出正确的问题也需要规模。还可以有效地研究已发布的现有模型；事实上，这导致 NLP 内部出现了一个大型子社区来探索这些模型 [Rogers et al.2020;曼宁等人，2020]。访问现有模型对于为下游应用程序提供动力或识别缺陷（例如偏差）可能很有用，但这可能不足以让我们为可以修复这些缺陷的基础模型设计更好的架构或训练目标（例如减轻偏差） ）。值得反思的是，当今的 NLP 研究中有多少是基于 BERT（一种特定的（并且有些任意的）基础模型）。鉴于需要将社会意识和道德设计融入这些模型的构建中，我们可能需要构建与当今存在的模型截然不同的基础模型。这将需要进行大规模的密集实验。 EleutherAI12 和 Hugging Face 的 BigScience 项目 13 等社区努力正在尝试训练大型基础模型，但行业可以训练的私有模型与向社区开放的模型之间的差距即使不扩大，也可能仍然很大。此外，如今的初创公司（OpenAI、Anthropic、AI21 Labs 等）比学术界拥有更丰富的资源，因此仍然有能力训练最大的基础模型（例如 OpenAI 的 GPT-3）。然而，大型科技公司在资源方面处于完全不同的水平，特别是在基础设施、用户和来自其市场地位的数据方面。基础模型的基本中心化性质意味着开发它们的进入壁垒将继续上升，因此即使是初创公司，尽管其敏捷性，也会发现难以竞争，这一趋势反映在搜索引擎的发展中[Radinsky] 2015]。缩小资源差距的一种方法是政府投资公共基础设施。我们可以将哈勃太空望远镜和大型强子对撞机等大型科学项目作为灵感，在这些项目中，大量投资使原本不可能的基础科学发现成为可能。人们可以想象一种类似的计算基础设施，基础模型的学术研究将从中受益匪浅。在美国，新生的国家研究云计划14就是朝这个方向迈出的一步。 10https://paperswithcode.com/rc2020 11https://worksheets.codalab.org/ 12https://www.eleuther.ai/ 13https://bigscience.huggingface.co/ 14https://hai.stanford.edu/policy /national-research-cloud12 基础模型研究中心 (CRFM) 另一种补充方法是依靠志愿者计算，其中数十亿个计算设备（节点）中的任何一个都可以连接到中央服务器并贡献计算。 Folding@home 项目已成功实施了这种模拟蛋白质动力学的方法 [Beberg et al.2009]。最近，Learning@home 项目正尝试利用志愿者计算来训练基础模型 [Ryabinin 和 Gusev 2020]。节点之间的高延迟连接以及训练基础模型的高带宽要求使这成为一个开放的技术挑战。概括。有巨大的经济激励来推动基础模型的能力和规模，因此我们预计未来几年技术将稳步进步。但一项主要依赖于突发行为的技术是否适合广泛部署到人们身上尚不清楚。很明显，我们需要保持谨慎，现在是时候建立专业规范，以便对基础模型进行负责任的研究和部署。学术界和工业界需要在这方面进行合作：工业界最终对如何部署基础模型做出具体决定，但我们也应该依靠学术界的学科多样性和围绕知识生产和社会效益的非商业激励来提供独特的指导开发和部署在技术和道德上都有基础的基础模型。 

## 1.4 本报告概述 

2021 年 3 月，我们在斯坦福大学创建了一个非正式社区，由对基础模型某些方面感兴趣的学生、教师和研究人员组成。从一开始，该社区不仅包括人工智能研究人员，还包括那些渴望应用的人其领域（例如，医疗保健和法律）的基础模型，以及那些对社会问题（例如，道德和经济学）感兴趣的人。随着讨论的进展，我们注意到相互之间的理解存在很多差距——技术如何运作、行业如何开发基础模型、如何考虑伦理问题等等，而现有文献只涵盖了零碎的内容。因此，我们希望更全面地了解基础模型，识别机会和风险，并为基础模型未来负责任的发展建立建设性愿景。这份报告的撰写是一项实验：我们有 100 多名来自不同背景的人聚集在一起，撰写一份涵盖基础模型各个方面的报告。本报告的很大一部分是对现有工作的调查，但通过多次讨论，我们将其统一在一份报告中，以突出所有跨学科的联系。结构。该报告分为 26 个部分，每个部分讨论基础模型的一个方面。这些部分分为四个部分：能力（§2：能力）、应用程序（§3：应用程序）、技术（§4：技术）和社会（§5：社会），尽管各部分之间存在许多联系。这些联系突出了一种综合方法，其中技术和功能的开发方式对实际社会问题敏感，同时受到应用的启发和扎根。虽然我们试图捕捉围绕基础模型的大多数重要主题，但这份报告不可避免地是不完整的，特别是当该领域发展迅速时。例如，许多应用程序（例如自然科学、音乐、金融、农业）不包括在内，尽管它们与我们选择讨论的应用程序一样可能受到影响。 15 有趣的是，这个社区促成了基础模型研究中心 (CRFM) 的成立，这是斯坦福大学以人为中心的人工智能研究所 (HAI) 的一项新的跨学科举措。关于基础模型的机会和风险 13 2. 能力 5. 社会 4. 技术语言视觉机器人推理交互哲学不平等误用环境合法性经济伦理建模训练适应评估系统数据安全鲁棒性人工智能安全与对齐理论可解释性 2.12.22.32.42.52.63。应用程序 HealthcareLawEducation3.13.23.3 4.14.24.34.44.54.6 4.114.104.94.84.7 5.15.25.35.45.55.6论文路线图 图 4. 本报告分为四个部分：能力、应用程序、技术和社会，每个部分包含一组部分，每个部分涵盖基础模型的一个方面。研究基础模型如何与神经科学、认知科学和心理学研究相关，以解释智力并帮助计算社会科学的努力来理解社会。14 基础模型研究中心 (CRFM) 作者贡献 Percy Liang 发起并概念化了框架和结构的总体报告。他和 Rishi Bommasani 共同领导了去中心化的写作工作，并为各个部分提供了指导。德鲁·A·哈德森 (Drew A. Hudson) 创建了报告中的所有图表，并与每个部分的作者讨论了它们的结构和内容。本报告的 26 个部分均由部分作者撰写，其姓名列于每个部分的开头。然而，许多讨论跨越多个部分，因此每个部分的实际贡献通常来自更广泛的集合。最后，我们注意到，并非所有作者都持有本报告中表达的所有观点。 

### 1.4.1 能力概述

基础模型获得了可为应用程序提供支持的各种功能。我们选择讨论五种潜在能力：处理不同模式（例如语言、视觉）、影响物理世界（机器人）、执行推理以及与人类互动（交互）的能力。最后，我们对它们的能力的潜在限制进行了哲学讨论。 
- §2.1：语言。 NLP 作为一个领域为基础模型开辟了道路。虽然这些模型主导了标准基准，但这些模型当前获得的能力与将语言描述为人类交流和思维的复杂系统的能力之间存在明显差距。针对这一点，我们强调全方位的语言变异（例如，不同的风格、方言、语言），鉴于某些变体的数据有限，这带来了机遇和挑战。此外，儿童语言习得比基础模型的训练样本效率更高；我们研究了文本和接地之外的信号如何帮助弥合这一差距。语言的这两个特征为未来基础模型研究提供了明确的方向。 
- §2.2：愿景。计算机视觉引领了人工智能中深度学习的采用[Russakovsky et al.2015]，证明在大型注释数据集上预训练的模型可以转移到许多下游设置。现在，基于网络规模的原始数据而不是精选数据集进行预训练，基础模型在计算机视觉领域正在兴起[例如，Radford et al.2021]。这些模型在该领域的标准任务（例如图像分类和对象检测）中显示出了有希望的结果，并且对图像之外的多模态和具体数据进行训练可能会在重大挑战（例如 3D 几何和物理理解、常识推理）上取得进展。我们还讨论了建模（例如，有效扩展到视频的能力）和评估（例如，高阶能力的测量）以及应用程序（例如，医疗保健的环境智能）和社会考虑（例如，监视），这将决定计算机视觉基础模型未来的影响。 
- §2.3：机器人技术。机器人研究的一个长期目标是开发能够在不同的物理环境中执行无数任务的“多面手”机器人。与语言和视觉不同的是，由于有大量的原始数据来训练这些模型，并且有虚拟应用程序可以应用这些模型，因此机器人技术由于锚定于物理世界而面临着根本性的挑战。 。开发新型机器人基础模型（本质上与语言和视觉模型不同）的主要挑战是获取有利于学习的足够的正确形式的数据：我们探索如何丰富的数据（例如，人类的通用视频、不特定于特定环境且跨模式（例如语言、视觉）的技术可能有助于弥合这一差距。这些新的机器人基础模型可以使任务规范和学习变得更加容易，从而引入新的应用（例如，更好的机器人辅助家务劳动）并提高稳健性和安全性的重要性（例如，正式的安全评估）。
- §2.4：推理和搜索。定理证明和程序综合等推理和搜索问题一直是人工智能领域长期存在的挑战。组合搜索空间使得传统的基于搜索的方法变得棘手。然而，众所周知，即使在最数学化的领域，人类也能凭直觉进行操作 [Lakoff 和 Núñez 2000]，事实上，AlphaGo 等现有工作已经表明深度神经网络可以有效地引导搜索空间。但人类还可以跨任务传递知识，从而促进更有效的适应和更抽象的推理能力。基础模型提供了缩小这一差距的可能性：它们的多用途性质以及强大的生成和多模式能力为控制搜索固有的组合爆炸提供了新的杠杆。 
- §2.5：互动。基础模型显示出改变人工智能系统开发人员和用户体验的明显潜力：基础模型由于其适应的样本效率降低了原型设计和构建人工智能应用程序的难度阈值，并由于其多模式和生成性而提高了新颖的用户交互的上限能力。这提供了我们鼓励向前发展的协同作用：开发人员可以提供更适合用户需求和价值观的应用程序，同时引入更加动态的交互形式和反馈机会。 
- §2.6：理解哲学。基础模型可以了解其训练数据的哪些内容？着眼于自然语言的案例，我们确定了关于理解本质的不同立场，并探讨了它们与我们的中心问题的相关性。我们的初步结论是，对未来基础模型理解自然语言的能力的怀疑可能还为时过早，尤其是在模型接受多模态数据训练的情况下。 

### 1.4.2 应用概述

目前，基础模型研究主要局限于计算机科学和人工智能，基础模型及其支持的应用的影响主要集中在科技行业。展望未来，基础模型呈现出明显的潜力，可以改变人工智能并将其影响范围扩展到科技行业以外的许多领域，这表明人工智能对人们的生活产生更普遍的影响。虽然有许多应用程序和领域需要考虑，但我们选择了三个应用程序——医疗保健、法律和教育——因为它们代表了我们社会的基本支柱。为了使基础模型对这些应用领域做出重大贡献，模型将需要特定的能力（§2：能力）以及技术创新（§4：技术）来考虑每个领域的独特考虑因素。此外，由于这些领域对社会功能至关重要（§5：社会），因此在这些领域应用基础模型需要深入研究社会技术问题，例如与数据（§4.6：数据）、隐私（§4.7：安全）相关的问题、可解释性（§4.11：可解释性）、公平性（§5.1：公平性）和道德（§5.6：道德）。 
- §3.1：医疗保健和生物医学。医疗保健任务（例如，通过疾病治疗来护理患者）和生物医学研究（例如，新疗法的科学发现）需要有限且昂贵的专业知识。由于跨多种模式（例如图像、文本、分子）训练基础模型的数据丰富，以及由于专家时间成本而提高样本适应效率的价值，基础模型在这些领域提供了明显的机会和知识。此外，基础模型可能允许改进医疗保健提供者和患者与人工智能系统交互的界面设计（§2.5：交互），并且它们的生成能力表明药物发现等开放式研究问题的潜力。同时，它们也带来了明显的风险（例如，加剧医学数据集和试验中的历史偏差）。为了负责任地释放这一潜力，需要深入参与社会技术基础模型研究中心 (CRFM) 的数据源和隐私问题以及模型的可解释性和可解释性，同时对医疗保健和生物医学基础模型的使用进行有效监管。 
- §3.2：法律。法律申请要求律师阅读并撰写长篇连贯的叙述，其中包含不断变化的背景并解读模糊的法律标准。基础模型可能会在这个领域带来好处：充足的数据以法律文件的形式存在，它们的生成能力非常适合法律所需的许多生成任务，但基础模型需要进行重大改进才能可靠地进行推理。各种信息来源以生成真实的长格式文档。与医疗保健领域（§3.1：医疗保健）一样，考虑到法律领域的专家时间和知识成本，基础模型适应的样本效率具有更高的价值，这可能允许将专业知识重新分配给紧迫的问题司法和政府服务问题。法律基础模型的负责任发展将需要对隐私进行具体考虑，并强调现有基础模型的核心局限性，这将需要在其行为的起源和对其生成的真实性的保证方面取得根本性的进步。 
- §3.3：教育。教育是一个复杂而微妙的领域；有效的教学涉及对学生认知的推理，并应反映学生的学习目标。基础模型的性质在这里带来了在人工智能教育领域尚未实现的希望：虽然教育中的某些数据流单独而言过于有限，无法训练基础模型，但利用领域外相关数据的能力（例如，互联网）并利用多种形式的数据（例如，教科书、数学公式、图表、基于视频的教程），共同为广泛适用于教育任务的基础模型带来了希望。如果基础模型能够显着提高教育相关能力，那么与基础模型的开放式生成（例如，问题生成）和交互（例如，向教师反馈）方面相一致的新应用程序就有明显的潜力；基础模型的样本有效适应表明适应性和个性化学习的能力更强。在这种情况下，需要重新考虑将技术应用于教育的标志（例如，学生隐私），同时某些问题变得更加关键（例如，教育中获得技术的不平等、技术辅助的剽窃）。 

### 1.4.3 技术概述

现在我们讨论构建更好的模型架构、训练和适应程序，当然还有扩展系统背后的技术。一个至关重要但经常被忽视的话题是数据——它来自哪里以及它的组成是什么？此外，我们希望基础模型能够应对分布变化并抵御攻击者。最后，我们希望从数学角度和经验角度理解基础模型为何有效。 
- §4.1：建模。基础模型的结构特性是什么？在建模部分，我们探索基础模型背后的底层架构并确定 5 个关键属性。首先，我们首先讨论计算模型的表达能力（捕获和吸收现实世界信息）和可扩展性（以熟练处理大量高维数据）。这些属性已通过现有架构成功实现，例如支撑迄今为止大多数基础模型的变压器网络 [Vaswani et al.2017]。然后，我们继续讨论对于下一代模型可能至关重要的属性，包括：多模态——消费、处理和可能产生来自不同来源和领域的内容，记忆容量——有效存储和检索所获得的知识，最后，组合性，促进对新场景和环境的成功推广。我们相信，要实现《论基础模型的机遇与风险》17 为基础模型设想的全部潜力，将取决于满足这些需求的建模进展。 
- §4.2：训练。训练目标以数学方式指定模型应如何从训练数据中学习和获取能力。训练基础模型的当前现状涉及特定模态的目标（例如，针对文本的掩码语言建模 [Devlin et al.2019] 和针对图像的 SimCLR [Chen et al.2020c]），这些目标通常是启发式选择的。我们预计基础模型的未来训练目标将反映两个变化：从系统证据和评估（§4.4：评估）中得出的原则性选择，以及跨数据源和模式提供丰富、可扩展且统一的训练信号的领域通用性。我们还讨论了重要的设计权衡，包括生成训练与判别训练、输入数据表示的选择以及涉及目标明确表示的未来训练目标的潜力。 
- §4.3：适应。基础模型是中介资产；它们尚未完成，通常不应直接使用，而是需要针对特定的下游任务进行调整。事实上的适应方法一直是微调，最近的工作表明轻量级微调替代方案和基于提示的方法可能会实现有利的准确性-效率权衡。展望未来，我们设想了一种更广泛的适应观点，而不仅仅是专门的基础模型来执行感兴趣的任务：适应将减轻独立基础模型的缺陷（例如，时间适应以反映世界随时间的变化） ）或引入限制（例如，与被遗忘权相关的 GDPR 合规性；§4.7：安全性）；这种对适应的更广泛的视角与对新的评估协议（§4.4：评估）的需求相一致，该协议系统地评估适应方法，同时控制适应所涉及的资源（例如，运行时、内存）和访问要求。 
- §4.4：评估。评估通过提供跟踪进度、理解模型并记录其能力和偏差的方法，为基础模型提供背景信息。基础模型挑战了机器学习中标准评估范式实现这些目标的能力，因为它们距离特定任务只有一步之遥。为了设想适合基础模型的新评估范式，我们讨论了（a）直接评估基础模型以衡量其固有能力并告知如何训练基础模型，（b）通过控制适应资源和访问来评估特定于任务的模型，以及(c) 更广泛的评估设计，以提供超出准确性衡量标准的更丰富的背景（例如稳健性（§4.8：稳健性）、公平性（§5.1：公平性）、效率（§4.5：系统）、环境影响（§5.3：环境）） 。评估实践的改革将允许评估充分服务于基础模型范式中涉及的不同目标和利益相关者。 
- §4.5：系统。虽然训练数据（§4.6：数据）决定了基础模型可用的理论信息，而模型架构（§4.1：建模）和训练目标（§4.2：训练）决定了可以提取多少信息，但计算机系统决定了实际可以实现的目标。系统是数据和模型大小扩展的关键瓶颈，这两者似乎都能可靠地跟踪功能的改进。为了确保我们能够有效地训练下一代基础模型（在时间和成本方面），我们需要算法、模型、软件和硬件的协同设计。这种协同设计已经开始以各种形式出现，从精心调整的并行策略到新的架构，例如基于检索和专家混合模型。除了训练之外，我们还考虑在基础模型之上部署应用程序需要什么（例如，高效推理）。18 基础模型研究中心 (CRFM) 
- §4.6：数据。数据是基础模型的命脉；这些模型的训练数据很大程度上决定了这些模型能够获得哪些能力。数据的中心性并不是基础模型所独有的。最近呼吁以数据为中心的人工智能 [Press 2021; Ré 2021]指出了管理、理解和记录用于训练机器学习模型的数据的普遍重要性。特别是对于基础模型，当前的操作方式是使用未指定或不明确的原则来选择训练数据，并且关于训练数据的性质普遍缺乏透明度。我们认为需要一种替代方法来重新想象围绕基础模型的数据生态系统：我们利用数据可视化和管理方面的工作，为基础模型提出一个数据中心。我们阐明了该提案如何与基础模型的许多以数据为中心的相关考虑因素相关：选择、管理、文档、访问、可视化和检查、质量评估和法律监管。 
- §4.7：安全和隐私。目前，基础模型的安全性和隐私性在很大程度上还是未知的。从根本上说，基础模型是一个高杠杆的单点故障，使其成为攻击的主要目标：现有的工作展示了各种安全漏洞（例如，对抗性触发器产生不良输出）或隐私风险（例如，训练数据的记忆） ）对于这些模型。此外，基础模型的普遍性加剧了这些问题，加剧了功能蠕变或双重用途（即用于非预期目的）的风险。就安全性而言，我们将基础模型视为类似于传统软件系统中的操作系统；我们讨论了实现安全基础模型的步骤，如果实现了该模型，将为构建可靠的机器学习应用程序提供强大的抽象层。对于隐私而言，通过利用公共数据的知识转移，基础模型可以实现对敏感数据分布的更有效的样本适应，即，当使用基础模型构建时，隐私保护应用程序可能会导致更少的准确性下降。 
- §4.8：分布变化的稳健性。标准机器学习的一个主要限制是它生成的模型对分布变化不稳健，其中训练分布与测试分布（对于下游任务）不匹配。现有的工作表明，采用在广泛的未标记数据上训练的基础模型可以提高适应模型在各种班次中的稳健性。这为改进基础模型的训练和适应性以实现鲁棒性开辟了一组新的有前景的方向。然而，我们并不认为基础模型是稳健性的灵丹妙药——跨时间外推和虚假相关性等挑战不太可能得到完全解决。 
- §4.9：人工智能安全和一致性。在考虑这些模型的潜在实际应用时，确保基础模型可靠（§4.5：系统）、稳健（§4.8：鲁棒性）和可解释（§4.11：可解释性）变得越来越重要。除了关键和直接的考虑之外，我们还考虑基础模型与更大规模的风险、危害和危害之间的关系，随着模型能力的不断进步，这些风险、危害和危害有可能增加相关性。例如，我们考虑调整基础模型的重要性，这样它们就不会被错误指定的目标或价值观所部署。我们还讨论了预测基础模型的突发行为（例如，欺骗或战略规划的能力）的相关性，这可能会使使它们适应特定任务的尝试变得复杂，并且可能需要新的可解释性方法（§4.11：可解释性）或评估（§4.4：评估）。 
- §4.10：理论。学习理论为应用机器学习中遇到的各种环境提供了广泛的基础；理论提供了理解、原则和保证来补充经验发现。目前，基础模型的研究主要是实证性的：标准监督学习的理论虽然相对成熟，但不足以充分解释基础模型。具体来说，基础模型体系内的训练阶段和适应阶段On the Opportunities and Risks of Foundation Models 19阶段之间的差异指出了现有理论的不足，因为这些阶段对应于（可能）完全不同的任务和数据分布。尽管如此，我们努力在理论上取得进展来解决这种差异，即使是在简单、有限的环境中，也将提供有用的见解。 
- §4.11：可解释性。可解释性为基础模型提供了清晰度：支撑基础模型的深层神经网络的不透明性，以及基础模型预期的普遍性，增加了理解这些模型及其功能的需要。目前的可解释性方法通常是为了解释和解释特定任务模型的行为而设计的。基础模型的性质（即这些模型有益于的广泛任务以及它们获得的意想不到的涌现属性）给可解释性研究带来了新的挑战。为了框架对基础模型可解释性的讨论，我们提出了一个模型-多模型范式，其目的是确定一个模型（基础模型）及其多个模型（其适应的衍生物）共享决策构建的程度块。除了解释所涉及的决策组件之外，我们还进一步讨论基础模型背景下的可解释性（例如，模型生成的事后解释的有效性）以及驱动模型行为的机制（这可能会阐明模型行为的程度）。理解基础模型可以扩展到理解其适应的导数）。鉴于我们赋予可解释性在基础模型研究中的关键作用，我们最后评估了可解释性和不可解释性的社会影响。 

### 1.4.4 社会概况

我们相信，基础模型的快速发展，适应和部署到各种应用，将对社会的健康产生广泛的影响。这些模型既令人兴奋又令人不安的是它们的任务不可知性。当我们谈论部署给用户的特定系统时，社会影响更容易（但仍然很重要）理解和推理，但在开发基础模型时，我们如何考虑所有可能的系统和用例的社会影响？ 
- §5.1：不平等和公平。在许多情况下，机器学习已被证明会加剧并可能加剧社会不平等。基础模型可能会延续这一趋势，即进一步加剧对历史上受到歧视的人的不公正待遇。然而，理解不平等与基础模型之间的关系需要考虑基础模型的抽象；基础模型是适用于影响用户的应用程序的中间资产。因此，我们描述了内在偏差（即基础模型中预示危害的属性）和外在危害（即在使用基础模型构建的特定应用程序的背景下产生的危害）。我们对引起这些偏见和危害的各种来源（例如，训练数据、基础模型开发人员缺乏多样性、更广泛的社会技术背景）进行分类，强调来源追踪的重要性和技术难度，以了解道德和法律责任。我们并不认为基础模型范式中的不公平是不可避免的：为了解决基础模型产生的不公平结果，我们双重考虑主动干预（例如，反事实数据增强等技术方法）和反应性追索（例如，反馈传播和反馈机制）道德/法律责任的归属）。 
- §5.2：滥用。我们将基础模型滥用定义为使用基础模型，因为它们在技术上是有意的（例如，生成语言或视频），但其目的是造成社会危害（例如，生成虚假信息、开发深度伪造品以进行骚扰）。我们认为，基础模型的进步将带来更高质量的机器生成内容，这些内容将更容易被基础模型研究中心 (CRFM) 创建和个性化以用于滥用目的。例如，虚假信息行为者可能会使用它们快速生成针对不同人口群体（例如国籍、政党、宗教等）的文章集合。虽然这些新功能可能会限制现有的有害内容人类检测方法（例如，跟踪不同来源的相似文本），但基础模型本身可能提供作为自动滥用检测器的有希望的潜力。 
- §5.3：环境。基础模型是计算成本高昂的训练方案的副产品，现有的轨迹有利于更密集的模型；这种训练所需的能量与更多碳释放到大气中和环境退化同时发生。目前，当前的讨论集中在这些巨大的单次训练成本以及在重复使用中摊销这些成本的潜力。我们试图通过确定影响基础模型环境影响计算的假设来澄清这些讨论。此外，我们设想围绕基础模型的生态系统需要采取多方面的方法：（a）计算效率更高的模型、硬件和能源网格都可以减轻这些模型的碳负担，（b）环境成本应该是明确的告知如何评估基础模型的因素（§4.4：评估），以便基础模型可以与更环保的基线更全面地并置，以及（c）围绕环境影响的成本效益分析需要在整个过程中进行更多的记录和测量社区。 
- §5.4：合法性。目前基础模型的法律基础还很薄弱；法律对这些模型的开发和使用有何影响尚不清楚。特别是针对基础模型的法律和监管框架，以及更广泛的人工智能技术的法律和监管框架，将需要影响、约束甚至促进研究、开发和部署的实践。以美国的法律格局为中心，目前对算法工具的考虑仍然存在广泛的不确定性，我们强调了模型预测的责任和模型行为的保护的相关问题。关于这两个问题，我们描述了考虑到基础模型的中间地位（而不是面向用户的特定任务模型），需要如何改进法律标准来解决这些问题。 
- §5.5：经济学。由于其新颖的功能以及在各种行业和职业中的潜在应用，基础模型可能会产生重大的经济影响。我们考虑基础模型的开发和使用对美国和全球经济未来的影响，重点关注生产率、工资不平等和所有权集中度。 
- §5.6：规模伦理。除了冒着增加不平等的风险（如第 5.1 节：公平性中所述）之外，基础模型的广泛采用还带来了其他伦理、政治和社会问题。我们讨论与基础模型应用规模相关的伦理问题，如同质化和权力集中，以及解决这些问题的规范和发布策略。论基础模型的机会和风险

# 2 能力 

基础模型获得能力，其中一些令人惊讶地从他们的学习过程中出现，为下游应用程序（§3：应用程序）提供动力。具体来说，我们讨论语言（§2.1：语言）和视觉（§2.2：视觉）能力，以及影响物理世界（§2.3：机器人）、执行推理和搜索（§2.4：推理）以及与人类交互的能力。 - 人（§2.5：互动）。此外，我们还讨论了自我监督（用于学习最新基础模型的技术方法）如何在哲学上与理解能力相关（§2.6：哲学）。22 基础模型研究中心 (CRFM) 

## 2.1 语言

作者：Isabel Papadimitriou，克里斯托弗·D·曼宁 

### 2.1.1 人类语言的本质

语言是大多数人类交流和互动的基础。然而，它不仅仅是人类实现共同目标的一种手段：语言对于人类思想、社会和情感关系如何形成、我们如何在社会和个人方面识别自己以及人类如何记录知识和发展社会智力都至关重要。 。每个人类社会都会出现口语或手语，世界上的语言在表达和构建所传达的信息的方式上都极其多样化，同时在语言的丰富性方面也表现出令人惊讶的一致性 [Comrie 1989] 。语言是非常复杂但有效的系统，儿童在短时间内持续习得，并且不断发展并包含语言社区不断变化的需求和条件。由于语言在人类活动中的中心地位，语言理解和生成是人工智能研究的关键要素。自然语言处理（NLP）是与语言相关的人工智能的子领域，与自动语音识别（ASR）和文本转语音（TTS）相关领域一起，其目标是赋予计算机理解和理解的能力。以与人类大致相同的方式生成人类语言。 2021 年迄今为止，NLP 是受基础模型影响最深远的领域。第一代基础模型展示了令人印象深刻的多种语言能力，以及对各种语言情况的惊人适应性。自 2018 年引入早期基础模型 ELMo [Peters et al.2018] 和 BERT [Devlin et al.2019] 以来，NLP 领域已主要集中在使用和理解基础模型上。该领域已转向使用基础模型作为主要工具，并转向更通用的语言学习作为中心方法和目标。在本节中，我们回顾了自然语言处理中基础模型的最新成功，详细介绍了基础模型如何改变了训练语言机器学习模型的整体流程和心态，并讨论了基础模型面临的一些理论和实践挑战。适用于更广泛的语言以及更现实和复杂的语言情况。 

### 2.1.2 基础模型对NLP的影响

基础模型对 NLP 领域产生了巨大影响，现在是大多数 NLP 系统和研究的核心。首先，许多基础模型都是熟练的语言生成器：例如，Clark 等人[2021] 证明，非专家很难区分 GPT-3 编写的简短英语文本和人类编写的英语文本。然而，在 NLP 中最有影响力的基础模型的特征不是它们的原始生成能力，而是它们令人惊讶的通用性和适应性：单个基础模型可以以不同的方式进行调整，以实现许多语言任务。 NLP 领域历来专注于定义和设计用于具有挑战性的语言任务的系统，其愿景是擅长这些任务的模型将为下游应用带来有能力的语言系统。 NLP 任务包括整个句子或文档的分类任务（例如，情感分类，如预测电影评论是正面还是负面）、序列标记任务，其中我们对句子或文档中的每个单词或短语进行分类（例如，预测如果每个单词是动词或名词，或者单词的跨度指的是一个人或一个组织），跨度关系分类（例如，关系提取或解析，例如一个人和位置是否通过“当前居住地”链接）关系，或“主谓”关系的动词和名词）和生成任务，生成以基础模型的机遇和风险为条件的新文本 23 图 5。目前仅代表了世界上一小部分语言在基础模型中。世界上有超过 6,000 种语言，由于构成独立语言的固有不确定性，估计值有所不同 [Nordhoff 和 Hammarström 2011]。该地图显示了世界上的语言，每个点代表一种语言，其颜色表示顶级语系。数据来自 Glottolog [Hammarström 等人。 2021]。我们在地图上标记了一些语言作为示例。强烈依赖于输入（例如，生成文本的翻译或摘要、识别或生成语音、或在对话中做出响应）[Jurafsky 和 Martin 2009]。过去，NLP 任务有不同的研究社区，这些社区开发了特定于任务的架构，通常基于不同模型的管道，每个模型执行语言子任务，例如标记分割、句法解析或共指解析。相比之下，执行每个任务的主要现代方法是使用单个基础模型，并使用相对少量的特定于每个任务的注释数据（情感分类、命名实体标记、翻译、摘要）对其进行稍微调整，以创建适应的模型。事实证明，这是一种极其成功的方法：对于上述绝大多数任务，稍微适应某项任务的基础模型的性能大大优于以前的模型或专门为执行该任务而构建的模型管道。仅举一个例子，在基础模型之前，2018 年回答开放式科学问题的最佳系统可以在纽约摄政八年级科学考试中获得 73.1% 的成绩。一年后的 2019 年，改编后的基础模型得分为 91.6% [Clark 等人，2019]。 2019]。主要经过训练来生成语言的基础模型的出现，构成了语言生成在 NLP 中的作用的重要转变。直到 2018 年左右，生成通用语言的问题被认为非常困难，并且基本上无法解决，除非通过其他语言子任务 [Paris et al.2013]。相反，NLP 研究主要集中在语言分析和理解文本上。现在，可以通过简单的语言生成目标来训练高度连贯的基础模型，例如“预测这句话中的下一个单词”。这些生成模型现在构成了完成语言机器学习的主要工具，包括曾经被认为是生成的先决条件的分析和理解任务。 Foundation24 基础模型研究中心 (CRFM) 模型所展示的成功生成也导致了摘要和对话生成等语言生成任务的研究蓬勃发展。基础模型范式的兴起已经开始在口语和书面语言中发挥类似的作用。像 wav2vec 2.0 这样的现代自动语音识别 (ASR) 模型仅在大型语音数据集上进行训练，然后针对 ASR 任务对具有相关转录的音频进行调整 [Baevski 等人，2017]。 2020]。由于基础模型范式带来的变化，NLP的研究和实践的重点已经从为不同任务定制架构转向探索如何最好地利用基础模型。对适应方法的研究已经蓬勃发展（参见§4.3：适应以详细了解适应），基础模型的惊人成功也引起了研究兴趣转向分析和理解基础模型（参见§4.11：可解释性和可解释性）基础模型分析）。 

### 2.1.3 语言变异和多语言性

尽管基础模型通过预训练获得的语言知识具有惊人的通用性，但这种适应性是有限的：目前尚不清楚当前的基础模型在处理语言变化方面有多成功。语言差异很大。除了世界上有数千种不同语言这一事实之外，即使在一种语言内或在一个说话者内，语言也会有所不同。举几个例子，非正式对话的表现与书面语言不同，人们与朋友交谈时使用的语法结构与与权威人士交谈时使用的语法结构非常不同，并且同一语言中的使用者社区使用不同的方言。社会和政治因素植根于如何看待和评价语言变异，以及 NLP 研究中代表了多少不同的变异（例如，参见 Blodgett 和 O'Connor [2017] 关于非裔美国英语 NLP 的失败，以及§ 5.1：公平性，以更深入地讨论基础模型中的不平等现象）。由于基础模型具有学习语言信息和灵活调整这些知识的巨大能力，因此有望扩展 NLP 以包含更多的语言多样性。它仍然是一个开放的研究问题，以了解是否有可能建立一个基础模型，稳健而公平地代表语言的主要和微妙变化，对每种语言变体的独特之处给予同等的重视和敏锐度[提出和解决这个问题的研究包括庞蒂等人。 2019；本德尔2011；乔希等人。 2020]。继英语基础模型取得成功之后，多语言基础模型已经发布，将这种成功扩展到非英语语言。对于世界上 6000 多种语言中的大多数来说，可用的文本数据不足以训练大规模基础模型。举个例子，有超过 6500 万说富拉语（一种西非语言）的人，但可用于富拉语 NLP 的资源却很少（如果有的话）[Nguer et al.2020]。多语言基础模型通过同时联合训练多种语言来解决这个问题。迄今为止的多语言基础模型（mBERT、mT5、XLM-R）均经过大约 100 种语言的训练 [Devlin et al.2019；戈亚尔等人。 2021 年；薛等人.2020]。联合多语言训练依赖于合理的假设，即语言之间的共享结构和模式可以导致从高资源语言到低资源语言的共享和迁移，使得我们无法训练独立语言的语言的基础模型成为可能模型。使用和分析多语言基础模型的实验表明，多语言基础模型中不同语言之间确实存在大量的传输和并行编码。然而，这些模型的多语言鲁棒程度仍然是一个悬而未决的问题。目前尚不清楚基于这些数据训练的模型有多少可以代表与英语截然不同或可用语言资源很少的语言的各个方面[Wu and DredzeOn the Opportunities and Risks of Foundation Models 25 2020]，以及它们明显的多语言性能是否更多地依赖于同化 [Lauscher et al.2020; Virtanen 等人，2019； Artetxe 等人.2020]。多语言模型在与训练数据中资源最高的语言相似的语言中表现出更好的性能，并且已经表明，多语言模型中的语言会竞争模型参数，这使得我们不清楚单个模型可以容纳多少变化[Wang等人.2020d]。一个突出的问题源于我们用来训练多语言基础模型的数据：在许多多语言语料库中，英语数据不仅比资源较低的语言丰富几个数量级，而且通常更干净、更广泛，并且包含示例展示更多的语言深度和复杂性 [Caswell et al.2021]（参见 Nekoto et al.[2020] 关于构建参与性和稳健的多语言数据集）。然而，答案不仅仅在于创建更平衡的语料库：语言变异的轴如此之多，以至于创建一个在所有方面都平衡且具有代表性的语料库是不可行的。尽管数据不平衡，但基础模型的未来、多功能性和公平性都取决于对语言变化的稳健处理[例如，Oren 等人，2017]。 2019]。目前的多语言基础模型的原始形式，以及天真的无监督多语言训练方法，可能无法充分模拟语言和语言变体的微妙之处。尽管如此，它们对于某些多语言应用程序仍然有用，例如通过针对不在原始训练集中的低资源语言调整多语言模型 [Wang 等人，2017]。 2020b]。此外，（非公开）GShard 神经机器翻译模型的结果显示，最低资源语言相对于单语言基线的收益最大，并且收益随着模型大小的增加而增加 [Lepikhin et al.2021]。研究界应该批判性地研究基础模型如何处理语言变异，了解基础模型在为 NLP 带来公平性和代表性方面的局限性，而不是决定推广消除语言变异并在训练数据中基本符合大多数语言的基础模型。 

### 2.1.4 来自人类语言习得的启发

尽管基础模型在创建更像人类的 NLP 系统方面取得了巨大进展，但它们获得的语言系统以及学习过程仍然与人类语言存在显着差异。了解机器语言学习和人类语言学习之间这种差距的影响是发展了解基础模型的语言限制和可能性的研究社区的必要组成部分。人类语言习得非常高效：像 GPT-3 这样的基础模型所训练的语言数据比大多数人听到或读到的语言数据多大约三到四个数量级，当然也比儿童在接触语言时接触到的语言数据多得多。主要是语言能力强。基础模型和人类语言习得之间的一个显着区别是人类语言植根于现实世界 [Saxton 2017]。例如，婴儿和看护者在语言发展过程中指向物体[Colonnesi et al.2010]，婴儿在学习语言系统的许多其他方面之前先学习指代常见物体的单词的基础含义[Bergelson and Swingley 2012] ]。另一方面，NLP 中使用的大多数基础模型都是从原始、无根据的文本的分布信息中学习，并且（与人类学习者相反）Zhang 等人[2021] 表明，RoBERTa 模型在可用含义之前表达抽象句法特征。强大的无根据的统计学习确实也存在于婴儿身上[Saffran et al.1996]，因此它无疑是习得的一个重要因素。尽管如此，推进基础模型的扎根语言学习仍然是提高人类习得效率的重要方向 [Dupoux 2018;谭和班萨尔 2020； Zellers et al.2021a，除其他外]（参见§2.2：视觉和§2.3：基础模型多模式潜力的机器人技术，以及§2.6：讨论基础模型是否可以在没有基础的情况下理解语言的哲学）。另一个重要方向是检查foundation26基础模型研究中心（CRFM）中的归纳偏差。图6.人类和基础模型的语言习得。虽然人脑和基础模型之间肯定存在不同的归纳偏差，但它们学习语言的方式也有很大不同。最重要的是，人类与物理和社会世界互动，他们在其中有不同的需求和愿望，而基础模型主要观察和建模他人产生的数据。模型以及它们如何与人类思维中的归纳偏差相关，包括特定于语言学习的偏差和一般于人类认知的偏差[Linzen and Baroni 2021]。尽管人脑在架构上可能更加专门用于高效的语言习得，但基础模型并不是白板学习者 [Baroni 2021]，理解和调整这些语言归纳偏差是基础模型研究的一个重要未来方向。语言习得效率的一个重要因素是人类习得了系统的、可概括的语言系统。尽管关于人类语言系统产生什么类型的理论抽象有许多不同的理论[例如，Comrie 1989；乔姆斯基 2014;克罗夫特2001； Jackendoff 2011]，人们普遍认为人类学习语言的方式使他们能够轻松地将新知识融入现有的抽象中并有效地创建新的语法句子。例如，一个十岁的孩子已经获得了许多关于他们的语言如何运作的抽象概念，尽管他们产生的实际单词和结构将在未来十年内发生巨大变化。另一方面，基础模型通常无法获得我们期望从人类那里获得的系统抽象。例如，当基础模型一次准确地产生语言结构时，不能保证该结构的未来使用将基本一致，特别是在主题发生重大领域转变之后[检查基础模型系统性局限性的工作示例包括 Lake 和 Baroni 2018； Kim 和 Linzen 2020； Bahdanau 等人，2018； Chaabouni 等人，2021]。 NLP 面临的挑战是在基础模型的习得过程中开发某种系统性，而又不回归到过于依赖严格语言规则的系统。语言学习在说话者的一生中持续进行：人类语言的语法不断发展，人类灵活地适应新的语言环境 [Sankoff 2018]。例如，当成年人的生活中出现新的术语和概念时，他们可以相对轻松地在语法上使用它们关于基础模型 27 的机会和风险句子，并且人类经常调整其语法模式以适应不同的社会群体[Rickford et al .1994]。另一方面，基础模型的语言系统主要由训练数据设置，并且相对静态[Lazaridou et al.2021; Khandelwal 等人.2020]。尽管适应方法可以为不同的任务准备基础模型（参见§4.3：适应），但仍然不清楚如何在不进行大量训练的情况下改变基础模型的更基本的语言基础。制作自然反映类人语言适应和语言进化的适应性模型是基础模型未来的一个重要研究领域。基础模型极大地改变了 NLP 的研究和实践。基础模型为社区带来了许多新的研究方向：将生成理解为语言的一个基本方面，研究如何最好地使用和理解基础模型，了解基础模型可能增加 NLP 不平等的方式，检查基础模型是否能够令人满意地涵盖语言的变化和多样性，并找到利用人类语言学习动态的方法。研究社区在基础模型之前关注的大多数复杂 NLP 任务现在都可以使用少数公开发布的基础模型之一来最好地处理，达到几乎人类的水平。然而，这种性能与在复杂的下游环境中有用且安全地部署基础模型的需求之间仍然存在显着差距。28 基础模型研究中心 (CRFM) 

## 2.2 视觉

作者：Shyamal Buch、Drew A. Hudson、Frieda Rong、 Alex Tamkin、Xikun 张、Bohan Wu、Ehsan Adeli、Stefano Ermon、Ranjay Krishna、Juan Carlos Niebles、Jiajun Wu、Li Fei-Fei 

图 7. 通过大规模利用自我监督，视觉基础模型有可能提炼出将原始的多模态感官信息转化为视觉知识，这可以有效地支持传统的感知任务，并可能在挑战时间和常识推理等高阶技能方面取得新的进展（§2.2.1：视觉能力）。这些输入可以来自各种数据源和应用领域，这表明医疗保健和具体的交互式感知设置中的应用前景广阔（§2.2.2：视觉挑战）。图片来源 [扎米尔等人。 2018；哈克等人。 2020]。视觉是生物体了解其环境的主要模式之一。

视觉能力能够近乎持续地远距离收集密集信号，这是一种在多种生命形式的进化时间尺度上发展起来的关键能力[Parker 2003；张和舒 2021]。对于即使是简单的生物也可以轻松执行的技能来说，将相同的能力转移到机器上已被证明是非常具有挑战性的，领先的计算机视觉和机器人研究人员 Hans Moravec 在 1988 年观察到一个悖论：在人工智能中，（被认为是）困难的问题很容易，而同样，简单的问题也很困难，其中“最简单”的问题之一是我们每天使用的视觉敏锐度，我们每天用它在几毫秒内不断地解释复杂的场景[Moravec 1988；索普等人。 1996；飞飞等人。 2007]。这一艰巨挑战的另一端是计算机视觉所关键的变革性应用的广泛范围：可以使通勤者摆脱交通拥堵的自动驾驶汽车（第 2.3 节：机器人技术）、可以协助的救生人工智能工具通过检测罕见的医疗事件（第 3.1 节：医疗保健）、下一代多媒体创建和编辑工具（第 2.5 节：交互）等，使专家过度劳累。反思人类感知发挥作用的应用和环境，可以让人们了解计算机视觉可以提供帮助和变革的潜在领域。

计算机视觉领域和我们定义的挑战从很多方面汲取灵感来自人类的感知能力。几种经典理论[例如，Biederman 1972；麦克莱兰和鲁梅尔哈特 1981； Marr 1982]提出，人类可以通过将各个部分作为一个更大的整体来感知现实世界场景，并为计算机视觉技术以不断增长的抽象水平逐步建模物理世界指明了道路[Lowe 1992； Girshick 等人.2014]。 Gibson [1979]认为人类视觉是固有的，交互的生态环境可能在其发展中发挥关键作用。这些想法继续推动计算机视觉系统的不断发展，不断迭代以实现对世界的情境化、交互式和具体化感知。在计算机视觉的背景下，基础模型将来自不同来源和传感器的原始感知信息转换为可适应多种下游设置的视觉知识（图 7）。在很大程度上，这项工作是过去十年该领域出现的关键想法的自然演变。 ImageNet的介绍[Deng等人。 2009]和监督预训练的出现导致了计算机视觉领域深度学习范式的转变。这一转变标志着一个新时代，我们超越了早期的经典方法和特定于任务的特征工程[Lowe 2004；贝等人，2006； Rosten 和 Drummond 2006] 致力于建立可以在大量数据上进行一次训练的模型，然后适应各种任务，例如图像识别、对象检测和图像分割 [Krizhevsky et al.2012； Szegedy 等人，2015；他等人，2016a；西蒙扬和齐瑟曼 2015]。这个想法仍然是基础模型的核心。通往基础模型的桥梁来自于先前范式的局限性。传统的监督技术依赖于昂贵且精心收集的标签和注释，限制了其稳健性、泛化性和适用性；相比之下，自我监督学习的最新进展[Chen et al.2020c； He et al.2020]提出了开发基础模型的另一种途径，可以利用大量原始数据来获得对视觉世界的上下文理解。相对于该领域更广泛的目标，视觉基础模型的当前能力目前还处于早期阶段（§2.2.1：视觉能力）：我们已经观察到传统计算机视觉任务的改进（特别是在泛化能力方面）[雷德福等人，2021； Ramesh et al.2021]并预计近期进展将延续这一趋势。然而，从长远来看，基础模型减少对显式注释依赖的潜力可能会导致基本认知技能（例如常识推理）的进步，而这在当前的完全监督范式中已被证明是困难的[Zellers 等人。 2019a；马丁-马丁等人，2021]。反过来，我们讨论了基础模型对下游应用的潜在影响，以及未来必须解决的核心挑战和前沿（§2.2.2：愿景挑战）。 

### 2.2.1 关键能力和方法

从高层次来看，计算机视觉是人工智能的核心子领域，它探索赋予机器解释和理解视觉世界的能力的方法。它包含大量任务、子域和下游应用程序，社区在过去几十年中不断取得进展[Zamir et al.2018]。示例任务选择16：
- (1) 语义理解任务，旨在发现视觉场景中实体之间的属性和关系；其中包括图像分类、对象检测、语义分割、动作识别和场景图生成等[例如，Krizhevsky et al.2012；他等人。 2016a；克里希纳等人，2017；鲁萨科夫斯基等人，2015；克里热夫斯基等人，2009；凯等人，2017；林等人。 2014]。
- (2) 几何、运动和 3D 任务，寻求表示几何、姿态和结构 16 这当然是一个粗略的选择：请参阅计算机视觉和模式识别 (CVPR) 年会的类别以获得更完整的信息（但不断发展）该领域任务的图片。30 静止或移动物体基础模型研究中心 (CRFM)，包括深度估计、运动结构、表面法线检测、曲率线和关键点估计等任务，仅举几例[例如，Laina et al.2016；阿加瓦尔等人，2011； Wang 等人，2015a；扎米尔等人，2018；厄尔曼 1979]。 
- （3）多模态集成任务，将语义和几何理解与自然语言等其他模态相结合；例如，这些包括视觉问答、图像字幕和指令遵循[例如，Antol et al.2015； Chen 等人，2015b；安德森等人，2018； Goyal 等人，2017b；哈德森和曼宁 2019b；约翰逊等人，2017；罗等人.2020;阿克巴里等人，2021； Huang 等人.2021c；钦普凯利等人。 2021]。我们在图 7 中强调了传统核心任务的子集。

在 2010 年代初期 ImageNet [Deng et al.2009] 的出现推动下，解决这些任务的主要范例往往围绕一个熟悉的核心思想：首先，预训练一个基于大量仔细注释的数据集的模型 [Russakovsky et al.2015]，具有完全监督的训练任务，例如图像分类。然后，在特定于任务的数据集和域上调整模型下游[Lin et al.2014； Chen 等人，2015b； Antol et al.2015]通过微调达到最先进的性能[Krizhevsky et al.2012；西蒙扬和齐瑟曼，2015；他等人，2016a； Xu 和 Saenko 2016]。这种先训练后适应的概念仍然存在于我们现在考虑的基础模型的定义中（§1：简介）。这种完全监督范式的局限性促使人们向基础模型过渡：对外部监督注释的依赖限制了以前方法以可扩展、稳健和可概括的方式捕获各种视觉输入的上限能力。视觉合成和无监督学习领域的最新发展提供了一种引人注目的替代方案。例如，GAN 学习生成高保真度、真实性和多样性的视觉内容，方法是通过两个相互竞争的生成器网络和鉴别器网络来学习生成高保真度、真实性和多样性的视觉内容，这两个网络可以仅从图像集合中相互监督[例如，Goodfellow 等人，2014 年； Hudson 和 Zitnick 2021]。其他神经模型通过采用变分自动编码、对比学习或其他自监督技术来推断对象和场景的视觉属性，而无需明确注释的监督[例如，Kingma 和 Welling 2014； Chen 等人.2020c；他等人，2020]。例如，何等人。 [2021]建立在先前关于使用掩模图像编码进行表示学习的工作的基础上[例如，Pathak et al.2016； Vincent et al.2008]，部分是将灵活架构（例如视觉转换器 [Dosovitskiy et al. 2021；Zhai et al. 2021]）的最新进展与增加的扩展性相结合。通过基础模型，这种自我监督技术的发展使得能够在更大范围的视觉数据上进行训练[Changpinyo et al.2021]，无论是在其范围还是潜在的多样性方面。因此，我们已经看到了传统视觉任务在标准精度指标和小样本泛化方面取得进展的早期指标。对于图像分类和目标检测，自监督技术已报告了与之前的完全监督方法相比的竞争性能[He et al.2019; Chen 等人.2020c；雷德福等人，2021； Hénaff et al.2021]，在训练过程中没有明确的注释，在适应过程中样本效率更高。对于视觉合成，著名的例子包括 DALL-E [Ramesh et al.2021] 和 CLIP 引导生成 [Radford et al.2021； Galatolo et al.2021]，研究人员利用多模态语言和视觉输入来渲染引人注目的视觉场景。短期内，随着训练目标的细化，我们预计这些基础模型的能力将继续沿着这些方向提高[Chen et al.2020a; Hénaff 等人.2021；塞尔瓦拉朱等人。 2021]并且架构被设计为包含额外的模式[Jaegle et al.2021]。 2021b]。值得注意的是，当前的计算机视觉基础模型相对于 NLP 模型（§2.1：语言）来说还处于新生阶段：有希望的早期工作仍然主要集中在 RGB 图像输入和核心传统视觉任务的子集上。然而，该领域在以具体化和交互式感知设置为中心的更广泛挑战方面继续取得进展（对于机器人技术的基础模型至关重要[Bohg et al.2017，§2.3：机器人技术]）。我们在图 7 中注意到这些高阶目标的子集，包括物理场景理解、视觉常识推理和时间基础模型 31 事件的机会和风险，以及对社会可供性的感知。其中每一个都是完全监督系统的目标，但事实证明它们具有挑战性，部分原因是大规模注释这些任务的难度。例如，视觉问答的标准系统很难回答需要常识性理解的问题，因为这些问题通常需要超出像素本身存在的外部知识[Zellers et al.2019a]。以稳健的方式感知人类凝视和社会可供性仍然是交互式代理中的具体视觉系统面临的持续挑战[Martin-Martin et al.2021]。通过减少对显式注释的依赖，基础模型可以比以前更进一步地实现这些目标。

语言基础模型（§2.1：语言）的相关进展已经能够捕获一定程度的语言事件常识[Brown et al.2020]，也表明了在多模态视觉输入上实现类似能力的潜在途径。虽然如何在基础模型中实现这些功能的确切路线图仍然是一个悬而未决的问题，但新的高效灵活的架构（§4.1：建模）、大规模训练（§4.5：系统）、自我监督技术（§4.5：系统）的组合仍然是一个悬而未决的问题。 4.2：训练）和小样本适应方案（§4.3：适应）可能会为迄今为止难以达到的能力打开大门。 

### 2.2.2 主要研究挑战

我们对研究挑战的讨论是由下游应用领域推动的，其中基础模型可能会进一步促进视觉模型的集成和影响。我们重点介绍几个这样的领域：
- (1) 医疗保健和家庭环境的环境智能：建立在这些环境中现有的环境智能方法的基础上[Haque et al.2017； Lyytinen 和 Yoo 2002； Hong 和 Landay 2004]，基础模型可能提供更好地检测细粒度人类活动和医疗事件的潜力，以及改进临床医生、患者和日常消费者的辅助交互（另请参见§3.1：医疗保健）。 
- (2) 移动和消费者应用：具有更强的多模态基础的基础模型可以在移动环境中实现更强大的服务交互性，并且视觉和语言输入生成能力的根本改进可以有利于计算摄影和内容编辑应用程序[Delbracio et al.2021] ;拉梅什等人，2021； Park et al.2019]（另请参见§2.5：交互）。 
- (3) 体现的交互式代理：感知模型已经被证明作为输入[Sermanet et al.2018]和奖励函数[Chen et al.2021c]是有效的； Shao et al.2020]在机器人设置中；在大量以自我为中心（真实/模拟，人类/机器人）视觉数据集上训练的基础模型[Damen et al.2018； Chen et al.2021e] 可能会通过捕获更广泛分布的视觉场景、对象和动作来进一步推动这一进步（另请参见§2.3：机器人技术）。

基础模型可能进一步影响这些应用程序设置的程度取决于第 2.2.1 节中概述的功能：视觉功能的实现程度。为了弥合当前、短期和长期预期能力之间的巨大差距，我们必须解决视觉基础模型当前的局限性，包括其训练和评估。下面是相应的关键挑战的子集：语义系统性和感知鲁棒性。人类具有非凡的能力，可以将视觉理解推广到看不见的构图，并推理新物体和场景的物理和几何特性[Lake et al.2015]。虽然当前的基础模型已经显示出图像合成的良好能力以及泛化到细粒度语言输入的早期结果，但这些模型仍然难以泛化到简单形状和颜色的组合[Ramesh et al.2021;雷德福等人，2021；荣2021]。普遍性也超越了语义。视觉场景和物体的物理动力学和几何特性具有自然的规律性。基础模型已经显示出理解场景和物体几何形状的早期迹象[Ramesh et al.2021]。此外，早期对物理场景和32基础模型研究中心 (CRFM) 感知模型几何理解的努力可能为正在进行的基础模型开发提供指导[Yi et al.2019;巴赫金等人，2019； Li 等人.2020b]。事实上，在基础模型中持续纳入多种模式（例如音频）可能有利于实现这些目标[Zhang et al.2017；高等人.2020b； Jaegle 等人.2021a]。然而，能够将初始观察到的能力稳健地推广到人类水平的广泛自然场景和物体的具体技术仍然是基础模型的开放研究挑战。计算效率和动力学建模。人类在处理支持理解事件动态所必需的对象、场景和事件的连续视觉流方面出奇地高效[Zacks et al.2001；特沃斯基和扎克斯 2013]。语言基础模型（§2.1：语言）已经展示了对事件的长期一致性进行建模的初步步骤；捕获视觉输入中的远程时间相关性和因果一致性的类似能力将有利于机器人等下游设置[Dai et al.2019；阿利亚姆金等人，2019； Goel 等人.2020b； Feng et al.2019，§2.3：机器人]。然而，相对于语言中的单词标记级输入，低级计算机视觉输入的维度极高：单个 1080p 帧包含超过 200 万个像素。在这种情况下，对长距离视频序列中更丰富的事件动态进行建模似乎是一项艰巨的任务，尤其是在附加模式（例如语音、光流等）和不断提高的分辨率的情况下。可以理解的是，完全处理每个单独像素的简单方法可能会令人望而却步。当前的视觉模型[例如，Radford et al.2021； Sun 等人，2019a；谭和班萨尔 2019； Kim et al.2021a]经常通过处理嵌入来解决这个问题，嵌入总结图像块甚至帧组，但这具有丢失细粒度细节的潜在缺点[Ramesh et al.2021]。除了考虑原始输入空间之外，视觉基础模型可能需要重新审视基本架构基元的设计（第 4.1 节：建模），以实现高效且有效的建模：3D 卷积的替代方案可能会更好地解决其立方复杂性 [Fan 等人.2020; Sitzmann et al.2019]，而基于粒子的表示可能被证明对于物理动力学建模更有效[Bear et al.2021]。此外，将这些视觉模型部署到下游应用程序设置也需要系统设计的进步（§4.5：系统）。总而言之，大规模、动态视觉输入的高效建模的瓶颈仍然是一个必须解决的多方面研究方向。训练、环境和评估。对于实现基础模型的潜力同样重要的是训练和评估它们的支持元素。当前的视觉基础模型主要集中在图 7 中所示的一小部分模式（例如 RGB 图像和文本的数据集），因为这些可能是最容易访问的 [Changpinyo 等人 2017]。 2021 年； Radford 等人，2021]。这激励了额外的大规模训练数据集的开发和使用，其中包含跨多种模式的多样化输入集合。虽然附加注释可能不是严格必要的，但输入质量会影响模型的学习效率；利用其他类型（例如语言）的基础模型来帮助提高质量的技术是一条有前途的前进道路[Zellers et al.2021b]。我们还想考虑静态数据集之外的设置：经典研究表明，人类的感知理解与其体现和交互式生态环境有关[Gibson 1979]。作为实现体现和交互的长期能力（§2.3：机器人技术）的踏脚石，持续开发以多种方式和观点捕捉物理、视觉和生态现实主义的模拟环境可能在提供可扩展和高保真度方面发挥重要作用这一目标的视觉输入[Kolve et al.2017a； Savva 等人.2019b； Gan 等人 2020； Shen 等人，2021a； Srivastava 等人.2021]。最后，还有一个度量问题：我们如何评估生成基础模型输出在语义方面的可信度？ Fréchet Inception Distance 等标准指标存在已知缺陷 [Bińkowski et al.2018]；此类问题关于基础模型的机遇和风险 33 个自然语言处理中的并行问题（例如，像 BLEU 这样的指标与人类的因果判断不相关）。将人类判断作为评估的一部分可能是一种途径，但会产生巨大的成本，并且可能不具有可扩展性[Zhou et al.2019； Khashabi 等人.2021]。围绕视觉基础模型的训练（§4.2：训练）、数据（§4.6：数据）和评估（§4.4：评估）设置的突出和开放的挑战确实非常微妙，并将成为研究的中心领域向前。结束语。在本节中，我们探讨了计算机视觉背景下的基础模型，从识别以前的计算机视觉范式的根源，到将其当前和预期的功能置于背景中，再到提出未来的研究方向。最后，我们简要讨论了计算机视觉基础模型及其持续发展的一些更广泛的社会影响（另见§5：社会）。相机在我们的社会中无处不在，这意味着计算机视觉技术的进步具有巨大的颠覆性影响潜力。这就带来了相应的责任负担，需要仔细考虑其风险。计算机视觉模型中习得偏差的历史已有充分记录，导致代表性不足的群体的准确性较低和相关错误，从而导致不恰当和过早地部署到某些现实世界环境中[例如，Buolamwini 和 Gebru 2018，§5.1：公平性]。当前的基础模型中仍然存在许多相同的根本问题[Agarwal et al.2021]。随着来自其他传感器模式（例如，可穿戴或环境传感器，图 7）的数据被纳入这些基础模型中，围绕隐私和监视的担忧变得至关重要（参见§5.6：道德）。此外，随着视觉基础模型的语义和生成能力不断增长，生成的深度伪造图像和错误信息会带来更大的风险[Dolhansky et al.2020； Ramesh 等人.2021，§5.2：滥用]。虽然计算机视觉和基础模型面临着巨大的开放挑战和机遇，但同时解决这些和相关风险仍然至关重要。34 基础模型研究中心 (CRFM) 

## 2.3 机器人 

作者：Siddharth Karamcheti、Annie Chen、Suvir Mirchandani、Suraj Nair、Krishnan Srinivasan、Kyle Hsu、Jeannette Bohg、Dorsa Sadigh、Chelsea Finn 

图 8。构建新型机器人基础模型将需要跨越不同环境和行为的海量数据集。模拟、机器人交互、人类视频和自然语言描述都可以成为这些模型的有用数据源。尽管获取数据面临挑战，但开发新的机器人基础模型对于任务规范和机器人学习中的各种问题表述具有巨大潜力。图片来源：[Finn 等人。 2016b；佐特等人。 2021]。机器人研究的一个长期挑战是赋予机器人处理现实世界中遇到的各种条件的能力。在本节中，我们将讨论基础模型背后的想法如何潜在地帮助实现“多面手”机器人，例如，它们可以在新房子里用新厨房做饭。为了实现这一目标，现有的基础模型是不够的。我们需要在多种数据源上进行训练的新型模型，涵盖地面机器人交互数据到人类执行任务的视频等。我们重点关注如何将此类基础模型应用于机器人控制其自身物理实体以成功执行不同任务的问题。这是一个高维闭环决策问题：机器人采取的动作直接影响它接下来感知到的内容，进而影响机器人的下一个动作。这种闭环方面传统上并未在语言和计算机视觉领域进行研究，其中大型离线数据集占主导地位，并且基础模型已经取得了成功。我们重点关注如何在这种新的闭环数据机制中利用基础模型（大规模、自我监督学习）所展示的优势。新型机器人基础模型的前景在于它能够放大机器人的潜力，以改善从制造到日常生活的关键方面[1999 年 11 月； Sanneman et al.2020]，构建[Khoshnevis 2004； Bock 2007]，自动驾驶[Thorpe et al.1988； Badue et al.2020]，家庭援助[Thrun and Mitchell 1995；布鲁克斯2002；迪尔曼2004；古德里奇和舒尔茨 2007；古普塔等人，2018；施里达尔等人。 2020] 和个人协助 [Dragan 和 Srinivasa 2013； Javdani 等人.2018]等。我们在本节中的讨论主要集中于用于家庭任务的移动操纵机器人，但我们希望其本质广泛适用于上面列出的机器人技术的其他用例。构建新型机器人基础模型的关键路径是抓住任务规范和任务学习的机会，同时应对数据采集、安全性和鲁棒性方面的挑战。考虑以下机器人学习范式：从关于基础模型 35 的机遇和风险开始，描述一个捕获用户可能希望机器人做什么的任务（例如，“做早餐”）——学习相应的策略来生成所需的机器人行动。虽然策略可以通过不同的方式参数化，但常见的选择是将任务表示和环境观察（例如，来自固定或以自我为中心的相机的场景图像，或来自激光雷达等替代传感器的输入）映射到机器人动作的函数。 Andrychowicz 等人，2017； Nair 等人，2018]。当机器人以任务条件方式行动时，后续状态将反馈给策略，生成更多动作，直到任务得到满足。然而，在实践中实施这样的范例是困难的。首先，描述目标的正确界面是什么？对于某个上下文中的特定用户来说，“做早餐”意味着一顿丰盛的早餐，包括煎鸡蛋、烤面包和一杯橙汁；对于另一个用户来说，“做早餐”可能意味着 idlis、水鹿和一杯滴滤咖啡。一般来说，像这样的高层次的上下文相关目标并不是孤立的，并且可能会带来许多歧义。如何足够清晰地指定一个目标（以及相应的子目标）来解决这些模糊性，并让机器人在给定的任务上取得进展？此外，我们如何制作一般任务表示，以帮助泛化到类似的目标（例如，取一杯牛奶而不是橙汁）。更进一步，我们如何构建方法来帮助机器人学习新任务和新环境的策略（在本例中，是一个配备新器具、电器、布局等的全新厨房）？最近在应用语言和视觉基础模型（§2.1：语言和§2.2：视觉）方面取得的突破表明，大规模自监督预训练对于提高泛化能力具有一些潜在的好处。利用不同的数据流来学习有意义的表征先验（类似于 BERT 和 GPT-3 等模型学习到的先验知识）的能力有望为学习任务规范的强大机器人基础模型带来希望。各种机器人交互数据可用于学习动作条件动力学模型或索引一般和语义上有意义的技能的策略，从而为任务学习带来希望。然而，尽管存在这些机会，但关键的障碍是收集正确的数据。与语言和视觉数据不同，机器人数据既不丰富，也不代表足够多样化的实施例、任务和环境——我们（作为一个领域）仍然没有集中在对实现通用机器人技术最有用的数据类型上（例如，离线演示、人类的第三人称录音、以自我为中心的视频、自主体验等）除了获得正确规模和数据多样性的问题之外，还有确保安全性和稳健性的问题：我们在新环境中如何表现而不造成损害？因此，为机器人技术构建新型基础模型包含机遇和挑战的二分法：任务规范和学习的机会与数据收集和安全部署的挑战之间的平衡。本节通过展示机器人基础模型如何帮助我们开发通用机器人来探讨这两个问题，这种方式不仅有意义地解决与构建此类系统相关的挑战，而且还包含了多模态的潜力——结合感知、驱动和语言——以及用于规范和学习的人机交互。 

### 2.3.1 机会

机器人基础模型可以采取多种形式：机器人技术中的问题不容易符合一刀切的模型，因为不同的问题具有不同的输入输出特征——这与 NLP 等领域可以转换许多问题形成鲜明对比变成一般的“文本输入，文本输出”签名。我们专注于可概括的任务规范以及跨任务、环境和机器人实施例的学习机会。36 基础模型研究中心 (CRFM) 任务规范的基础模型。在机器人能够学习如何以通用方式解决任务之前，它们必须了解所需的任务是什么：例如，为了在新厨房中发挥作用，机器人需要知道我们希望它做什么，以及我们希望它避免的行为。因此，开发多面手机器人必要的第一步是建立一种新型的基础模型来实现可靠的任务规范，即任务目标、偏好和约束的直观有效的沟通。我们将任务规范形式化为一个过程，将人类提供的任务描述转换为衡量机器人任务完成和进度的定量指标（例如奖励函数）。该信号对于优化机器人行为、诊断故障和提示人类反馈至关重要。由于描述任务的最自然的方式可能会根据用户、环境或任务的不同而有所不同，因此用于任务规范的机器人基础模型应该接受各种描述模式，例如目标状态[Fu et al.2018; Singh et al.2019]，自然语言[MacGlashan et al.2015； Karamcheti 等人，2017； Misra 等人，2017b； Co-Reyes 等人，2019； Shao et al.2020]，人类视频[Shao et al.2020； Chen 等人，2021c； Liu et al.2018]，成对或排名比较[Biyik and Sadigh 2018]，交互式修正[Co-Reyes et al.2019； Karamcheti et al.2020] 和物理反馈[Ross et al.2011；巴杰西等人。 2017]。任务规范的通用模型的一个重要要求是能够转移到新的环境和任务。将任务描述可靠地转换为机器人学习的通用奖励信号仍然是一个悬而未决的问题[Taylor et al.2016]——机器人基础模型可以说非常适合这个问题。当应用于任务规范时，此类模型应该通过从大型和广泛的数据集中学习来提供更强大（§4.8：稳健性）的奖励信号 - 甚至利用上面列出的多种描述模式。用于任务规范的新基础模型的一种可能的实例可能是通过对不同语言和视觉数据集进行训练来学习从任意（语言、当前观察）对到奖励信号的映射[Bahdanau et al.2019；傅等人.2019; Chen 等人.2021c]。通过从这些广泛、多样化的数据集中学习信息先验，这样的模型可能能够推广到看不见的环境中看不见的语言指令和观察结果。总的来说，新的基础模型能够巧妙地桥接模式并广泛推广的潜力使其对通用任务规范具有吸引力。任务学习的基础模型。除了实现更通用的任务规范之外，机器人基础模型还可以使学习解决新任务变得更加高效和可靠。在这种情况下，这些新型基础模型可能采取行动、传感器观察、奖励和其他感兴趣属性的联合分布的形式。对该联合分布的不同维度进行调节可以恢复不同的推理问题，每个推理问题对应不同的签名：动力学建模：𝑝（未来观察|行动，过去观察）[Finn and Levine 2017；哈夫纳等人。 2019；吴等人。 2021d]。政策学习：𝑝（行动|观察，目标）[Kaelbling 1993； Schaul 等人，2015；丁等人。 2019]。逆强化学习：𝑝（奖励函数|观察、行动）[Ng and Russell 2000；齐巴特等人。 2008年；芬恩等人。 2016a]。机器人基础模型的一个合理的训练目标是以自回归方式预测上述关节分布的不同元素 [Janner et al.2021;陈等人。 2021b，§4.1：建模]。然而，这些并不是唯一的选择。特别是，机器人数据集包含大量未标记的数据，其中包括来自许多不同传感器模式（例如，RGB 和深度相机、触觉传感器、麦克风等）的同步观察结果以及机器人为生成这些观察结果而执行的一系列动作。除了上述目标之外，关于基础模型的机遇和风险 37 可以训练机器人基础模型来预测一种传感器模式对另一种传感器模式的观察，或者预测两个感官观察流是否来自同一时间段。这些类型的自我监督目标可以利用多模态对应来生成高维数据的低维表示，甚至可以与上述目标相结合，在这些表示之上产生模型、策略和奖励。这些目标可以促进从未标记数据中训练强大的机器人基础模型——只要数据表现出多样化、有意义的行为。 §2.3.2：机器人挑战讨论了进一步收集此类数据的挑战。在语言和视觉方面，基础模型已经证明了从大型、多样化的数据集中学习广泛适用的先验的能力，这些先验可以随后适应下游任务（§2.1：语言，§2.2：视觉）。机器人基础模型有潜力类似地通过利用与现有语言和视觉模型研究不同的数据、自我监督目标和模式，使感知和控制能够适应新环境、任务和实施例。考虑一下我们运行厨房的例子。为了在新厨房做饭，机器人需要适应特定的环境——空间布局、可用的设备等。先验者从人类的离线视频中了解到，机器人交互、文本和/或模拟可能会编码厨房的一般方面，例如炉子通常靠墙放置，必须打开才能产生热量。这些常识性知识、物理先验和视觉先验可以使适应新环境的样本效率更高。同样，开发用于机器人任务学习的新基础模型可能会在其训练数据集中使用大量烹饪视频，以根据特定用户的偏好调整通用技能（例如“煎鸡蛋”）的策略。演示数量少——允许样本有效适应。最后，凭借学习前面描述的跨模式表示的潜力，机器人基础模型可以帮助适应新的实施例。适应的这一方面对于使这些模型广泛使用至关重要。 

### 2.3.2 挑战和风险

尽管有这个令人兴奋的愿景，但仍需要克服多项挑战。为了实现上面讨论的泛化，我们必须收集足够大小和多样性的机器人数据集。此外，我们需要机制来确保我们可以在现实世界中安全地部署学到的行为。数据需求和挑战。传统上，学习机器人通过传感器感知环境状态并采取行动完成任务的策略需要机器人在现实世界中交互的大量数据集。另一方面，计算机视觉和自然语言处理中的许多学习任务依赖于可以轻松从网络上抓取的大型且多样化的离线数据集。受现有语言和视觉基础模型进步的推动，我们对利用大型离线数据源来训练新的机器人基础模型的可能性感到兴奋。实现这一目标的一种途径是收集用于离线学习的大型数据集，例如使用远程操作[Mandlekar et al.2019]、动觉教学[Sharma et al.2018]或自主方法[Pinto and Gupta 2016；古普塔等人，2018；莱文等人，2018；达萨里等人，2019；卡拉什尼科夫等人2021； Chen et al.2021d]，它们在泛化方面显示出了一些有希望的迹象。同时将机器人数据收集扩展到视觉和语言数据集的大小[Deng et al.2009；克里希纳等人，2017；拉斐尔等人，2019； Gau et al.2020a]仍然是一个开放的挑战，机器人数据集的规模和质量不断提高表明它们可以在学习机器人基础模型中发挥重要作用。此外，由于机器人有能力主动、自主地塑造其环境，因此它们应该能够大规模生成有针对性的未标记数据。 38 基础模型研究中心 (CRFM) 鉴于学习控制具有挑战性的闭环性质，对于机器人技术来说，收集与视觉和语言中使用的数据集相当的数据集可能是不够的。一种令人兴奋的选择是额外利用外部非机器人数据源，例如人类视频或现有视觉和自然语言数据集。这些数据多种多样，并且在网络上大量存在[Deng et al.2009；李等人，2012；海尔布隆等人，2015；戈亚尔等人，2017a；达门等人。 2018；高等人.2020a； Grauman et al.2021]，如果利用得当，则提供广泛推广的可能性。优雅地解决机器人领域与网络视频或语言中发现的领域之间的差距仍然是一个开放的挑战；然而，领域适应的最新进展[Smith et al.2019； Schmeckpeper 等人 .2020] 并在机器人技术中使用预训练的视频和语言模型 [Lynch 和 Sermanet 2020；邵等人.2020; Chen et al.2021c] 提出了缩小这一差距的有希望的方向。最后，模拟提供了机器人可以学习的无限丰富的交互式数据源，包括渲染视觉、点云和模拟触摸/音频等一系列传感器模式。然而，一个主要的挑战在于弥合模拟与现实世界之间的差距，无论是在基础物理方面还是在环境和任务的语义分布方面。最近的工作表明，通过使用广泛的域随机化，任务范围从飞行 [Sadeghi 和 Levine 2017] 到富含接触的操作 [Mahler et al.2017； OpenAI et al.2019] 和运动[Peng et al.2020； Hwangbo et al.2019]在模拟中学到的技能可以成功地转移到真实的机器人上，并且可以通过将现实世界扫描到模拟中来模拟现实世界的语义和视觉分布[Chang et al.2017； Kolve 等人，2017b； Savva 等人，2019a； Szot 等人，2021； Shen 等人.2021a]。虽然这些都是缩小模拟与真实差距的有希望的步骤，但有效且通用的模拟与真实学习操作和运动技能仍然是一个开放的挑战。模拟数据、真实机器人数据、人类视频和自然语言数据对于学习机器人基础模型都至关重要。安全性和稳健性。确保机器人新基础模型在现实世界中训练或部署时的安全性和稳健性，使新机器人基础模型的开发变得更加复杂。鉴于实体代理有权在物理世界中直接操纵周围环境并与之交互，我们可以预期这些机器人模型的安全风险将不同于其语言模型。基于学习的系统面临的一个核心安全挑战是先有鸡还是先有蛋的问题，即在收集数据之前需要指定系统的安全约束，之后可能会出现需要额外约束的不可预见的不安全行为。例如，适应训练分布之外的新厨房的智能体需要足够的安全保证来确保安全的数据收集，这可能会对任务性能产生不利影响或导致智能体以新的方式失败。解决这个问题的一种方法是限制环境的复杂性或增加机器人的复杂性，从而通过构造来避免不可恢复的状态或不安全的动作。机器人还可以负责自动重置环境，以促进大规模数据收集的不间断学习（或适应）[Eysenbach et al.2017； Gupta 等人.2021b]。这要么意味着确保厨房里没有任何东西是易碎的，要么确保并更换代理在尝试收集数据时可能损坏的物品。为了解决机器人基础模型无法概括或对新刺激产生意外行为所带来的风险，未来潜在的方向包括开发代理的因果分析[Déletang et al.2021]、新的正式安全评估工具和现实模拟环境[Corso]等2020； Dreossi 等人，2017；朱利安和科亨德弗 2019]。最后，为机器人基础模型导出正式的安全保证，例如安全集的 Hamilton-Jacobi 可达性 [Chow et al.2018; Fisac 等人，2019； Herbert et al.2021]或开发人类操作员可解释的学习安全边界（§4.11：可解释性），可以帮助减少此类模型在基础模型 39 的机会和风险上带来的风险[Berkenkamp et al.2017]。随着这些新型基础模型的开发和研究的进展，解决这些挑战将至关重要。结论。虽然机器人基础模型的前景很多——涵盖从任务规范到任务学习的机器人管道的多个层面——但挑战也是巨大的。在物理世界中大规模收集涵盖不同环境和实施例的数据是一个相当大的障碍，确保此类系统的安全性和稳健性同样紧迫。尽管如此，我们仍保持乐观态度。在开发模型之前解决这些挑战使我们有机会找到从正确的来源、以正确的规模收集正确数据的方法，以构建具有我们所需功能的安全可靠的机器人基础模型。支撑本节的是多模态的主题。机器人基础模型——在所有可能的实例中——已经并将继续受益于人工智能其他子领域的工作，例如语言和视觉（§2.1：语言，§2.2：视觉）。然而，当我们考虑整合来自其他领域的这些扩展时，即将出现的跨学科挑战涉及基础模型的其他方面：用于训练和部署实时机器人模型的系统创新（第 4.5 节：系统）、用于实时机器人技术的接口创新强大的人机交互（§2.5：交互），以及我们更好地掌握此类模型的安全性和鲁棒性时要吸收的经验教训（§4.9：人工智能安全，§4.8：鲁棒性）。围绕基础模型（特别是机器人基础模型）建立可靠的生态系统和深思熟虑的研究实践是实现这些目标的关键。40 基础模型研究中心 (CRFM) 

## 2.4 推理和搜索 

作者：Yuhuai Wu、Frieda Rong、 Hongyu Ren、Sang Michael Xie、Xuechen Li、Andy Shih、Drew A. Hudson、Omar Khattab 

图 9. 多模态不仅可以让基础模型使用形式符号语言进行推理，还可以利用问题的视觉方面，例如等价性、对称性和欧几里得几何，以修剪无限搜索空间并找到有希望的解决方案结构（§2.4.1：推理任务），模仿人类推理几何问题的方式。

推理和搜索一直是人工智能历史上的中心主题。从策略游戏到抽象数学发现，经典的智力测试都是鼓舞人心的目标，通过设计更智能的方法来寻找获胜解决方案，从而突破了“机器智能”的极限。在早期，符号方法是推理的主要方法 [Russell 和 Norvig 2020]，但所涉及的工程工作以及形式化启发式方法来解决棘手的搜索空间的需要很快被证明是麻烦的。最近，使用神经网络的数据驱动方法已经显示出令人鼓舞的结果，例如，通过利用统计结构和学习有用的启发法。本节概述了现有的推理任务，这些任务需要扩展到更大的搜索空间并广泛地理解世界（§2.4.1：推理任务）。然后，我们在§2.4.2：推理角色中论证，基础模型应该在一般推理中发挥核心作用，作为捕获无界搜索空间的统计规律（生成性）的工具，允许跨任务和场景的积极转移（普遍性），以及利用多模式环境中的知识基础（接地）。 

### 2.4.1 当前的任务是什么？

许多推理问题都会带来无限的搜索空间，系统必须处理多种开放式替代方案。考虑尝试证明对于等腰三角形 △𝐴𝐵𝐶 和𝐴𝐵=𝐴𝐶，角度 ∠𝐵 和 ∠𝐶 相等（图 9）。系统可以在推理的每个步骤中执行任意数量的操作。例如，系统可以添加具有任意构造的新辅助点，例如垂直线、平行线或切圆，并且搜索空间只会随着图表变得更加复杂而变得更大。证明这个定理的一种方法是画一条线𝐴𝐷，它是𝐴的角平分线，并使用两个三角形△𝐴𝐵𝐷和△𝐴𝐶𝐷的全等来显示∠𝐵=∠𝐶，但是系统如何在不进行大量搜索的情况下找到它呢？更一般地说，数学家并不局限于搜索图表结构和欧几里得定理：数学家可以应用来自不同分支的大量定理关于基础模型的机遇和风险 41 图 10。左：1,6- 的反应路线基于机器学习的药物逆合成规划器 AiZynthFinder 预测 Heptadiene-3,5-dione [Genheden et al.2020; Yoshikawa 等人.2021]。右：命题逻辑中的示例证明树，其中绿色概述的公式代表公理。尽管它们来自不同的领域，但两棵树在结构上是相同的。数学知识、做出高级猜想、形式化新的数学概念或找到反例。这与围棋等更加结构化的人工智能挑战形成鲜明对比，围棋的搜索空间被认为要小得多。17除了定理证明之外，许多现实世界的问题都涉及无界搜索空间，例如程序综合[Gulwani et al.2017]，药物发现 [Drews 2000]、化学合成 [Segler et al.2018]、计算机辅助设计 [Haigh 1985]、组合优化 [Bengio et al.2021] 等等。这些推理问题往往表现出相似的结构，例如药物发现中的逆合成与命题逻辑中的定理证明之间的双射，如图 10 所示：在这两个问题中，一个人都在构建一棵合成树，其节点一方面是化学产品一方面是命题，一方面是叶节点，另一方面是结束公理。在这些问题中，通常会提供模拟环境，它允许求解器运行多个搜索线程来构建解决方案树。模拟器通常会提供中间反馈，例如，在证明被认为完成之前通知求解器需要建立的剩余命题。求解器反过来需要选择最有希望的搜索线程并根据中间反馈继续进行。最近，人们对应用基于学习的方法来解决推理问题的兴趣激增。为了克服无界搜索空间的挑战，研究人员首先从受限的搜索空间开始，以使问题易于处理[Huang et al.2018； Bansal 等人，2019]。但这种方法受到求解器可以发出的操作类型有限的影响。例如，求解器只能应用已知数据库中的定理来证明目标定理，而不是综合新的定理和引理。由于大型语言模型提供了将输出空间建模为序列的通用方法，因此它们很快成为更受欢迎的选择，允许生成任意类型的动作。研究人员已将这些基于语言模型的方法应用于各种应用，例如预测蛋白质结构 [Senior et al .2020]、证明形式定理 [Polu and Sutskever 2020; Han et al.2021]，猜想定理[Urban和17小于围棋棋盘上的网格点数量（即19×19棋盘的361个动作）。42基础模型研究中心（CRFM）Jakubuv 2020；拉贝等人.2021; Li et al.2021b]，从自然语言合成程序[Chen et al.2021f； Ling et al .2016]，修复、生成和理解代码[Yasunaga 和Liang 2021； Lu 等人.2021b；郭等人，2020； Svyatkovskiy 等人.2020； Kim 等人.2021b； Zügner 等人.2021]。研究还表明，缩放模型大小可以显着提高推理能力 [Polu and Sutskever 2020]，此外，语言建模的标准技术（例如预训练）也可以极大地提高这些任务的性能 [Rabe et al. 2020]。 2021 年； Polu 和 Sutskever 2020]。 

### 2.4.2 基础模型的作用是什么？

生成性。我们相信基础模型的生成能力对于有效推理至关重要。由于搜索空间无限，枚举各种可能性变得很困难。相反，通过基础模型，人们可以对最佳决策的分布进行建模，并生成合适的候选者以继续下一步。特别是，由于基础模型提供了一种将输出空间建模为序列的通用方法，因此下一代决策完全不受约束，因此是通用的。这种灵活性对于我们讨论的许多推理挑战至关重要，以允许在数学猜想 [Li et al.2021b] 和合成新颖程序 [Chen et al.2021f] 等领域进行创造性的生成。随着基础模型规模的扩大，捕获此类统计结构的能力也会大大增强 [Polu 和 Sutskever 2020]。普遍性。正如我们在上一节中提到的，许多推理问题都表现出类似的潜在结构。我们相信，基础模型强加的统一框架可以跨任务传输和共享重要的启发式方法，从概括适用于一项任务的低级技术到新场景，一直到直接找到适用于众多任务的元技术。各种问题。此外，由于基础模型是跨多个领域进行训练的，因此它可以跨任务和领域积极地传输基础模型权重中编码的元知识[Papadimitriou and Jurafsky 2020; Wu 等人.2021f； Lu 等人.2021a]。基础模型训练和适应框架鼓励关注点分离，其中基础模型训练学习元知识，例如药物逆合成和命题逻辑证明之间的共享搜索树结构，而适应阶段可以专注于学习任务特定词汇。因此，基础模型可以降低适应阶段学习问题的复杂度，提高样本复杂度和泛化能力。接地。推理问题通常很容易用符号语言表达（例如，数学、代码、分子的 SMILE 表示）。然而，这些符号具有深刻的潜在语义——“等腰三角形”在人类头脑中描绘了一个生动的形象。基础模型可以实现深层基础和语义意义。首先，以其他形式（例如视觉或物理）为基础的表示对于掌握推理任务中的抽象概念并赋予它们具体的含义至关重要[Larkin and Simon 1987；贾姆尼克 2001]。由于模型可以在多种模式上进行训练，因此基础模型可以帮助理解一系列数据源（例如图像、文本）。因此，在几何示例中，通过对自然图像中学习到的几何形状的理解，基础模型可以有效地利用问题的图形表示。然而，推理中对齐的多模态数据很少，并且基础模型是否能够以无监督的方式发现不同模态之间的联系（例如，发现具有相应代数方程的交换图）仍然是一个悬而未决的问题。此外，即使在符号领域内，符号也可以有不同层次的解释。例如，高级编程语言可以翻译为低级汇编代码。基础模型可以学习包含这些不同视图的共享表示。过去的工作表明，自我监督任务 [Han 等人。论基础模型的机遇和风险 43 2021；彭等人，2021； Li et al.2021a] 允许模型了解高级代码脚本背后的内部工作原理，并进一步协助下游任务。 

### 2.4.3 推理方面的未来挑战

由于这些问题的内在困难，与原始图像和文本相比，高质量的注释数据稀缺且更难收集。为了缓解这个问题已经进行了多次尝试。在数学中，研究人员提出生成综合定理，希望能够推广到现实定理 [Wang and Deng 2020; Wu 等人.2021a； Firoiu 等人，2021； Zhou等人，2021c]。另一种方法是设计自我监督任务来扩充数据集 [Yasunaga and Liang 2020;任等人.2020; Han 等人，2021； Rozière 等人.2021； Yasunaga and Liang 2021]，或更好的预训练目标[Wu et al.2021f]。然而，我们仍然缺乏设计自监督任务的一般原则方法，因为大多数现有工作都是针对特定问题设置量身定制的[Yasunaga and Liang 2020;任和莱斯科维奇 2020； Han 等人，2021]。构建基础模型将鼓励构建一套可应用于所有推理问题的自我监督任务的统一框架。此外，交互性（§2.5：交互）可以通过足够的可扩展性，通过将人类带入循环中以最低程度地指导学习课程或数据增强过程来缓解数据稀缺问题，例如，选择要添加的公理或要探索的猜想，而交互式工具本身就是对基础模型进行推理的激励[Han et al.2021; Chen et al.2021f] 帮助那些认知要求最高或最费力的人。易于解释的交互式工具可以在功能强大的基础模型的帮助下帮助人类学习，从而在教育中找到进一步的应用（§3.3：教育）。提高高级推理能力是现有基础模型的核心挑战。人类在解决困难的问题解决任务时执行抽象推理和高级规划[Miller et al.1960]。例如，在构建软件工具或证明定理时，我们通常从高级草图开始，然后再深入研究低级细节 [Koedinger 和 Anderson 1990]。现有的基础模型没有经过训练来生成此类高级计划。相反，他们通常只专注于预测接下来的低级步骤 [Polu 和 Sutskever 2020； Han 等人，2021； Chen 等人.2021f]。不幸的是，为了训练基础模型来模拟类人推理，我们再次面临数据收集的挑战。尽管此类数据确实存在于有限的环境中[Li et al.2021b]，但总的来说，用于高级推理的数据稀缺且难以收集。研究方向之一是让抽象和模块化的层次结构在学习过程中自行出现[Ellis et al . 2021 年； Hong et al.2021]，但如何将这些方法扩展到更普遍和更现实的环境仍然是一个悬而未决的问题。除了这些挑战之外，还存在许多悬而未决的问题，这些问题对于其他部分讨论的主题也至关重要。什么构成了可靠推理的良好架构（§4.1：建模）？我们如何从理论上理解和解释这些模型（§4.10：理论和实践§4.11：可解释性）？我们能否训练能够推广到域外问题的鲁棒推理模型（§4.8：鲁棒性和§4.3：适应性）？我们相信，对这些前沿领域的基础模型的研究可以极大地扩大其对推理领域的影响。44 基础模型研究中心 (CRFM) 

## 2.5 交互 

作者：Joon Sung Park、Chris Donahue、Mina Lee、Siddharth Karamcheti、Dorsa Sadigh，Michael S. Bernstein 

图 11。基础模型将通过降低构建人工智能应用程序的难度阈值，为开发人员带来重大机会，并通过提高可实现的交互类型的上限，为应用程序用户带来重大机会。在某些情况下，开发人员和用户之间的界限将开始模糊，用户也许能够轻松开发自己的人工智能应用程序，例如使用自然语言。基础模型的早期形式，例如 GPT-3 [Brown 等人。 2020] 和 DALL ·E [Ramesh 等人 2021] 展示了高水平的多功能性，无论是在让非机器学习专家也能够构建强大的人工智能应用程序原型的能力，还是无缝集成各种模式的能力，文本到图像。随着基础模型开发的成熟，模型的容量将继续扩展，其多功能性可能最终导致我们与人工智能交互方式的根本改变，使我们能够快速原型化并构建高度动态和生成的人工智能应用程序。在本节中，我们从两个重要利益相关者的角度讨论这些变化带来的机会：（1）将与基础模型交互以设计用户体验的应用程序开发人员，以及（2）将使用或受其影响的最终用户由基础模型支持的人工智能应用程序。最后，我们考虑了当今严格区分开发人员和最终用户的界限可能开始变得模糊的场景，从而为创建更紧密地满足用户需求和价值的人工智能应用程序提供了新的机会。 

### 2.5.1 对人工智能应用程序开发人员开发流程的影响

基础模型将如何改变开发人员创建人工智能应用程序的方式？尽管机器学习算法和系统基础设施取得了巨大进步，但一些人指出，设计新颖且积极的人机交互形式仍然很困难[Dove et al . 2017年；库珀等人，2014]。创建强大的特定任务模型所需的大量数据、计算资源和技能经常与引发和满足用户需求和价值观所需的迭代原型制作过程发生冲突[Yang et al.2016]。这一挑战进一步加剧了基础模型 45 的机遇和风险，因为人工智能的反应可能是不可预测的，并且模型可以产生巨大的生成输出空间，使人们很难建立有效的绩效心理模型。在以交互式机器学习（例如，Crayon [Fails and Olsen 2003]、Regroup [Amershi et al. 2012]）和设计框架的形式解决这些挑战方面已经取得了一些进展，这些框架用于将人工智能中的不确定性传达给终端。用户（例如，混合主动原则[Horvitz 1999]）。然而，仍然需要做更多的工作来克服这些障碍[Yang et al.，2017]。 2020]。基础模型为解决上述许多挑战提供了重要机会。例如，基于语言的基础模型能够将自然语言作为输入，并推广到许多下游任务，可以显着降低应用程序开发的难度“阈值”[Myers et al.2000]，即通过启用开发无需收集大量数据并从头开始训练大型模型。这甚至可以让非机器学习专家快速原型化人工智能应用程序。与此同时，基础模型强大的生成能力和潜在的多模态能力可以为可实现的交互类型在质量和多样性方面提供更高的“上限”[Myers et al.2000]，正如我们将要实现的那样下面讨论。然而，我们如何成功地利用这些能力将取决于我们如何有效地将基础模型转化为更易于应用程序开发人员管理的形式。不幸的是，赋予基础模型优势的通用性和高上限也可能使这些模型难以使用，因为它们可能比单一用途的人工智能模型更加不可预测和复杂。事实上，最近的工作表明，让像 GPT-3 这样的模型始终如一地执行预期任务可能很困难 [Reynolds 和 McDonell 2021]，而了解它的能力仍然是一个活跃的研究领域 [Hendrycks 等人 2021]。 2021a]。为了提高人工智能应用程序的可靠性和可信度，我们建议未来的工作应继续研究如何从基础模型中实现更可预测和更稳健的行为（例如，通过微调，或者在主模式交互的主要内容是自然语言提示，通过提示工程 [Reynolds and McDonell 2021; Liu et al.2021d]、校准 [Zhao et al.2021] 或预先格式化特定于任务的端点。18请参阅§4.8：鲁棒性更多细节）。 

### 2.5.2 对最终用户与人工智能应用程序交互的影响

除了开发人员创建人工智能应用程序的新方法之外，基础模型还将给最终用户与这些应用程序交互的体验带来哪些变化？正如 Douglas Engelbart [Engelbart 1963] 所描述的那样，用于开发面向用户的人工智能应用程序的现有设计框架侧重于增强（而不是取代）用户的能力 - 我们期望这些框架应该并且将继续与未来人工智能应用程序的开发相关。 - 阳离子。例如，维护用户的代理权并反映他们的价值观将继续成为基础模型驱动的应用程序的中心主题。此外，需要仔细权衡允许 AI 代理采取主动并自动化用户例程的好处与等待用户直接操作的好处 [Shneiderman 和 Maes 1997] [Horvitz 1999]。此外，用户的价值观应通过参与式[Lee et al.2019]和价值敏感设计[Smith et al.2020]等流程直接收集和反映，这些流程提倡在人工智能注入的设计过程中积极让所有利益相关者参与进来应用程序。对于基础模型，这些问题可能变得尤其突出，因为模型的行为方式可能会让用户和社区感到惊讶和失望。生成能力可能会暴露与社区目标相反的偏见或观点，或者更阴险的是，18https://beta.openai.com/docs/guides/classifications46 基础模型研究中心 (CRFM) 利用此类关联他们的行为在社区不知情的情况下。这将给使用基础模型来监控模型行为并尽可能调整模型以适当方式行事的团体带来巨大负担。虽然考虑人工智能应用程序以增强用户能力的设计框架应该保持不变，但由于基础模型强大的生成和多模式能力，可实现的实际交互形式可能会大大多样化。早期用于多媒体创建和编辑的基础模型驱动的软件工具已经开始推动一个新的领域，即使是新手内容创建者也能够根据粗略、直观的规范生成高质量的多媒体（例如，协作创作作家 [Lee 等人 .2022]，数字艺术家的文本到图像生成，19音乐家的母带处理，20 以及程序员的代码完成）。21改进的基础模型可能会启用更雄心勃勃的工具（例如，粉丝可能会为然后将以他们最喜欢的乐队的风格生成歌曲，或者企业主可能会提供其产品的简单描述，这些描述将用于创建完整的网站）。此外，基础模型将用于丰富静态多媒体（例如，自动将遗留多媒体内容重新制作成新格式，或为新视频游戏中的每个玩家生成独特的体验），甚至可能导致使用以下界面的新形式的多模式交互：它们本身混合了不同的方式，例如视觉和基于手势的交互。我们开始看到基础模型如何具体化为 AI Dungeon22 到 Microsoft PowerApps23 和 CoPilot 等应用程序中的具体交互。24 当我们开始设想新的交互形式时，批判性地思考其潜在影响就变得越来越重要这些互动将对个人用户和社会产生最大程度的积极影响。例如，基础模型驱动的应用程序将如何改变我们彼此沟通的方式？一个强大的模型会代替我们写电子邮件吗？如果是的话，这将如何重塑人们的信任、可信度和身份，因为我们知道电子邮件的作者可能不是自己写的，这将如何改变我们的写作风格[Hancock et al.2020] ]？谁将拥有模型生成内容的作者身份，以及如何滥用同意的责任和所有权转移 [Weiner 2018]（请参阅第 5.5 节：经济学以进行更深入的讨论）？基础模型将对我们的工作、语言和文化产生哪些长期影响[Hancock et al.2020;布什克等人。 2021]？与最后一个问题特别相关的是，基础模型是根据观察到的数据进行训练的，并不一定能告诉我们因果关系。因此，我们如何确保基础模型的使用引导我们走向理想的未来，而不是重复过去？尽管这些问题不一定是基础模型所独有的，但随着基础模型加速创建有效的人工智能应用程序，这些问题将会被放大并变得更加普遍。 

### 2.5.3 模糊开发者和最终用户之间的界限

如今，人工智能模型开发人员和最终用户之间的界限已经很严格——最终用户很少拥有数据、计算资源和专业知识来开发适合自己价值观和需求的新模型。需要很好。虽然通用模型（即不特定于特定用户或社区的模型）在某些情况下可能就足够了，但近年来，此类模型无法为用户提供服务的场景越来越多。例如，文本分类 19https://github.com/nerdyrodent/VQGAN-CLIP 20https://www.landr.com/ 21https://copilot.github.com/ 22https://play.aidungeon.io/main /home 23https://powerapps.microsoft.com/en-us/ 24https://copilot.github.com/On the Opportunities and Risks of Foundation Models 47 旨在识别某个在线社区有问题的评论的模型可能对此很有效社区，但在规范和文化可能存在显着差异的其他社区中会失败（例如，Reddit 上的 NSFW 社区可能更能容忍某些内容，而科学社区可能会拒绝并非基于科学研究的看似平凡的轶事）[Chandrasekharan et al.2018 ]。在另一个例子中，为一个目标人群设计的人工智能传感器和机器人工具可能会失败，因为无法快速适应具有不同能力和需求的用户的上下文[Karamcheti et al.2021]。虽然最近的工作为未来研究最终用户如何通过手动提供模型参数或数据集来共同创建人工智能模型提供了有希望的途径（例如，WeBuildAI [Lee et al.2019]），但结果仍然是初步的并且经常关注基本模型。如果基础模型能够充分降低构建人工智能应用程序的难度阈值，那么它们可以通过允许用户积极参与模型的开发过程，从而提供一个重要的机会，将用户的需求和价值观与模型的行为更紧密地结合起来。例如，最近的工作表明，当在自然语言提示中给出足够的任务描述时，GPT-3 可以以几次甚至零射击的方式稳健地执行分类任务 [Brown et al.2020]。试图审核自己内容的在线社区可能能够利用这种能力来创建定制的人工智能分类器，根据社区已经同意的分类任务描述来过滤内容（当然，这种能力也可能被滥用来压制社区内某些成员的声音 - 我们指出§5.2：滥用以进一步讨论该主题）。此外，基础模型将展示的强大的上下文学习功能可能允许基础模型驱动的应用程序更有效地针对每个用户优化其界面。这可以为解决人机和机器人交互中的许多突出问题打开大门，例如在混合自主设置中平衡用户直接操作和自动化的能力。当然，我们仍然需要克服一些重要的挑战，才能真正实现这种模糊用户和开发人员之间界限的潜力。这些挑战包括减轻基础模型中现有的偏差，以及使模型的行为更加稳健和易于管理，即使对于非机器学习专家来说也是如此（与机器学习专家相比，非机器学习专家可能更难以理解全部能力）和基础模型的机制，这可能会导致开发周期中出现意想不到的陷阱[Yang et al.2018]）。未来的工作应该探索如何将基础模型置于交互式机器学习的背景下，并研究我们如何支持那些机器学习经验有限的人以稳健的方式利用这些模型。尽管如此，最终用户参与开发人工智能应用程序的能力是一个令人兴奋的机会，它可以为我们未来如何与这些应用程序交互引入新的范例。48 基础模型研究中心 (CRFM) 

## 2.6 理解哲学 

作者：Christopher Potts、Thomas Icard、Eva Portelance、Dallas Card、Kaitlyn Zhou、John Etchemendy 

基础模型可以理解其所训练的数据的哪些内容？这个问题的答案将为基础模型对智能系统做出贡献的整体能力提供非常丰富的信息。在本节中，我们重点关注自然语言的情况，因为语言使用是人类智力的标志，也是人类经验的核心。目前最好的基础模型可以极其流畅地使用和产生语言，但它们总是陷入一种不连贯的状态，这表明它们只是“随机鹦鹉”[Bender et al.2021]。这些失误是固有局限性的证据，还是未来的基础模型可能真正理解它们处理的符号？我们本节的目的是澄清这些问题，并帮助围绕这些问题展开辩论。我们首先解释基础模型的含义，特别关注基础模型的训练方式，因为训练制度限制了模型获取有关世界的信息。然后我们讨论为什么澄清这些问题对于进一步开发此类模型很重要。最后，我们试图澄清理解的含义，解决理解是什么（形而上学）以及我们如何可靠地确定模型是否已经实现理解（认识论）。最终，我们得出的结论是，对未来模型理解自然语言的能力的怀疑可能还为时过早。仅靠基础模型就能实现理解这一点绝不是显而易见的，但我们也不知道认为它们不能实现的明确理由。 

### 2.6.1 什么是基础模型？

基础模型没有精确的技术定义。相反，这是一个大型模型系列的非正式标签，并且该模型系列可能会随着时间的推移而增长和变化，以响应新的研究。这对推理它们的基本属性提出了挑战。然而，可以说所有基础模型都有一个共同的决定性特征：它们是自我监督的。我们的重点是自我监督是模型唯一正式目标的情况。在自我监督中，模型的唯一目标是学习其所训练的符号序列中的抽象共现模式。这项任务还使许多模型能够生成看似合理的符号字符串。例如，许多基础模型的结构使得人们可以用“三明治含有花生”之类的序列来提示他们，并要求他们生成一个延续 - 例如，“黄油和果冻”。其他模型的结构使得它们能够更好地填补空白；您可能会提示模型“三明治包含 __ 和果冻”，并期望它填充“花生酱”。这两种功能都源自这些模型从训练数据中提取共现模式的能力。这种自我监督并没有明显的意义告诉模型这些符号的含义。它直接给出的唯一信息是有关哪些单词倾向于与哪些其他单词同时出现的信息。从表面上看，知道“三明治含有花生”很可能会继续“黄油和果冻”，但并没有说明三明治是什么、果冻是什么、这些物体如何组合等等。这似乎表明基础模型所能实现的目标的固有限制。但是，我们不需要将模型限制为仅查看文本输入。基础模型可以在各种不同的符号上进行训练：不仅是语言，还包括计算机代码、数据库文件、图像、音频和传感器读数。只要它只是学习它所接触到的序列的共现模式，那么根据我们的定义，它就被视为基础模型。作为学习的一部分，该模型可能会代表给定文本片段与特定传感器读数之间的强关联，或者代表基础模型 49 像素值序列与数据库条目之间的强关联。这些关联可能反映了我们所居住的世界的重要方面以及我们用来谈论它的语言。 

### 2.6.2 有什么利害关系？

在考虑分析什么是理解之前，值得反思一下为什么我们会关心基础模型能否实现理解这一问题。这些模型准备用于具有各种功能的多种用途。我们的一些部署目标可能只有在模型能够理解的情况下才能实现。这里我们列出了一些这样的目标： 信任：有人可能会说，我们不能信任一个系统的语言行为，除非它理解它所使用的语言。当然，我们目前相信工程系统可以做事（例如，制造汽车零部件），甚至不会出现理解问题，但语言在这方面可能很特殊，因为它是人类独有的。此外，语言可以用来欺骗和歪曲，因此仅理解并不意味着信任。总体而言，理解可能被视为语言使用环境中信任的必要条件。可解释性：如果真正的自然语言理解在某种程度上涉及维护和更新世界的内部模型（包括例如语音上下文），并且我们（作为工程师）是否能够分析语言输入和输出如何与这个内部模型可以在这些系统的可解释性、可预测性和控制方面带来巨大的收益。问责制：与前面的观点不无关系，未来我们可能会发现以某种方式让人工智能体对它们产生的语言负责[HAI 自适应代理组 2021]。根据我们如何思考责任、责任、代理等概念，语言理解可能会成为先决条件。理解在任何这些问题中都将发挥不可或缺的作用，这一可能性本身就提供了开发理论框架的强大动力。 

### 2.6.3 什么是理解？

我们的核心问题是基础模型是否能够理解自然语言。有了上述内容，我们现在可以尖锐化它：自我监督是否足以理解，记住用于这种监督的数据没有限制？为了解决这个问题，我们首先需要定义什么是理解。首先，我们发现明确区分有时会在主题讨论中混淆的区别是有帮助的。区别在于形而上学和理解的认识论之间。形而上学关注主体实现理解意味着什么（“原则上”）。相比之下，认识论关注的是我们如何（“在实践中”）知道一个主体已经实现了相关类型的理解。简而言之，形而上学更多的是关于我们的最终目标，而认识论更多的是关于我们如何（如果有的话）知道我们何时达到了目标。因此，我们的认识论在某种程度上取决于我们的形而上学。50 基础模型研究中心 (CRFM) 理解的形而上学。语言哲学为理解自然语言提供了多种选择。25为了简洁起见，简化一下情况，以下三大类观点都与 AI 和 NLP 的研究方向有关：26 内在主义：语言理解量根据语言输入检索正确的内部表征结构。因此，如果没有丰富的正确类型的内部概念库，语言理解甚至是不可能的。指涉主义：粗略地说，当代理能够知道该语言中的不同句子如何为真（相对于上下文）时，他们就理解了语言。也就是说，词语具有所指对象，并且（陈述性）话语是可以进行真实评估的，并且理解涉及相对于情况或场景的呈现来评估它们的能力。实用主义：理解不需要任何内部表示或计算的方式，真理和参考不是基础。相反，重要的是代理能够以正确的方式使用语言。这可能包括推理或推理模式的倾向、适当的对话动作等等。至关重要的是，相关的语言能力构成了理解。27虽然这是可能性空间的简化图景，但我们已经看到它们如何以完全不同的方式与上述目标相关。例如，根据实用主义的观点，实现语言理解并不意味着我们信任或解释系统的能力，因为它不能保证主体的内部结构或其与（非语言）世界的关系。相比之下，内在主义者的观点至少强烈建议一种相当稳健的内部/因果解释性。基础模型原则上是否能够理解语言的问题呈现出截然不同的特征，具体取决于我们采用哪些形而上学特征。内在主义和指称主义都可以定义为映射问题：将语言符号与“含义”或“语义值”相关联。对于内在主义来说，这将是一种表示或概念、一个用于计算值的程序或某种其他类型的内部对象。对于指称主义，它可能是从单词到外部指称的映射，或者从情况到真值的映射（都与上下文相关）。自我监督是否足以在基础模型中实现所需的映射？在这里，训练示例的性质可能是相关的。如果模型仅接收语言输入，那么它学习这种映射的能力可能会从根本上受到限制，从而阻止它学习相关意义上的指称。 （事实上，Merrill 等人 [2021] 确定了一些理论限制，尽管是在关于学习符号含义的非常强烈的假设下。）但是，如果输入符号流包含世界上事物的不同数字痕迹 -图像、音频、传感器等——那么共现模式可能包含足够的信息，使模型能够为所需的映射引入高保真代理。28相对于 25，科学哲学中有大量文献关注“理解，主要是与科学解释有关。参见格林[2021]。 26我们将其他可能与理解形而上学相关的问题放在一边，例如意识或某种形式的主观经验是否必要。这些都是紧迫的哲学问题，但它们很难与人工智能和自然语言处理的研究联系起来。 27 对于内部主义观点和参考观点的简单介绍，我们推荐 Elbourne [2011]。这个版本的实用主义可以说是在维特根斯坦[1953]中找到了根源，但图灵[1950]最简洁地表达了它，其中图灵建议用有关特定行为测试的问题来代替机器是否可以思考的问题（这是被称为图灵测试）。 28就映射体现因果信息而言，我们还必须应对有关从相关（甚至实验）数据得出因果推论的可能性的理论限制（参见Spirtes等人，2001年；Bareinboim等人，2020年）。关于基础模型 51 参照主义的机遇和风险，仍然存在一个进一步的问题，即这些代理如何与现实世界相关，但对于人类语言用户来说，也会出现同样的问题。 Bender 和 Koller [2020] 给出了一个有趣的论点，将指称主义与实用主义结合起来。他们想象一个代理 O 拦截两个使用自然语言 L 的人之间的通信。O 生活在一个与人类截然不同的世界中，因此不具备以指称主义所要求的方式为人类的话语奠定基础所需的经验。尽管如此，O还是从人类的话语模式中学习，以至于O甚至可以成功地假装成人类中的一员。 Bender 和 Koller 然后试图激发这样的直觉：我们可以很容易地想象出 O 无法在人类世界中将 L 扎根的情况，而这反过来又会揭示 O 不理解 L。指导性假设似乎是世界是如此的复杂，任何文本交换都无法完全覆盖它，而间隙最终会暴露出来。用我们定义的术语来说，无法指代意味着代理人不处于理解的正确处置状态。从根本上来说，本德和科勒所描述的场景中，一些用于理解的关键信息被认为缺失，而一个简单的行为测试就揭示了这一点。我们可以同意这一评估，但不会得出基础模型一般无法理解的结论。这再次让我们回到所涉及的训练数据的细节。如果我们修改本德和科勒的场景，使传输包括来自人类世界的数字编码图像、音频和传感器读数，并且 O 能够学习这些数字痕迹和语言单位之间的关联，那么我们可能会更加乐观 -可能是一个涉及 O 获取足够数据进行概括的能力的实际问题，但也许不是对 O 所能实现的目标的原则限制。29 我们初步得出结论，没有简单的先验理由认为理解的多样性属于 O 的范畴。我们的三个立场中的任何一个都无法以相关方式学习。由于这种可能性仍然存在，我们面临着认识论上的艰巨挑战，即澄清我们如何希望评估潜在的成功。理解的认识论。实用主义的一个积极特征是，通过将成功与具体行为的表现等同起来，关于如何测试它就不存在巨大的概念难题。我们只需让自己相信，迄今为止我们对系统行为的有限观察表明，我们对作为目标的更一般的行为类别有可靠的倾向。当然，就适当的目标达成一致是非常困难的。当提出具体建议时，它们总是会遭到反对，而且往往是在证明了假定的成功之后。图灵测试的历史在这里很有启发性：尽管许多人工智能体已经通过了实际的图灵测试，但它们都没有被广泛认为是智能的。同样，近年来，NLP 中的许多基准任务被提出来评估理解的特定方面（例如，回答简单问题、执行常识推理）。当系统超出我们对人类表现的估计时，社区的反应通常是测试有缺陷，而不是达到目标。可能有一些行为是我们真正的目标，但很难限制或转化为实际测试。30话又说回来，这可能表明内在主义或指涉主义是我们一直以来的想法。 29根据我们的阅读，Bender 和 Koller [2020] 认为多模式数据可能会改变场景，特别是如果允许 O 与人类就共享场景和主题进行合作交互。 30部分困难还可能与这样一个事实有关：典型的人类在许多这些领域中经常犯错误，但不一定与当前系统所犯的错误类型相同。因此，表征目标行为可能不仅仅涉及识别“正确”行为。52 基础模型研究中心 (CRFM) 如果我们将内在主义或参照主义作为最终目标——我们理解的黄金标准——那么行为测试将作为评估是否已达成理解的一种手段，充其量永远是不完美的。缺陷有两个方面。首先，行为测试总会存在一些漏洞，可能会让不复杂的模型漏掉。其次，系统可能已经实现了这些视图所需的映射，但我们可能无法通过行为测试来证明这一点。最近使用 GPT-3 模型的经验表明，这可能会变得多么具有挑战性：根据所使用的提示，人们可能会看到令人惊讶的连贯输出或完全无意义的输出，因此即时工程需要深厚的专业知识 [Rong 2021]。因此，内在主义和指称主义都需要结构评估方法，使我们能够研究它们的内部表征，探究它们以获取信息[Tenney et al.2019; Manning et al.2020]，研究它们的内部动态[Sundararajan et al.2017]，并且可能根据支持因果推理的特定实验协议积极地操纵它们[Vig et al.2020；盖格等人。2020]。我们从复杂基础模型的内部运作的实际实验中学到的东西可能存在根本性的限制，但很明显，只要我们的目标与内在主义或参照主义一致，这些方法就会很有用。 

### 2.6.4 推动讨论

很明显，对于基础模型是否能够理解语言的问题，没有简单的答案。为了开始解决这个问题，我们必须解决一个困难的形而上学问题，对此存在许多实质性不同的观点。然后，形而上学问题会引发认识论问题，从而带来许多实际挑战。尽管如此，上述讨论确实得出了一个实际结论：如果将基础模型作为人工智能体语言理解的一种途径，那么多模态训练体系很可能是最可行的策略，因为它们似乎最有可能为模型提供必要的信息。那么自我监督是否足够是一个完全悬而未决的问题。 关于基础模型的机遇和风险

# 3 应用 

基础模型的能力（§2：能力）表明它们有潜力改变各个部门和行业，扩展作用人工智能在社会中发挥作用（§5：社会）。在可以应用基础模型的无数应用中，我们将重点关注三个学科——医疗保健（§3.1：医疗保健）、法律（§3.2：法律）和教育（§3.2：法律）——它们都是社会功能的基础。在每个模型中，我们讨论了基础模型为此领域带来的机遇以及挑战（例如，可解释性；§4.11：可解释性）和关注点（例如，隐私；§4.7：安全性）。54 基础模型研究中心 (CRFM) 

## 3.1 医疗保健和生物医学 

作者：Michihiro Yasunaga、Jing Huang、Camilo Ruiz、Yuhui Zhang、Giray Ogut、Saahil Jain、William Wang、Yusuf Roohani、hongyu Ren、Antoine Bosselut、Ehsan Adeli、Jure Leskovec、Russ Altman 

图 12. 基础模型医疗保健和生物医学。我们可视化了一个交互式框架，其中基础模型在接受医疗保健生态系统中各种来源生成的多模态数据的训练后，可以实现医疗保健和生物医学领域的各种任务。第一列列出了多个数据源，包括护理提供者、付款人、机构（大学、非营利组织和政府）、制药公司、可穿戴设备和医疗出版物/论坛。第二列显示了数据源生成的几种数据模式。它们包括图像（例如胸部 X 光检查）、视频（例如超声波）、化合物图表、电子健康记录 (EHR) 表格、临床记录等文本、心电图等时间序列以及遗传数据。第三列可视化了根据此类数据进行训练的基础模型，然后将其应用于第四列中列出的医疗保健和生物医学下游任务。这个过程可以生成新的数据，进一步改进基础模型，从而建立基础模型和任务之间的双向关系。例如，医疗保健和生物医学是社会中一个巨大的应用领域，其支出占美国国内生产总值 (GDP) 的 17% [Swensen et al.2011; van Hartskamp 等人，2019； Keehan 等人，2020]。医疗保健（侧重于通过诊断、治疗和健康管理向患者提供护理）和生物医学研究（侧重于对疾病的科学理解和新疗法的发现）都需要大量费用、时间和全面的医学知识[于等人.2018; Korngiebel 和 Mooney 2021]。我们设想基础模型可以成为医学知识的中央存储，并根据医学数据的不同来源/模式进行训练[Krumholz 等人，2017]。 2016年；苏丹-扎德 2019； Suresh 等人 .2020]（图 12 左），并且可以由医疗专业人员交互式查询/更新（例如，医疗保健提供者和生物医学研究人员访问已发表的研究结果并上传新出版物）[Ionescu 等人 .2020] 并由公众查询。由于基础模型具有很强的适应能力（例如，微调、提示[Brown et al.2020]），它们可以有效地适应医疗保健和生物医学中的各种单独任务（例如，问题On the Opportunities and Risks of Foundation Models 55的回答）患者使用的应用程序[Klasnja and Pratt 2012；Zhu et al.2019；Daniel et al.2019；Liu et al.2020a]，临床试验匹配系统[Ni et al.2015；Harrer et al.2019；Beck et al.2020a]。 2020] 由研究人员和患者访问；图 12 右）。这样，基础模型可以成为支持医疗保健和生物医学中数据、任务和人员之间的各种交互的中央接口，从而提高医疗保健/生物医学应用的效率和准确性[Elbattah et al . 2021]。我们在第 3.1.1 节：医疗保健任务和第 3.1.2 节：生物医学任务中详细阐述了这些机会。与此同时，医疗保健/生物医学应用提出了独特的挑战，推动了基础模型的进一步研究，例如在医疗保健/生物医学中集成多模式数据[Miura et al.2021; Liu et al.2021a]并遵守医学伦理和法律法规（隐私、安全和可解释性）[Guan 2019；徐等人.2019]。我们在第 §3.1.3 节中详细阐述了这些挑战：healthcare-biomed-challenge。 

### 3.1.1 医疗保健领域的机会

基础模型可以改善通过医疗保健提供者和医院向患者提供的护理服务。目前，医疗保健成本每年都在增加 [Keehan et al.2020]，研究估计，由于行政效率低下和可预防的医疗错误，30% 的医疗保健支出可能会被浪费 [Kocher 2021]。此外，随着医疗保健需求的增加，社会面临医疗保健提供者的严重短缺[Kirch and Petelle 2017]。医疗保健领域的低效率和短缺需要为医疗保健提供者和患者开发快速、准确的界面，例如用于诊断/治疗、总结患者记录和回答患者问题的自动化辅助系统[Davenport and Kalakota 2019;聂等人.2018; Wang 等人.2021b]。特别是，在新冠肺炎 (COVID-19) 等紧急大流行危机中，快速诊断/筛查（例如，自动分析胸部 X 光图像）以及为患者（例如，症状检查和护理）和公众提供自动问答（例如，症状检查和护理）。例如，疾病预防）对于减少疾病传播和为危重患者分配医疗资源、挽救更多生命至关重要[Lalmuanawma et al.2020]。由于基础模型具有作为集成知识库的强大能力，因此可以查询并适应医疗保健中的各种单独任务。以下是医疗保健领域将从基础模型中受益的重要任务的示例。医疗保健提供者的界面。基础模型可以提高提供者护理的效率和准确性。医疗保健提供者花费不必要的时间编辑电子健康记录 (EHR) [Kocher 2021]，而可预防的医疗错误（例如，再入院、手术错误）会造成医疗保健浪费 [Shrank et al.2019； Shah 等人.2020]。基础模型可以作为 EHR（临床记录、实验室价值历史和影像文件）的高效、准确的接口进行调整 [Li et al.2020c；斯坦伯格等人，2021； Percha 2021]，帮助医疗保健提供者创建患者探视摘要[Krishna et al.2020]，检索相关病例和文献，并建议实验室检查、诊断、治疗和出院[Zhang et al.2019b； Rasmy 等人.2021]。基础模型还可以进行调整，以帮助手术机器人监控并实现准确的手术[Diana 和 Marescaux 2015； Agrigoroaie 和 Tapus 2016； Yu等人.2019]。有关机器人基础模型的更多讨论，请参阅§2.3：机器人技术。患者界面。基础模型可以调整为患者的界面，提供有关临床预约的相关信息[Bates 2019]，回答患者与预防性护理相关的问题[Demner-Fushman et al.2020]，以及相关的医学解释信息（例如，解释病情的文本和图形）[Chaix et al.2019]，并帮助患者使用辅助护理机器人[Jeong et al.2015； Abdi 等人.2018]。有关用户交互基础模型的更多讨论，请参阅§2.5：交互。基础模型还可以作为与公众的接口，回答与公共卫生和流行病预防相关的问题（例如 COVID-19 病例）[Bharti et al.2020； Herriman 等人，2020]。同时，56 基础模型研究中心 (CRFM) 我们注意到，界面必须保证事实准确性，以确保公众对医疗建议的信任 [Kreps 和 Kriner 2020]（参见第 3.1.3 节：医疗保健-生物医学-挑战） 。 

### 3.1.2 生物医药领域的机会

基础模型可以促进生物医学研究，例如药物的发现和疾病的理解，最终转化为改进的医疗保健解决方案[Hanney et al.2015]。目前，生物医学发现需要大量的人力资源、实验时间和财务成本。例如，药物开发涉及一个复杂的过程，从蛋白质靶点识别和有效分子发现的基础药物研究到临床开发（例如临床试验）再到最终的药物批准，通常需要10年以上的时间，成本超过10亿美元[Wouters 等人，2020]。利用现有数据和已发表的研究结果促进和加速生物医学发现是生物医学中的一个迫切问题[Yu et al.2018]。特别是，像 COVID-19 这样的新型疾病爆发造成了数百万人的生命和数万亿美元的损失 [Lalmuanawma et al.2020; McKibbin 等人.2020]；如果我们能够加快针对新疾病的药物开发，那将非常有帮助。基础模型在两个方面对生物医学发现特别有帮助。首先，基础模型具有强大的生成能力（例如，GPT-3中的连贯文本生成），这可以帮助生物医学研究中的生成任务，例如根据现有数据生成实验方案（临床试验）和设计有效的分子（药物发现） [Kadurin 等人，2017 年； Harrer 等人，2019]。其次，基础模型有可能整合医学中的不同数据模式，这使得能够从多个尺度（使用分子、患者和人群水平的数据）和多个知识源（使用成像、文本）研究生物医学概念（例如疾病）。和化学描述）。这有助于生物医学的发现，而如果使用单一模态数据则很难获得这些发现[Lanckriet et al.2004； Aerts 等人，2006 年； Kong等人，2011；里贝罗等人，2012； Wang 等人，2014，2015c；鲁伊斯等人.2020; Wu 等人.2021h]。基础模型还可以跨模式转移知识。 Lu 等人 [2021a] 展示了如何将在自然语言（一种数据丰富的模态）上训练的 Transformer 模型适用于其他基于序列的任务，例如蛋白质折叠预测，这是生物医学领域长期研究的预测任务 [Jumper等人.2020]。以下是将从基础模型中受益的生物医学重要任务的示例。药物发现。为了发现治疗疾病的药物或疗法，研究人员必须首先确定目标（例如与疾病相关的蛋白质、基因、RNA），然后必须寻找与目标结合的分子（例如化合物、抗体）。瞄准并治疗疾病。通常，识别适当的靶标并生成相应的分子需要多年昂贵的湿实验室实验[Hughes et al.2011； Schenone 等人，2013；施耐德2018]。基础模型的生成性可以提高搜索空间和效率（参见§2.4：推理），这不仅减少了实验量，还有助于发现新的更好的药物[Jin et al.2018; You et al.2018；沃尔特斯和巴齐莱 2020； Stokes 等人，2020]。此外，通过单一基础模型同时解决相关药物发现问题（即靶点识别、功效预测、副作用预测等）可能会改进每个问题的解决方案[Ramsundar et al.2015；卡马乔等人，2018； Duran-Frigola 等人，2020； Huang 等人.2021a]。例如，基础模型显示出影响治疗设计的巨大潜力的一个领域是使用语言模型对蛋白质进行建模。成功的应用范围从预测可以逃避疫苗诱导的免疫反应的病毒突变到预测蛋白质对接潜力以更好地设计治疗性抗体[Bepler and Berger 2021;希等人。 2021 年；查班等人。 2021 年；吴等人。 2021b；里夫斯等人。 2021]。个性化医疗。个性化医疗旨在根据患者的健康史、遗传学、影像学和其他个人测量结果为个体患者选择最佳治疗方法 [CollinsOn the Opportunities and Risks of Foundation Models 57 and Varmus 2015; CollinsOn the Opportunities and Risks of Foundation Models 57 and Varmus 2015;阿什利 2016]。例如，给定一组药物和患者基因组，基础模型可能有助于预测哪种药物最有可能以最小的副作用治疗患者[Whirl-Carrillo et al.2012；塔托内蒂等人，2012； Gerstung 等人，2017 年；格林菲尔德等人，2018；亚当等人。 2020]。基础模型具有独特的强大功能，能够集成从 EHR [Rajkomar et al.2018] 到医学成像 [Bera et al.2019； Ouyang et al.2020] 药物和分子测量[Gottlieb et al.2011； Ruiz et al.2020] 做出最佳预测。临床试验。临床试验研究治疗或候选药物的功效和安全性。传统的临床试验效率低下且成本高昂：80% 的试验由于无法显示疗效/安全性或患者匹配问题而失败[Ali et al.2020; Liu 等人.2021c]。基础模型可以在以下方面提供帮助：根据现有研究预测潜在的失败并设计有前景的临床试验方案（例如患者资格标准）；根据患者个人资料自动匹配符合条件的患者，这些资料是多模式数据，包括 EHR、基因序列等。 2019]。 

### 3.1.3 基础模型的挑战和未来研究

虽然基础模型有潜在的机会提供帮助，但医疗保健/生物医学应用也带来了独特的挑战，推动了基础模型的进一步研究。多模态。医学数据是高度多模态的，具有各种数据类型（文本、图像、视频、数据库、分子）、尺度（分子、基因、细胞、组织、患者、群体）[Kong et al.2011； Ruiz et al.2020]，以及风格（专业和外行语言）[Lavertu 和 Altman 2019；李等人。 2019]。当前的自我监督模型是针对每种模态开发的（例如，文本[Lee et al.2020b]、图像[Chaitanya et al.2020]、基因[Ji et al.2021]、蛋白质[Jumper et al.2020]），并且不从不同的模式中共同学习。为了从这些不同的多模态医学数据中学习跨模态和跨模态信息，我们需要研究基础模型训练中的特征级和语义级融合策略。如果做得有效，这有可能统一生物医学知识并促进发现，如第 3.1.2 节：生物医学任务中所述。可解释性。可解释性——为决策提供证据和逻辑步骤——在医疗保健和生物医学中至关重要[Holzinger et al.2019]，并且根据《通用数据保护条例》(GDPR) 具有强制性。例如，在诊断和临床试验中，必须将患者症状和时间相关性解释为证据。这有助于解决系统和人类专家之间潜在的分歧。医疗保健领域的知情同意也需要可解释性[Amann et al.2020]。然而，当前基础模型的训练目标不包括可解释性，需要在这个方向上进行未来的研究[Linardatos et al . 2021]。知识图的结合可能是进一步提高模型可解释性的一个步骤[Roberts et al.2020;徐等人.2020;金等人.2021]。读者可参阅第 4.11 节：可解释性，以获取有关可解释性的更多讨论。法律和道德规范。医疗保健应用程序必须遵守法律和道德法规并提供保证，例如患者安全、隐私和公平。例如，在安全性方面，基础模型做出的预测必须根据既定的医学知识准确无误，并且必须量化不确定性或在不确定时选择听从专家的意见[Challen et al.2019;莫扎纳尔和桑塔格 2020]。出于隐私考虑，患者健康记录的使用必须遵守隐私法，例如美国的 HIPAA [1996 年法案]。联邦学习是在基础模型训练中保持原始敏感数据私密性的一种潜在解决方案[Chamikara et al.2021]。为了公平起见，研究人员需要注意常见的陷阱，否则可能会加剧现有的社会不平等[Chen et al.2019；维恩斯等人，2019； Chen 等人.2020b]。他们必须58基础模型研究中心（CRFM）确保基础模型的训练和评估数据充分代表不同性别、种族、民族和社会经济背景；医学数据集和临床试验长期以来存在偏见的领域[Martinez-Martin et al.2020； Kaushal 等人，2020]。还需要进行研究来消除偏差和规范模型，以确保代表性数据稀缺时的公平性[Zhao et al.2020a]。基础模型开发人员还需要咨询道德和法律研究人员，并遵守其部署所在的特定环境（例如国家、地区）的法规。我们还建议读者参阅第 4.7 节：安全性、第 4.8 节：稳健性、第 5.1 节：公平性、第 5.4 节：合法性，以了解有关隐私、稳健性、公平性和合法性的详细信息。外推法。生物医学发现的过程涉及外推。例如，基础模型必须能够快速适应新的实验技术（例如，新的分析方法、新的成像技术，如高分辨率显微镜）或新的设置（例如，新的目标疾病，如 COVID-19）[Jaroch 等2018 年； Benam 等人，2019]。利用现有数据集并推断新设置的能力是生物医学领域机器学习的一个关键挑战[Snell 等人。 2017年； Ma 等人.2021b]。虽然 GPT-3 表现出一些外推行为（例如，生成以前未见过的新文本），但其机制尚不清楚并且仍处于起步阶段。需要进一步研究来提高基础模型的外推能力，特别是在考虑医疗保健和生物医学固有的各种数据模式和任务时，但在当前的 GPT-3 和相关模型中并未普遍研究。另请参阅§4.8：稳健性。关于基础模型的机会和风险 59 

## 3.2 法律

作者：Peter Henderson、Lucia Cheng、Jenny Hong、Neel Guha、Mark Krass、Julian Nyarko、Daniel E. Ho 

图 13. 民事案件中各个步骤的示例美国以及基础模型可能有所帮助的地方。在每个阶段，可能需要处理不同的模式，并需要适应新的法院或法律视角。从家庭法庭到刑事司法，从环境政策到公司交易，法律的影响范围非常广泛。在美国，31有超过 130 万律师 [美国律师协会 2021]，法律服务的年收入超过 $300B [MarketLine 2021]。然而，“诉诸司法”对大多数人来说仍然遥不可及。法律服务可能非常昂贵。例如，在美国，大约 86% 遇到民事法律问题的低收入个人表示没有获得足够的法律帮助或没有获得足够的法律帮助 [Legal Services Corporation 2017]。即使指定了律师，律师也可能会因案件数量不断增加而感到紧张。例如，研究表明公设辩护人经常工作过度且资金不足 [Lefstein 和 Spagenberg 2009；舒姆2012；美国律师协会，2004 年]。美国司法部报告称，2007 年，73% 的县公设辩护办公室超过了每位律师受理案件的最高建议限额，19 个州公设辩护计划中的 15 个州超过了每位律师受理的重罪或轻罪案件的最高建议限额[法罗尔和兰斯顿 2010；兰斯顿和法罗尔 2010]。即使在一个人均律师比例最高的国家之一，正义也似乎遥不可及。美国总统吉米·卡特曾说过：“我们百分之九十的律师为百分之十的人民服务。我们的律师过多，代表性不足”[Carter 1978]。根据诉诸司法的主要声音，技术可能提供一条前进的道路 [Rhode 2014]，许多其他人也赞同这一观点 [Cabral 等人，2014]。 2012]。基础模型在法律中可以发挥什么作用？ 32一个主要承诺是，基础模型可以通过消除法律服务的程序和财务障碍来改善诉诸司法和政府服务的机会。法律应用带来的挑战反过来又可以激发基础模型的基础研究问题。许多法律应用对计算解决方案提出了独特的挑战。法律语言是专业化的，法律结果往往依赖于对各种和以前未见过的事实模式应用模糊和不明确的标准。同时，由于成本高昂，带标签的训练数据稀缺。根据具体任务，这些特质31由于作者的专业知识，我们将讨论限制在美国的法律应用。然而，这里的一些讨论可能适用于全球的法律场所。 32我们注意到，就本节而言，我们认为基础模型是任何自我监督的预训练模型，用于在几乎没有监督学习的情况下快速适应新环境。另请参阅§1：引言和§2.6：扩展定义的哲学中的讨论。60 基础模型研究中心 (CRFM) 可能会对传统模型的成功部署造成难以克服的障碍。相比之下，它们的灵活性和从少数例子中学习的能力表明，基础模型可以具有独特的定位来解决上述挑战。在本节中，基础模型可以将多种模式作为证据：审判程序期间的音频、发现期间的视频和图像以及进行法律研究时的文本。然而，大多数依赖基础模型有益的法律任务都涉及基于文本的输入和输出。因此，我们主要关注基于文本的领域，而仅简要讨论其他领域。为了奠定讨论的基础，图 13 描述了美国民事诉讼的各个阶段以及基础模型在此过程中可能发挥作用的地方。图 14 显示了仅生成法律摘要的一个段落的一部分所需的逻辑流程，这可以作为基础模型有一天可能用于的任务的具体示例。一个重要的考虑因素。在继续之前，我们注意到，在应用法律或政府环境中使用基础模型之前，在第 5.6 节：道德、第 5.4 节：合法性和第 5.1 节：公平中扩展的道德、法律和公平性考虑因素尤其重要，因为这些应用程序通常会对受影响的人产生重要的现实后果 [Surden 2020]。基础模型在部署之前还必须进行彻底审查，如第 4.4 节：评估中所述。例如，法律体系特别强调——甚至可能强制——透明度、问责制和可解释性。因此，当前的模型是否能够解决许多最紧迫的法律问题值得怀疑。尽管如此，扩大和改善获得法律和政府服务的机会的需要为基础模型提供了一个有价值的目标。 

### 3.2.1 法律机会

法律应用的范围包括在政府环境中使用机器学习 [Engstrom et al.2020;科利亚尼斯和本多尔 2020； Re 和 Solow-Niederman 2019] 协助律师提供法律服务 [Zheng et al.2021； Huang 等人.2021b；奥斯坦多夫等人，2021；沃尔德和康拉德 2021]。我们注意到，之前的工作还调查了基于文本领域的机器学习辅助法律任务[Zhong et al.2020； Chalkidis et al.2020]，尽管人们注意到最近的法律人工智能研究主要集中在美国以外的地理区域[Zheng et al.2021]。虽然我们在这里讨论的许多主题可能适用于不同的法律体系，但由于我们团队的专业知识，我们主要关注美国，特别是，我们专注于可能受益于美国基础模型的三大类法律应用法律体系：私法或民事司法（私人之间因合同、财产或侵权行为等而提出的索赔）、刑法（即对个人的犯罪行为进行起诉）和（非刑事）公法（例如，政府机构对私人行为的监管）。民法。在美国民事诉讼中，当事人通常必须寻找并付费聘请代理律师。结果，许多人，特别是低收入者，很难获得足够的法律代理[Rhode 2004]。基金会模式有潜力通过降低成本、提高质量和扩大法律服务的范围来改善诉诸司法的机会。在图 13 中，我们描述了在美国法院提起民事诉讼的流程，以及基础模型可以在帮助律师和法官方面发挥作用的过程。即使在律师参与法律程序之前，客户也可以从基础模型的部署中受益。最近的工作使用机器学习模型来识别客户提供的事实的简单语言描述中包含的相关法律问题。33此类工具可以帮助为解决当前问题或解决问题所需的法律行动类型提供建议。推荐专业律师。许多其他类似的努力也试图通过提供适合客户特定需求的信息来增加 33https://spot.suffolklitlab.org/On the Opportunities and Risks of Foundation Models 61 诉诸司法的机会[Cabral et al.2012;布雷西亚等人。 2014年；奎多等人。 2020；韦斯特曼等人。 2019]。一旦客户在民事诉讼之前与律师交谈，律师可能会寻求避免昂贵的审判。在这个阶段，他们可以依靠基础模型来评估合同、审查服务条款、查找相关专利以及进行其他诉讼前流程，以确保其客户处于优势[Betts and Jaep 2017;埃尔瓦尼等人，2019；里皮等人，2019；李和祥2019； Hendrycks 等人.2021c；黑格尔等人.2021]。值得注意的是，最近的工作描述了使用基础模型进行合同审查的挑战和好处[Leivaditi et al.2020;黑格尔等人，2021； Hendrycks 等人.2021c]。除了审查和起草法律文件之外，还可以翻译客户互动和文件，以减少提供法律服务的成本和障碍 [Cuéllar 2019]。但法律文件的翻译需要精确性和对高技术语言的理解，这使得收集训练数据的成本很高。此外，翻译客户陈述或审判程序通常需要了解当地方言和语言。这也使得收集足够的地面实况翻译数据进行训练变得困难。因此，传统的监督方法很少达到法律领域所需的准确性水平[Vieira et al.2020]。通过在这些资源匮乏的环境中快速适应，基础模型可以通过完全监督的机制来提高该领域的性能。在诉讼过程中，基础模型可以帮助律师进行法律研究、起草法律语言或评估法官如何评估其主张[Zheng et al.2021； Huang 等人.2021b；奥斯坦多夫等人，2021；沃尔德和康拉德 2021； Chalkidis 等人，2020，2019]。这可能会降低法律服务的成本并改善法律服务。例如，最近的工作利用预训练模型在撰写法律文本时推荐相关引文和保留声明[Zheng et al . 2021 年； Huang 等人.2021b； Ostendorff 等人.2021]。其他工作使用预训练模型来改进法律问答，为常用的法律搜索引擎提供支持，并帮助律师进行法律研究 [Vold 和 Conrad 2021]。各种各样的工作还检验了自动化合同起草和审查，这项任务同样可以从基础模型中受益[Hendrycks et al.2021c；贝茨和杰普 2017]。也许最引人注目的基础模型可以帮助律师生成法律摘要（书面论点）。这些模型可能会发现新颖的论点或识别律师撰写的案情摘要部分中的问题。例如，Tippett 等人[2021]根据从提交的案情摘要中提取的特征来预测法律诉讼的结果。可以利用基础模型来使用原始语言作为输入而不是提取的特征。这可能会为律师提供更丰富的建议，告诉他们如何改进他们的陈述以确保取得有利的结果。提交开场陈述和答复摘要后，各方开始发现过程，该过程在十年的大部分时间里已经使用了简单的机器学习模型 [Grossman 和 Cormack 2010]。律师使用这些系统来标记是否应向对方当事人出示文件。这些文档本质上是多模式的，通常包含视频、图像、音频和文本。当前的系统成本高昂，因为它们使用监督学习和主动学习来将文档标记为响应式 [Grossman and Cormack 2010; Oard 等人，2018；杨等人。 2021]。相反，基础模型可能实现的少样本或零样本文档检索功能将有助于缓解人们对当前过程的巨大成本的担忧。 34为了避免发现过程中出现耍花招的可能性，Cui [2018] 提出了一种零样本（或少样本）适应过程只能通过使用基础模型来操作。发现后，一旦审判开始，基础模型可以通过预测法官在审问期间可能关注的内容来帮助当事人为审判做好准备 [Dickinson 2018]，适应 34https://www.kirkland.com/publications/article/2020/04 /technology-auxiliary-review-framework62 基础模型研究中心 (CRFM) 根据法官之前发表的意见提供当前背景。在法庭上，基础模型可用于检查法庭诉讼的音频和视频，以确定结果是否因被告的种族或方言而存在偏见。 35 审判结束后，基础模型可以帮助法官和书记员正确评估双方使用类似技术的法律主张，或使用基础模型的上下文嵌入可能有助于法律解释 [Nyarko 和 Sanga 2020；崔2020]。最近的工作（不依赖基础模型或 NLP）研究了是否可以根据一组提取的特征（例如引用计数和关键词的出现）来预测上诉决定[Katz et al.2017； Boniol 等人，2020]。这些模型有可能使用基础模型进行改进，并通过标记法官意见中的明显错误来帮助法官起草决定，正如在裁决机构的背景下讨论的那样[Engstrom et al.2020;雷和鲁伯斯 2014]。它们还可以用来识别法律意见中的种族偏见，并帮助法官相应地修改他们的意见[Rice et al.，2017]。 2019]。刑法。一个特别有争议的领域是在政府环境中使用风险评分，特别是在刑法中。有些人可能希望使用基于语言的基础模型来帮助根据给定的基于文本的事件叙述做出指控决定或假释决定。由于可能存在偏差，在使用基础模型进行风险评分之前必须仔细考虑，特别是在包含语言数据时 [Bender et al.2021; Berk 等人，2021；劳弗 2020]。但基础模型可能在刑事司法的许多其他方面发挥作用。检察官和辩护律师也可以使用与上述民事诉讼相同的工具。这可以帮助指定律师更有效地完成工作并减少不必要的开销。因此，他们或许能够更有效地平衡本已繁重的案件量。例如，公设辩护人通常被视为工作过度且资金不足，这将导致可避免的程序错误。36基础模型可以通过识别错误和自动化简单任务来帮助减少其中一些资源限制。然而，它们本身并不是解决方案。在其他领域，基础模型可以充当减少结构性不平等的监督机制。预训练模型已用于处理假释听证会笔录，以查找异常结果的实例 [Bell et al.2021]。最近的工作还消除了警方报告中嫌疑人种族的语言线索，以促进不考虑种族的指控决定并避免带有种族偏见的起诉[Chohlas-Wood et al.2020]。其他工作有助于识别不尊重的警察沟通[Voigt et al.2017]。在这些情况下，标记数据的成本非常高，因为注释者必须有权访问敏感数据，并且通常需要适当的背景检查。为了降低这些成本，基础模型可用于预训练并快速适应标签稀缺的下游任务。公共法。政府机构对社会的大部分进行监管，基础模型在公法中具有广泛的潜在适用性。这包括：在通知和评论过程中分析公众评论、协助专利审查、根据《信息自由法》的要求检索相关文件、协助大规模裁决等。最近的工作在各种背景下调查了这些政府应用程序，我们建议读者参考相关来源进行深入讨论[Engstrom et al.2020; Coglianese 和 Ben Dor 2020]。在许多此类应用中，基础模型可以提高质量、效率和效用，并且 35 例如，在法庭上说非裔美国人方言英语已被证明是审判期间潜在的偏见来源。 https://www.nytimes.com/2019/01/25/us/black-dialect-courtrooms.html 36 例如，参见 People v. Superior Court (Vasquez) , 27 Cal.App.5th 36 (2018)由于公设辩护人办公室预算严重削减且人手不足，一名被告长达 17 年没有接受审判。法院裁定，公设辩护人办公室的系统崩溃构成了正当程序违规，被告的案件被驳回。 论基金会模式的机遇与风险 63 政府服务的可及性：标签稀缺、资源受限、情境不断转移。因此，通常需要基础模型的适应性和灵活性来提高效率和性能。仅举一个此类应用程序的说明性示例，现有工作已利用 NLP 在公共评论论坛中进行促进性调节。在此用例中，预测模型可帮助非专业用户改进论点并识别评论中的错误陈述。这样的系统已经部署在美国交通部的规则制定过程中[Park et al.2012]，尽管它可以通过基础模型的语言推理能力来改进。但政府机构必须遵守宪法、法定和行政义务（参见第 5.4 节：合法性），因此在这些情况下需要额外注意。 

### 3.2.2 基础模型如何提供独特的帮助？

上述法律应用的例子在几个方面都是独特的。首先，标注数据的成本非常高。通常，创建高质量标签的专业知识只能由律师提供，而律师每小时的收费可能高达数百美元。即使获得标签后，某些数据也可能是敏感的，无法汇集在一起来训练大型语言模型。鉴于最近在小样本学习方面取得的进展 [Brown et al.2020]，基础模型是具有有限注释的学习模型最有前途的路径之一。其次，法律决策需要不同层面的背景：了解所有历史决策和标准、了解当前仍然相关的判例法以及了解当前个案的细微差别。基础模型具有独特的潜力，能够学习历史和法律背景的共享表征，并具有对个案进行建模的语言能力和精确度。 

### 3.2.3 基础模型还缺少哪些需要更多研究的内容？

为了说明当前基础模型需要克服才能实际部署的缺陷，我们以自动创建提交给法院的法律摘要为例。听证会前，案情摘要向法官陈述了论点。一旦一方提交了开场简报，对方就会做出回应。然后，法官在做出决定之前在听证会上评估案情摘要并询问双方问题。图 14 直观地展示了此类法律摘要的结构及其一些特征。自动摘要生成机制可能会将案件的相关文件和事实（由律师指定）以及所需结果的粗略草图作为上下文。然后，它会生成一份包含复杂法律论据的法律摘要，提交给法院。长文档和叙述。为了实现这一目标，模型必须能够读取长上下文并生成长叙述。法律文件往往比任何其他情况下的文件长得多。美国最高法院的意见平均包含约 4,700 个单词，37 提交给最高法院的案情摘要可多达 15,000 个单词，38 法律评论文章通常包含 20,000 至 30,000 个单词，39 假释笔录可长达数百页 [Bell et al. al .2021]，审判记录甚至可能更长。当前的基础模型一直在努力应对如此长的上下文和输出（请参阅§4.1：建模以获取更多讨论）。检索、概念漂移、论证形成和逻辑推理。除了阅读特定案例的文件外，基础模型还必须检索相关判例法并了解其中的内容 37https://www.americanbar.org/groups/public_education/publications/teaching-legal-docs/how-to-read-aus – Supreme-court-opinion/ 38https://www.supremecourt.gov/casehand/courtspecchart02162010.aspx 39https://www.stanfordlawreview.org/submissions/article-submissions/64 基础模型研究中心 (CRFM) 14. 摘自本文作者之一的虚构简介。法学院学生被要求撰写摘要的典型形式包括：（1）介绍论点； （二）以具有说服力的方式陈述法律规则； （三）对案件事实适用法律规定； （4）有说服力地总结论点。这通常涉及对先前案例和当前案例事实的信息检索和解释。判例法仍然有效，但考虑到自训练以来潜在的概念漂移，判例法已被否决。随着判例法的发展，将需要在基础模型中编辑基础信息方面开展更多工作[De Cao et al.2021]。使用检索到的法律标准，基础模型必须了解如何将它们编织成有说服力的论点。新兴研究研究了使用基础模型来测量、检测和生成有说服力的文本的方法 [Duerr 和 Gloor 2021； Li等人.2020a； Longpre et al.2019]，这可能为实现这一目标提供垫脚石。但法律简报还需要能够识别相关法律规则并确定其如何适用于新情况。例如，Holzenberger 等人[2020]提供了一个数据集和方法，用于在基于税法的当前模型中评估这种能力。给定一组税法规则和对某人情况的描述，模型必须确定这些规则如何应用于个人，以便确定他们的纳税义务。40Holzenberger 等人。[2020]发现领域专家可以表达相关的信息税法作为序言求解器，可以完美准确地确定个人的纳税义务。然而，他们发现，仅仅使用 BERT 模型，即使是在域内法律文本上进行预训练，也不足以让模型推断并应用相关规则，这一过程也称为“法律推理”。 40 例如，规则可能如下所示：“对于未婚且不是未亡配偶的个人，应通过用‘750 美元’代替‘600 美元’来适用第 (1) 和 (2) 款。”美国国内税收法 (IRC) §63(f )(3)。关于基础模型的机遇和风险 65 这些挑战可以通过与 GPT-3 的真实交流来进一步说明，表明当前模型甚至无法执行涉及法律推理的相对简单的任务。使用 GPT-3 进行法律推理。 “违约金”是当事人在协议中规定的违约金的一种形式。一般而言，如果合同中规定的违约金金额过高，以致其充当一种惩罚形式，而不仅仅是补偿性的，美国法院将不会执行违约金条款。首先，我们看看该模型是否编码了有关执行违约金条款的正确规则。问：违约金条款可以强制执行吗？ a：违约金条款通常是可执行的，除非规定的金额过高或不合理。该规则看起来是正确的。但随后我们将该规则应用于一组事实。在这里，模型需要推断出 100 万美元的赔偿对于丰田卡罗拉来说过高，因此不应执行违约金条款。 q：X 和 Y 就丰田卡罗拉的销售签订了合同。他们的合同规定，如果 X 未能按时交付汽车，X 必须向 Y 支付 100 万美元的赔偿金。 X 没有按时交付汽车。 Y是否有权获得损害赔偿？如果有，数量是多少？答：是的，Y 有权获得 1,000,000 美元的损害赔偿。损害赔偿旨在补偿受害方因违约行为而遭受的损失。由于 X 没有按时交付汽车，Y 损失了 1,000,000 美元。该模型无法推断丰田卡罗拉 100 万美元的价格过高。即使模型提供了抽象规则及其对提示中上下文的精确翻译（因此应用程序应该是微不足道的），GPT-3 仍然无法进行所需的逻辑推理。 q：X 和 Y 就丰田卡罗拉的销售签订了合同。他们的合同规定，如果X未能按时交付汽车，X必须向Y支付一百万美元的巨额赔偿金。 X 没有按时交付汽车。法律要求执行违约金条款，除非规定的金额过高或不合理。 Y是否有权获得损害赔偿？如果有，数量是多少？ a：是的，Y有权获得违约金。本案的金额并不高，因为价格是双方自行确定的。精确。基础模型也不能在此过程中创建虚假事实，这是当前模型中出现的问题[Gretz et al.2020; Zellers 等人.2019b]。在法律背景下，具体性和真实性尤为重要，不精确的陈述可能会产生严重的、意想不到的后果，而虚假陈述可能会导致对律师的制裁。少镜头学习。除了上述示例中描述的挑战之外，基础模型还面临一些挑战，包括小样本学习，它仍处于起步阶段 [Perez et al.2021]。对小样本学习技术的研究和增加法律语料库的获取可以同时进行。由于基础模型需要精确，不仅在事实真相上如此，如上所述，而且在技术法律语言上也如此，因此目前还不清楚从一个语料库获得的信息在多大程度上可以在不同领域的另一个语料库中使用。因此，小样本学习仍然很重要。66 基础模型研究中心 (CRFM) 适应。在未标记的法律语料库上进行领域自适应预训练已经观察到了一些成果。当预训练语料库与下游任务高度相关且标记训练数据有限（法律中常见的设置）时，这些收益似乎最为明显[Zheng et al.2021]。尚未全面研究这是否扩展到一系列不同的法律任务，但利用未标记的特定领域语料库对基础模型进行自我监督训练可能会为小样本方法提供补充改进。访问干净的域内数据。最近的一些努力试图通过自动化[Zheng et al.2021]或志愿者法律专家的手动注释[Hendrycks et al.2021c]来为更具挑战性的法律基准任务创建大型标记数据集。这些努力表明，与在其他设置中观察到的更有限的收益相比，在更多数据上进行预训练的更大语言模型可以在某些具有挑战性的任务中实现性能收益[Chalkidis et al.2020;埃尔瓦尼等人，2019；钟等人.2020]。这项工作表明，可能需要更大的法律基准数据集来观察将迁移学习技术应用于基础模型的进一步收益。然而，从 NLP 角度来看，为具有法律意义且困难的任务创建基准数据集本身就具有挑战性，因为人类专家注释可能成本高昂，而且利用传统标记化和句子分割技术的自动化方法可能无法考虑法律文本的独特方面，例如法律引文的结构 [Bommarito et al.2018; Savelka 等人.2017]。由于这些挑战，许多现有的法律领域特定标记数据集都很小，无法公开获得，或者反映了通常通过早于基础模型开发的方法解决的更简单的任务。 41许多可用的法律数据也可能是不具代表性。由于只有一小部分案件最终形成了法律意见，因此尚不清楚公开数据中的争议是否能够代表实践中模型中出现的典型争议[Priest and Klein 1984]。更具代表性场景的昂贵训练数据可能集中在最大的律师事务所。这些律师事务所可能有能力保留和积累许多案件和客户的数据。那么一个问题是，基础模型可能会将权力更多地集中在少数有资源在域内数据上训练模型的参与者身上——除非模型能够足够好地泛化。可靠性。最后，我们再次指出，即使基础模型可以成功地执行法律领域的所有任务，部署仍然是一个重大挑战：法律中基础模型的失败将对客户和律师产生真正的、破坏性的后果（另见讨论）关于公平、合法性和道德（参见第 5.1 节：公平、第 5.4 节：合法性和第 5.6 节：道德）。因此，机器翻译软件已被认为在一些法院用作证据不可靠，42尽管它继续在其他法院得到依赖。43鉴于所有这些复杂性，法律简报和推理可能超出了当前模型的能力，但似乎在未来的可能性范围内。因此，它们可以作为基础模型持续开发的潜在主星。 41对于律师事务所和法律技术公司来说，已经可以实现高性能并因此可以更立即产品化的任务可能被认为更值得投入昂贵的手动标记工作。 42 参见 Vieira 等人的讨论。 [2020]。 43 例如，在 Vasquez 诉美国案，第 3 号：16-cv-2623-D-BN（德克萨斯州北达科他州地方法院，2019 年）中，律师依靠谷歌翻译来证明前任（母语人士）律师翻译错误关于基础模型的机会和风险 67 

## 3.3 教育 

作者：Ali Malik、Dorottya Demszky、Pang Wei Koh、Moussa Doumbouya、Drew A. Hudson、Allen Nie、Hamed Nilforoshan、Alex Tamkin、Emma Brunskill、Noah Goodman、 Chris Piech 

图 15。教育中的基础模型可以在多个数据源上进行训练，以学习教育所需的能力：了解各种主题和不同的教学技术。这些基础模型可以以通用的方式应用于一系列任务和目标，例如理解学生、协助教师和生成教育内容。 2000年，世界领导人最大规模的聚会在联合国千年峰会上召开，思考未来的理想愿景。代表们得出的结论是，首要重点应该是教育，并宣称教育是“人类实现、和平、可持续发展、经济增长、体面工作、性别平等和负责任的全球公民的基础。”这场讨论最终被重新编入联合国可持续发展目标，即“确保全民包容和优质教育，促进终身学习”[联合国大会 2015]。然而，大规模提供高质量、全纳教育带来了严峻的社会和经济挑战。每个学生的教育价格增长速度快于整体经济成本 [Bowen 2012]，限制了可用于支持学生学习的资源。在美国，一个症状是学生持有的私立教育债务已达到 1.6 万亿美元，超过信用卡债务总额 [Friedman 2020]。考虑到提供成人再训练的需求不断增长，教育需求与我们提供教育的能力之间的差距惊人地大，并且受保护人口的成就差距令人担忧。随着数字时代的到来和数字学习的快速发展，计算教育方法在提高学习者和教师的效率方面显示出了希望。几个核心方向已经出现，成为人工智能在教育领域潜在影响力的应用[Woolf et al.2013]，例如可以为学生提供有意义的反馈的系统[Malik et al.2021]，帮助教师改进[Jensen et al.2020； Demszky 等人，2021； Suresh 等人 .68 基础模型研究中心 (CRFM) 2021]，甚至创建个性化和适应性学习体验，根据个别学生的需求和性格定制学习过程 [Connor 2019]。尽管存在这种潜力，但事实证明，构建技术解决方案来有效扩展包容性和教育质量却异常困难。一个特殊的挑战是，现有的工作重点是针对高度特定任务的定制解决方案，为此必须从头开始收集大量训练数据。由于创建大型数据集的难度和成本，使用这种方法独立解决每项教育任务从根本上受到限制。相反，是否有可能创建可在各种任务和主题中重复使用的通用方法？基础模型已经开始提高教育领域一些特定旗舰任务的性能。最近的例子包括使用 MathBERT [Shen 等人。 2021b] 推动“知识追踪”（根据学生过去的反应跟踪学生随时间的理解的挑战）和“反馈挑战”，其中算法必须解释学生对结构化开放式任务的答案，例如编码问题 [Wu et al.2021e]。基础模型能否在该领域带来更具变革性的变化？基础模型应用于教育的已知和想象的风险是什么？在本节中，我们首先围绕道德考虑构建对话。然后，我们将讨论围绕两个具体任务：（1）理解学生的误解，（2）通过指导提高学生的理解。 3.3.1 将基础模型置于教育研究的重要关注点。人工智能教育的未来是令人兴奋的，特别是在基础模型的背景下。然而，我们提醒读者要特别考虑人工智能研究应用于教育的影响。44教育目标与复杂、长期的社会影响深深交织在一起。在我们积极努力改善数字教育的同时，我们必须认真思考，尝试并想象该领域任何颠覆的复杂性 [Piech 和 Einstein 2020]。道德挑战包括数据偏见、法律约束和数字社交化的影响等问题。这些问题并非基础模型所独有，但随着研究在人工智能教育领域取得实质性进展，这些问题值得定期反思。当研究开始时询问“新的人工智能技术能提供什么？”时，反思影响就显得尤为重要。 “§5.6 中的许多问题：道德适用于教育。例如，与许多其他领域一样，基础模型训练数据中的小偏差可能很难追踪[Dixon et al.2018；Bolukbasi et al.2016]，但对教育机会的公平性具有重要影响。此外，这些系统可能会经历高度的“反馈”，收集的数据不断强化模型的决策。这种偏见问题超出了收集数据的范围，还包括对研究人员选择研究的应用程序的担忧。下面，我们讨论其他特定于教育的问题。许多问题都围绕着这样一个问题：“谁受益？ ” 新技术是为谁而创造的？将教师从循环中移除 数字教育（尤其是基于人工智能的数字教育）的目标之一是提高学习体验的生产力，以便在单位时间或单位成本上进行更多学习。想象一下，决策者可以利用这种生产力的提高来将人类教师从循环中移除。此类决策的长期影响很难先验地知道。与优化以最大化“学习”的教育系统互动是否会对社会情感技能发展产生不利影响? 它会不会为 44 创造更少的机会？2013 年，Facebook 启动了 Free Basics，这是一个向世界提供免费互联网的项目，从而传播机会和互联。现在，联合国人权理事会报告说，在缅甸，Facebook 努力贯彻落实在没有适当人类节制的情况下，这种愿望加速了仇恨言论，煽动分裂，并煽动罗辛亚种族灭绝中的线下暴力。 Free Basics 现在是对技术对社会影响的复杂性的警告。关于基础模型的机遇和风险 69 图 16。该图说明了一个嵌入来自各种模式（图像、语音、符号、文本）和语言进入通用特征空间。这样的特征空间允许思想跨模式和语言联系起来。教学相关的链接类型包括类比（跨语言的相似性）和对比（跨语言的不同概念），两者都可以以相同的方式或不同的方式出现。与他人互动？年轻一代的孤独感正在上升 [Cigna 2018]，而教师是人工智能研究人员可能没有预见到的压力的调节力量。这项工作是由学习者还是基础模型完成的？另一个挑战是如何有效地教授能够使用基于基础模型的工具的学生。例如，如果学生与强大的生成模型一起工作，教师要了解学生的贡献程度，或者规范无效的合作并检测抄袭，将会变得更加复杂。 Visual Studio 最近发布了 GitHub CoPilot，这是一个基于 GPT-3 构建的 AI 结对编程器 [Chen et al.2021f]。这将如何改变计算机科学教育？初学者程序员面临的许多挑战对于 CoPilot 或其技术继承者来说可能是微不足道的，这可能会破坏新手的学习体验。研究扰乱某些学科教育的技术进步的其他例子是有启发性的，例如数学课堂上的计算器和语言课程中的谷歌翻译，这两者现在都与传统教学共存。隐私和安全。关于学生作业隐私的严格法律准则凸显了在教育中使用人工智能的一个重要道德问题。例如，在美国，学生信息受《家庭教育权利和隐私法》(FERPA) 的保护。这些法律法规对于 13 岁以下的儿童尤其重要，他们的数据隐私和安全还受到《儿童在线隐私保护法》的保护。除此之外，FERPA 还限制教师分享可识别个人身份的学生作业。这可能会直接影响共享用于训练和评估基础模型的数据的举措。此外，还有一个悬而未决的问题，即基础模型的权重是否会以某种方式泄漏其训练数据（可能是私有的）[Nasr et al.2018;宋等人.2017]。这些问题及其相应的方法与第 3.1 节：医疗保健中描述的挑战类似。该列表并不详尽，不同项目的道德考虑因素也有所不同。70 基础模型研究中心 (CRFM) 3.3.2 学生思想的基础模型。在构建用于包容性、快乐教育的人工智能工具时，基础模型可以在许多任务中发挥作用。其中许多任务要求我们首先了解我们试图帮助的学习者，特别是在开放式工作的背景下。基础模型需要什么才能推理出学生的理解？很容易想象一个适合正确回答数学问题的基础模型，但不太清楚如何构建一个可以根据学生的答案诊断学生理解错误的模型。为了探索这个主题，我们考虑了向正在完成开放式任务（例如写短段落、绘制物理图或编写代码）的学生提供反馈的案例研究。这种“反馈挑战”例证了基础模型如何为学习者提供现成的帮助，并且还展示了基础模型研究的开放领域。为了有效地向学生提供反馈，需要两项核心能力：（1）理解任务的主题（例如物理或编码），以及（2）“注意”的诊断能力：教育中的一个技术术语用于推断学生犯错的原因。对于典型课堂上的典型学生互动，没有足够的数据让人工智能模型从头开始学习这两种核心功能。即使对于拥有数百万学生的大规模课程，监督算法几乎无法理解即使是简短的四行程序背后的复杂学生推理[Malik et al.2021]。因此，反馈任务本质上需要从外部数据和经验中转移理解。目前存在的基础模型，对第一个能力有直接帮助：理解特定的主题。例如，当学习为简短的编程问题提供反馈时，GPT-3 等基础模型可以通过几个示例有效地理解流畅的代码是什么样子。这个方向的一些研究已经开始探索可以快速适应新主题领域问题的基础模型[Wu et al . 2021 年； Condor 等人.2021]。同样，基础模型还可以集成多种信息模式，例如任务提示的文本、问题中的图表，甚至是提供给助教的评分标准的内容。这种统一的表示能力可以帮助基础模型通过更丰富的信息源理解主题。作为一个具体的案例研究，其中许多见解被用作算法的核心组件，该算法能够对斯坦福大学的计算机科学入门期中考试进行评分，其有效性与人类助教相同[Wu et al.2021e]。在这种情况下，主题编码建立在一个基础模型上，该模型已在 GitHub 代码上进行了调整，并为每个问题的主题提供了相应的小数据集。一般来说，我们可以想象利用各种数据源来使基础模型适应不同的主题。例如，数学适应可以使用数学网站或教科书[Shen et al.2021b]或Gradescope等平台上的历史学生答案；口语理解可以利用广播档案或播客；像创意写作这样的领域可以向古腾堡计划这样的大型数字档案馆寻求帮助。与主题相比，采用基础模型来完成将观察到的错误映射到学生思维过程中的缺陷的任务还没有得到很好的探索。教师“注意到”学生犯特定错误背后原因的能力是反馈挑战的关键组成部分。例如，想象一下，一名学习两位数加法的学生回答了“26 + 19 是多少？”的问题。 ”，响应为“315”。花点时间尝试猜测他们为什么给出这个答案以及他们有哪些误解。45。这种注意能力可以作为基础模型的适应任务（§4.3：适应），甚至可以作为推理任务（§2.4：推理）。 45 这位学生犯了一个常见错误，将个位数字和十位数字相加的结果串联起来。关于基础模型的机遇和风险 71 虽然很困难，但训练人工智能系统进行注意是一个可以实现的目标。在不同的课堂和特定领域的学习任务中，学生如何得出答案存在着普遍的模式。可直接用于此适应任务的标记数据，例如 [Wu et al.2021e] 中教师对学生作业的书面反馈，通常由教师在不同的数据集中私下保存。然而，可公开访问的数据（例如 StackOverflow 交互）也可以创造性地用于调整基础模型以引起注意。一些研究还探索了从教师那里提取学生如何犯错误的生成描述的有效方法[Malik et al.2021; Gulwani 和 Singh 2013]——这些手写的生成模型也可以用来生成适应数据，以帮助基础模型诊断学生的错误。 3.3.3 教学基础模型。推理学生的理解是实现第二个目标的重要一步：提供包容性的、高质量的教学。计算教学方法侧重于不同的任务，例如内容个性化[Connor 2019]、问题生成[Guo et al.2016；威利斯等人，2019； Srivastava 和 Goodman 2021]，适应性课程设计 [Mandel 等人 2014；多鲁迪等人。 2017]，并预测讲师干预[Chandrasekaran 和 Kan 2019； Alrajhi 等人.2021]。在本小节中，我们将讨论基础模型如何在学生教学中发挥作用。由于有效的教学需要对学生的理解进行推理，因此之前关于理解主题和“注意”的讨论非常相关。然而，提供有效的指导需要额外的能力：理解教学法[McKenzie 2003]。这包含了对指导学生的技术的有效理解，例如提出苏格拉底式问题或提供类比/对比案例；使用鼓励或支持性语言；根据学生的情况调整问题的难度；并生成与学生的兴趣和背景相关的示例。如何调整基础模型来理解良好的教学法？一个想法是考虑使用数据源进行适应，其中指令是主要作用。例如，来自 StackOverflow 等问答论坛的数据可能会被用来构建一个能够重复常见苏格拉底问题的导师。同样，根据维基百科等百科全书改编的基础模型可能能够为学生的问题提供（通常）事实上正确的答案。还有教科书、讲座视频、课程计划和评分反馈等公共数据源，它们共同包含可以通过基础模型进行调整的重要教学行为（图 15）。基于基础模型的教学的另一个适应挑战是学习如何像老师一样与学生交谈。教师使用的语言通常与一般人群使用的语言不同。理想情况下，教师接受过训练，能够以尊重的方式与学生交谈，并有意帮助他们对所学科目形成积极的认同[Truax 2018]。像微软 2016 年 Twitter 机器人“Tay”这样的警示性例子，这是一个在部署后 24 小时内开始生成仇恨言论的聊天机器人，向我们展示了在教育中明确考虑这一因素的重要性。课堂上的专业教师，我们也许可以使基础模型适应讲座视频或录制的办公时间视频等数据源。上述适应问题因以下事实而变得更加复杂：不同的教育环境在合适的语言类型上存在很大差异：例如，五年级科学课的有效教学看起来与大学物理课有很大不同，更不用说大学文学课了。这提出了超出典型 NLP 领域转移设置（例如，基于问题回答的问题）所面临的技术挑战。新闻文章与 Reddit 帖子），因为基础模型需要在语气和语言方面具有流畅的适应性，而不仅仅是其生成的事实内容。72 基础模型研究中心 (CRFM) 超越合理的教学技术和教学语言，基础模型如何提供更有洞察力的教学形式？ §2.1：本文的语言强调了这样一个事实：婴儿可以在短时间内习得极其复杂的语言。正如作者指出的那样，基础模型训练和人类语言习得之间的一个显着区别是“人类语言植根于现实世界：例如，婴儿的看护者在谈论物体时会指着物体。”同样的见解也可以激发关于如何将基础模型用于生成教育的想法。当人类面对现实世界的类比和对比时，他们似乎能学得很好，这些类比和对比可能是他们当前背景和过去经验之间的交叉点。例如，在教授手语时，教师可能会使用诸如“‘早晨’这个词的手形看起来像太阳升起”之类的类比，或者指出“你刚刚制作的手形看起来与另一个词非常相似，所以让我们关注差异。”另一个例子，当向已经了解阿拉伯语和英语的学习者教授斯瓦希里语时，教师可能会指出，斯瓦希里语中表示 8（发音为 nane）的单词是一个“假朋友”，在语音上与英语单词 9（发音为 9）相似。 ）。可以集成多模态数据的基础模型有可能进行儿童语言学习中常见的丰富类比和比较（图 16）。关于基础模型的机遇和风险 73 

# 4 技术 

基础模型的技术基础产生了决定其潜力的能力（§2：能力）。为了了解开发中使用的技术，我们考虑用于训练（§4.2：训练）的数据（§4.6：数据）、模型架构（§4.1：建模）和系统（§4.5：系统），并进一步适应，（ §4.3：适应）这些模型与理论（§4.10：理论）一起应该被开发来理解这种范式。为了理解生成的模型，我们讨论如何评估（§4.4：评估）和解释（§4.11：可解释性）以及鲁棒性（§4.8：鲁棒性）、安全性和隐私（§4.7：安全性）的重要性，和长期人工智能安全（§4.9：ai-safety），以确保这些模型在社会中部署时的可靠性（§5：社会）。74 基础模型研究中心（CRFM）

## 4.1 建模

作者：Drew A. Hudson , Antoine Bosselut, Alex Tamkin, Omar Khattab, Jared Quincy Davis, Jiaxuan You, Trevor Gale 

图 17. 基础模型的五个关键属性： 表现力 — 灵活捕获和表示丰富的信息；可扩展性——有效地消耗大量数据；多模态——将各种模态和领域连接在一起；记忆容量——存储大量积累的知识；和组合性——推广到新的背景、任务和环境。过去几年，新兴的基础模型范式在人工智能领域取得了令人瞩目的成就，像 BERT [Devlin et al.2019] 这样的模型在广泛的语言理解任务中大放异彩：从文本分类和蕴涵到问答和阅读理解力，而 GPT-3 撰写了关于独角兽的丰富而流畅的故事 [Brown et al.2020]，而 DALL-E 显示了视觉创造力的迹象，从头开始生成了极其逼真的鳄梨椅图片 [Ramesh et al.2020]。 2021]。最近的基础模型的这些实例和其他实例不仅在众多不同的下游任务和应用程序中实现了卓越的性能[Rajpurkar et al.2018；王等人。 2019a]，但也表现出值得注意的可解释性行为[Karras et al.2020]、鲁棒性[Devlin et al.2019]、可控性[Patashnik et al.2021]和泛化性[Brown et al.2020]。一个模型需要什么才能展示这些品质？哪些架构能够消耗大量潜在的多模态信息并将其转化为丰富的世界知识？总的来说，网络应该具备哪些理想的属性才能产生基础模型？在这里，我们确定并讨论了五个这样的属性，涵盖表达性、可扩展性、多模态、记忆容量和组合性，我们认为这些属性对于基础模型至关重要，以便：（1）从各种来源和领域提取和积累知识，（ 2）以有效且可扩展的表示方式组织它，并且（3）灵活地将其推广到新的环境中。对于这些特性中的每一个，我们都激发了它们的必要性，提供了结合它们的当代模型的例子，并探索了未来研究和开发的关键挑战和有希望的途径。概览图请参见图 17。论基础模型的机遇和风险

### 4.1.1 表现力

表达能力涉及网络对所训练的数据分布进行建模并以灵活的方式表示它的理论和实践能力。先前的工作已经提出了正式的表达性度量来表征网络可以计算的功能的复杂性，或者更准确地说，近似，这本质上受到其深度、宽度、连接性和结构模式的影响[Raghu et al. 2017]。 2017]。正如“没有免费的午餐”定理所表明的那样，不存在最适合所有情况的单一模型或算法 [Wolpert 和 Macready 1997]，因此，就我们的目的而言，我们特别感兴趣的是确定哪些模型可以有效地捕捉自然的各个方面。信息，例如人类语言或现实世界图像[Goodfellow et al.2016]。这些模态要么是连续的（如在视觉中），要么是离散的（如在语言中），具有明显的层次结构和高维性，并且在其组成元素之间呈现出一组复杂的关系和相互作用，无论这些元素是像素、单词还是物理对象。事实上，生成模型的最新突破为神经网络的高表达能力提供了强有力的证据，因为它们成功地表达了文本的分布[Brown et al . 2020；德夫林等人，2019； Lieber 等人 2021； Wang 和 Komatsuzaki 2021]、听觉 [van den Oord et al.2016] 和视觉 [Karras et al.2020； Brock et al.2018] 域，并生成高保真度、多样性和真实性的样本。归纳偏差。过去十年神经网络在自然数据建模方面的成功很大程度上归功于网络的高深度，这可以通过它们组成的堆叠非线性层的数量或它们的计算步骤的数量来粗略地衡量。在他们的推理链中采取。深度在增强网络的表达能力方面发挥着至关重要的作用，使它们能够形成强大的分层和分布式表示，这些表示可以从训练数据推广到新的未见过的例子[He et al. 2017]。 2016b；莱文等人。 2020]。通用逼近定理 [Lu et al.2019b] 确实指出，即使是简单的多层感知器 (MLP) 也可以表示广泛的函数，而不同的归纳偏差，如在循环神经网络 (RNN) 或卷积神经网络 (CNN) 中实现的那些[Goodfellow et al.2016]，可以提高学习效率并增强给定网络对不同形式信息进行建模的能力：语言、语音和时间序列共有的序列数据，对于前者或空间不变信息，如在图像或视频中，对于后者。变压器网络和注意力。与此同时，最近推出的变压器网络 [Vaswani et al.2017] 证明了捕获元素之间的远程依赖关系和成对或高阶交互的重要性。它们建立在自注意力机制的基础上 [Vaswani et al.2017; Bahdanau et al.2014]，它可以缩短计算路径，并提供直接方法来比较输入数据中的元素（例如句子中的代词及其先行词，或引用同一主题的两个句子）。从另一个角度来看，注意力和门控结构中体现的乘法交互（如 LSTM [Hochreiter 和 Schmidhuber 1997] 或 Mixture-of-Experts [Shazeer et al.2017]）为刚性固定结构提供了更灵活的替代方案。 MLP 和 CNN 的权重计算，动态调整计算以适应手头的输入。事实证明，这对于语言建模特别有用，例如，给定一个句子“她用 X 吃了冰淇淋”，而前馈网络总是以相同的方式处理它，这是一个基于注意力的模型可以根据输入调整其计算——如果介词短语（PP）附件X是“勺子”，则更新单词“ate”的上下文表示，或者如果X指的是“例如”，则将其链接到“冰淇淋”草莓”[Zavrel et al. 1997].76 基础模型研究中心 (CRFM) 通用计算。注意力相对于先前架构的最后一个显着优势源于其更强的通用性，即与特定任务没有紧密联系或域，就像卷积的局部感受野或循环网络的顺序假设一样，两者分别反映了特定于视觉和语言模态的固有属性。我们假设注意力和变压器的通用性质有助于它们在广泛的研究问题和应用中的广泛适用性[Liu et al.2019;多索维茨基等人。 2020； Hudson 和 Zitnick 2021]。这种对比体现了任务专业化和表现力之间更普遍的权衡：具有更强结构先验的模型可以利用它们来提高受益于这些假设的特定任务的样本效率；相反，整合较弱归纳偏差的模型学习速度更慢，但反过来可以扩展到更大的数据量并适应不同的领域，因为它们不依赖于限制性或特定于任务的假设。随着数据和计算变得更加容易获取，我们观察到，探索具有最小归纳偏差集的模型，可以“让数据自己说话”，似乎是该领域未来研究更有前途的方法。未来方向。尽管神经网络总体上取得了显着的进步和成就，特别是基础模型，但在表达能力方面，仍然存在显着的挑战。领先的方法 [Choromanski et al.2020; Dosovitskiy et al.2020] 仍在与建模作斗争极长范围的依赖性，例如书籍、电影甚至 DNA 序列中出现的依赖性，这可能归因于当代基于 Transformer 的方法的二次计算 [Wang et al. 2020c; Lin et al. 2021]。挑战本质上反映了效率和表现力之间的权衡：通过短而直接的计算路径对长距离交互进行显式建模一方面提高了表现力，但由于增加的连接性所带来的计算而牺牲了可扩展性。其他[Child et al.2019； Kitaev 等人 2020； Choromanski 等人，2020]。 GANformer [Hudson and Zitnick 2021] 和 Perceiver [Jaegle et al.2021b,a] 等模型探索了平衡这两个属性的方法，并提出了依赖二分或瓶颈注意力的具有线性复杂度的变压器，从而在提高计算效率的同时保持高表现力。我们相信，确定这两个目标之间的有效平衡为未来的研究提供了一条有趣的途径。另一个重要的研究方向涉及基础模型的扩展，到目前为止，该方向主要集中在语言领域[Peters et al.2018;德夫林等人，2019； Brown et al.2020]，以不同的方式，例如结构[Scarselli et al.2008；维利科维奇等人。 2017]和感知[Tolstikhin et al.2021； Jaegle 等人.2021b； Tan 和 Le 2021]，每个都涉及一组独特的相关挑战。同样，我们相信探索推理架构（第 2.4 节：推理）需要迭代计算链以及与符号信息的交互，这构成了未来基础模型研究的一个有价值的目标。 

### 4.1.2 可扩展性

与模型的表现力密切相关的是可扩展性的概念。随着来自不同来源的丰富数据变得更加容易获得，并且计算资源变得更强大、更高效（§4.5：系统），我们应该寻找与这种进展速度相匹配的方法，并利用它来提高人工智能的能力和多功能性。为了使基础模型有效地适应图像或文本的复杂和高维分布，它们应该在所有维度上都可扩展：包括模型的深度和宽度以及它们的训练时间、参数数量和数据量。可以处理。论基础模型的机遇和风险 77 优化。具体来说，基础模型应该是：（1）易于训练（§4.2：训练），对数据中的噪声或缺陷具有弹性，并且对消失等不稳定性具有鲁棒性[Helfrich et al.2018； Glorot 和 Bengio 2010] 或梯度爆炸 [Hochreiter 和 Schmidhuber 1997； Nair and Hinton 2010]，而且（2）易于适应（§4.3：适应），通过克服灾难性遗忘现象[Kirkpatrick et al.2017]并支持小样本学习[Sung et al.2018] ]。我们仍处于理解驱动学习算法可扩展性的原理的早期阶段，尽管最近的工作已经开始阐明这些主题 [Liu 等人，2017]。 2020c；库迪蒂普迪等人。 2019；纳基兰等人。 2019]，还有很多工作要做。硬件兼容性。除了稳健性和优化方面之外，基础模型还应该具有实际效率（第 4.5 节：系统），并利用当代和未来的硬件 [Hooker 2020]。其中一个例子是并行性，这是表征 GPU 支持的计算的一个重要属性。事实上，变压器相对于以前占主导地位的循环方法的巨大成功很大程度上是由它们更高程度的并行性推动的。展望未来，鉴于系统开发的快速进展，我们应该进一步确保模型的设计能够适应未来硬件的进步。因此，理想情况下，基础模型应该适合诸如分布式训练之类的方案，这种方案越来越受欢迎，例如专家混合的情况，并且可能利用计算或表示的稀疏性等属性，就像这种情况对于 Longformer [Beltagy et al.2020]、BigBird [Zaheer et al.2020] 和 Sparse Transformer [Child et al.2019] 方法，这些方法可能会在未来的硬件和处理器中变得更加核心。 

### 4.1.3 多模态

传统上，计算机视觉、机器人技术和 NLP 领域以独立的方式取得进展，不同的社区开发适合每种模式的特定方法。深度学习的兴起带来的一个有利后果是，它帮助在人工智能的各个社区和研究领域之间建立了桥梁，因为看似不同的问题现在可以通过密切相关的方法来解决，并且原本遥远的主题的研究开始汇聚到人工智能领域。共同点。这一突破开辟了一系列新的可能性，促进了对多模态主题的开拓性探索，涵盖语言基础 [Lynch 和 Sermanet 2020]、视觉语义 [Conser 等人 2019]、体现环境 [Savva 等人 2019] 等多种领域。 2019a] 和交互代理 [Gray 等人。 2019]。从本质上讲，多模态是智能的关键组成部分，是发展对世界的全面和广泛理解的关键因素。具体来说，语言学习在扎根的环境中比在真空中更有效。相反，从视觉角度来看，语言鼓励抽象概念的出现，将低级感知信号和统计数据与对象、属性、代理和动机的语义概念联系起来，从而丰富和提升视觉表征。根据这些观察，我们认为基础模型应该理想地将不同的模式连接在一起，将它们的具体信息提炼成共享的多方面表示，并捕获它们之间的全方位的相互联系和关系，以便提供广泛的信息。能力（参见§2.1：语言，§2.2：视觉，§2.3：机器人，§2.4：推理）。通用性和特殊性。多模态基础模型的一个重要设计选择是专业化程度，或者负责每种模态的模块之间的结构共享。当然，不同领域的数据表现出不同的结构和属性——例如，语言是离散的，而视觉是连续的。乍一看，这种变化暗示为每种模态量身定制的专门归纳偏差可能会有所帮助。然而，随着训练规模78基础模型研究中心 (CRFM) 的规模不断扩大，模型有机会更少地基于结构先验，而更多地基于数据本身，证明只维持少数广泛的一般假设的通用方法事实上比针对特定任务的替代方案要成功得多。因此，正如最近跨不同模态的变压器等通用模型的成功所证实的那样——无论是语言学的[Liu et al.2019； Lan et al.2019] 和视觉[Dosovitskiy et al.2020； Hudson 和 Zitnick 2021]，我们看到通用性对于提高人工智能能力至关重要。多模式交互。多模态模型的另一个关键考虑因素涉及权重共享：各种模态是否受益于对其各自组件使用相同或不同的参数？先前的研究表明，富有成效的转移肯定可以跨模式发生，但理想的共享程度仍不清楚，发现共享的原则性方法是否存在也不清楚。最后，一个主要的设计问题涉及模型支持的多模态交互的形式，这些形式在具体案例和示例之间差异很大：跨模态或后期融合模型，例如 ConVIRT [Zhang et al.2020a] 和 CLIP [Radford et al.2021]为每个数据源维护完全独立的编码器，并且仅在最终计算阶段使用例如简单的点积来比较它们的空间。同时，早期融合模型，例如 ViLBERT [Lu et al.2019a; Cho et al.2021]，对视觉推理和问答任务所需的多种模式进行联合推理。确定合并各个向量空间的最佳阶段和形式 [Nagrani 等人。 2021]仍然是一个开放的研究问题。总体而言，虽然社区内部似乎对多模态的重要性达成了共识，但超越视觉和语言浅层对齐的模型尚不存在，而在具体环境中扎根语言学习的主题仍然有很大的探索空间。 

### 4.1.4 内存

到目前为止，我们已经讨论了基础模型的目标，即大规模收集和积累来自不同模式的信息。这些知识既包括对世界的广泛理解，也包括对利基主题或特定事实的具体掌握。表示如此大量的学习信息绝非易事，并且引发了有关访问、存储、检索和操作特定项目或记忆的有效机制的有趣问题。显式存储。可以实现这些需求的一个重要设计原则是将计算与内存分开[Weston et al.2014；格雷夫斯等人，2016； Hudson 和 Manning 2018，2019a]，通过将先前获得的抽象技能应用到新的具体环境中来增强模型转移知识的能力。在这种情况下，区分显性事实（可以存储在外部存储器中）和隐性知识（通过网络的可训练权重反映）非常重要。与通过网络权重将所有信息隐式编码在一起的替代方案相比，这种显性知识和隐性知识的解耦具有多种优势。这种分离缓解了存储不断增长的知识量所需的模型大小和参数数量的膨胀[Guu et al.2020]，通过增加知识来源来提高模型的信任和可靠性[Cheney et al.2009]，并且大多数值得注意的是，它是记忆更新、操作或适应的关键[Lewis et al.2020b]（§4.3：适应），这反过来又可以泛化到新的上下文和下游任务。事实上，过去几年，内存和计算之间的解开一直是深度学习和 NLP 研究中反复出现的目标，包括内存网络等模型 [Weston et al.2014; Sukhbaatar 等人 2015 年）、神经图灵机 [Graves 等人 2014 年、2016 年]、神经状态机 [Hudson 和 Manning 2019a] 以及 MAC [Hudson 和 Manning 2018]。此外，On the Opportunities and Risks of Foundation Models 79 使用键值结构 [Miller et al.2016] 访问外部存储器已被证明对于建模长期依赖性非常有效 [Henaff et al.2016; Bosselut 等人，2018； Lample 等人，2019]。 Transformers 是迄今为止大多数基础模型的著名架构，同样展示了涉及键值内存访问和它们逐渐构建的上下文单词表示之间的计算的操作[Geva 等人，2017]。 2020]。信息检索。一旦模型在训练后完成信息收集，就有多种方法可以检索下游应用程序和任务所需的特定事实或记忆。有些采用显式提示技术，通过输入序列查询模型的知识[Petroni et al.2019； Kassner 等人，2021； Jiang et al.2020]而其他方法涉及通过适应阶段隐式回忆和重塑先验知识[Bosselut et al.2019； Hwang 等人.2021]。第三类方法更进一步，将基于神经的计算与符号聚合以及从非结构化文本存储库中检索信息相结合[Karpukhin et al.2020； Lewis 等人，2020b；哈塔布等人。 2020]甚至是结构化资源，例如知识图谱[Zhang et al.2019a；彼得斯等人，2019；刘等人。 2020 年；维尔加等人。 2020；安永等人。 2021]。然而，一方面检索机制提供的强大记忆技能与另一方面存在信息瓶颈时学到的更丰富的表示之间存在权衡。事实上，过度依赖检索会减少学习如何以紧凑和抽象的方式表示信息、从模型暴露的大量输入信息中提取关键见解和概念的机会，以及基本上将小麦与小麦分开的机会。糠。例如，GPT-3 的上下文学习能力可能是强制网络通过其有界内存架构表示输入序列数据的副产品 [Brown et al.2020]。总的来说，虽然它们确实有一些优点 [Guu et al.2020]，但依赖外部检索机制的模型可能无法像有界、紧凑和抽象表示那样有效地学习泛化。知识操纵。最后，在考虑长时间的大规模学习时，至关重要的是要注意知识的动态性质，随着世界的不断发展，事实的正确性和有效性可能会随着时间的推移而改变——而昨天真实或相关的内容可能并不如此明天。因此，对于模型来说，以支持有效更新或操作事实作为其终身学习的一部分的方式表示其知识至关重要。 

### 4.1.5 组合性

组合性可以被定义为整体的含义从其组成部分的含义导出的原则，以及用于组合它们的规则[Janssen and Partee 1997;博图2014]。它是人类智能的重要组成部分 [Lake et al.2017]，是我们从少数例子中轻松有效地进行计划、推理和学习的能力的基础。组合性可能是实现分布外（或者具体而言）组合泛化的关键。它借鉴符号人工智能的经典思想，鼓励和增强神经网络内的理想特性，例如可解释性、可控性和数据效率[Lake et al.，2017]。 2017]，并且可以采取不同的形式，表征各种元素：模型。组合性可以在模型级别反映在其架构属性、结构和模块化程度方面，这可以提高大型神经模型的训练和推理效率[Shazeer et al.2017]。它还与可解释性和多模态主题相关，因为它涉及模型组成的不同模块之间的接口、它们采用的交互模式以及它们的透明度。80 基础模型研究中心 (CRFM) 计算。模块网络 [Andreas et al.2016] 和 Mixture-of-Experts [Shazeer et al.2017] 等模型沿着这个方向走得更远，不仅表现出结构模块化，而且表现出由子网络专业化支持的组合计算以适应和定制模型行为以适应手头输入的方式来适应不同的操作。虽然一些方法依赖于手工设计的模块的串联[Andreas et al.2016]，但替代方法使网络专业化能够通过学习自然地出现[Shazeer et al.2016]。 2017]。其他模型，例如 MAC [Hudson 和 Manning 2018] 和动态记忆网络 [Xiong et al.2016] 执行显式迭代计算，其中给定任务被分解为多个推理步骤，逐一执行，表现出从一组最初的事实到新颖的推论和结论。训练和数据。不仅模型或其计算可以是组合的，数据或训练过程也可以是组合的 [Andreas 2020]。我们可以将其拆分或分解为子集，在每个子集上独立训练不同的模型，并最终在测试时通过各种集成技术将它们重新组合，而不是在完整的数据集上训练一个模型 [Dietterich 2000]。这些方法可能对基础模型的训练和部署程序产生深远的影响，无论是在实践还是社会方面。表示。我们讨论了不同元素的组合性，例如模型、计算、训练方案或数据。但最值得注意的是，在模型训练和适应过程中出现的学习表示本身也可以是组合的 [Andreas 2019]。事实上，一种有前途的知识表示方式是通过结构化的、可能基于图的、面向对象的表示[Zhang et al.2019a； Wang et al.2021a]，其核心是识别实体和事件节点并在它们之间形成联系、类比和关系边缘。它反映了一种组织世界信息的自然方式，来自不同模式的输入可以围绕语义多方面概念进行引导和聚合。这种表示可以支持多跳推理和推理[Washington et al.1995； Sun 等人，2020b； Yu et al.2020c]，并且还可能通过重组实现更强的分布外泛化。然而，组合性也会阻碍表示的表达性，并阻碍其解释特质、例外和上下文相关性的能力[Misra et al.2017a]。换句话说，整体有时可能大于部分之和，例如，红酒与红洋葱不同。但是，尽管过去十年占主导地位的许多方法往往主要关注光谱的一端，并学习整体分布式表示，但我们相信探索在上下文性和组合性之间达到更好平衡的方式是未来研究的一条有前途的途径。 

### 4.1.6 总结

我们引入了我们认为对于下一代基础模型至关重要的五个属性，以便有效地提取我们周围的大量信息，从而成功地解决下游任务：表现力——灵活捕获和同化现实世界的信息、可扩展性- 熟练地处理大量高维数据，多模态 - 消费、处理和潜在地产生来自不同来源和领域的内容，记忆容量 - 有效地存储和检索所获得的知识，最后，组合性，以促进成功的泛化新颖的任务、设置和环境。我们相信，正如本报告中所设想和详细讨论的那样，基础模型的全部潜力的实现将依赖于新架构和建模进步的研究来满足这些需求。 关于基础模型的机遇和风险 81 

## 4.2 训练

作者：Alex Tamkin 

训练目标是数学函数，描述如何将模型架构和大量广泛数据转换为基础模型。例如，GPT-3 是通过语言建模目标进行训练的，该目标会奖励正确预测下一个单词的模型 [Shannon 1948]。我们首先列出这些训练方法的一些目标，描述当前方法中的重要设计权衡，并概述未来道路的重要目标。

### 4.2.1 训练目标的目的

鉴于这些方法和模型最近的快速进展，我们在此概述了训练算法的一些关键目标。46 利用广泛的数据。自监督学习算法的兴起释放了互联网规模数据集的力量，而这些数据集很难手动注释。这种广泛的数据有多种形式，包括图像、音频记录和视频（§2.2：视觉）；机器人和传感器数据（§2.3：机器人技术）；和文本，无论是单独的还是与图像等其他形式配对（§2.1：语言）。由于这些数据缺乏外部注释，研究人员的主要关注点是设计定制的自监督算法，利用每种数据中的独特结构为基础模型生成训练信号。域完整性。基础模型训练算法的一个重要目标是领域完整，从某种意义上说，解决训练任务需要对该领域中的下游任务广泛有用的能力（参见§2.1：语言，§2.2：视觉，§2.3：机器人） ）。该属性对于基础模型的通用性至关重要。例如，当模型学习预测文档中的下一个单词时，语言建模可能需要模型获得共指、情感和翻译等广泛的能力。相反，像情感分类这样的监督学习任务可能会导致能力范围更窄（参见§2.1：语言）。尽管这种质量很重要，但哪些任务将导致领域完整的能力，甚至如何评估模型能力的全部范围，都不是显而易见的先验（参见§4.4：评估和§4.10：理论）。扩展和计算效率。训练基础模型的过程必须可靠地将数据、模型架构和计算转换为具有广泛功能的模型。为了最大限度地发挥基础模型的能力，我们可以识别该过程的瓶颈，并提出新的训练算法来消除它们。自监督算法的兴起使模型大小和计算资源成为日益突出的瓶颈[Kaplan et al.2020； Henighan et al.2020]，导致了一种转变，即模型的评估不仅取决于其能力，还取决于达到这些能力所需的计算量和类型（第 4.4 节：评估）。训练目标的效率可能差异很大，47这突显了训练方法的设计对于在固定计算预算的情况下出现强大功能的重要性。因此，训练研究人员的一个主要目标是设计具有更丰富训练信号的训练目标，从而产生学习速度更快并获得更强能力的模型。48帮助这一发展的一个力量是令人惊讶的可预测性，即能力如何随着不同类型的架构、数据而扩展。 46我们使用“训练”而不是预训练来强调基础模型本身的首要地位，因为一些使基础模型适应下游任务的方法不涉及任何后期训练阶段。47例如，ELECTRA 的 4x [Clark 等人.2020] vs BERT [Devlin et al.2019]，CLIP 训练的对比与生成方法的 12 倍 [Radford et al. 2021] 48当然，计算机系统设计者的一个关键目标是减轻计算作为训练瓶颈的程度（参见§4.5：系统）并且训练方法的选择最终也受到各种高质量数据82基础模型研究中心（CRFM）大小和计算的可用性的限制[Hestness et al.2017； Kaplan et al.2020]，这是一个引人注目的现象，它使模型开发人员能够根据更清晰的趋势做出选择，而不是更昂贵的随机搜索。 

### 4.2.2 当前 SSL 方法的设计权衡

当前用于训练基础模型的**自监督学习（SSL）方法**多种多样，但它们的共同点是它们可以根据未标记的数据产生预测问题，而不需要人工注释者。 SSL 目标通过精心设计的约束从这些数据中产生丰富的训练信号，无论是数据本身（例如，编辑或噪声）还是模型能够表示或处理数据的方式（例如，潜在瓶颈）。在某种程度上，这些约束“融入”了使模型适应下游任务时所需的功能（§4.3：适应）。49在这里，我们描述了当前模型探索的三个重要的设计选择，以及它们各自在以下方面的权衡：他们产生的能力。我们应该在什么抽象级别建模？一个基本问题是基础模型的输入表示应该是什么。一种选择是在原始字节级别对输入进行建模。然而，这种高维度可能会导致模型专注于预测输入的较少语义方面，50 减慢了它获得更通用功能的速度。当训练像 Transformer [Vaswani et al. 2017] 这样的模型时，这些方法也会变得棘手，因为其计算成本随输入呈二次方增长51另一种选择是使用领域知识来减少模型的输入空间——此类策略包括补丁嵌入[Dosovitskiy et al.2020]以及固定或学习的标记化[Schuster and Nakajima 2012； Sennrich 等人，2016；工藤和理查森 2018；范登奥尔德等人，2017； Ramesh 等人，2021]。这些方法可以缓解生成方法面临的一些挑战，但需要权衡它们可能会放弃输入中可能有用的信息。52连续输入与离散输入的选择也需要权衡适应（§4.3：适应）；需要做更多的工作来发挥这两种方法的优势。生成模型与判别模型生成训练方法在概念上是优雅而强大的——它们训练模型来学习训练输入的联合或条件分布。生成基础模型的两个主要系列包括自回归基础模型 [van den Oord et al.2016;雷德福和纳拉辛汉 2018； Chen 等人.2020d；杨等人，2019； Ramesh et al.2021]，逐段生成输入，以及去噪基础模型[Devlin et al.2019； Raffel et al.2019] 破坏然后恢复输入。训练过程中执行的特定类型的生成决定了可用的交互类型（§4.6：数据），这仍然是许多领域的主要挑战，包括机器人技术（§2.3：机器人技术）和低资源语言（§ 2.1：语言）49例如，用于训练 GPT-3 [Brown 等人 .2020] 的因果语言建模目标可以通过前缀对其进行调节。对比学习期间使用的颜色抖动增强 [Chen et al.2020c] 鼓励对下游任务无用的属性的不变性。更好地理解这些约束的特定选择和结构如何影响模型获得的能力是未来工作的一个重要领域（§4.10：理论）。 50例如，草叶、音频压缩伪影或单词的拼写 51参见§2.2：视觉和§4.1：用于讨论高维序列（例如图像和视频）的训练成本的建模 52例如，标记文本可能会使学习押韵、双关语或其他受益于字符级信息的任务变得更加困难 [Branwen 2020]关于基础模型 83 在适应过程中的机遇和风险53（参见§2.5：交互和§4.3：适应），未来的模型可能54 虽然生成训练方法有其好处，但一些歧视性方法最近也受到关注。这些方法无法实现基于生成的交互，但它们可以在图像、音频和视频等高维连续设置中更有效地学习基于分类或回归的任务。这些方法中的大多数为输入（部分）输出向量，这些向量被训练为对于输入的不同“视图”是相似的[Wu et al.2018；范登奥尔德等人，2018； Chen 等人.2020c；他等人2020；烧烤等人。2020；卡隆等人，2021；张等人。 2020a； Radford et al.2021]或用于预测部分输入是真实的还是假的[Clark et al.2021] 2020； Iida 等人.2021]。更好地理解生成训练和判别训练之间的权衡，以及充分利用两种方法的优点，仍然是未来研究的有趣途径。捕获多模式关系。另一个日益重要的研究领域是捕获多种数据之间的关系。根据建模者的上下文和目标，这意味着什么可能会有所不同。例如，CLIP [Radford et al.2021] 和 ViLBERT [Lu et al.2021]。 2019a] 都是多模态视觉语言，但其多模态的精确方式有所不同。55 前者将图像和文本分别编码为向量，使拥有单一模态示例的用户能够检索、评分或分类其他模态的示例情态。第二个在模型的早期阶段联合处理图像和文本，支持下游应用程序，例如视觉问答，其中提供对相关图像和文本对（例如，图像和有关它们的问题）的推理。多模态基础模型仍然是一个新兴的研究领域；关于模型多模式的不同方式以及更好地理解这些附加模式带来的功能，还有很多尚未探索。 

### 4.2.3 前进的道路

我们最后提出了基础模型训练未来的一些重要目标。开箱即用的 SSL 目前，SSL 目标是高度特定于领域的：目前在自然语言处理、计算机视觉和语音处理中盛行不同的方法。这有两个主要缺点：首先，这些不同的技术使得掌握每种方法起作用的共同线索和科学原理变得具有挑战性。其次，这种领域特异性要求从头开始为每个新领域开发新的基础模型训练方法，包括医学、科学和新的多模式环境。在任何类型的数据上有效训练基础模型的更普遍目标将代表基础模型训练社区的一个重要里程碑[Tamkin 等人。 2021b]。获得丰富的训练信号 显然，并非所有训练目标都是平等的 - 有些训练目标比其他训练目标更加有效，对于给定的计算预算，可以转化为能力更强的基础模型。是否有比目前已知的训练方法更有效的数量级？如果是这样，我们怎样才能找到他们？这些调查将受到多种力量的影响，包括未来软件和硬件的进步（§4.5：系统）使之成为可能。我们也不需要将数据（§4.6：数据）和训练算法视为独立因素：53例如，GPT-3等自回归模型不仅支持基于前缀的调节，而T5或BERT等去噪模型则有助于使用双向上下文来替换任意长度的跨度或修复拼写错误。54在基础建模环境中研究较少的其他类型的生成方法包括扩散模型和基于评分的模型[Sohl-Dickstein et al.2015；宋和尔蒙 2019； Ho et al.2020]、VAE[Kingma and Welling 2014]、流动模型[Dinh et al.2015； Kingma 和 Dhariwal 2018]，以及 GAN [Goodfellow 等人 2014]——这些或其他未来的方法是否也能够像自回归或去噪方法一样实现多种能力的学习，还有待观察。 55请参阅第 2.2 节：视觉和第 2.1 节：语言，以详细讨论视觉和语言中的多模态84 基础模型研究中心 (CRFM) 数据的质量和可用性会影响训练信号，56 但训练算法本身可以自适应地寻找或随着模型的改进构建更丰富的训练示例以加速学习 [Tamkin 等人。 2021c]。基础模型的目标导向训练。诸如提示（第 4.3 节：适应）之类的适应方法利用了几乎是训练后的突发特性。我们是否可以训练基础模型，将在复杂世界中理解并可靠地实现目标的能力作为模型训练目标的一部分？对开发通用能力的关注将这一方向与通过强化学习使现有基础模型适应特定任务的目标区分开来（例如，Stiennon 等人[2020]）。相反，人们可能会想象当前方法的更复杂版本，这些方法从原始在线获取各种现实世界的能力[Klyubin et al.2005；辛格等人，2005； Salge 等人，2013；沙基尔·穆罕默德 2015；弗洛伦萨等人，2017； Pathak 等人，2017； Haber et al.2018] 或离线 [Precup et al.2000；兰格等人，2012；阿贾伊等人.2021;杨和纳胡姆 2021； Schwarzer et al.2021] 交互，无需人工注释或任务构建。此类方法可能使用与现有 SSL 算法非常相似的技术：例如，在目标导向的上下文中训练序列模型，可以直接要求它们通过调节来执行某些任务（例如，UDRL [Schmidhuber 2019; Srivastava et al.2019] 或Decision Transformer [Chen et al.2021b]；另请参阅§2.3：机器人技术）。简单交互环境中已经出现的复杂行为[Baker et al.2020]表明基础模型的多任务、多智能体和多模式目标导向训练是未来研究的一个有趣途径。 56包括任何不良或有偏见的能力（§5.1：公平性）关于基础模型的机会和风险 85 4.3 适应 作者：Xiang Lisa Li*、Eric Mitchell*、Sang Michael Xie、Xuechen Li、Tatsunori Hashimoto 图 18. 在适应过程中，基础模型被转换为适应模型（底行），以反映更新的信息、期望的行为或部署约束。虽然基础模型为处理多模态信息提供了强大的通用引擎，但对于某些应用程序来说，在使用前调整基础模型是必要的。从广义上讲，适应过程通过根据附加信息调节基础模型来生成适应模型，或者通过在其输入中包含新数据或提示来启动基础模型，或者通过更新部分或全部基础模型参数以反映基础模型的参数。新的信息。例如，在文本摘要中，在输入文章中附加诸如 TL;DR 之类的提示可以通过充当基础模型的任务规范来提高基础模型性能 [Radford et al.2019]。或者，使用组织的内部特定领域数据微调基础模型的参数可以通过添加与组织用例相关的信息来提高模型的准确性。在本节中，我们描述现有的适应方法以及确定特定适应程序是否适合特定环境的几个因素。我们还描述了基础模型适应的各种用例，包括相对充分研究的设置，例如基础模型针对特定任务或领域的专门化，以及更具推测性的设置，例如测试时数据删除[Bourtoule et al.2019]和编辑特定输入上的模型行为 [Sinitsin 等人。 2020]。最后，我们提出了基础模型适应未来研究的长期目标。 

### 4.3.1 基础模型适配方法

已经提出了许多用于适应基础模型的方法，使得决定**针对特定问题**或计算环境使用哪种适应过程变得困难。我们强调从业者在选择适应过程时要考虑的三个特别重要的因素：（1）计算预算（特别是存储和内存）； (2) **可用的特定任务数据量**； (3) 获得基础模型梯度的范围。86 基础模型研究中心 (CRFM) 因素 1：计算预算。对于具有数十亿或数万亿参数的基础模型，微调所有模型参数可能需要非常大的内存。此外，对许多任务进行单独微调可能会产生不可接受的存储成本。有许多工作提出了减少适应基础模型的存储的方法，我们将此类轻量级适应方法称为低存储适应。通常，此类中的方法会冻结大部分预训练基础模型参数，仅学习相对少量的特定于任务的参数（通过微调一些预训练参数或添加全新模块），从而减少每个任务的存储开销。此类算法的关键设计决策是选择要调整的参数。也许最简单的方法是仅调整预训练模型的最后一层，而其他作品仅调整模型的偏差向量 [Zaken et al.2021]、模型权重张量的低阶残差 [Hu et al.2021]，或模型参数的掩码[Zhao et al.2020b]。另一条研究调子“软”提示[Li and Liang 2021;秦和艾斯纳 2021；刘等人.2021e；莱斯特等人，2021； Hambardzumyan et al.2021]，对应于任意参数向量序列而不是模型词汇的嵌入，并通过在输入层或所有层连接输入激活来根据这些提示条件基础模型。另一种方法冻结所有模型参数，并将新的 MLP 模块与现有模型层之间的可训练参数交错[Houlsby et al.2019]。虽然这些轻量级适应技术似乎在下游任务上权衡了参数效率和性能，但它们有时会实现与完全微调相当的性能，尽管更新了 1000 ×更少的参数 [Zaken et al.2021;李和梁2021；胡等人.2021]。 Lester 等人[2021]展示了一个实例，随着模型大小的增加，完全微调和轻量级自适应之间的性能差距消失。我们仍然不确定轻量级适应技术如何随着模型大小的增加而扩展[Aghajanyan 等人。 2020]。由于 GPU 内存通常是比磁盘存储更有限的资源，因此低内存适应过程对于基础模型的民主化可能比低存储适应过程更为重要。人们已经提出了各种用于低内存模型训练的技术，这些技术可以直接应用于基于微调的适应过程（§4.5：系统）。然而，一些低内存程序，例如梯度检查点 [Chen et al.2016] 会权衡计算和内存，可能会加剧基础模型的显着能耗 [Bender et al.2016]。 2021]。因素 2：数据可用性。任务专业化主要需要特定于任务的标记数据作为训练信号。57然而，不同任务和语言的注释成本差异很大；例如，注释 MRI 数据需要专业的医学知识，而标记英文文本的情感只需要常识判断。当适应数据充足时，我们可能会应用传统的微调方法或其轻量级对应方法。在基于语言的资源匮乏的环境中，结合提示和微调已被证明是一个有前途的方向 [Schick 和 Schütze 2021a,b;高等人.2020c；佩雷斯等人.2021； IV 等人 2021；敏等人。 2021]。 Le Scao 和 Rush [2021] 表明，一个经过精心调整的提示可以相当于大约 100 个训练示例，并且微调经过仔细提示的基础模型比微调无条件基础模型的数据效率要高得多。

因素 3：访问基础模型梯度。尽管基础模型对一些研究团体产生了重大影响，但大规模基础模型（具有超过 500 亿个参数）的分布实践几乎没有标准化。随着我们逐渐意识到滥用基础模型带来的潜在危险（参见§5.2：滥用），提供对所有 57 个提示的访问是一个例外，尽管我们可能会考虑提示隐式表示一批标记数据中包含的信息[ Le Scao 和 Rush 2021]。关于基础模型的机会和风险 87 对基础模型的参数进行微调可能会导致伦理问题。此外，大多数用户没有足够的计算资源来利用其完全访问权限。例如，基础模型的内存要求可能会妨碍许多组织和机构对其进行直接微调。因此，未来的基础模型提供商可能会限制对模型完整参数的访问，转而提供代理 API 访问，如早期基础模型 GPT-3 所示。在一种极端情况下，如果基础模型提供者仅允许访问模型输出（例如，提示的文本延续、生成的图像或评估图像与文本描述之间的一致性的分数），则可以调整基础模型使用情境学习[Brown et al.2020]。上下文学习冻结基础模型参数，并通过调节（通常是自然语言）提示来引导基础模型的输出，该提示可能由任务指令或演示组成。为了提高情境学习的性能，需要仔细设计提示，无论是通过手动搜索还是自动化程序[Jiang et al.2020; Shin et al.2020]，并根据适应数据进行了验证。在另一个极端，如果基础模型提供者授予对模型参数的梯度的访问权限，则可以应用完全微调，其中更新所有模型参数以提高下游任务的性能。作为中间立场，我们可能仅获得对基础模型输入的梯度访问，58其维度比基础模型参数低得多。在这种情况下，我们可以部署轻量级适应技术[Liu et al.2021e;李和梁2021； Lester et al.2021]，它冻结模型参数并优化每个任务的连续前缀或提示。 

### 4.3.2 适应用例

当模型所需的用例与用于基础模型训练（§4.2：训练）的相对通用的训练目标不同时，适应就很有用。最常考虑的情况是基础模型适用于执行特定任务（例如，文本摘要或图像中的动物分类），从而缩小模型的范围。事实上，本节前面描述的绝大多数现有方法都针对此设置。然而，其他形式的适应也是有用的，例如进行本地模型编辑以纠正特定输入的不良预测或向训练后的基础模型添加隐私约束，这些都是与任务无关的。在本小节中，我们描述了各种适应用例、最适用于它们的方法，以及解决这些设置中仍然存在的挑战。任务专业化。基础模型适应最广泛研究的案例是任务专业化，其中基础模型经过调整以优化特定任务或任务集的性能。例如，专门从事摘要任务将引发基础模型行为，从输入文档中提取关键思想并将其重新组织为简短的摘要句子。事实证明，各种适应程序对于任务专业化是有效的，与未适应模型的性能相比显示出显着的改进[Howard and Ruder 2018;布朗等人，2020]。除了相对广泛研究的针对特定任务的专门基础模型的设置之外，由于其规模和计算需求，其他与任务无关的适应问题对于基础模型来说变得越来越具有挑战性（但同样重要）。例如，训练基础模型的成本使得随着时间的推移持续训练以使模型的预测与当前事件保持同步特别昂贵。此外，收集用于训练基础模型的大量匿名数据集（§4.6：数据）的挑战使得个人信息泄漏到训练集中的可能性非常大；因此，需要一种事后有效地从基础模型中删除训练数据的机制。 58假设基础模型提供者使输入空间是连续的。88基础模型研究中心（CRFM）时间适应。理想情况下，基础模型存储的知识密切代表世界的状态，与模态无关。然而，世界在不断变化；新的国家元首当选，服装风格发生变化，社会规范和信仰发生变化（§5.6：道德），语言的使用发生变化，导致输入分布、目标预测分布或两者都发生变化。由于引起的分布变化，这种时间变化提出了一个具有挑战性的统计问题，如第 4.8 节：稳健性中所述。对于基础模型来说，时间平移也带来了一个特别困难的计算问题。由于训练基础模型的计算要求较高[Shoeybi et al.2019； Brown et al.2020]，频繁的从头开始重新训练可能会带来不可接受的财务或环境影响[Bender et al.2021]（§5.3：环境），或者只是需要太长时间才能成为保持模型最新的可行方法。在视觉领域，跨中间时间点对未标记数据进行渐进式自我训练可以弥补较长时间段内的时间变化，但仍然是一个昂贵的再训练过程 [Kumar et al.2020a]。在语言模型的背景下，时间分区的诊断数据集有助于量化大型语言模型过时的速度 [Lazaridou et al.2021; Hombaiah 等人，2021； Dhingra et al.2021]，表明重新加权训练数据和动态评估（在生产时用新数据更新模型参数[Mikolov et al.2010]）等经典技术可以部分缓解但不能完全解决这个问题。另一种技术已显示出一些前景，即根据语言模型要建模的时间段显式调节语言模型 [Dhingra et al.2021]。解决时间变化的另一种策略是设计基于检索的（半参数）模型，该模型使用从大型人类可解释数据库（例如维基百科文章）检索的附加上下文来增强模型输入[Karpukhin et al.2020; Lewis 等人，2020b； Guu 等人.2020; Khandelwal 等人，2020； Khattab 等人.2020]。对于基于检索的模型，适应对应于更新数据库中的各个信息单元（例如，百科全书文章的单个段落），而无需重新训练模型。虽然很有希望，但基于检索的方法在训练准确的检索机制和准确调节检索到的信息方面仍然面临挑战[Lewis et al.2020b]。我们将在本节后面的持续学习的更一般背景下重新讨论时间适应问题。领域专业化。除了任务专门化之外，通常还需要将基础模型专门化到特定领域（例如法律文档），而不限制基础模型可以完成的任务的广度。这种专业化导致基础模型训练和适应数据分布之间不匹配（§4.8：鲁棒性），这可能需要新的适应方法来处理。先前的工作发现，多样化和通用的预训练数据可能会导致当前适应方法的负迁移。例如，Cole 等人 [2021] 表明，微调仅在 iNaturalist 动物分类数据集上预训练的模型比微调在 iNaturalist 和 75 万张其他图像上预训练的模型提供更好的下游性能；类似地，LegalBERT [Chalkidis et al.2020] 仅在法律文档上进行预训练，比 BERT [Devlin et al.2019] 有所改进，BERT 在文本分类和序列标记的下游任务上使用更加多样化的训练集进行训练合法文件。领域专业化的一种方法是包括中间适应步骤，其中基础模型继续对来自专业领域的未标记数据进行训练。例如，这种方法显着提高了卫星图像和专门文本主题的下游性能[Reed et al.2021； Gururangan 等人，2020]。然而，在某些领域（例如法律文件），持续的基础模型训练可能比从头开始重新训练表现更差 [Chalkidis et al.2020]。阐明持续训练对绩效有好处或无好处的场景是未来工作的重要方向。本地模型编辑。在某些设置中，局部调整基础模型很有用，这意味着模型的预测分布应仅针对单个输入或单个输入周围的局部基础模型邻域进行调整，而不改变模型的预测分布。模型对不相关输入的行为。例如，当基础模型对特定输入短语和目标语言产生特别有问题的误译时，需要纠正这种误译而不影响模型对不相关短语的行为。过去的工作研究了通过新的预训练目标将近似局部更新应用于大型神经网络的问题，这些目标可以使用标准梯度下降进行轻松编辑[Sinitsin et al.2020]，预测基础模型参数编辑的高阶网络[Cao et al. 2021 年；米切尔等人，2021； Hase et al.2021]，以及约束微调程序[Zhu et al.2020]。然而，现有方法的可靠性各不相同，它们可以在不损害全局模型性能的情况下执行模型编辑。此外，由于这些方法的规模和需要计算高阶梯度的训练目标的计算成本，将这些方法扩展到大规模基础模型并不简单[Sinitsin 等人，2017]。 2020；曹等人。 2021 年；米切尔等人。 2021]。应用约束。在某些情况下，需要调整基础模型以满足隐私约束。例如，Carlini 等人[2021] 证明，现有的基础模型能够记住训练数据中的敏感信息，并且在通过标准 API 查询时可以反省此类数据。虽然这种现象需要改进数据管理，但开发消除或减少特定数据示例对训练模型的影响的适应程序将是一种补充解决方案。在这个方向上改进的适应策略（以及更好的预训练方法）也将使在通用数据保护条例（GDPR）下使用基础模型的机构受益，因为该授权赋予用户被遗忘的权利。在研究机器取消学习这一主题时 [Bourtoule et al.2019; Cao and Yang 2015]已经开始受到关注，但该问题尚未针对基础模型进行深入研究。此外，在较少管理的互联网数据上训练的基础模型已被证明表现出针对特定群体的有害偏见（例如性别和种族偏见）[Bender et al.2021；巴斯塔等人，2019；栗田等人，2019； Shen et al.2019]并且可以产生有毒输出[Gehman et al.2019]。 2020]（§5.2：滥用）。虽然在精心策划的数据集上进一步微调基础模型（可能存在多代）[Solaiman and Dennison 2021]或应用可控生成技术[Keskar et al.2019]等策略在减轻有害行为方面取得了一些成功，但框架为了训练公平和安全的基础模型（§5.1：公平性），可能需要在数据收集、训练和适应阶段进行集体努力以及与领域专家的协商进行进一步研究。 

4.3.3 基础模型适应研究的长期目标

就适应而言，适应 (adaptation) 涉及**将模型的现有知识与新数据或目标有效地整合起来**，适应的自然延伸就是**持续学习**[McCloskey and Cohen 1989； Parisi et al.2019] 或持续适应。不断调整基础模型的能力是可取的，无论是使模型的知识不断更新世界事件或文化发展，不断添加来自全新领域或模式的数据，还是不断编辑模型的记忆随着社会价值观或法律的发展，遵守隐私或法律约束。然而，持续学习问题通常会导致灾难性遗忘[McCloskey and Cohen 1989；拉特克利夫 1990； Kirkpatrick et al.2017]在神经网络中，随着训练分布的变化，旧的任务或数据很快就会被遗忘。我们认为基础模型的持续适应是未来基础模型适应研究的巨大挑战。应对这一挑战需要缩小在来自不同任务、领域或时间段的非平稳数据流上持续训练的基础模型与从聚合混合物的 iid 数据训练的相同基础模型之间的性能差距 [Lopez-Paz 和 Ranzato 2017]。为此，模型架构和训练 90 基础模型研究中心 (CRFM) 目标方面的创新可能是必要的。例如，虽然记忆机制长期以来一直被认为是成功持续学习的关键[French 1999]，并且已经显示出对基础模型的一些希望[Lewis et al.2020b； Guu 等人.2020; Borgeaud et al.2021]，有效利用更新的记忆仍然是一个具有挑战性的问题[Zhang and Choi 2021]。此外，还有用于在基础模型中本地化知识的技术，以便进行有针对性的参数更新[Dai 等人。 2021a]或者学习这样的更新规则[Cao et al. 2021 年；米切尔等人。 2021]可能有助于防止遗忘，但重复应用此类更新仍然会导致严重遗忘[Hase et al.2021]。持续的基础模型适应可能还需要对灾难性遗忘问题如何在基础模型规模上表现出来有新的理解，以及开发新的元学习技术[Schmidhuber 1987；桑托罗等人，2016； Finn et al.2017] 改进已学信息到新设置的前向传输。持续对已部署的基础模型收集的经验进行训练，甚至对许多不同模型收集的汇总经验进行训练，可能会加快基础模型开发的进度，但会带来反馈循环的风险，并削弱模型行为与利益相关者价值观的一致性。尽管存在上述挑战，持续的基础模型适应有望更快地响应社会文化价值观的变化，更好地利用现有知识来学习新概念，减少对环境的影响，并通过消除计算负担来提高基础模型的可访问性。基础模型的机遇与风险 91 

## 4.4 评估 

作者：Rishi Bommasani、Kawin Ethayarajh、Omar Khattab 4.4.1 

简介。评估为机器学习模型提供了背景：它作为一种手段（1）跟踪进度——我们如何衡量模型的性能以及如何设计改进的模型（§4.1：建模）； (2) 理解——模型表现出哪些行为（§4.11：可解释性）以及它们在不同数据切片上的表现如何（§4.8：稳健性）； (3) 文档——我们如何有效地总结模型行为并将其传达给不同的利益相关者。对于基础模型来说，评估的每一个目的都至关重要，但基础模型的性质带来了其他人工智能或机器学习设置中通常不会遇到的新挑战：（1）跟踪进度需要相对比较，但比较基础模型很复杂事实上，基础模型必须进行调整（可能以不同的方式）来执行任务。 (2)理解需要对正在评估的内容有特定的预先知识（例如分类法），但基础模型获得在设计评估时很难预测的新兴技能（例如上下文学习）。 (3)文档需要明确的需求来为决策提供有意义的信息，但基础模型可以适应无数的应用，这使得全面的文档具有挑战性。为了引导评估基础模型的讨论，我们区分了从基础模型的抽象中产生的两类评估：基础模型的内在评估，由于这些模型的任务不可知性，它本质上脱离了特定的任务，以及特定任务模型的外在评估，必然依赖于基础模型和适应机制。此外，我们认识到，由于基础模型的预期影响和范围，各种利益相关者（例如基础模型提供商和应用程序开发人员、审计师和政策制定者、从业者和研究人员）将需要对基础模型和特定任务衍生品进行评估，这些评估服务于不同的目的，并涉及基于利益相关者的不同需求。考虑到这一点，机器学习模型评估的标准范式并不是专门为基础模型的设置而设计的。因此，我们强调内在评估（§4.4.2：评估-内在）、外在评估中适应的重要性（§4.4.3：评估-适应）和评估设计（§4.4.4：评估-设计）。迈向更适合基础模型的评估框架的步骤。这一讨论有助于围绕机器学习系统评估的作用进行更广泛的对话[Galliers 和 Spärck Jones 1993；利普顿和斯坦哈特 2019；里贝罗等人，2020；林岑 2020; Kiela 等人，2021；米利等人，2021；雅各布斯和瓦拉赫 2021；鲍曼和达尔 2021； Dehghani 等人，2021； Ma et al .2021a，除其他外]，并且考虑到评估的复杂性，可能会受益于机器学习之外存在的测量和评估理论[Messick 1987；杰克曼2008；洛文格 1957；梅西克1988；手2010；布鲁尔和克拉诺 2014]。 

### 4.4.2 内在评价

传统上，机器学习系统的评估是以任务为基础的，通常是那些被设想为对应用程序特别有用的功能（例如翻译、对象识别）的任务。相比之下，由于基础模型是中间资产，必须进一步调整或专门化才能执行有用的任务，因此必须改变标准评估范式，以促进对基础模型的直接理解和比较。 92 基础模型研究中心 (CRFM) 1方法是根据与训练目标相关的任务来评估基础模型。例如，像 GPT-3 这样的语言模型是通过在给定先前上下文的情况下预测下一个单词来训练的，可以根据在保留的测试数据中给定其先前上下文的情况下分配单词的概率来评估（即，语言的困惑度）建模基准，例如 LAMBADA [Paperno et al.2016]）。到目前为止，这种方法在 NLP 领域已经显示出前景，但我们发现它表现出两个基本局限性。首先，依靠训练目标进行评估缺乏通用性：使用不同的不兼容目标训练的基础模型无法在一致的框架中轻松比较或理解。其次，以这种方式进行的评估依赖于有意义的代理关系，即，根据训练目标进行的测量应该与其他更有意义和更容易理解的量（例如，通过基础模型生成的内容的质量）相关。虽然这种代理关系过去在某些情况下已被证明是稳健的，但在评估基础模型的更多样化的功能、它们在更多样化的环境或领域中的行为以及超出域内准确性的考虑因素时，它可能会崩溃（我们讨论了这一点）更广泛地参见§4.4.4：评估设计）。鉴于这些限制，我们预计需要考虑两种方法，以提供互补的优势。从广泛的外在评价中推算出内在评价。评估基础模型的一种途径是使它们适应广泛的任务，并测量所得到的特定任务模型的性能。由于基础模型是所有这些模型的共享基础，因此总体性能反映了该共享基础的性质和质量。目前，人工智能的许多子领域已经开始构建元基准，即整合多个不同任务或领域的个人评估的单一评估[Wang et al.2019b,a;胡等人.2020; Santurkar 等人，2020； Gehrmann 等人，2021；亨德里克斯等人。 2021b； Koh 等人，2021； Tamkin 等人.2021b]。鉴于这种范式的日益普及及其既定的优势，我们在这里指出为什么它可能不足以完全满足基础模型的评估目标。元基准评估需要适应（至少将基础模型专门化到元基准中的每个任务），考虑到所涉及的添加过程（即适应），这使得对基础模型本身的推理变得具有挑战性。具体来说，这使得进展问题变得复杂，无论是在跟踪方面（例如，性能是否归因于有效的基础模型或精心设计的适应实践），还是在识别用于学习基础模型的过程中的改进（例如，基本模型）方面。通过比较两个基础模型在元基准上的性能，可能很难识别数据选择（§4.6：数据）、训练目标（§4.2：训练）和模型架构（§4.1：建模）方面的改进。此外，这种评估范式使得理解或记录特定于基础模型的属性和功能变得困难，这可能导致难以向某些利益相关者传达信息（例如，SuperGLUE 的性能可能没有提供足够的信息，或者可能会误导政策制定者） ）或用作预测他们在新任务或领域的行为的基础。直接评估内在特性。为了补充元基准的使用，我们还论证了为什么直接测量基础模型的属性（例如，特定能力或偏差）是有价值的，脱离特定任务。59例如，我们可能会努力直接测量基础模型的语言能力识别句法有效和无效句子的基础模型。为了激发这种方法的价值，我们回到评估的目的。值得注意的是，59严格来说，这些直接评估可能仍然涉及将公式化为任务和基础模型专门化来执行任务，但目标更类似于尝试尽可能直接地测量基础模型的探索（参见§4.11：可解释性）关于基础模型的机会和风险 93 阐明能力、技能和偏见的存在和强度，确定需要改进的具体领域（进展），阐明当前潜力（理解），并有效地表达相关方面（文档）。这种方法还服务于广泛可理解的评估，即技术专家、非技术专家（例如政策制定者或社会科学家）和一般目的都可以理解的评估。例如，表征这些模型的说服或修辞能力对于内化其虚假信息和滥用的潜力可能特别直观（§5.2：滥用）[Buchanan 等人。 2021]。直接评估属性也是更好地处理基础模型的新兴属性的重要途径；为了证明这一点，我们以情境学习作为案例研究。特别是，Brown 等人[2020]不仅证明了 GPT-3 强大的上下文学习的标志性能力，而且是第一个明确将上下文学习识别为适应模型并与模型交互的特定方式的人（通过其GPT-3 的探索）。传统的基于任务的外在评估没有提供明确的方法来识别情境学习；在这种情况下，与基础模型直接交互似乎是必要的。更一般地说，虽然通过对这些模型及其能力的非结构化或松散结构的探索，许多意想不到的现象（例如情境学习）似乎不可避免地会被认识到，但我们认为应该寻找新的评估方法来构建这种探索，或者更多雄心勃勃地提出新的特性，然后对其进行更严格的测试。内在评估还可能降低展示基础模型潜力的门槛；如果基础模型的新方法能够证明内在评估方面的改进，即使它们没有立即伴随相应的非常适合的适应方法来在外在评估中引发这些能力，它们也可能是足够有前途的。如何实施内在评价是一个重要的悬而未决的问题；这种评估的机制尚不清楚。我们列举了一些可能有助于指导内在评估的设计和执行的一般原则和注意事项。 (1)来自人类评价的启示。我们对基础模型感兴趣的许多相关属性、能力和偏差也对人类感兴趣，这表明测量人类这些属性的方法可能对评估基础模型具有指导意义，甚至可以直接转化。例如，可以修改人类语言能力的心理语言学测量来评估基础模型语言能力[Levy 2008；弗兰克等人，2013； Linzen 等人，2016；埃廷格和林岑 2016；马文和林岑 2018；范·斯金德尔和林岑 2018； Futrell 等人，2019；普拉萨德等人。 2019； Ettinger 2020]或人类社会偏见的心理测量可以被修改以评估基础模型社会偏见[Greenwald et al.1998； Caliskan 等人，2017；梅等人。 2019；郭和 Caliskan 2021]。 (2)人在环评估。人机交互评估可能被证明对于提供一种更具探索性的方法来理解基础模型至关重要，包括评估其生成或交互能力。特别是，人类与基础模型的直接交互可以更好地识别其新兴的能力和局限性，并且对基础模型的直接审计[例如，Raji 和 Buolamwini 2019，§5.6：道德]可以推进文档和透明度的目标。 (3)内在措施的有效性。虽然内在测量允许在源头进行直接测量，即独立于适应和特定任务来测量和评估基础模型的属性，但它们对建立对评估有效性的信任提出了挑战[Messick 1987, 1988]。特别是，外在评估结果对于验证内在措施设计也可能很重要，例如，内在94基础模型研究中心（CRFM）措施的预测有效性（即，它们（统计）预测相关下游结果的能力）可能被证明成为一个中心标准。 

### 4.4.3 外部评估和适应

评估特定于任务的模型历来涉及报告模型在特定测试集上的性能（通常指准确性）。虽然这种范式可能部分足以理解或记录模型，但它通常相当于对使用不同（并且可能是不平等的）资源生成的特定于任务的模型进行不公平的比较，使得很难衡量已经取得了多少进展。在基础模型体系中，不公平比较的担忧更加严重：不同的基础模型（例如 BERT 和 GPT-3）可能构成不同特定任务模型的基础，并且这些基础模型可能涉及截然不同的训练数据量和计算量。为了说明实现特定绩效水平所需的资源，Linzen [2020] 认为，应在评估中承认和跟踪（预）训练资源。我们认为这是一个有科学原则的建议；在不考虑训练资源的情况下比较不同的训练基础模型方法可能会产生误导。然而，鉴于创建基础模型的过程特别昂贵（例如，需要大量的人力和财务资本），并且除了科学因素之外，通常还受到社会因素（例如，商业激励）的控制，因此基金会可能会面临这样的情况：实践中的模型在提供的训练资源方面会有很大差异，这使得控制比较变得困难。在这里，我们考虑一种可能更普遍可行的替代方案，部分考虑补充 Linzen [2020] 提案所涉及的资源。特别是，我们考虑为什么外在评估应该承认适应资源，这对于确保外在评估能够识别最有效的适应方法至关重要（从根本上说，内在评估无法做到这一点）。我们提请注意这样一个事实，即适应资源通常被解释为用于适应模型的数据，但附加资源[例如，用于选择适应方法的数据； Perez et al.2021]和约束（例如，适应基础模型所需的访问级别；参见§4.3：适应和§5.6：进一步讨论的伦理）。适应资源的核算。考虑到使基础模型适应特定任务所花费的资源需要完全了解不同适应方法使用了哪些资源或约束，即，努力考虑这些资源的评估必须随着资源使用情况的发展而发展。适应（§4.3：适应）。在现有的特定于任务的评估中，大多数评估指定可用于使（基础）模型适应任务的数据量。然而，Perez 等人[2021]在这里发现了一个在过去的工作中被忽视的关键细微差别，因为这应该封装用于通知适应的所有数据，即用于适应基础模型的数据和用于适应基础模型的数据。选择适应方法。此外，在基础模型体系中，不同适应方法的接入要求的概念也是评估中应考虑的新考虑因素。具体而言，一些适应方法通常可能优于其他方法，但与其他方法相比可能需要更大的访问或修改基础模型的能力（例如，微调需要基础模型梯度来修改基础模型，而提示可能只需要在指定输入时进行黑盒访问） ）。考虑适应过程中涉及的资源丰富了从特定任务模型的评估中可以合理得出的结论。目前，特定于任务的评估可以为特定特定任务工件（即正在评估的确切模型）的某些类型的理解或记录提供足够的清晰度，但没有提供关于不同适应方法如何执行以及如何选择适应方法的清晰信号。给定上下文中的特定适应方法。在《关于基础模型的机遇和风险》95 中，相比之下，通过考虑适应所涉及的资源和获取要求，评估可以更好地使研究能够确定哪些适应方法或过程能够最好地利用所提供的资源，即，提供信号不仅仅是为了正在评估的特定工件，以及派生它们的更一般的过程。因此，拟议的评估协议显然致力于确定应使用哪些适应方法；我们注意到，所有这些结论都应始终被视为特定于给定的基础模型，因为这种形式的评估不能提供足够的证据来得出结论，适应方法在所有基础模型中都是最佳的。

### 4.4.4 评估设计

理论上，评估的目标是衡量和表征各种理论构造（例如，准确性、稳健性（§4.8：稳健性）、公平性（§5.1：公平性）、效率（§4.5：系统）、环境影响（§5.3：环境））服务于各种目的（即进步、理解、文档）。然而，在实践中，评估的效用将取决于评估的设计和执行方式。例如，对基础模型的生成能力（例如，它们的事实正确性）的自动测量可能很难捕捉到这些质量的本质，相反，人机交互评估可能会更好地将这些能力置于上下文中。在考虑我们设想的基础模型及其改编衍生品的评估设计时，我们从评估机制开始。传统上，机器学习模型的评估涉及用于学习模型的大型训练集、用于设置超参数的可选验证集以及用于评估学习模型对保留数据的泛化能力的测试集[主教2006]。因此，创建基准来评估模型历来需要大量数据，其中大部分数据分配给训练，当数据稀缺或获取成本昂贵时，这会使某些诊断或细致评估的设计变得复杂[Rogers 2020，2021]。相比之下，由于基础模型的好处通常与适应的样本效率（即少样本或零样本能力）以及可能应用的多样性相一致，因此我们设想了一种制度，其中单个任务的基准要小得多（因为需要作为“训练”提供的数据少得多，即适应数据）并且更加多样化（既捕获内在评估的各种能力，又以生态有效的方式进行更强有力的地面评估[Bronfenbrenner 1977; de Vries et al .2020]在外部评估期间）。这表明基础模型的性质可能会导致基准（以及构建基准的心态）性质的转变，不再强调数量作为基准的关键优先事项，而不是质量和NLP 社区已经开始看到这样一个制度的开端，它具有广泛且多样化的基准，如 BIG-Bench61 和 FLEX [Bragg et al.2021]；这种范式降低了基准设计的障碍，从而使更广泛的社区能够参与进来评估设计。62 除了评估机制之外，评估结果的呈现和界面还告知如何使用这些结果来指导决策（例如，新的建模方法、模型选择、审计）。排行榜已成为机器学习事实上的范例，其中模型根据特定且单一的标准（通常是准确性的一种形式）进行排名。随着时间的推移，这种方法通常会导致系统质量显着而快速的进步[例如，Wang 60当前的结果相反，表明不同的适应方法更适合不同类型的基础模型和训练目标[Liu et al. 2017]。 2021 年；莱斯特等人。 2021]。 61https://github.com/google/BIG-bench 62传统上，ImageNet [Deng et al.2009] 和 SQuAD [Rajpurkar et al.2016] 等基准测试的设计是由资源丰富的研究实验室进行的，这些实验室有能力通过众包支付创建这些数据集的费用 [Rogers 2020].96 基础模型研究中心 (CRFM) 等人 2019a]，但人们对这是否会产生更普遍的改进提出了重大担忧 [例如，Linzen 2020 ; Bowman 和 Dahl 2021].63 正如所有机器学习模型一样，基础模型及其衍生模型的需求很少是单一的；相反，我们预计其应用的广度和社会影响需要更多地考虑准确性以外的标准（例如稳健性、公平性、效率和环境影响）。为此，我们注意到基础模型的评估应该报告这些不同方面的测量结果；现有的基准越来越多地被设计为不仅仅反映准确性（例如，鲁棒性[Koh et al.2021；Goel et al.2021]、公平性[Nadeem et al.2021；Nangia et al.2020]、效率和环境影响[Coleman]等人.2017]）。此外，我们注意到，如果以排行榜的形式报告不同类别的表现，那么消除潜在权衡歧义（以产生排名）的机制将特别必要[Ethayarajh 和 Jurafsky 2020]。特别是，由于不同的利益相关者会有不同的偏好（例如，他们赋予不同属性的权重）和价值观[Birhane et al.2020]，排行榜设计应允许利益相关者互动并操纵排名的完成方式以符合他们的价值观; Ma 等人[2021a] 提出了一种早期尝试，通过使用基于用户指定效用函数的经济框架来比较模型的效用，从而实现这一目标。 

### 4.4.5 要点

评估扮演着对所有机器学习范式（包括基础模型范式）至关重要的多种角色（即进展、理解、文档）。基础模型给现有评估框架带来了新的挑战；设计直接针对基础模型制度的评估不仅可以更好地服务于评估的多重目的，而且可以服务于众多利益相关者。 (1)虽然机器学习评估传统上考虑了特定于任务的模型，但评估基础模型涉及到这些模型并不特定于任务的事实。这些模型的评估可能涉及整合两种互补的方法：（a）通过对特定任务导数的广泛评估来估算基础模型的属性，以及（b）直接测量基础模型中的这些属性。 (2)现有的评估框架往往没有考虑创建被评估模型所需的资源，导致比较不公平。对于基础模型，我们讨论了一种评估范式，强调考虑适应资源（例如，适应中使用的所有数据、基础模型的访问要求），这似乎会导致信息更丰富的评估，从而更好地塑造适应的进行方式。(3)现有的评估设计往往受限于所考虑的指标的多样性，并且需要大量的适应数据集。对于基础模型，我们呼应越来越多的呼吁，要求评估考虑更广泛的需求（例如稳健性、公平性、效率、环境影响），以捕获广泛的利益相关者价值观/偏好，并强调适应的样本效率如何通过重新分配设计评估所涉及的资源，模型可以允许更加多样化的评估。 63我们注意到与斯特拉森定律 [Strathern 1997]（有时称为古德哈特定律 [Goodhart 1984]）的联系：“当一项措施成为目标时，它就不再是一个好的措施。”关于基础模型的机遇和风险 97 

## 4.5 系统

作者：Deepak Narayanan、Trevor Gale、Keshav Santhanam、Omar Khattab、Tianyi 张、Matei Zaharia 图 19. 显示基于 Transformer 的语言模型的参数数量和训练操作 (FLOP) 数量增长的图（如图所示）蓝色），以及 NVIDIA P100、V100 和 A100 GPU 的内存容量和峰值设备吞吐量（红色）随时间的变化。最先进语言模型的增长率（每条线的斜率）（大约 10 ×年）远远超过硬件计算能力的增长速度（四年约10×），激发了对大量加速器并行性以及算法、模型、软件和硬件协同设计的需求来驱动参数数量和训练操作数量从相关论文[Brown et al.2020]中获得，内存容量和峰值吞吐量从GPU规格表中获得。计算机系统是开发基础模型的最大瓶颈之一。基础模型通常太大，无法容纳单个加速器（例如 GPU）的主内存，并且需要大量计算来训练（例如，GPT-3 >1000 petaFLOP/s-days [Brown et al.2020] ]）。此外，随着时间的推移，这些模型可能会变得更大：例如，最先进的语言模型的计算和内存需求在过去三年中增长了三个数量级，并且预计将继续以更快的速度增长比硬件功能（图 19）。一旦经过训练，这些大型模型执行推理的成本高昂，并且在生产应用程序中难以调试、监控和维护。我们相信，基础模型的性能和可用性的进一步进步将需要跨算法、模型、软件和硬件系统的仔细协同设计，以及用于编程和部署机器学习应用程序的新接口。在本节中，我们讨论开发和生产大型基础模型时面临的关键计算机系统挑战。 

### 4.5.1 通过协同设计提高性能

如今，训练大规模基础模型[Brown et al.2020； Rae 等人 2021； NVIDIA 和微软 2021； Wang 和 Komatsuzaki 2021] 通常需要定制软件系统，例如 Megatron、DeepSpeed 或 Mesh Transformer JAX [Shoeybi et al.2019；拉斯利等人.2020; Wang 2021]，构建在 PyTorch、TensorFlow 和 JAX 等标准框架之上 [Paszke et al.2019；阿巴迪等人，2016；布拉德伯里等人。2018]。这些软件系统依赖于整个堆栈中的大量创新来大规模有效地训练模型：新的并行化维度，例如 pipeline98 基础模型研究中心 (CRFM) 并行性 [Huang et al.2019; Narayanan et al.2019]在保持设备繁忙的同时限制通信，状态分片优化器可减少内存使用[Rajbhandari et al.2020]，即时（JIT）编译器可优化计算图[PyTorch 2021]，以及cuDNN 和 NCCL 等优化库 [NVIDIA 2021]。 Megatron 和 DeepSpeed 对于特定规模而言是高效的；例如，Megatron 可以在具有万亿个参数的模型上提取大约 3000 个 GPU 的现代硬件理论峰值吞吐量的 52% [Narayanan 等人，2017]。 2021b]。然而，扩展到具有更多 GPU 的更大模型仍然具有挑战性，因为现有的并行化策略在更大的 GPU 数量下会崩溃。数据并行性受到批量大小的限制 [Li et al .2020e]，管道并行性受到模型层数的限制 [Huang et al .2020e]。 2019； Narayanan et al.2019]，以及单个服务器中 GPU 数量的张量模型并行性 [Shoeybi et al.2019]。 2019]。虽然我们将继续从新硬件中实现性能提升，但大型模型的资源需求增长远远超过了各代硬件的改进[Brown et al.2020]。为了促进模型能力的下一次重大飞跃并使模型质量的进步民主化，协同设计训练算法、模型、软件和硬件将变得越来越重要，因为许多显着提高性能的途径改变了模型的语义。训练计算。例如，以较低精度执行操作（例如 fp16 ）有助于提高现代硬件的吞吐量（例如，V100 和 A100 GPU 具有用于较低精度矩阵乘法的专用张量核心单元），但也会影响优化过程的数值[Micikevicius 等人.2017]。同样，利用权重稀疏性可以显着缩短训练和推理时间 [Elsen et al.2020； Gale et al.2020]仅对模型中的非零点执行数学运算，但需要不同的训练算法[Jayakumar et al.2020]。 2021 年； Evci 等人.2020; Dettmers 和 Zettlemoyer 2019]。协同设计的其他示例包括更有效地映射到硬件的模型架构[So et al.2019; Child 等人，2019；王等人。 2020c； Lee-Thorp 等人，2021； Kitaev 等人 2020； Beltagy 等人 2020； Tay 等人 2020； Ren et al.2021]，高效优化器[Anil et al.2020； Shazeer 和 Stern 2018]，新颖的标记化替代方案 [Xue et al.2021； Tay et al.2021]，专门设计的硬件训练平台[Jouppi et al.2017； Mudigere 等人，2021； Selene 2021]，以及具有宽松权重更新语义的分布式并行化策略 [Narayanan 等人。 2019、2021a]。案例研究：有效的知识表示。作为成功协同设计、基于检索的模型（如 REALM、RAG、ColBERT-QA 和 RETRO）的具体案例研究 [Guu et al.2020; Lewis 等人，2020b； Khattab 等人 2020； Borgeaud et al.2021] 采用了不同的模型设计方法，而不是简单地增加模型参数的数量。基于检索的模型不是尝试将越来越大的数据集中的隐性知识直接积累到具有数十亿个参数的 DNN 模型（如 GPT-3）中，而是以文本段落的形式存储模型参数之外的知识，捕获段落内的知识具有密集的向量表示。然后，这些模型使用可扩展的 top-𝑘 搜索机制来提取与每个输入相关的知识，同时保持 DNN 模型本身较小（第 4.1.4 节：建模内存）。这种设计提高了计算效率以及模型在生产中的可维护性：例如，开发人员只需替换文本段落即可更新模型的知识，而无需重新训练大型 DNN。基于检索的模型通过利用几个新的跨功能想法，包括在训练期间通过检索器反向传播损失，取得了有希望的初步结果[Guu et al 2017]。 2020]（这需要通过由数百万个段落组成的知识存储来近似梯度）并对查询和段落之间的细粒度交互进行建模[Khattab 和 Zaharia 2020； Khattab et al.2020]（这需要将计算分解为向量级最近邻搜索操作）。这些技术允许基于检索的模型准确且高效，但流行的 ML 框架和最近邻索引（例如 FAISS [Johnson 等人，2017]）不容易支持所需的功能。 2019]。 

### 4.5.2 自动优化

系统中的另一个重要挑战是自动化跨算法、模型、软件和硬件的优化应用。虽然许多优化和并行化策略是互补的，但确定最有效的优化组合具有挑战性，因为联合搜索空间以组合方式增长，并且优化以非平凡的方式相互作用[Narayanan et al.2021b]。基础模型提高了对自动优化的需求，因为在数千个 GPU 的规模上进行手动实验极其昂贵且耗时。该领域最近的工作重点是针对语义保留优化的系统。特别是，已经提出系统可以自动发现数学上等效的图替换[Jia et al.2019a； Wang et al.2021c]，通过高级 API 和低级编译器促进运算符图的分布式执行[Rasley et al.2020；曼迪普·贝恩斯 2021；布拉德伯里等人，2018； Shazeer 等人，2018； Lepikhin et al.2020]，并自动选择混合分发策略[Jia et al.2019b； Santhanam 等人，2021]。这些系统帮助在行业中部署了许多基础模型 [Fedus et al .2021; M2M-100 2020；图灵-NLG 2020]。不幸的是，在组合语义改变优化时（§4.5.1：系统协同设计），自动优化变得更加困难，因为通常不清楚如何联合建模这些技术的统计影响（例如，需要多少次训练迭代）达到特定的精度？）。因此，我们需要新的软件工具、库和编译器来自动识别针对诸如准确时间等综合指标的优化组合[Coleman et al.2017;马特森等人。2020]。构建此类工具需要系统和机器学习专家之间的密切合作。

### 4.5.3 执行和编程模型

基础模型独特的多任务性质提供了在许多应用程序中摊销训练和推理成本的机会。特别是，诸如适应之类的范式意味着模型实例之间的更多共享[Raffel 2021]。例如，来自同一预训练模型的两个经过前缀调整的模型 [Li and Liang 2021] 可以共享相同的模型“主干”，从而减少存储占用空间（共享主干只需存储一次），同时还可以跨前缀调整模型共享和批处理执行[Shen et al.2019； Narayanan 等人，2018]。因此，所使用的特定适应机制会影响系统优化（第 4.3 节：适应）。应该使用什么编程接口来指定各种适应模型源自相同的预训练模型（例如，模型𝑌和𝑍源自相同的预训练模型𝑋），或者两个模型的各个组件共享参数（例如，两个模型𝐴和𝐵共享相同的茎直到层𝑖）。 Ludwig [Molino et al.2019] 和 PyTorch 的模块提供了在模型中组合功能的简单方法，但目前没有系统支持跨模型依赖项。为用户提供提供注释的机会将使训练和推理系统能够更有效地优化和编排计算；如果没有这样的注释，系统将无法了解可以在模型实例之间共享哪些计算和参数。模型的“适应历史”（该特定模型改编自哪些模型）也可用于调试：适应模型在特定类型输入上的错误可能源自预训练模型，指出预训练过程与适应过程中的问题。 PyTorch 等框架以及 100 基础模型研究中心 (CRFM) 训练基础模型（例如 HuggingFace Transformers [Wolf et al.2020]）的软件库不允许指定整个模型实例的细粒度谱系信息。构建和维护数千个加速器集群也需要付出巨大的努力。新的训练范例，如 Learning@Home [Ryabinin 和 Gusev 2020； Diskin et al.2021]探索利用互联网上的志愿者计算来协作训练基础模型。这种全新的执行模型可以降低任何一个实体的训练成本，但需要跨多个不同领域进行协作，例如安全性（以确保恶意志愿者无法显着改变训练过程）、分布式系统（以处理容错问题）志愿者下降的问题）和众包。 

### 4.5.4 基础模型的产品化

随着社区不断推动基础模型的功能，实现其潜力将需要解决与在生产中部署这些资源密集型模型相关的挑战。这些挑战包括以严格的延迟目标执行模型推理，并确保以自动化方式监控模型和数据。对于具有严格成本和延迟限制的应用程序，可以使用蒸馏等模型压缩技术[Hinton et al.2015； Li等人.2020d； Sanh et al.2019]，量化[Polino et al.2018；古拉米等人，2021； Zhou et al.2018]，修剪[LeCun et al.1990；戈登等人.2020;麦卡利等人。 2019； Wang 等人.2019c； Sajjad et al.2020]，以及稀疏性[Gale et al.2020； Elsen et al.2020]可以通过转换更大的模型来帮助部署以获得所需的推理时间属性。这些技术最初适用于低内存环境（例如移动电话）中的较小模型（例如 BERT-L），但现在需要处理数据中心部署中现代基础模型的极端规模。传统上用于训练的张量模型并行性 [Shoeybi et al.2019] 等并行化技术也可能有助于减少推理延迟，并且还可以跨 GPU 提供额外的内存容量以适应模型的参数。除了这些实际限制之外，基础模型以及用于训练它们的数据集的大小和复杂性的增加给模型和数据集生命周期管理带来了新的挑战。由于具有大量参数的模型很难由人类手动检查，因此我们需要更好的系统来自动数据集管理（第 4.6 节：数据）和模型质量保证。行为测试等技术 [Ribeiro et al.2020] 和模型断言 [Kang et al.2020] 2020] 通过为最终应用程序中部署的模型提供单元测试、运行时监控（以测试时断言的形式）和持续模型改进（随着新输入的出现）的模拟，促进生产中更轻松的模型维护。这些工具可以帮助解决公平性和偏见问题（§5.1：公平性），并减少模型错误预测。关于基础模型的机遇和风险 101 

## 4.6 数据

作者：Laurel Orr、Simran Arora、Karan Goel、Avanika Narayan、Michael Zhang、 Christopher Ré 基础模型标志着范式转变，越来越多的数据被“输入”到这些模型中，以提高适应性能 [Devlin et al.2019；雷德福德等人，2021； Tolstikhin et al.2021]，总体经验法则是“数据越多越好”[Kaplan et al.2020]。正如前面几节提到的，对数据管理的关注引发了对基础模型数据生命周期的担忧，包括 (1) 管理如此大规模的数据（§1：简介），(2) 跨新模式集成数据（§2.3） ：机器人技术，§3.1：医疗保健），（3）对许可和治理法规的推理 - 特别是在考虑基础模型训练中使用的大规模网络爬虫时 - （§3.1：医疗保健，§5.4：合法性），以及（4）理解数据质量（§4.4：评估）。虽然基础模型给这些挑战带来了新的困难，但我们看到这些问题与数据管理和数据分析以及工业机器学习管道等社区的核心挑战之间存在相似之处。例如，数据管理长期以来一直在研究用于数据分析、版本控制、出处和集成的可扩展声明性系统，以解决挑战 (1) 和 (2) [Zaharia et al.2012; Cudré-Mauroux 等人，2009 年；斯通布雷克和韦斯伯格，2013；斯通布雷克和伊利亚斯 2018；海勒斯坦和斯通布雷克 2005]。行业拥有应对挑战 (3) 的管道，以管理不同的数据许可证并帮助减少数据违规。有一个完整的研究和系统生态系统来应对挑战 (4)，以支持交互式数据分析和可视化 [Hohman 等人 .2020]。64虽然这些解决方案不一定是“基础模型就绪”，但我们相信这是更好地管理基础模型数据生命周期应该从这些现有系统中汲取灵感。在本节中，我们将讨论管理基础模型数据生命周期。我们首先概述了四个需求，包括大规模数据管理、对异构数据源的支持、数据治理和数据质量监控。然后我们设想如何将所有这些需求集成到称为数据中心的整体数据管理解决方案中。数据中心简单来说就是一个数据管理工具包，可供私营或公共部门使用，以更好地支持基础模型数据生命周期的交互式管理。 

### 4.6.1 数据管理需求

当前基础模型开发的实践通常是在整个生命周期中临时进行的，从数据管理和数据文档到模型监控和修补[Gebru et al.2018;班迪和文森特 2021；本德和弗里德曼 2018]。数据管理社区的研究表明，定义明确的数据管理平台通过数据摄取、数据版本控制、数据来源、高效分析和模型监控促进了大规模 ML 模型开发 [Hellerstein 和 Stonebraker 2005；阿格拉瓦尔等人，2019； Vartak 等人，2016； Ikeda and Widom 2010].65从数据管理社区中汲取灵感，我们在为基础模型构建整体数据管理平台时考虑了核心需求。 (1)可扩展性。基础模型正在越来越多的数据上进行训练 [Kaplan 等人 .2020]，WuDao 2.0 模型正在 4.9 TB 的多模态数据上进行训练。 66随着大多数最新模型主要在公共 64VIS 上进行训练，这一规模预计会增加。 、CHI、SIGGRAPH 是研究交互式数据分析方法和系统的几个社区。 Pandas、Matplotlib 和 Seaborn 等软件系统和库也可以帮助用户进行交互式探索。像 Michelangelo 这样的 65 个特征商店也支持端到端 ML 模型构建 https://eng.uber.com/michelangelo-machine-learning-platform/。 66https://www.scmp.com/tech/tech-war/article/3135764/us-china-tech-war-beijing-funded-ai-researchers-surpass-google-and102 基础模型研究中心 (CRFM)面对数据集。与每天收集并用于工业基础模型管道的 PB 级商业和个人数据相比，公共数据仅占数据的极小一部分 [Marr 2017]。因此，对能够处理多模式基础模型数据集的高度可扩展的技术的需求不断增长。 (2)数据整合。最近使用基础模型的工作表明，利用集成的结构化和非结构化数据可以帮助模型更好地泛化到罕见概念[Orr et al.2020]并提高事实知识回忆[Orr et al.2020； Logeswaran 等人，2019；张等人.2019a；彼得斯等人，2019； Poerner 等人，2020]。尽管最近取得了这些成功，但集成基础模型的数据集仍然是一个挑战。许多作品使用具有结构化实体知识或图像数据的非结构化文本数据[Antol et al.2015]。人们越来越需要集成跨不同模式的数据集，例如文本、视频、眼球追踪 [Hollenstein et al .2020] 和机器人模拟 [Lynch and Sermanet 2021]（参见§2.3：机器人技术）。我们需要能够在工业规模上应用于多种模式和多个领域（例如政府、商业和科学）的数据集成解决方案。 (3)隐私和治理控制。用于基础模型的训练数据可能存在侵犯数据主体隐私的风险；他们的数据可能会在未经他们同意的情况下被披露、收集或使用 [Jo and Gebru 2020] 或在最初同意的范围之外。同意和使用问题与基础模型尤其相关，因为下游应用程序并不总是可预测的。正如第 5.4 节：合法性中所解释的，这些问题随着用于基础模型训练的网络抓取数据集的流行而变得更加复杂。由于网络爬取数据的管理和版权问题仍存在悬而未决的法律问题，67公共和私营部门的基础模型提供商仍不清楚使用网络数据的后果。我们需要工具来帮助基础模型提供商适应新兴的法规和指南，以确保安全和负责任的数据管理。 (4)了解数据质量。数据质量影响模型性能[Lee et al.2021b]；然而，系统地、可扩展地理解训练数据和相关数据子集的工具包或方法仍处于起步阶段。数据创建过程可能很混乱，并且数据可能包含不同类型的偏差 [Blodgett et al.2020; Bender et al .2021]（参见§5.1：公平性）并且包含有毒、虚假或重复的信息[Chang et al.2020；卡利尼和特尔齐斯 2021；布坎南等人，2021； Lee 等人.2021b]。数据也在不断更新和完善[Kiela et al.2021]，并且可能具有新兴实体[Fetahu et al.2015]、分布转变[Chen et al.2021a]和概念意义转变[Kenter et al.2015； Lazaridou 等人，2021]。此外，一旦部署，基础模型可能会在关键的、细粒度的数据子群上出现不良行为，而基础模型提供商需要检测和缓解这些行为[Goel et al.2021；霍曼等人，2018； Ré 等人，2019； Oakden-Rayner 等人，2019]。我们需要能够检测并可能减少不同类型的不良数据的工具包，以交互式和迭代的方式提高模型性能。此类工具包还需要适应训练数据的动态特性。 

### 4.6.2 数据中心解决方案

凭借多年的数据管理、数据科学和数据分析工作，我们设想了一个基础模型生命周期数据管理解决方案，我们将其称为数据中心。虽然示例 67 最近围绕 Copilot 的 Codex 工具中使用 GitHub 数据来帮助开发人员编码的争论引发了这些问题：https://www.pwvconsultants.com/blog/questions-around-bias-legalities-in-githubs- copilot/关于以 ML 为中心的数据中心 68 的基础模型 103 的机遇和风险以及更传统的数据管理系统的存在，69 它们要么 (1) 不将数据集成视为第一类原语，(2) 本身不支持具有模型预测的端到端生命周期，或者（3）不允许交互驱动的数据管理和细化，其中基础模型提供者可以根据访问控制指南动态探索和更新可能的数据集。我们现在讨论数据中心如何解决这四个需求。数据规模。为了应对大规模管理挑战，数据中心将需要标准数据管理解决方案[Armbrust et al.2009]，例如用于存储和维护随时间变化的大规模数据集的基础设施，以及用于查询、选择和过滤的可扩展接口数据集。该中心应支持异构计算和云基础设施，以支持不同环境中的可扩展解决方案。数据整合。该中心应将数据集成作为一等公民。它将需要先进的数据集成解决方案 [Stonebraker 和 Ilyas 2018；阿比特布尔 1997；董等人.2020； Rekatsinas 等人.2017a]70 允许跨模式和领域合并结构化和非结构化知识。此外，这意味着中心需要支持异构数据集和源的存储和查询。访问控制。考虑到中心的访问控制，中心将需要支持不同的文档，例如数据集表[Gebru et al.2018]或数据声明[Bender and Friedman 2018]，以允许数据管理者反思他们的流程并保持透明关于其数据集的预期用例、潜在偏差和局限性。数据中心需要决定上传数据需要哪些文档（例如，数据源和数据描述）以及推荐哪些信息（例如，数据可以用于什么任务）。此外，随着数据集的发展，文档可能需要更新[Goel 等人。 2021]。数据源通常与许可证相关联，中心需要整合具有不同法律问题和条件的不同来源 [Masur 2018]。71此外，某些数据集具有保护数据主体隐私的法律指南。该中心需要采取方法来确保数据集不会泄露个人身份信息 (PII)，72 匿名或去识别化数据的聚合不会泄露 PII，73 并且数据主体已对其数据的传播给予知情同意。 74 借鉴数据集成的思想[Rekatsinas et al.2017b]，该中心应支持能够高效、安全地维护和共享数据资源的机制。特别是由于某些公共数据集（例如网络转储）的合法性仍在决定中（§5.4：合法性），该中心迫切需要工具来帮助识别许可违规行为并减轻任何治理违规行为的影响。由于某些违规行为可能与模型行为有关，因此我们需要系统来支持更好地理解模型行为，如下所述。数据质量工具。利用数据分析和探索领域，当用户交互式地选择、过滤和提炼数据以用于训练或适应时，该中心将需要工具来快速68一些公共数据中心包括：https://data.world/、 https://dataverse.harvard.edu/dataverse/harvard、https://datacommons.org/、https://www.data.gov/、https://www.kaggle.com/、https://huggingface .co/datasets，https://www.ldc。 upenn.edu/ 69一些用于基础模型的传统数据管理系统包括：https://aws.amazon.com/big-data/datalakes-and-analytics/、https://eng.uber.com/michelangelo-machine-learning -platform/，https://kafka.apache.org/ 70https://www.tamr.com/ 71https://content.next.westlaw.com/4-532-4243 72https://www.justice.gov /opcl/privacy-act-1974 73http://www2.ed.gov/policy/gen/guid/fpco/ferpa/library/georgialtr.html 74https://www.dhs.gov/sites/default/files/publications /privacy-policy-guidance-memorandum-2008-01.pdf104 基础模型研究中心 (CRFM) 了解用户当前的数据集及其对模型行为的影响 [Hohman 等人 .2020].75 此外，这些系统可以允许结束通过最近关于切片（子群体）发现的工作结合模型性能来进行端到端基础模型监控[Chung et al.2019]，相关子集的模型验证[Goel et al.2021； Ribeiro et al.2020]，以及数据评估[Ghorbani and Zou 2019]。最近的工作还提出了使用模型来检测哪些数据子群对给定输出贡献最大的方法，以进一步帮助模型调试 [Keskar 等人，2017]。 2019]。一旦用户可以监控模型行为（尤其是罕见但关键的子群体），该中心就应该为用户提供通过纠正模型错误来维护模型的方法和指导。尽管“模型修补”[Goel 等人 .2020a] 仍然是一个悬而未决的问题，但 [Orr 等人 .2020] 的工作首次描述了使用数据工程来维护生产自我监督系统，该系统通过以下方式纠正不良行为更改的是数据，而不是模型。我们认为数据中心需要支持用户注入有针对性的数据修改以进行模型维护的接口。我们还承认，数据管理和探索并不是孤立进行的，并且相信数据中心应该支持围绕共享有用指标和分析管道的社区。受到 Hugging Face 的 ModelHub76 或 Tableau Public 的可视化共享平台等类似社区共享平台的启发，我们希望用户分享有关基础模型训练数据的见解。开放式问题。尽管我们描述的数据中心受到现有工具包和解决方案的启发，但我们认为它们尚未准备好应对基础模型的挑战。特别是，围绕设计数据中心的一些悬而未决的问题是：我们应该如何支持数据版本控制，以便可以更新数据集，同时维护旧版本以实现可重复性[Agrawal et al.2019]？一旦部署模型并识别错误桶，可能需要更新数据集以包含这些错误桶中的更多示例。应该如何收集这些新的、有针对性的例子？正如第 4.2 节：训练中所述，我们想象从头开始训练的模型会更少，而需要微调的模型会更多。我们如何支持出处或谱系信息以了解原始数据的来源，同时维护受试者隐私 [Chen 等人。 2015a]？在公共部门，数据中心可以由由数据管理员和基础模型提供商组成的个人开源社区组织和运行。在此设置中，回答诸如谁存储数据？谁支付计算费用？如果违反许可，谁负责？都特别阴暗。数据中心如何提供正确的工具，以便一旦解决了此类问题的答案，就可以轻松实施它们？什么是正确的数据统计数据集，以提供足够的文档，而又不会太昂贵或难以获取？数据中心如何支持有针对性的数据修改，例如增强[Ma 2019； Shorten 和 Khoshgoftaar 2019] 或数据编程 [Ratner 等人。 2017]？监控工具包如何更好地检测由于动态变化的评估数据性能不佳而需要更新基础模型的情况？我们对数据中心的愿景并不完整或不详细。然而，我们提出了对数据挑战的初步想法，以及一种解决方案，以促使思考如何改进基础模型生命周期的数据管理。 75 以数据为中心的交互式工具包的示例包括 https://www.tableau.com/ 和 https://www.paxata.com/。 76https://huggingface.co/models 77https://public.tableau.com/en-us/s/about关于基础模型的机遇和风险 105 

## 4.7 安全和隐私 

作者：Florian Tramèr*、Rohith Kuditipudi*、Xuechen Li * 

图 20. 机器学习系统安全和隐私基础模型带来的风险和机遇。作为关键数据驱动决策系统的核心组件，机器学习模型必须解决各种安全和隐私威胁。78这些威胁可以使用计算机安全的传统“CIA 三元组”来表征。机器学习系统应保护用户数据的机密性，免受推理和重建攻击[Fredrikson et al.2015； Shokri 等人，2017； Carlini 等人，2019，2021]。此外，经过训练的模型本身的保密性可能面临模型窃取攻击的风险 [Tramèr et al.2016; Papernot 等人.2017]。机器学习系统的完整性可能会受到对抗性示例的影响 [Biggio et al.2013; Szegedy et al.2014] 和数据中毒攻击[Biggio et al.2012；陈等人.2017]。最后，资源耗尽攻击[Shumailov 等人。 2020；洪等人。 2020a] 可能会威胁机器学习系统的可用性。针对这些威胁，我们认为未来机器学习系统中基础模型的安全角色将类似于传统软件系统中操作系统所扮演的角色。由于其通用性和普遍性，基础模型可能成为单点故障，从而成为针对源自该模型的应用程序的攻击的主要目标。然而，反过来，具有强大安全性和隐私属性的基础模型可以成为设计各种安全可靠的机器学习应用程序的支柱。当然，这些应用程序可能仍然需要设计为强制执行特定的安全和隐私保证（就像软件设计者不能依赖安全的操作系统来防范所有安全风险一样）。 

### 4.7.1 风险

单点故障。适合各种应用程序的基础模型代表了这些应用程序的单点故障。例如，对基础模型的数据中毒攻击（对手将恶意示例插入训练数据）也可能会影响所有适应的应用程序。类似地，针对基础模型的对抗性示例（即，导致模型输出非常不同的特征的小输入扰动）可以更容易地转移到适应的应用程序。 Wallace 等人[2019]甚至发现单个对抗性触发器78在本节中，我们重点关注基础模型的安全性。 §5.2 中讨论了基础模型在安全方面的一些应用（例如，有毒内容的检测）：误用 .106 添加到任何输入的基础模型研究中心 (CRFM) 可能会导致诸如 GPT-2 之类的语言模型输出一段预定义的文本。基础模型也可能成为数据隐私的单点故障。如果基础模型是根据公司的私有数据进行预训练的，并且该模型会记住部分数据，则所有下游应用程序都可能面临暴露这些数据的风险 [Carlini et al.2021]。基础模型的提供者也可以是应用程序数据隐私的单点信任。例如，当前的 GPT-3 API 要求将所有用于微调或推理的（可能敏感的）数据上传到 OpenAI 的服务器。设计一个避免这种信任集中化的基础模型服务是一个有趣的问题。如果基础模型的参数是公开的，则可以促进对改编应用程序的模型窃取攻击，因为攻击者只需要对公共基础模型的“增量”进行逆向工程[Krishna et al.2019]（例如，根据从公共冻结模型中提取的特征进行训练的线性模型）。最后，对基础模型提供者的拒绝服务攻击也可能是一个问题，并且可能会因使用特殊的高成本输入查询模型而加剧[Shumailov et al.2020]。数据中毒。迄今为止，成功的基础模型都是在从网络上抓取的大型且通常未经整理的数据集上进行训练的[Radford et al.2021, 2019]。这种宽松的数据收集，再加上缺乏直接的训练监督，促进了对基础模型训练数据的中毒攻击（例如，将针对特定个人或公司的仇恨言论注入到 Reddit 的一些出站页面中）。更糟糕的是，当今模型规模和准确性的不断增长可能会加剧中毒攻击的威力 [Carlini 2021]。为了说明这一点，Schuster 等人[2021] 表明，在 Github 数据上使用 GPT-2 训练的代码自动完成系统可能会被毒害，仅通过注入少量恶意文件来建议不安全的代码片段。 Carlini 和 Terzis [2021] 进一步表明，针对 CLIP 式 [Radford et al.2021] 模型的针对性攻击只需要修改 300 万个训练样本中的两个即可。功能蠕变和双重用途。基础模型学习一般特征，使它们能够轻松适应各种任务。然而，这种灵活性引起了人们的担忧，即基础模型的使用可能超出其最初预见的目的——这种风险通常被称为功能蠕变或双重使用。机器学习中功能蠕变的例子包括过度学习 [Song and Shmatikov 2019] 和对抗性重编程 [Elsayed et al. 2019]。 2018]。举例来说，CLIP 最初被训练来解决预测图像文本对的通用任务，但在此过程中也学会了捕获丰富的面部特征 [Goh et al.2021]。虽然 CLIP 的“模型卡”79 明确将面部识别和其他监控技术置于范围之外，但 CLIP 当然可以重新用于此类任务 [Radiya-Dixit 和 Tramèr 2021]。这个例子说明，在设计基础模型时限制（甚至预见）基础模型可能的恶意用途可能具有挑战性。 §5.2：滥用提供了关于基础模型的双重（滥用）使用的进一步讨论。多模式不一致。多模态可能会增加基础模型的攻击面，使对手能够利用跨模态的不一致性。此类攻击的可能性在一个（臭名昭著）的 CLIP 示例中得到了证明，该示例将带有“iPod”一词的苹果分类为 iPod [Goh et al.2021]。更一般地，只要可以使用不同的模态来表达一个概念，这些模态之间的不一致就可以被利用。当基础模型适用于主要依赖于一种学习模式的任务时，这种不一致尤其令人担忧。例如，考虑使用从 CLIP 中提取的特征进行面部识别。这是一个纯粹的视觉任务，但改编后的模型是 79https://github.com/openai/CLIP/blob/main/model-card.md。访问日期：2021 年 6 月 30 日关于 Foundation Models 107 的机遇和风险 107 个功能仍然对文本信号敏感（因此，攻击者可能能够通过穿着印有文本的衣服来逃避面部识别）。或者，考虑一个自动驾驶系统（也主要依赖于视觉的应用程序），它看到一个带有“绿色”一词的广告牌，并错误地将其解释为绿灯。 

### 4.7.2 机会

安全瓶颈。如果改编后的应用程序可以继承基础模型的漏洞，那么它们也可以继承所需的安全特性，例如对对抗性示例或中毒攻击的鲁棒性。因此，基础模型可能成为安全瓶颈。例如，对对抗性示例具有鲁棒性的模型在适应其他任务时可以保持其鲁棒性[Shafahi et al.2019]。同样，能够（以某种方式）防御中毒、模型窃取或资源耗尽攻击的基础模型提供商可以为其客户的应用程序提供此类安全保证。基础模型作为单点故障或安全瓶颈之间的权衡让人想起软件堆栈中其他抽象层（例如，操作系统、数据库系统或Web 浏览器）中的类似安全权衡。由于服务于许多不同的应用程序，抽象层是攻击的主要目标，但与任何单个应用程序相比，通常也可以利用更多的资源来增强其安全性。私人学习更便宜。当前的基础模型通常是通过从公共可用来源（例如，来自开放网络）收集大量数据来训练的。这种做法可能会引起人们对隐私的担忧——从广义上讲，将用户数据带出其预期的上下文 [Nissenbaum 2004; Nissenbaum 2004; Carlini 等人，2021]。虽然一些现有的工作旨在减轻模型记忆训练数据的倾向（例如，通过删除重复训练数据[Lee et al.2021b]，或通过差异隐私下的预训练[Anil et al.2021]），但这样的解决方案不太可能满足用户对文本数据的广泛隐私期望 [Brown et al.2022]。另一方面，公共预训练最终也可能成为处理稀缺和敏感数据的应用程序（例如医疗保健领域）中用户隐私的胜利。作为一个例子，考虑训练差分隐私模型的问题 [Dwork 等人。 2006]用于医疗保健任务。目前，“端到端”（即不利用任何预训练）训练这样的模型以实现适当的隐私与效用权衡需要大量的隐私敏感数据[McMahan et al.2018；巴苏等人.2021]。相比之下，在许多情况下，对公共数据进行预训练的基础模型可以适用于使用明显较少的机密数据来执行特定任务[Bommasani 等人，2017]。 2019；特拉梅尔和博内 2021 年；李等人。 2022 年；于等人。 2022]。对大规模对抗性例子的鲁棒性。有证据表明，与标准训练相比，训练一个对对抗性示例具有鲁棒性的模型需要更多的数据[Schmidt et al.2018]，但未标记的数据可能足以弥补这一差距[Carmon et al.2018]。 2019； Uesato 等人.2019]。此外，增加模型大小和容量（即过度参数化）也被证明对于在某些设置中实现对抗鲁棒性是必要的[Madry et al.2018； Bubeck 和 Sellke 2021]。了解如何最好地利用过度参数化和未标记数据来实现对抗鲁棒性是未来研究的一个重要方向。鉴于其前所未有的规模（无论是模型大小还是训练集大小），基础模型都处于独特的地位，可以从这一研究中受益。尽管规模空前，但不幸的是，当前的基础模型在应对最坏情况的对抗性扰动的鲁棒性方面几乎没有什么进展 [Fort 2021;华莱士等人.2019]。然而，诸如 CLIP 之类的多模态模型对于（非对抗性）分布变化具有惊人的鲁棒性（参见§4.8：鲁棒性）。分布式鲁棒性的这些进步是否可以转化为增强抵御现实世界攻击的能力是另一个令人兴奋的悬而未决的问题。特别是在基础模型研究中心 (CRFM) 的环境中，对手受到各种限制（例如，有限的查询访问或计算预算），有理由乐观地认为，增强的分布式鲁棒性可能会带来整体安全性的伴随收益——甚至如果基础模型仍然容易受到最坏情况的“白盒”攻击。论基础模型的机遇和风险 109 4.8 分布变化的稳健性 作者：Sang Michael Xie, Ananya Kumar, Rohan Taori, Tony Lee, Shiori Sakawa, Pang Wei Koh，Tatsunori Hashimoto 现实世界的机器学习系统需要对分布变化具有鲁棒性——它们应该在与训练分布不同的测试分布上运行良好。高风险应用，例如资源贫乏国家的贫困地图[Xie et al.2016； Jean et al.2016]，自动驾驶汽车[Yu et al.2020a； Sun et al.2020a]，以及医学诊断[AlBadawy et al.2018； Dai 和 Gool 2018] 都需要模型能够很好地泛化到训练数据中未出现的情况，例如来自不同国家、不同驾驶条件下或来自不同医院的测试示例。先前的工作表明，即使在最先进的模型中，这些类型的分布变化也会导致性能大幅下降 [Blitzer et al.2006;道梅三世 2007；杉山等人，2007；加宁和伦皮茨基 2015；彭等人.2019; Kumar 等人.2020a； Arjovsky 等人，2019； Szegedy 等人，2014 年；亨德里克斯和迪特里希 2019；佐川等人.2020a； Recht 等人，2019；阿布尼 2007；罗德普朗克2018； Geirhos 等人，2018； Kumar 等人.2020b； Yu 等人.2020b； Geirhos 等人 2020； Xie 等人.2021a；科等人。 2021]。在本节中，我们考虑基础模型对分布变化稳健性的作用。基础模型是在从分布 𝑝pre 中采样的大型且多样化的未标记数据集上进行训练的，并且可以适应许多下游任务。对于每个下游任务 T，基础模型适应从分布内 (ID) 训练分布 𝑝T ID 采样的标记训练数据，然后在分布外 (OOD) 测试分布 𝑝T OOD 上进行评估。例如，贫困预测模型[Xie et al.2016； Jean et al.2016]可以对来自世界各地的未标记卫星数据进行预训练，以学习对所有国家有用的特征，然后对来自尼日利亚的标记示例进行微调，最后在标记示例稀缺的马拉维进行评估。我们认为 1) 基础模型是一种特别有前途的鲁棒性方法。现有的工作表明，对未标记数据进行预训练是提高 OOD 测试分布准确性的有效、通用方法，这与许多仅限于狭窄类型的分布变化的稳健性干预措施形成鲜明对比。然而，我们还讨论了为什么 2) 基础模型可能并不总能减轻分布变化，例如由于虚假相关性或随时间变化而导致的变化。最后，3）我们概述了利用和改进基础模型以实现鲁棒性的几个研究方向。我们注意到，基础模型改进外推的方法之一是为适应模型提供归纳偏差（通过模型初始化），这些偏差是在超出下游训练数据的多样化数据集上学习的。然而，同样的归纳偏差也可以编码来自预训练数据的有害关联，并在存在分布偏移的情况下导致代表性和分配危害。有关此类危害和缓解方法的进一步讨论，请参阅第 4.6 节：数据和第 5.1 节：公平性。 

### 4.8.1 优点

通过学习大型且多样化的基础模型训练分布 𝑝pre 上的表示，基础模型可以提高下游测试分布 𝑝T OOD 上适应导数的准确性。 OpenAI 的 CLIP 模型是在一组不同的图像和自然语言文档上训练的基础模型，已被证明对 ImageNet 上的一些基准分布变化具有鲁棒性 [Radford et al.2021]：例如，CLIP 和标准ResNet50 在 ImageNet 上获得了 76% 的准确率，但 CLIP 在 ImageNetV2 [Recht et al.2019] 上实现了 6% 高的准确率，在 ImageNet Sketch [Radford et al.2021] 上实现了 35% 高的准确率，这两者与原始 ImageNet 相关但又不同训练分配。相比之下，许多其他稳健性干预措施，例如对抗性训练 [Madry 等人 .2018]、不变风险最小化 [Arjovsky 等人 .2019]，110 基础模型研究中心 (CRFM) 图 21. 分布内 (ID) ）和各种分布变化的分布外（OOD）输入。这里描述的预测任务是图像的图像分类和文本的事实验证。尽管基础模型学习到的表示提高了许多转变（例如常见腐败）的下游鲁棒性[Hendrycks 和 Dietterich 2019； Xie 等人.2021a； Radford et al.2021]，一些变化，例如虚假相关性（其中草预示着牛）[Beery et al.2020]和跨时间的外推（随着时间的推移而变化的事实）[Lazaridou et al.2020]。 2021] 基础模型可能仍然没有解决这个问题。或使用更大的模型对这些 ImageNet 任务的有效稳健性（定义为分布内和分布外性能之间的差距）几乎没有影响，特别是在没有明确了解分布变化的情况下 [Taori et al.2020; Santurkar 等人，2020；雷德福等人。 2021 年；米勒等人。 2021]。许多其他工作表明，对大型数据集进行预训练可以提高对常见图像损坏、标签移位和标签损坏的鲁棒性[Hendrycks et al.2019a,b]；卫星图像任务中真实世界的空间变化[Xie et al.2021a; Kumar 等人.2022]；以及自然语言理解任务中跨主题的转变[Hendrycks et al.2020； Fisch 等人，2019； Yogatama 等人，2019]。另一个例子是，使基础模型训练数据多样化以包含多种语言（如多语言 BERT [Liu et al.2020b]），可显着提高未见过的语言对的性能。 

### 4.8.2 持续的挑战

尽管有希望的迹象表明基础模型将显着提高稳健性，但我们预计基础模型并不是解决分布变化的灵丹妙药。我们在下面两大类分布变化的背景下讨论这个问题。虚假相关性。虚假相关性是特征和标签之间的统计相关性，对训练分布具有预测能力，但对测试分布没有预测能力 [Heinze-Deml 和 Meinshausen 2017； Arjovsky 等人，2019；佐川等人.2020a]。众所周知的例子包括依赖背景颜色进行对象识别[Xiao et al.2020]、用于医学诊断的手术标记[Winkler et al.2019]、众包数据中的注释者偏差[Tsuchiya 2018； Gururangan 等人，2018；波利亚克等人，2018； Geva 等人 2019]，以及人口统计学偏见 [Abid 等人 .2019]。 2021 年； Nadeem 等人，2021； Gehman 等人，2020]。模型学习这些虚假相关性很大程度上是因为基础模型训练和适应数据表现出这些偏差 [Nagarajan 等人。论基础模型的机遇与风险 111 2020； Gehman et al.2020]，并且这个问题不能简单地用更大的模型来解决[Sakawa et al.2020]。 2020b]。基础模型可能会加剧或减轻虚假相关性的影响，但这取决于特定下游任务的性质及其与基础模型训练数据和算法的关系。通过使用不同的数据集进行训练，基础模型可以提高对仅在训练数据的子集中发现的虚假相关性的鲁棒性：例如，现有研究发现，预训练的语言模型可以通过快速从反例学习到虚假相关性来避免虚假相关性。 Tu 等人.2020]。然而，基础模型也可能通过引入基础模型训练数据中存在的偏差而加剧问题，正如在 GPT-3 和其他 NLP 模型中观察到的人口统计偏差一样 [Abid et al.2021;纳迪姆等人。 2021 年； Gehman 等人，2020]。此外，仅大规模训练并不能完全解决识别和不依赖下游训练集而非下游测试集的预测特征的根本问题[Heinze-Deml and Meinshausen 2017]。解决这些挑战需要我们理解和管理基础模型训练中的归纳偏差，并开发能够抵抗学习虚假相关性的适应算法。外推和时间漂移。最后，基础模型的少样本和零样本能力将意味着这些模型将越来越多地应用于远远超出训练分布的范围。虽然大规模基础模型训练可以帮助对新分布进行某些形式的外推 [Papadimitriou 和 Jurafsky 2020]，但它们的外推能力可能有限。例如，现有语言模型无法在不重新训练的情况下处理世界知识的变化或语言变化[Lazaridou et al.2021; Dhingra et al.2021]，CLIP 中的零样本传输在卫星图像领域受到很大影响 [Radford et al.2021]。 2021]，并且 ImageNet 预训练并没有显着提高大型模型在医学图像上的性能 [Raghu 等人，2021]。 2019； Ke等人.2021]。我们认为，不能假设基础模型能够在给定模态（例如所有图像）内自动外推，并且定义和区分基础模型新启用的外推形式与那些仍然无法实现的外推形式将变得越来越重要。尽管现有的分布转移分类法已被普遍提出[Quiñonero-Candela et al.2009； Ye et al.2021]，充分理解和定义基础模型有效的分布变化类型是鲁棒性研究的一个主要开放问题。 

### 4.8.3 机会

基础模型作为分布变化的通用稳健性干预措施具有巨大的前景，并为稳健性研究开辟了新的途径。我们在下面概述了一些机会和开放性问题。了解基础模型表示。现有对基础模型稳健性的研究主要是经验性的，对稳健性增益背后的机制知之甚少。 Sun 等人 [2019b] 假设预训练表示使不同的域（例如 ID 和 OOD 分布）更加紧密地结合在一起，这反过来又可以提高从标记的 ID 数据到 OOD 数据的泛化能力 [Ben-David 等人 2010]。测量有或没有预训练的域表示之间的距离的受控实验可以阐明这种效应。在表征基础模型训练（例如，作为谱图分解的对比学习[HaoChen et al.2021a]）及其归纳偏差方面存在初步有希望的方向[Saunshi et al.2020a； Lee等人2020a；张和桥本2020；谢等人.2020]。然而，这些理论是有限的，无法解决其他经验上有效的基础模型，例如完全生成语言模型（例如，GPT-3 [Brown et al.2020] 和 image-GPT [Chen112 基础模型研究中心 (CRFM) 等人） .2020d]）。进一步了解这些归纳偏差在分布转移下如何发挥作用可能会产生关于基础模型如何提高鲁棒性的更完整的理论（第 4.10 节：理论）。基础模型训练中的数据增强。虽然在不了解下游任务的情况下训练基础模型可以避免一些特定于任务的偏差并通常提高稳健性，但由于基础模型的训练方式而产生的某些统计偏差可能会持续存在。作为一个具体的例子，许多当代的自我监督算法在很大程度上依赖于选择一组适当的数据增强[Chen et al.2020c]，这反过来又在适应阶段赋予了不同类型的鲁棒性。例如，Xiao 等人[2021]表明，通过旋转增强对比学习训练的视觉基础模型可以提高具有旋转不变性的适应任务的 OOD 性能，但可能无法提高 OOD 泛化需要其他不变性的任务的鲁棒性。进一步研究哪些类型的数据增强可以提高各种下游任务的鲁棒性——包括从数据中学习的数据增强[Wong and Kolter 2020; Tamkin et al.2021c] 或设计为普遍适用于各种数据模式 [Verma et al.2021] — 将提供更好的基础模型训练算法（§4.2：训练）。基础模型训练中的编码结构。总的来说，探索对数据中已知结构和不变性进行编码的新方法是基础模型训练的重要途径。许多现实世界的任务都有额外的元数据（例如，空间位置坐标、贫困预测示例中来自辅助卫星的气候信息），这可能为 OOD 泛化提供额外的结构（例如，跨地理区域）[Xie et al.2021a ; Koh 等人，2021]。例如，Xie 等人[2021a]表明，元数据可以用作预训练的目标，以提高下游 OOD 准确性。在语言中，对 HTML 数据中的标签进行建模提供了额外的下游任务相邻监督，允许新形式的提示（例如，填写 <title> 标签以提供标题建议），并提高数据效率 [Aghajanyan 等人 2017]。 2021]。虽然当前的数据增强方法对手工制作的知识进行编码，但其他途径（例如利用元数据）可以提供更自动化的方法来确定将哪些结构和不变性纳入基础模型训练。基础模型训练数据的专业化与多样性。基础模型训练数据的选择会对下游产生影响——对更多样化的数据集进行训练并不总是比更专业的基础模型更好地提高下游性能[Cole et al.2021; Chalkidis et al.2020]（参见§4.3：更详细讨论的改编）。在一些领域，例如卫星图像和专门的文本主题，对专门领域的持续预训练可以显着提高下游性能[Reed et al.2021; Gururangan 等人，2020]。这是一个潜在的紧张来源：一方面，我们可能希望在大型、多样化的数据集上训练基础模型，以便在分布变化下获得更稳健的性能，而另一方面，我们可能需要专门化基础模型模型来提高其在下游任务上的分布内和分布外性能。更好地理解专业化如何影响基础模型的分布内和分布外性能将使我们能够设计和收集更有效的基础模型训练集。适应方法。尽管基础模型提供了一个强有力的起点，但适应方法如何使用预训练信息会影响鲁棒性。例如，语言模型的轻量级调优方法（例如，适配器/前缀/提示调优 [Houlsby et al.2019; Li and Liang 2021; Lester et al.2021]），通过优化一个小的参数来使模型适应新任务。一组参数（例如连续提示），同时保持其他基础模型参数冻结，似乎可以带来 OOD 性能优势（第 4.3 节：适应）。 Xie 等人 [2021b] 在一个特殊情况下解释了这一点，其中将学习模型与冻结的基础模型组合在一起关于基础模型 113 的机会和风险可以降低学习模型的复杂性，从而提高 ID 和 OOD 的泛化能力。关于视觉数据集，Wortsman 等人[2021]； Kumar 等人[2022]发现，与微调整个模型相比，冻结基础模型并仅训练头部可以带来更好的 OOD 性能。 Kumar 等人 [2022] 通过证明即使在简单的设置（两层线性网络）中，完全微调也可以扭曲预训练的特征，从而从理论上解释了这一点。然而，人们对于为什么冻结参数似乎可以提高 OOD 性能仍然知之甚少。最后，虽然当前的自适应方法可能足以实现良好的 ID 泛化，但这些方法在其设计中并未明确考虑分布变化。第一步，我们可以研究分布变化的方法（例如领域适应、领域泛化和半监督学习方法）在用于适应时如何与基础模型交互。这些方向的进展可以带来适应方法，可以更好地利用基础模型来实现鲁棒性。114 基础模型研究中心 (CRFM) 

## 4.9 人工智能安全和对齐

作者：Alex Tamkin、Geoff Keeling、Jack Ryan、Sydney von Arx

人工智能 (AI) 安全关注先进人工智能模型的潜在事故、危险和风险，特别是对社区或社会造成的大规模风险。当前的基础模型可能远不会带来此类风险；然而，它们的功能和潜在应用的广度是惊人的，并且与以前的机器学习范式有明显的转变。虽然人工智能安全历来在人工智能研究中占据着更边缘的地位，但目前向基础模型及其相应通用性的转变为人工智能安全研究人员提供了一个机会，以新的眼光重新审视该领域的核心问题，并重新评估其直接或近乎严重的问题。未来的相关性。80 

### 4.9.1 人工智能安全的传统问题

人工智能安全研究的一个主要分支涉及先进人工智能系统的影响，包括那些在广泛的认知任务中可能匹配或超过人类表现的系统[Everitt et al.2018]。81在这种情况下安全研究的中心目标是减轻先进人工智能开发带来的大规模风险。82这些风险可能比第 5.2 节：误用、第 4.8 节：稳健性和第 4.7 节：安全性中考虑的风险更具推测性；然而，它们的规模要大得多，并且至少在原则上可能是未来高性能系统的结果。特别值得关注的是全球灾难性风险：粗略地说，是全球性或跨代范围的风险，导致死亡或以其他方式显着降低受影响者的福利（例如核战争或快速的生态崩溃）[Bostrom 和 Cirkovic 2011]。那么，人工智能安全研究就是一系列项目，旨在描述先进人工智能的发展所带来的灾难性风险（如果有的话），并开发合理的技术解决方案来减轻这些风险的可能性或严重性。从人工智能安全的角度来看，最好的情况是控制问题的解决方案：如何开发一个先进的人工智能系统，使我们能够获得该系统的计算优势，同时让我们拥有足够的控制权，例如该系统的部署不会导致全球灾难 [Bostrom 和 Cirkovic 2011]。然而，技术解决方案不足以确保安全：确保安全算法实际上是在现实世界系统中实施的算法，并且不部署不安全的系统可能需要额外的社会技术措施和机构。强化学习（RL）研究针对奖励进行优化的决策代理，在过去十年中一直是人工智能安全的主要焦点。这里的问题是，在不构成全球灾难性威胁的最低意义上，为人工智能指定和实例化一个与人类价值观一致的奖励函数是困难的。83而这个问题被称为价值对齐[Gabriel 2020； Yudkowsky 2016]，乍一看似乎微不足道，人类价值观是多种多样的，84无定形的，并且难以定量捕捉。因此，一个突出的问题是奖励黑客行为，即人工智能发现了一种不可预见的政策，可以最大限度地提高人类福祉的代理奖励，但其错误指定会导致重大损害。85许多努力来打击 80See Amodei 等人。 [2016] 和 Hendrycks 等人。 [2021d] 对人工智能安全中的开放问题有更广泛的视角。 81这被一些人称为 AGI 或通用人工智能，尽管术语使用有所不同 [例如，参见 Karnofsky 2016]。 82请注意，这并不需要相信构建某些类型的高级人工智能是一个理想的目标，甚至也不需要确定它是一个可以实现的目标。 83参见 Hubinger 等人[2019]，讨论了奖励规范和奖励实例化之间阈值处出现的一些挑战。 84参见 Gabriel [2020]，了解人类多样性、道德和价值取向问题的扩展讨论。 85参见此电子表格，了解奖励黑客的真实示例列表，包括飞机着陆算法，该算法通过输出巨大的力量来获得完美分数利用模拟器中的缺陷。关于基础模型 115 的机遇和风险价值调整问题集中于最大化可校正性，即系统运行后可以纠正系统设计中的错误[Soares et al.2015] 。这可能远非直截了当——在强化学习环境中，具有特定目标的智能体将被激励禁止改变该目标的尝试，因为任何改变该目标的尝试都可能不利于目标的实现 [Omohundro 2008]。然而，纯强化学习并不是实现高级人工智能的唯一理论途径。基础模型还可以使用简单的（自）监督目标（例如下一个标记预测）进行训练，但仍然可以以交互和目标导向的方式使用，无论有或没有额外的 RL 训练。此外，这些方法中的许多方法似乎可以通过直接扩展计算、参数数量和数据集大小来增强功能[Hestness et al.2017；卡普兰等人。2020]。在更广泛的基础模型背景下，诸如价值对齐和可修正性之类的概念在几个方面与纯强化学习案例有所不同，因此必须进行仔细的理论化。 

### 4.9.2 当前的基础模型和人工智能安全

强化学习环境中的许多风险都是由为实现目标而优化的模型造成的。然而，针对最新基础模型的人工智能安全研究的一个关键挑战是，尽管没有明确优化，但目标导向的行为可能会出现（另见§4.2：训练）。例如，大型语言模型可以在语料库上进行训练，其中代理以目标导向的方式使用语言，例如在有说服力的文本中。为了很好地预测下一个标记，模型可能会获得推理和产生论证的一般能力，这些能力可以在适当的上下文中出现。在其他类型的人类数据上训练的基础模型可以捕获数据中存在的其他类型的目标导向行为；例如，如果训练数据包含拳击比赛的视频，被训练在视频中模仿人类的机器人代理可能会尝试拳击或击倒人类操作员。最近的工作还尝试直接训练智能体产生目标导向的行为；例如，Decision Transformer 在带有返回值的轨迹上训练序列模型 [Srivastava 等人。 2019；施米德胡贝尔 2019; Chen 等人.2021b]。然后，我们可以通过以高回报“提示”该模型来生成高回报轨迹，这在 RL 环境中提出了类似的奖励黑客问题。然而，目标导向模型安全研究的一个主要目的是获得对智能体所追求的行为更有原则的控制和可解释性，而不是依赖于黑盒神经网络的难以理解的决策。 86这使得当前的基础模型成为令人兴奋的模型。人工智能安全研究的研究途径，因为调整它们可能是调整更先进模型的有用先驱[Christiano 2016；科特拉 2021； Kenton 等人，2021]。挑战之一是基础模型的训练目标与期望行为之间的不一致；例如，可以训练语言模型来预测训练语料库中所有文档的下一个单词，而不管其准确性如何，但用户可能希望模型仅输出真实或有帮助的文本[Tamkin et al.2021a]。引导目标导向的智能体实现期望行为的一种潜在方法可能是用自然语言描述动作来训练它们——这可以用语言来引导它们，并使它们能够输出可解释的语言来描述它们“相信”自己正在执行的任务，类似于可控生成和来源归因的方法[例如，Keskar et al.2019，另见§2.3：机器人学，§2.5：交互，和§4.11：可解释性]。然而，为了确保此类模型在野外的可靠性和自我一致性（§4.8：鲁棒性），以及对这些模型如何运作获得更机械的理解，还需要进一步的进步[Cammarata et al.2020，另请参阅§4.11：可解释性]。即使对未来基础模型基于自然语言的控制能够实现更好的任务规范86有关理解和语义之间关系的更多信息，请参阅§2.6：哲学116基础模型研究中心（CRFM）和监控，模型也可能会出现欺骗性或其他不良行为来自人类数据——识别和消除这种行为是未来研究的另一个重要方向。虽然上一段中描述的自我监督目标训练模型来捕获数据中的人类行为，但新的训练范式可能会产生目标导向的基础模型，能够在复杂的环境中执行各种任务，并展示出多种能力。在不同领域优于人类（参见§4.2：训练）。例如，目标导向的基础模型可以在类似于 AlphaGo 的开放式自我对弈环境中进行训练，或者在大型多任务单智能体强化学习设置中进行训练。这可能会导致出现新的能力，使代理人实现目标的努力变得复杂化，特别是如果许多代理人在一个丰富的世界模拟器中一起接受训练，该模拟器鼓励欺骗、误导、掩饰、说服和战略规划等技能的发展。除了打击欺骗行为之外，目前还不清楚如何有效评估和控制非常有能力的模型的行为，即可扩展的监督或对齐[Amodei et al.2016;雷克等人.2018]；例如，对化学基础模型提出的新反应进行评分（参见§4.4：评估）。因此，用于训练、指导、监控和理解这些模型的新的人机交互方法是令人兴奋的未来方向。最后，即使在这些更先进的功能出现之前，近期人工智能安全的一个重要研究领域就是表征和预测当前自我监督基础模型的功能。三个方面使得这一点具有挑战性。首先，基础模型的通用性意味着它们可以以意想不到的方式应用于无数不同类型的应用程序。列举基础模型当前和计划的应用不足以捕获它们的全部使用方式。其次，即使在特定的应用程序中，模型功能也会出现：随着模型的扩展，它们会以意想不到的方式增长和变化。例如，通过“提示”控制 GPT-3 的能力是一种新兴现象，在较小的 GPT-2 模型中只能看到最简单的一瞥[Radford et al.2019; Brown et al.2020]。未来基础模型的属性将是什么样子是未知的。第三，即使在特定的应用程序和规模内，模型的能力也不容易表征。例如，一旦将逗号添加到 GPT-3 中，GPT-3 执行加法的能力就会显着提高。输入 [Branwen 2020; Brockman 2020]。同样，提示的小改写会对任务性能产生很大影响。由于提示的空间难以枚举，因此很难明确断言任何任务超出当前提示的范围 - 

### 4.9.3 未来基础模型的潜在灾难性风险

当前模型广泛且快速增长的能力表明，尝试描述可能的灾难性风险的好处来自更先进的系统。我们看到先进的基础模型至少可以通过两种方式促成这种结果。灾难性的稳健性失败。 §4.8：稳健性讨论了模型在面对新类型数据时如何以意外或有害的方式表现[Amodei et al.2016； Yudkowsky 等人，2008]。如果将基础模型集成到利用基础模型快速适应许多不同任务和情况的能力的重要系统中，这些失败可能会特别严重。如果故障发生在战争系统（导致不必要的武器发射，可能引发冲突）、关键基础设施（关键能源或农业能力的意外破坏），或者如果它们对大部分经济活动至关重要（其意外失败可能导致生活水平突然崩溃和政治不稳定；另见§5.5：经济学）。事实上，与其他类型的基础模型 117 人工智能的机遇和风险相比，灾难性稳健性失败的威胁对于基础模型尤其相关。这是因为基础模型由单个模型组成，该模型可以适用于许多不同的用例，因此从模型学习的统计关联中得出的鲁棒性故障原则上可以在多个不同领域以相关的方式表现出来。如果将相同的基础模型集成到多个关键功能中，则模型缺乏鲁棒性可能会导致跨越多个关键功能或故障保护的相关故障。目标设定错误。使用基础模型可能会增加优化不一致但易于指定的目标的风险，通常称为古德哈特定律 [Kenton et al.2021;古德哈特 1984]。当前这些风险的一个例子是某些推荐系统的负面影响（例如，两极分化、媒体成瘾），这些系统可能会优化简单的参与度指标，而不是难以衡量的社会和消费者福祉的组合[Burr 等人] .2018; Milano 等人，2020]。未来的机构可能会利用无法解释的基础模型来最大化利润或 GDP 等简单指标，因为这些模型能够适应每个指标所依赖的许多不同子问题。然而，在更大范围内优化这些代理指标，而不是为人类福利设计更全面的目标，可能会无意中导致环境或地缘政治危害[Gabriel 2020；克里尔和赫尔曼 2021]。 

### 4.9.4 结论

总之，我们认为基础模型当前和未来潜在的新兴特性使它们成为人工智能安全领域成熟的研究对象。我们鼓励未来致力于描述和预测基础模型的确切能力和风险；开发新方法，使基础模型符合人类价值观和期望目标； 118 基础模型研究中心 (CRFM) 

## 4.10 理论

作者：Aditi Raghunathan、Sang Michael Xie、Ananya Kumar、Niladri Chatterji、Rohan Taori、Tatsunori Hashimoto, Tengyu Ma 

严谨的数学理论在许多工程和科学学科中发挥着基础作用（例如，电气工程中的信息论）。我们相信，基础模型理论对于指导技术决策和创新特别有益，因为基础模型实验需要巨大的计算成本。此外，理论见解有助于阐明基本局限性并解释令人惊讶的经验现象。然而，尽管最近取得了很多进展，但目前社区对基础模型的理论理解仍然有限[Arora et al.2019b；陈浩等人.2021a； Wei 等人.2021, 2020b;张和桥本 2021； Saunshi 等人.2020b； Dao 等人，2019； Tosh 等人.2020, 2021;蔡等人，2021； Lee等人2020a； Zimmermann 等人，2021； Bansal 等人 2020；王和伊索拉 2020；蔡等人。 2020；田等人。 2020a、b；特里普拉尼尼等人。 2020；杜等人。 2020]。深度神经网络构成了基础模型的支柱。即使在经过充分研究的监督学习环境中，训练和测试场景具有相同的分布，围绕深度网络也存在许多悬而未决的问题，例如理解非凸优化、优化器的隐式正则化效果和表达能力。基础模型提出的问题明显超出了监督深度学习设置的范围。理论上分析基础模型的核心问题是理解为什么对一个可能存在无监督/自监督损失的分布进行训练会在不同的下游分布和任务上产生良好的适应性能。我们将讨论一种直观的模块化来分析基础模型，该模型揭示了监督学习和基础模型、具体和核心技术问题之间的联系，以及解决这些问题的一些有前景的理论工具。这些新的核心问题可以为基础模型提供有用的见解，并且可以与监督深度学习理论并行研究。虽然我们专注于分析下游性能，但所提出的模块化和工具可能有助于分析其他感兴趣的指标，例如分布变化的稳健性（第 4.8 节：稳健性）和安全性（第 4.7 节：安全性）。图 22. 从不同数据的预训练到适应任务的下游性能的基础模型分析涉及捕获不同损失项之间的关系，如上所示。主要挑战是分析突出显示的预训练-适应界面，除了预训练和适应阶段的模型架构、损失和数据分布之外，还需要仔细推理种群损失（§4.10.2：理论界面）。泛化和优化的分析在很大程度上简化为标准监督学习中的分析。 87基础模型的理论与迁移学习理论密切相关，但也超越了迁移学习理论（迁移学习本身就是一个尚未充分探索的领域）：基础模型可能使用未标记的数据进行训练，并将适应许多或所有自然任务，而迁移学习通常研究标记了源任务和固定数量的目标任务。关于基础模型的机遇和风险 119 

### 4.10.1 理论表述和模块化

回想一下，基础模型是根据大量原始数据进行训练的（§4.2：训练），然后适应特定任务（§4.3：适应），因此可以自然地分解为训练和适应阶段。我们确定它们之间的接口，并将特定于基础模型的部分与需要标准深度学习理论的部分分开，以便它们可以独立工作。我们引入了模块化的分析框架，该框架也在最近的工作中隐式或显式地使用，例如 Arora 等人[2019b]；陈浩等[2021a]；魏等人。[2020b]； Tripuraneni 等人。[2020]。事实证明，这种模块化分析的关键组成部分是训练前适应接口。我们首先描述模块化，并讨论为什么我们认为这种模块化很有前途，最后讨论了一些局限性。我们将训练阶段明确称为“预训练”，以将其与适应阶段区分开来，后者还可能涉及对特定任务中的一些样本进行训练。预训练阶段。基础模型的预训练通常涉及数据分布𝑝pre（例如，自然文本的分布）和预训练损失函数ℓpre（𝑥;𝜃），用于测量输入𝑥的损失（例如GPT-3中的语言建模损失）参数为 𝜃∈θ 的模型。让 ˆ𝑝pre 表示来自 𝑝pre 的大量独立样本的经验分布。预训练最小化损失 ℓpreonˆ𝑝pre，我们称之为经验预训练损失 ，并产生模型 ˆ𝜃FM：b𝐿pre(𝜃)def=E𝑥∼ˆ𝑝pre[ℓpre(𝑥;𝜃)]，和 ˆ𝜃FMdef=arg min 𝜃εθb𝐿pre(𝜃)。 (1) 我们考虑总体分布上的相应损失𝑝pre，称为总体预训练损失，作为一个中心概念：𝐿pre(𝜃)def=E𝑥∼𝑝pre[ℓpre(𝑥;𝜃)]。 (2)基于优化的适应阶段。我们将适应框架视为依赖于 ˆ𝜃FM 的一般约束优化问题，抽象出那些基于优化某些损失函数（例如微调和提示调整）的适应方法（参见，例如，[Houlsby et al. 2019; Li 和Liang 2021；Lester et al. 2021]，以及§4.3：适应）。由于不同的适应方法可以修改模型参数的不同子集，因此我们用一些 Γ 表示适应模型参数的空间。给定下游任务分布 𝑝task（例如，在特定领域中回答问题）和一些从 𝑝task 采样的经验样本 ˆ𝑝task，我们将适应阶段建模为最小化一些适应损失 ℓadapt onˆ𝑝taskw.rt 适应参数 𝛾∈Γ: 𝛾task(ˆ𝜃FM)def= arg min 𝛾∈Γ,𝐶(𝛾;ˆ𝜃FM)≤𝑐0b𝐿adapt(𝛾,ˆ𝜃FM), (3) 其中 b𝐿adapt(𝛾,ˆ𝜃FM)def=E𝑥∼ˆ𝑝task[ℓadapt(𝑥;𝛾,ˆ𝜃FM)]是经验适应损失，并且 𝐶(𝛾,ˆ𝜃FM)≤𝑐0 是一个可选约束，控制适应参数的复杂性，包括显式正则化（例如，模型维度和范数）和适应过程的隐式正则化。我们列出了一些常见的适应方法，并讨论了相应的适应参数𝛾和约束𝐶（𝛾，ˆ𝜃FM）≤𝑐0。 (1)线性探测：在基础模型的表示之上训练线性分类器。这里 Γ=R𝑘 是维度 𝑘 表示的线性分类器集合，而𝐶(𝛾,ˆ𝜃FM) 可能是 𝛾 的 ℓ2orℓ1norm。120 基础模型研究中心 (CRFM) (2)微调：优化随机初始化的线性进行几个步骤，所有其他参数 𝜃 来自 ˆ𝜃FM 的初始化。这里𝛾是𝜃和线性头的串联。这样的过程可以对应于𝛾对由𝐶(𝛾,ˆ𝜃FM)≤𝑐0捕获的初始化ˆ𝜃FM的一些隐式正则化。确切的术语 𝐶(𝛾,ˆ𝜃FM) 将取决于所使用的优化算法，并且优化的隐式正则化的这种表征是一个活跃的研究领域 [例如，Gunasekar 等人 .2017；苏德里等人。 2018； Gunasekar 等人，2018；阿罗拉等人，2019a；布兰克等人，2019；伍德沃斯等人，2020； Wei 等人，2020a；陈浩等人.2021b；达米安等人.2021; Kumar et al.2022，以及其中的参考文献].88 (3)提示调整：优化一小组连续的特定于任务的向量，这些向量前置于任务输入。这里𝛾是连续提示向量，通常具有较小的维数，并且我们可以选择对𝛾的范数进行约束。需要注意的一个明显的限制是，该公式排除了适应方法，例如上下文学习[Brown et al.2020]，其中在适应阶段没有“训练”（即，最小化一些经验适应损失）。我们在§4.10.3：理论上下文中讨论这个和其他限制。适应阶段的两个中心量是种群适应损失 𝐿adapt(𝛾,ˆ𝜃FM)=E𝑥∼𝑝task[ℓadapt(𝑥;𝛾,ˆ𝜃FM)] (4) 和最小适应损失 𝐿 adjustment(ˆ𝜃FM)= min 𝛾∈Γ ,𝐶(𝛾;ˆ𝜃FM)≤𝑐0𝐿adapt(𝛾,ˆ𝜃FM) (5) 模块化阶段的单独分析。现有的标准监督学习泛化理论旨在表明 b𝐿pre ≈𝐿preandb𝐿adapt ≈𝐿adapt。专门针对深度网络解决这些问题是一个活跃的研究领域。我们还可以利用标准学习理论分解，通过过量泛化误差和最小适应损失来限制最终下游任务损失，如下所示。 𝐿适应（𝛾任务，ˆ𝜃FM）≤𝐿适应（𝛾FM）| {z } 最小适应损失+泛化误差 (6)，其中泛化误差捕获 𝐿adapt 和 b𝐿adapt 之间的接近度。89这些关键量之间的分解和关系如图 22 所示。如上所述，泛化和优化箭头在很大程度上简化为监督环境中的深度学习理论。我们剩下的是基础模型的主要挑战，即理解为什么最小适应损失 𝐿* adjustment(ˆ𝜃FM) 可能会因为预训练群体损失较小而很小，这在 §4.10.2 中进行了研究：理论-界面 。 Arora 等人的工作 [2019b] 通过在对比学习的背景下从上面的 𝐿 adjustment(ˆ𝜃FM) 到 𝐿pre(ˆ𝜃FM) 的边界，开创了对这个问题的追求，HaoChen 等人的工作。 [2021a]； Tosh 等人[2020, 2021]放宽了数据假设。其他预训练方法成功分析了 88，通过显式约束 𝐶(𝛾,ˆ𝜃FM)≤𝑐0 来表征适应的归纳偏差可能并不总是可行。我们提出的模块化也适用于这些情况，但为了符号简单起见，我们关注可以通过显式约束来近似隐式正则化的情况。 89更准确地说，泛化误差项是𝐿adapt(𝛾task,ˆ𝜃FM)−b𝐿adapt(𝛾task,ˆ𝜃FM)和b𝐿adapt(𝛾task,ˆ𝜃FM)−𝐿adapt(𝛾,ˆ𝜃FM)=b𝐿adapt(𝛾task,ˆ)之和𝜃FM)−𝐿 适应( ˆ𝜃FM)，其中𝛾任务是 (5) 的最小者。 (6) 很容易使用 b𝐿adapt(𝛾task,ˆ𝜃FM)≤b𝐿adapt(𝛾 task,ˆ𝜃FM)。关于该框架下基础模型 121 的机遇和风险（隐式或显式）包括使用语言模型进行预训练 [Wei 等人，2017]。 2021]或自我监督[Lee et al.2020a]，使用自我训练算法[Wei et al.2020b；蔡等人。 2021]，并具有多个监督任务[Tripuraneni 等人。 2020；杜等人。 2020]。 

### 4.10.2 为什么预训练-适应接口很有趣？

如图 22 所示，标准监督理论之外的主要缺失环节是：在什么条件下，小群体预训练损失 𝐿pre(ˆ𝜃FM) 意味着小的最小适应损失 𝐿 adjustment(ˆ𝜃FM)，为什么？实现成功界面的条件可能取决于几个量，例如预训练和适应分布、目标和训练方法以及模型架构。这个问题超出了标准泛化理论的范围，但它确实将我们的范围缩小到基础模型特有的几个重要因素，并抓住了基础模型上各种重要的开放问题的本质，如下文所述。首先，我们注意到该接口处理涉及两种不同分布的人口数量。因此，成功接口的条件可能涉及分布的特殊属性，例如预训练分布的多样性以及预训练和适应数据之间的结构转变。这使得界面分析具有挑战性（如下面第 4.10.4 节：理论工具中所述），因为我们需要对两个分布如何相互关联进行仔细的建模假设。然而，这表明为分析此类界面而开发的工具和技术可能有助于理解分布变化的影响并预测基础模型何时可以提高鲁棒性。其次，人口损失以及成功界面的可能条件取决于模型架构。这提出了打开神经网络黑匣子的挑战。特定分布上的小预训练损失告诉我们中间层的哪些属性？此类分析还将指导我们设计新的适应方法，更仔细地利用不同的中间表示。第三，小样本学习或适应的样本效率可以通过最小适应损失中复杂性度量 𝐶(𝛾,ˆ𝜃FM)的约束来捕获。我们需要正式描述这些复杂性度量（例如，通过理解适应过程的隐式正则化效应），并进一步理解为什么少量的预训练损失意味着低复杂性的适应参数𝛾任务。这个问题的令人满意的答案可能会让我们提高下游适应的样本效率。最后，也是重要的一点，接口的关键组成部分是预训练和适应损失的选择。我们希望了解如何最好地将预训练和适应目标结合起来以实现成功的适应。最能保证成功适应的预训练目标可能与预训练过程中明确最小化的目标不同——上面的界面允许人们在预训练分布上使用任何替代群体目标。此外，新的替代目标可以证明可以在广泛的任务中实现良好的适应，可以揭示使基础模型成功的基本方面。总而言之，该界面排除了泛化问题，并允许我们正式推理预训练和适应阶段的几个重要量之间的相互作用，这些量可以以重要的方式指导实践。 

### 4.10.3 挑战：分析情境学习和其他突发行为。

GPT-3 [Brown et al.2020] 展示了上下文学习的力量，这是一种不需要任何参数优化的适应方法。在适应阶段，预训练的语言基础模型接收一个提示——一系列连接任务中输入输出示例的标记序列122基础模型研究中心（CRFM）——后面是一个测试示例，并简单地生成该任务的标签。通过调节迄今为止看到的序列来测试示例（提示加上测试示例）。换句话说，没有对模型参数进行显式训练或更改。模型通过简单地使用示例作为输入来“学习”不同示例的机制是什么？之前的模块化并不直接适用，因为我们在适应过程中没有获得新的模型参数，而是仅通过执行结构设计的输入来使用基础模型的生成能力。然而，将无限数据预训练和有限数据预训练分开的想法仍然有用。例如，最近的一项工作从假设无限预训练数据和足够的模型表达力来研究上下文学习[Xie et al.2021c]。这些假设将上下文学习的特征简化为分析以上下文学习提示为条件的预训练分布，这些提示是从与预训练数据不同的分布中得出的。特别是，Xie等人[2021c]提出，上下文学习是从预训练分布中的长期一致性结构中产生的，它由具有一致性结构的潜变量生成模型来描述。更广泛地说，虽然本节提出的模块化提供了一个很好的框架来获得对基础模型的有用理论见解，但一些新兴行为（例如上下文学习和其他尚未发现的功能）可能需要超越模块化，例如，通过打开架构的黑匣子。 

### 4.10.4 挑战：适当的数据假设和数学工具。

与传统的监督学习相比，理解预训练和适应阶段之间的接口需要对数据分布进行更仔细的研究。这是因为预训练和任务适应分布本质上是不同的。根据定义，基础模型是根据原始数据进行训练的，这些数据通常极其多样化且与任务无关，而适应数据在很大程度上取决于任务。类似地，上下文学习是学习生成看起来像预训练分布的数据的结果，因此理解上下文学习需要对预训练数据进行仔细建模。因此，回答围绕基础模型的核心问题需要现实且可解释的假设，这些假设也易于分析。最近的工作要么假设人口数据的某些属性，例如，HaoChen 等人[2021a]中的扩展属性； Wei 等人[2020b]，或者人口数据是从具有某种结构的潜变量模型生成的[Saunshi 等人。 2020a；魏等人。 2021 年；阿罗拉等人。 2016年；李等人。 2020a；张和桥本2020；托什等人。 2021]。我们通常缺乏将基础模型的属性与人口数据分布的结构联系起来的数学工具。 haoChen 等人[2021a]应用谱图理论来利用总体分布中的内部类连通性。对于潜变量模型来说，通过概率和分析推导可以更精确地表征 ˆ𝜃FM，但到目前为止仅限于相对简单的模型。解决这个问题的更系统和通用的数学工具将使社区受益匪浅。定义简单的玩具案例也是非常可取的，以便理论家可以精确地比较各种工具和分析的优势。例如，HaoChen 等人[2021a] 和 Wei 等人。 [2020b]考虑流形混合问题，这可能是视觉应用的一个很好的简化测试平台。我们需要针对 NLP 等离散领域的更有趣的测试床。我们相信，捕捉真实数据集相关属性的易于处理的理论模型是将基础模型建立在坚实的理论基础上的关键一步。论基础模型的机遇和风险

## 4.11 可解释性

作者：John Hewitt*、Armin W. Thomas*、Pratyusha Kalluri、Rodrigo Castellon、Christopher D. Manning 

与大多数其他机器学习模型相比，基础模型的特点是训练数据和复杂性大幅增加，以及不可预见能力的出现：基础模型能够完成不可预见的任务，并在不可预见的方式。因此，越来越多地采用基础模型，为理解其行为带来了日益增长的愿望、需求和前所未有的挑战。与特定于任务的模型相比，基础模型是在巨大且通常高度不同的数据集上进行训练的，可能跨越许多领域和模式（参见§4.2：训练）。通过这种训练，基础模型学习了极其广泛的行为，这些行为在任务和领域之间可能存在很大差异，正如它们适应不同类型的下游任务并表现出针对每个任务的特定行为的能力所证明的那样（参见§4.3：适应）。以 GPT-3 为例，它被训练成一个巨大的模型来简单地预测文本中的下一个单词。虽然这是一项非常具体且易于定义的学习任务，但通过将其与包含各种类型的庞大训练数据集相结合，它使 GPT-3 获得了远远超出与下一个单词预测相关的能力。互联网文本。因此，当提供一些训练样本时，GPT-3 现在可以适应明显超出其原始训练任务范围的行为，例如简单的算术和计算机编程。这表明，即使回答有关基础模型的看似最简单的问题也具有挑战性：它具有哪些功能？此外，这些不同的功能在多大程度上依赖于不同或共享的模型机制（类似于模型中的算法构建块）也是一个悬而未决的问题。一方面，基础模型可以解释为单一模型，它利用一组可泛化的模型机制来跨任务和领域良好地执行。在这种情况下，可以通过识别和表征这些机制来充分了解他们的行为。另一方面，基础模型能够适应不同任务的截然不同的行为，这表明它们也可以被理解为大量独立专家模型的集合，每个模型都针对特定任务量身定制。例如，GPT-3 用于算术的模型参数似乎不太可能与用于从英语翻译成法语的参数有很大关系。在这种情况下，一项任务中的模型行为的解释不一定能够提供有关其他任务中的行为的信息。我们将此称为基础模型的单模型-多模型性质（参见图 23），并认为了解基础模型在一个模型和多个模型之间的范围内的位置对于理解其行为至关重要。为了使这一研究领域系统化，我们提出并讨论了三个层次的理解基础模型[受到 Marr 1982 的启发]：我们首先讨论理解模型能够做什么的挑战和机遇，然后讨论为什么它输出某些行为，最后讨论它是如何做到的。具体来说，问题的目的是什么，旨在描述模型在不窥视模型内部的情况下可以执行的行为类型，而为什么的问题，旨在根据数据中的潜在原因提供对模型行为的解释，以及如何旨在解释模型的行为。了解产生这些行为的内部模型表示和机制。在介绍了所有三个层次之后，我们最后讨论了基础模型的不可解释性和可解释性所产生的潜在后果。124 基础模型研究中心 (CRFM) 图 23. 基础模型的单一模型 - 多模型性质：A核心的可解释性问题是了解基础模型在一个模型和多个模型之间的范围内的位置。作为一种模型，可以通过识别和表征用于产生跨任务行为的有限数量的可概括模型机制（例如，为单词分配含义、比较数量和执行算术的机制）来使行为变得可解释。与许多模型一样，对一项任务中的模型行为的解释不一定能够提供有关其他任务中的行为的信息，因此需要对每个任务中的行为进行独立研究。 

### 4.11.1 表征行为

人们普遍认为对技术最简单的理解就是了解该技术的用途。这个看似简单的问题对于基础模型来说是一个巨大的挑战，因为这些模型能够执行无数不可预见的行为和任务。特定于任务的神经网络模型经过训练以在单个域中执行单个任务，例如图像分类。因此，他们的任务以及输入和输出域是明确的；然而，即使对于这些模型，在给定特定输入的情况下，准确了解模型将做什么也可能具有挑战性。例如，对于两个感知相似的输入，模型行为可能会出现意想不到的巨大差异 [Garg 和 Ramakrishnan 2020； Jin 等人 .2020] 或相同数据的两个亚群（例如，按种族或性别分层 [Hovy 和 Søgaard 2015；Blodgett 等人 2016；Tatman 2017；Buolamwini 和 Gebru 2018]）。对于基础模型来说，表征模型行为的挑战被放大了很多倍。模型能够执行的任务空间通常很大且未知，输入和输出域通常是高维且广阔的（例如，语言或视觉），并且模型对特定于域的行为或故障的限制较少模式。例如，考虑一下 GPT-3 令人惊讶的能力，它可以在大型语言语料库上进行训练，并随后开发出生成主要功能的计算机程序片段的能力。因此，表征基础模型行为的一个关键挑战是确定其具有的功能。更进一步，对于基础模型可以执行的每一项任务，可能有很多或无限多个，所有关于基础模型的机遇和风险 125，当人们试图理解更简单的、特定于任务的行为时，仍然面临挑战。楷模。基础模型可以执行的每项“任务”的特征由于其单一模型多模型的性质而变得更加复杂（参见图 23）。再次以 GPT-3 为例，结果表明它可以通过简单的提示来适应许多任务（参见§4.3：适应）。然而，每个任务都可以通过许多可能的提示来指定，并且提示中的细微变化可能会导致模型行为发生有意义的变化。例如，电影评论的情感分类任务可以通过呈现电影评论后跟“她对这部电影的情感是……”或“我的总体感觉是这部电影是……”来指定；尽管这些提示似乎提出了密切相关的任务，但 GPT-3 对每个提示都会表现出不同的响应精度 [Zhao et al.2021]。诸如此类的观察结果提出了有关提示特征与最终模型行为之间关系的重要问题。具体来说，对看似相似的提示的有意义的不同反应实际上是否可以被认为是由同一模型产生的，或者它们是由高度不同的模型机制产生的，并且在一项任务中描述基础模型（或其改编的衍生物）的行为确实有助于描述模型其他可能的适应行为的特征？为了确定基础模型具有和缺少的功能，研究人员可以利用受控评估。在这里，领域专家设计已知需要特定能力的提示，然后研究模型正确响应这些提示的能力 [Papadimitriou 和 Jurafsky 2020； Lu 等人.2021a； Kataoka 等人.2020; Wu 等人.2021c； Xie 等人.2021a； Koh 等人，2021]。例如，心理语言学家设计了提示，要求语言模型在语法正确的句子和具有特定语法错误的同一个句子之间进行选择；了解模型是否始终更喜欢语法正确的句子而不是语法不正确的句子，这可以告诉我们该模型是否具有识别这种不准确所需的特定语法能力[Linzen 等人。 2016]。鉴于基础模型的可能功能范围广泛，并且我们目前缺乏任何通用方法来先验确定基础模型是否具有给定的功能，因此此类定制评估至关重要。它们允许探索基础模型能够执行的行为范围，同时需要最少的模型访问：我们只需要呈现输入并接收模型输出，并且不需要依赖于对模型的实现或参数的访问。考虑到基础模型可能有（或无能力）无数期望和不期望的任务、子任务和行为，表征模型行为和能力将变得越来越具有挑战性和重要性。我们相信，与其依靠少数专家来制定和测试可能的行为，不如扩展这些类型的分析以测试更多的行为，这在一定程度上是通过向不同社区和专家开放这一探索路线来实现的。许多学科，以及通过增加这些评估的机会和规模。 

### 4.11.2 解释行为

除了描述基础模型正在做什么之外，我们还可以通过根据数据中的潜在原因提供对这些行为的解释来尝试描述其执行某些行为的原因。虽然当前提供此类行为解释的解释方法可以揭示影响模型响应的输入质量，但它们通常需要完全访问模型才能做到这一点，并且通常在阐明任何通用模型机制的能力方面受到限制，这奠定了基础模型用于响应许多输入、任务和领域。当前的解释方法通常可以理解为不同的模型，旨在提供对另一个黑盒模型的特定行为的解释。重要的是，126 基础模型研究中心 (CRFM) 这些方法与对其行为进行分析的模型是分开的，模型本身是不可解释的。这种分离可能会产生问题，因为所提供的解释可能缺乏忠实度[Jacovi and Goldberg 2020]，对行为的原因不可靠且具有误导性[参见。鲁丁2019]。更进一步，不合理的解释可能会诱使人类更加信任不合理的模型（有关人工智能信任的详细讨论，请参阅 Jacovi 等人[2021]）。随着我们从特定于任务的模型过渡到广泛采用基础模型，这些类型的担忧不断增加，因为它们的行为要复杂得多。当前的解释方法主要可分为提供模型行为的局部解释或全局解释 [Doshi-Velez 和 Kim 2017]。局部解释试图解释模型对特定输入的响应，例如，通过归因与行为的每个输入特征的相关性或通过识别与行为最相关的训练样本[Simonyan 等人。 2013年；巴赫等人，2015； Sundararajan 等人，2017； Shrikumar 等人，2017；斯普林伯格等人，2014；泽勒和弗格斯 2014；伦德伯格和李，2017； Zintgraf 等人，2017； Fong 和 Vedaldi 2017； Koh 和梁 2017]。相反，全局解释并不依赖于特定的输入，而是旨在揭示影响模型行为的整体数据的质量，例如，通过综合模型与行为最密切相关的输入[Simonyan et al.2013 ; Nguyen 等人.2016]。局部和全局解释为特定任务模型的行为提供了有用的见解[例如，Li et al.2015； Wang 等人，2015b； Lapuschkin 等人，2019；托马斯等人，2019；波普林等人。 2018]。在这里，由此产生的解释通常被视为对引起行为的模型机制的启发；例如，当模型读取手写数字“7”时，看到解释对水平线赋予高度重要性，很容易让人产生这样的印象：水平线是模型用来识别所有七或可能区分所有数字的普遍重要特征。然而，考虑到基础模型的一模型多模型性质，我们应该小心，不要从行为的具体解释跳到关于模型行为的一般假设。虽然当前的解释方法可能会阐明特定的行为，例如，通过识别强烈影响这些行为的数据的各个方面，但由此产生的解释不一定能够深入了解模型对其他（即使看似相似）输入的行为，更不用说其他输入了。任务和领域。另一种方法可能是通过以自我解释的形式利用基础模型的生成能力来完全回避这些类型的事后解释[cf.埃尔顿 2020； Chen et al.2018]，也就是说，通过训练这些模型不仅生成对输入的响应，而且共同生成该响应的人类可理解的解释。虽然尚不清楚这种方法在未来是否会取得成果，但有理由对此表示怀疑：语言模型和现在的基础模型在生成流畅、看似合理的内容方面表现出色，但没有任何事实依据。简单的自我生成的“解释”也可以效仿。因此，辨别模型创建听起来合理的解释的能力与提供对其行为的真实见解的能力之间的差异非常重要。 

### 4.11.3 表征模型机制

对系统的深入理解通常意味着理解系统如何执行：它包含哪些知识和机制，以及它们如何组装形成整体？如果这确实可能，那么描述基础模型中的表示以及对其运行的机制将是满足彻底理解这些增殖模型的愿望的核心。无论这些机制是多且具体的，还是少且可概括的，它们都是基础模型在不同任务和领域中采用广泛行为的能力的核心。论基础模型的机遇和风险 127 提出概念具体的模型表示和机制，考虑 GPT-3 表现出的一个简单行为：当提供小数字相加的示例时，很快观察到 GPT-3 做了什么，然后查询执行两个新数字的相加：概率很高，它预测了加法的正确结果 [Branwen 2020;布罗克曼 2020]。当询问为什么 GPT-3 表现如此时，人们可以在输入中找到证据，例如其提示的某些方面对其响应有很大影响（这可能是要添加的两个数字，但不一定），或者 GPT-3 的某些方面3 影响其响应的训练数据（这些可能是加法的示例，但不一定）。深入研究该模型，我们可能会更深入地了解 GPT-3 用于添加特定数字对的机制以及用于添加其他任意数字对的机制。我们还可以设想更深入地理解这些机制是否类似于“加法”的数学概念，或者仅仅与这个概念相关。通过理解各个模型机制，我们可以建立对基础模型复杂行为的组合理解。比数字加法稍微复杂一些的任务是解决数学文字问题，其中数字带有单位，并且问题以自然语言呈现。一旦我们了解了模型执行加法的机制，我们就可以研究该机制是否用作解决应用问题的中间步骤。如果使用加法机制，我们就已经建立了对模型如何解决应用问题的理解，我们就更有信心基础模型概括了数量和加法的概念（而不是另一种相关性或启发式），此外，我们有增强了我们对预测模型的原因（模型关注输入的哪些部分）和输出是什么（两个数字相加）的能力的信心。如果不使用加法机制，我们可能会对这是否是真正的加法保持健康的怀疑，并且我们可以研究使用哪些表示和机制。重要的是要意识到，存在许多更复杂和令人担忧的模型机制的潜在情况，例如，根据名称中的字符或图像中的像素来估计种族。在基础模型中建立这种机制的证据及其使用可以支持道德或法律责任，以禁止该模型执行预测性警务、营销、贷款申请和整体监视等任务。已经出现了大量的方法来研究神经网络模型的这些内部方面。通常，这些方法将模型分成节点（例如，神经元、层或层的一部分），然后询问节点中捕获的表示或节点组装的机制。有些方法是假设驱动的：通过假设节点可以捕获某些信息（例如，单词的语法特征或人的种族），人们可以探测所有节点以量化它们提供了多少信息[Alain 和本吉奥 2016； Veldhoen 等人，2016；别林科夫等人，2017；阿迪等人，2017； Conneau 等人，2018；休伊特和梁，2019；休伊特和曼宁 2019；沃伊塔和蒂托夫 2020； Pimentel 等人，2020]。其他方法建立在解释性方法的基础上，它们不是识别哪些数据导致某种行为，而是试图识别哪些数据导致某个节点激活，或者哪些节点导致模型中的另一个节点稍后激活，从而揭示模型的集合表述和机制 [Olah et al.2020；穆和安德烈亚斯 2020；卡特等人，2019； Goh 等人，2021]。总而言之，这些方法检查模型的内部，并为不断探索基础模型的行为提供基础。然而，基础模型中潜在的表示和机制的数量是巨大的，特别是考虑到它们的一个模型——许多模型的性质，并且这些类型的方法通常只捕获模型内部性的一小部分。因此，扩大表征和机制的发现并阐明与模型行为最相关或最普遍的表征和机制是一个公开的挑战。与解释基础模型的许多方法一样，这些类型的探索将受益于 128 基础模型研究中心 (CRFM)，包括和支持更多样化和跨学科的研究人员，以及更容易获得、灵活和可扩展的发现方法。总之，我们认为基础模型的单一模型-多模型本质（回想图 23）为当前的可解释性研究提供了新的机遇和挑战：单一基础模型有多种适应，而我们根本不知道其适应程度它们具有共同的机制。在机制共享的情况下，理解基础模型可能是描述这些机制及其关系的一个棘手问题。就机制是独立的而言，基础模型的每次适应都必须独立分析，从而导致基础模型的任何新适应的性质存在极大的不确定性。 

### 4.11.4 不可解释性和可解释性的影响

最后，我们想强调的是，基础模型的广泛采用与许多跨学科研究人员最近的呼吁相矛盾，即不要使用复杂的黑盒模型进行高风险决策[例如，Rudin 2019]，而是将重点放在长期决策上。 - 更本质上可解释的模型的持续开发和应用。在这些请求中，旨在解释基础模型的工作是一把双刃剑。大型机器学习模型，以及现在的基础模型，通常由强大的公司和机构部署，可解释性的渐进式进步可能会被夸大为“道德清洗”，并继续使用模型，就好像它们已经实现了可解释性一样，掩盖了这样的现实：它们仍然远远低于算法可解释性的传统标准。此外，当可解释性方法通常假定可以轻松访问模型及其实现和参数时，可解释性不仅可以充当强大机构的掩护，而且可以将模型知识集中在同一个人手中。对于那些致力于基础模型可解释性的人来说，有责任不断地问自己是否正在努力使基础模型可以被研究人员和模型所有者解释，或者可以被每个人解释。同时，就基础模型已经被部署的程度而言，可解释性方面的工作提供了独特的机会，可以将基础模型的知识以及权力转移回数据化和评估的人们。解释可以促进模型的社会显着方面的发现。更根本的是，创建易于理解的方法，允许任何人解释基础模型的行为，将权力转移给不同的人，创造研究模型的机会，发现模型对个人或其社区重要的方面的机会，以及有意义地同意、改进的机会。 ，或者完全竞争基础模型的使用。最后，对于研究人员来说，重要的是，研究人员不仅将基础模型的可解释性视为一个目标，而且将其视为一个问题：研究可以探索和评估基础模型可解释性的缺乏是否是内在的，是否应该作为一个严重问题进行深入研究和广泛了解不鼓励使用（或加强监管）这些系统，或者未来的基础模型是否有可能为所有人维持高标准的可解释性。关于基础模型的机会和风险 129 

# 5 社会 

基础模型的社会影响，参考两者模型本身的构建及其在开发应用程序中的作用需要仔细检查。具体来说，我们预计基础模型将产生广泛的社会影响，这些影响很难理解：基础模型是不直接部署的中间资产，而是作为进一步适应的基础。因此，推理技术社会影响的传统方法可能很复杂。对于具有明确目的的系统来说，社会影响更容易（但仍然很难）掌握。在本章中，我们讨论如何应对并开始理解模型基础模型的社会影响的复杂性。具体来说，我们讨论 (i) 不公平（§5.1：公平）和滥用（§5.2：滥用）方面的危害，(ii) 对经济（§5.5：经济学）和环境（§5.3：§5.3：环境），以及（iii）有关法律（§5.4：合法性）和道德（§5.6：道德）的更广泛考虑。130 基础模型研究中心（CRFM） 

## 5.1 不平等和公平

作者：Rishi Bommasani、Fereshte Khani、Esin Durmus、Faisal Ladhak、Dan Jurafsky

图 24. 基础模型中存在的内在偏差是各种训练偏差源（左）的副产品，它与适应过程中引入的偏差一起决定了用户经历的外在伤害（右）在特定下游应用的背景下。我们强调同一个基础模型是许多不同应用的共享基础；结果，它的偏见传播到了许多应用程序。此外，由于用户经历的伤害是特定适应模型的结果，因此将这些伤害归因于该图中描绘的各种过程和来源既至关重要又具有挑战性。 

### 5.1.1 简介

基础模型有可能产生不公平的结果：人们受到不公正的待遇，特别是由于分配不均，加剧了历史歧视[Hellman 2021]。与任何人工智能系统一样，基础模型可以通过产生不公平的结果、巩固权力体系以及将技术的负面后果不成比例地分配给那些已经被边缘化的人来加剧现有的不平等[Sweeney 2013；凯等人，2015； Buolamwini 和 Gebru 2018；本杰明2019；阿君瓦2019；迪纳齐奥和克莱因 2020；克劳福德 2021]。在这里，我们询问哪些与公平相关的损害与基础模型相关，哪些来源对这些损害负责，以及我们如何干预来解决这些问题。我们在这里讨论的问题与更广泛的算法公平性和人工智能伦理问题有关 [Corbett-Davies and Goel 2018;乔尔德霍娃和罗斯 2020；赫尔曼 2020；约翰逊 2020； Fazelpour 和 Danks 2021]，种族和技术 [Benjamin 2019；汉娜等人.2020; 2021 年格布鲁； Field et al.2021]，以及社会与技术的共存[Abebe et al.2021]。 2020]。 

### 5.1.2 危害

基础模型是在调整之前没有特定用途的中间资产；了解它们的危害需要推理它们的特性以及它们在构建特定任务模型中所扮演的角色。我们描述了内在偏差，90即间接但普遍影响下游应用的基础模型的属性，以及外在危害，即在特定下游应用背景下出现的危害[Galliers and Spärck Jones 1993]。 90我们使用“偏见”一词来表示导致不平等的基础模型的属性；我们关注布洛杰特等人。 [2020] 在可能的情况下尝试描述谁受到伤害以及他们如何受到伤害。关于基础模型的机会和风险 131 内在偏见。基础模型的属性可能会对下游系统造成损害。因此，这些内在偏差可以在基础模型中直接测量，尽管危害本身只有在基础模型经过调整并随后应用时才会意识到，即这些是潜在的偏差或危害[DeCamp and Lindvall 2020]。我们关注最广泛研究的内在偏见形式，即代表性偏见，特别考虑误述、代表性不足和过度代表性。人们可能会被有害的刻板印象所歪曲[Bolukbasi et al.2016; Caliskan 等人，2017；阿比德等人2021； Nadeem 等人，2021；格曼等人。 2020]或消极态度[Hutchinson et al.2020]，这可以通过下游模型传播，从而强化社会中的这种误传[Noble 2018；本杰明2019]。例如，当 LGBTQ+ 身份术语出现时，人们可能会被低估或完全被抹去 [Strengers et al.2020;奥利瓦等人.2021; Tomasev et al .2021] 或描述非裔美国人的数据 [Buolamwini 和 Gebru 2018； Koenecke 等人.2020; Blodgett and O'Connor 2017] 被排除在训练数据中，下游模型将在测试时处理类似的数据。人们可能被过度代表，例如，默认情况下，BERT 似乎编码了一种以英国为中心的观点 [Zhou 等人 .2021a]，这可以放大大多数声音并有助于观点同质化 [Creel 和 Hellman 2021] 或单一文化 [Kleinberg 和 Raghavan 2021]（ §5.6：道德）。这些表征偏差适用于所有人工智能系统，但它们的重要性在基础模型范式中大大增强。由于相同的基础模型充当无数应用程序的基础，因此人员表示中的偏差会传播到许多应用程序和设置。此外，由于基础模型承担了大部分繁重工作（与通常旨在轻量级的适应相比），我们预计许多经历的危害将在很大程度上由基础模型的内部属性决定。外来的伤害。用户可能会遇到通过调整基础模型创建的下游应用程序带来的特定危害。这些危害可能具有代表性[Barocas et al.2017；克劳福德 2017; Blodgett et al .2020]，例如信息检索系统对黑人女性的性化描述[Noble 2018]，默认为男性代词的机器翻译系统对人的性别歧视[Schiebinger 2013，2014]，或者有害的产生刻板印象[Nozza et al.2021；盛等人，2019；阿比德等人.2021]。它们可能包括滥用，例如当基于基础模型的对话代理使用有毒内容攻击用户时[Dinan 等人。 2021 年； Gehman et al.2020] 或微侵犯[Breitfeller et al.2019； Jurgens 等人，2019]。所有这些面向用户的行为都可能导致心理伤害或强化有害的刻板印象 [Spencer 等人，2017]。 2016年；威廉姆斯 2020]。除了个人遭受的伤害之外，群体或亚群体也可能遭受群体层面的绩效差异等伤害。例如，系统可能在非裔美国英语的文本或语音方面表现不佳 [Blodgett and O'Connor 2017; Koenecke 等人 .2020]，从种族、性别和保险状况少数群体的临床记录中错误地检测出医疗状况 [Zhang 等人 .2020b]，或者未能检测到肤色较深的人的面部 [Wilson 等人 .2020b]。 2019； Buolamwini 和 Gebru 2018]。随着基础模型的应用更加普遍，包括在高风险领域，这些差异可能会导致进一步、更严重的危害。 Koenecke 等人 [2020] 讨论了如果非裔美国英语使用者无法可靠地使用语音识别技术（例如，由于底层基础模型的不平等），这可能意味着他们无法从某些衍生产品（例如语音助手、辅助技术）中受益），如果这些技术用于进行就业面试或转录法庭程序，则会处于不利地位。更一般地说，描述这些群体层面的伤害（并努力为那些受到伤害的人伸张正义）还需要人工智能社区提高对基于群体的偏见 [Allport 1954] 和社会群体的理解：我们指出社会科学和社会科学领域的相关工作其他社区关于超越性别二元处理132 基础模型研究中心 (CRFM) [Lindsey 2015；威斯布鲁克和萨珀斯坦 2015；理查兹等人，2017；达尔文 2017;凯斯2018；海德等人，2019；曹和多梅三世 2020； Dinan et al.2020]，对种族进行更细致的处理[例如，Penner 和 Saperstein 2008；弗里曼等人，2011；萨珀斯坦和彭纳 2012； Saperstein 等人，2013；彭纳和萨珀斯坦 2015； Field et al.2021]，更好地解决交叉身份问题[例如，Crenshaw 1989；纳什2008；吉尼斯 2011;彭纳和萨珀斯坦 2013；加瓦米和佩普劳 2013； Bright 等人，2016； Buolamwini 和 Gebru 2018；梅等人，2019；奥康纳等人，2019； Guo 和 Caliskan 2021]，以及更现代的残疾治疗方法 [例如，Batterbury 2012；斯皮尔等人。 2019；哈钦森等人。 2020]。其他注意事项。为了更全面地了解基础模型的危害，需要进一步记录其内在偏差和外在危害；未来的工作应该阐明内在偏见和外在伤害之间的关系[Blodgett et al . 2020、2021； Goldfarb-Tarrant 等人，2021]。该文件要求将利益相关者置于学术界和行业从业者之外：基础模型的不公平影响将主要由少数群体经历，而他们在学术界和行业中的代表性都不足。特别是对于基础模型来说，它们的创建和研究可能将由那些拥有所需渠道和资源的人进行，这进一步强调了集中边缘化声音的场所的重要性[D'Ignazio and Klein 2020，§5.6：道德]。特别是，当跨应用程序汇总时，对特定适应模型的用户研究可以提供令人信服的个性化文档，说明基础模型的内在偏差所带来的危害，同时以个人用户为中心。通过这种方式，我们想象人机交互（HCI）中的方法，通过一些调整以适应基础模型中涉及的抽象，将有助于集中边缘化社区的声音（第 2.5 节：交互中的进一步讨论）。 

### 5.1.3 来源

为了充分表征并适当干预基础模型的危害，我们必须能够将其根源追溯到基础模型的属性和适应过程，并进一步分解到个体偏差来源的作用[Friedman和Nissenbaum 1996]。来源追踪对于归因所经历的伤害的道德和法律责任至关重要，尽管归因需要新颖的技术研究，以强调因果关系 [Pearl 2000] 和影响力 [Koh and Liang 2017] 等问题。数据。多种类型的数据根据基础模型塑造应用程序的行为以及相关的外在危害：用于训练基础模型的训练数据、用于适应基础模型的适应数据以及测试时用户数据/交互。对于所有这些数据源，数据的属性（例如，毒性和仇恨言论 [Henderson et al.2017]、辱骂性语言 [Waseem et al.2017]、微攻击 [Breitfeller et al.2019]、刻板印象 [Voigt et al.] al . 2018]）将体现在基础模型（及其改编衍生品）的偏差中。91由于训练数据是确定基础模型和相关内在偏差的关键数据源，因此我们在这里重点关注训练数据。目前，训练数据以及相关数据实践（例如，数据管理、数据选择和数据加权[Paullada et al.2020；Bender et al.2021；Rogers 2021]）与通过以下方法获得的内在偏差之间的关系基础模型仍不清楚；未来的工作迫切需要澄清这种关系。由于基础模型通常需要大规模的训练数据，这不仅对其文档提出了明显的挑战[Bender et al.2021]，而且对 91In 适应的全面科学探索提出了挑战，其中涉及标记的特定于任务的数据，因此选择的偏差标签空间 [Crawford 2021] 以及标记该数据的注释者的偏差 [Geva et al.2019； Sap et al.2019] 也可能导致用户遭受外在伤害。在基础模型 133 的机遇和风险中阐明了数据偏差和模型偏差的关系，我们预计需要新的协议来解决这一问题。建立偏差的标度法则，类似于准确度指标的标度法则[Kaplan et al.2020； Henighan et al.2020]，可能能够在较小规模上进行系统研究，为较大规模的数据实践提供信息。造型。建模决策（例如，训练目标（§4.2：训练）、模型架构（§4.1：建模）、适应方法（§4.3：适应））影响基础模型及其衍生物的偏差，从而影响所经历的外在伤害。现有的工作表明，基础模型放大了训练数据偏差，扩展了机器学习和深度学习模型的趋势[Zhao et al.2017；王等人.2019d;贾等人.2020; Hashimoto et al.2018]，尽管模型属性是什么以及如何导致这种偏差放大仍然不清楚。此外，考虑到直接应用基础模型可能不可行（由于其规模），压缩这些模型或使其更高效的努力似乎也会放大偏差[Hooker et al.2020； Renduchintala 等人.2021]。反馈循环也可能会加剧放大，在反馈循环中，基础模型会改变社会行为并引发社会学变化，从而修改后续的训练数据；这种形式的反馈效应往往会加剧其他机器学习应用中的不平等[Lum and Isaac 2016; Ensign 等人，2018；桥本等人.2018]。除了在训练和应用基础模型时做出的明确决策之外，社区价值观 [Birhane et al.2020] 和规范（§5.6：道德）间接和隐含地 [Liu et al.2021b] 塑造了构建模型的决策。因此，结合引入基础模型 [例如，Brown 等人 .2020] 和标准基准 [Friedman 和 Nissenbaum 1996，§4.4：评估] 的工作来测量偏差，以及对不同用户组进行用户研究以记录经历过的伤害，是确保最佳实践积极强调考虑偏见和不平等的步骤。模型师。与所有算法系统一样，开发或应用基础模型的决策机构中利益相关者和边缘化社区的代表性和多样性较差，这本质上是有问题的，并且可能会给这些社区带来更大的伤害。 92 虽然难以记录，但现有的努力开发基础模型表明这是一种可能性：Caswell 等人[2021]展示了用于训练多语言模型的多语言数据集中代表性较少的语言的有缺陷的数据处理，Hutchinson 等人[2020]表明模型往往包含对残疾人的不良偏见。在这两种情况下，通过在开发团队中更好地代表这些各方，这些偏见和危害可能会更早地被注意到。此外，由于最终用户可能比开发人员更加多样化，并且可能更早注意到这些问题，因此允许用户反馈为基础模型设计做出贡献（第 2.5 节：交互）是一个重要的前进方向。 

### 5.1.4 干预和追索

解决、减轻和纠正与技术相关的不平等需要整合社会和技术方法[Abebe et al.2020]。具体来说，对于基础模型，我们考虑主动方法（改变模型的开发和部署方式以预防性地减少伤害）和反应方法（对伤害做出响应并为未来做出改变）。从本质上讲，基础模型的抽象使两个方面都变得复杂：要了解基础级别的干预措施是否成功减少伤害，需要在特定部署的应用程序级别进行下游观察，并在事件中追索92我们注意到多样性，无论是尊重学科背景和人口特征，在这些高影响力的决策环境中至关重要，其原因远远超出了对公平相关危害的潜在改进认识。 134 危害基础模型研究中心 (CRFM) 需要上游传播对基础模型提供者的反馈和责任。干涉。管理技术系统干预的一般原则适用于基础模型设置：确定哪些来源对偏见或伤害负有最大责任，为有针对性的行动提供了所需的证据。例如，迫切需要提高设计、生产和控制技术（例如基础模型）及其应用的团队的多样性[Longino 1990；哈丁2015；尼尔森等人，2017；奥康纳等人，2019；霍夫斯特拉等人，2020；如果多样性的缺乏被证明与危害有关，那么 Katell 等人 2020] 就会进一步加剧 [Caswell 等人 2021]。此外，透明的文档[例如，Gebru et al.2018；本德和弗里德曼 2018； Mitchell 等人 .2019] 和审计 [例如 Raji 和 Buolamwini 2019] 在提供干预和变革动力方面同样至关重要 [Burrell 2016；立顿2018；鱼架2020；拉吉等人.2020;威尔逊等人，2021]。基础模型的规模及其可访问性的细节，给现有的文档和审计协议带来了新的挑战，我们将在第 5.6 节：道德规范中进一步讨论。迄今为止，为减少技术的不公平影响而考虑的许多干预措施，包括在基础模型制度中，都是以数据（以避免反映不平等或偏见）和建模决策（以避免放大数据偏见）为中心的技术缓解方法。涉及。在基础模型体系中特别重要的是认识到这些缓解方法可能针对管道中的不同步骤，例如训练数据[例如，Lu et al.2020]、建模目标[例如，Zhao et al.2018]），以及适应方法和测试时间使用[例如，Park et al.2018；赵等人.2019]。因此，不同的方法可能不仅或多或少有效，而且需要不同实体（例如基础模型提供商与应用程序开发人员）采取行动，并且或多或少强烈地影响这些模型昂贵的训练过程（例如，改变创建基础模型的过程与事后更改它的过程）。这种形式的技术干预也可能针对不同的目标：一些干预措施，例如改变训练数据，旨在减少内在偏差。另一方面，大多数关于算法/机器学习公平性缓解的工作反而考虑减少模型行为方面的结果差异，即与外在伤害更直接相关的下游系统的输出。目前所有形式的技术缓解都受到严重限制：衡量或消除内在偏见的方法是脆弱的或无效的[Gonen and Goldberg 2019; Ethayarajh 等人.2019;博马萨尼等人。 2020； Zhou等人.2021b； Antoniak 和 Mimno 2021]，衡量或消除外部结果差异的方法可能与利益相关者的目标不一致 [Saha 等人 .2020]，并且有一些证据表明某些类型的技术干预可能同时无法令人满意 [Corbett-Davies 和 Goel] 2018； Kleinberg et al.2017]，这是不可能的[Lechner et al.2021]，甚至可能加剧不平等[Xu et al.2021]。尽管存在这种情况，我们仍然相信技术方法仍将在解决基础模型制度中出现的危害方面发挥重要作用；总的来说，我们主张透明度，特别是考虑到技术缓解方法可能无法实现预期目标。更广泛地说，必须谨慎提出偏见和偏见缓解的主张，以便向具有不同专业知识的各个利益相关者清楚地传达现状（例如，在基础模型基础上构建的应用程序开发人员和规范技术的政策制定者；[Nissim et al. 2020]） 。追索权。不幸的是，主动干预不太可能完全解决因基础模型而可能产生的所有潜在伤害或不平等。当损害发生时，目前没有广泛采用的（或法律要求的）框架来解决受害方的适当追索问题。虽然特定应用程序可能存在某些协议，但基础模型的抽象再次引入了脱节：损害可能部分归因于基础模型提供商和下游应用程序开发人员，但将此关于基础模型的机遇和风险 135 的责任分配给其中之一党仍然充满挑战。更简单地说，甚至没有向基础模型提供商传达这些危害的机制（即使向应用程序开发人员提出反馈或投诉）。因此，需要新的规范和标准来确定应用程序开发人员和最终用户的反馈如何到达上游基础模型提供商、如何确定对这些危害负责的实体（例如基础模型提供商、应用程序开发人员）以及与法律责任的关系（§5.4：合法性）。为了在这个问题上取得进展，我们鼓励未来的工作参考其他领域中使用的实践（特别是那些具有类似抽象和多实体结构的实践），并且我们预计引入的任何标准可能需要相当动态，以便它们能够与这些模型及其应用快速变化的现状保持同步。 

### 5.1.5 要点

机器学习有着不公平影响的记录，其危害的大部分负担是由边缘化社区承担的。基础模型给这种计算带来了新的挑战，但最终，为了使其社会影响公平，需要进行大量研究和变革，以了解它们造成的危害，并有意义地解决和纠正这些危害：（1）一对多基础模型的性质，即在许多应用程序中使用相同的几个基础模型，意味着基础模型的内在属性遍及许多下游应用程序。因此，这些模型中的有害偏见对所经历的伤害产生了巨大的影响。 (2)基础模型制度中的偏差和危害有很多来源（例如，训练和适应数据、建模和适应决策、建模者多样性和社区价值观）。归因偏见和伤害的来源对于干预和责任问题至关重要；归因需要可靠地进行新技术研究。 (3)基础模型的不平等并非不可避免，但解决这些问题需要多管齐下，包括主动干预（例如以数据为中心和以模型为中心的变革）和被动追索（例如反馈和问责机制） .136 基础模型研究中心 (CRFM) 

## 5.2 滥用

作者：Antoine Bosselut*、Shelby Grossman*、Ben Newman 

图 25。该图显示了基础模型对操纵性和有害内容生成的影响，以及对检测的影响。在本节中，我们考虑基础模型的滥用——人们按照预期用途使用基础模型（例如，生成语言），但故意利用其能力对群体或个人造成伤害。该定义将滥用问题置于不平等（模型可能在没有恶意的情况下造成伤害；§5.1：公平）和安全（不良行为者利用模型中无意的能力或漏洞造成伤害；§4.7：安全）之间。下面，我们概述了基础模型如何实现新形式的滥用并支持用于滥用检测和缓解的新工具。 

### 5.2.1 基础模型将被滥用于有害目的

生成基础模型的规模（§4.2：训练）、多模态（§4.1：建模）和适应性（§4.3：适应）方面的进步将允许它们被滥用来生成高质量、廉价和个性化的内容出于有害目的。在本节中，我们将在两个恶意活动示例的背景下讨论这三个维度：操纵内容创建和骚扰。内容质量。与之前的人工智能方法相比，基础模型能够自动生成质量更高、更人性化的内容。它们可能会赋予虚假信息行为者权力，例如，国家可以创建内容来欺骗外国民众，但又不透明地表明该内容与国家有关。目前，创建此类内容通常需要雇用会说目标人群语言的人员。政府可能会将关于基础模型 137 的机遇和风险的内容外包给目标国家/地区的母语人士，93,94 但这一决定会给运营安全带来真正的风险。基础模型将允许创建通常与人类创建的内容无法区分的内容 [Kreps et al.2020; Clark et al.2021]——事实上，它将能够对多种语言做到这一点——实现创建引起共鸣的内容和维护操作安全的两个目标。除了欺骗外国人之外，基础模型生成高质量合成图像（深度伪造）或文本的能力还可能被滥用来骚扰个人。 

Deepfakes 已经被用于骚扰目的。例如，印度调查记者拉娜·阿尤布 (Rana Ayyub) 成为高质量深度伪造的目标，该伪造将她的脸叠加到色情视频上，导致她离开公共生活数月。95因为基金会模特通常是多模式的（第 4.1 节：建模），它们可能类似地冒充言论、动作或写作，并可能被滥用来使受害者难堪、恐吓和勒索。96 内容创建成本。基础模型将大幅降低内容创建成本，进一步降低恶意行为者实施有害攻击的准入门槛[Brundage et al.2018]。 

2017 年一项源自俄罗斯、针对美国人的影响力行动的预算为 1220 万美元 [DiResta et al.2018]。最近，作为虚假信息活动的一部分，俄罗斯的个人向美国自由职业者每篇文章支付 75-200 美元。97 基础模型将降低这些边际成本。虽然基础模型（例如 GPT-3）在生成内容时可能会出错 [Buchanan et al.2021]，但雇用少量编辑来修复这些错误比直接雇用内容创建者更为可行。训练基础模型的初始成本更为重要（第 4.5 节：系统），但这些费用对于大多数国家参与者来说应该是可以管理的 [Buchanan 等人，2017]。 2021]。除了金钱成本之外，基础模型需要更少的技术技能来实现高质量的结果。当前的工具（例如视频编辑软件）可以实现可信的照片或视频深度伪造，但需要熟练用户花费几个小时的时间才能生成高质量的内容。基础模型降低了使用障碍：它们的小样本适应能力（§4.3：适应）为应用程序用户提供了新的交互模式（§2.5：交互），这将允许用户快速迭代内容创建。个性化。基础模型将减少创建个性化内容的障碍。例如，2016 年俄罗斯个人针对美国发布的虚假信息包括高度定制的内容。社交媒体帖子的目的是推动有关叙利亚的叙述（例如，美国应该离开叙利亚），这些叙述与黑人生命也是命活动人士产生共鸣[DiResta et al.2018]（例如，建议美国应该关注黑人面临的问题）美国社区，而不是叙利亚问题）。同样的叙述被重新包装，以引起德克萨斯州分离主义者的共鸣 [DiResta et al.2021]。这样的内容创建工作是昂贵且耗时的。基础模型将允许类似的活动，但由于适应成本较低而规模化（§4.3：适应）。除了允许攻击者针对特定受众个性化内容的基础模型之外，它们还允许攻击者针对单个个体个性化内容——这种能力可能被骚扰者滥用。以个人属性或信息为基础的基础模型可以创建现实的个性化内容，这可能会更令人尴尬，使受害者处于更大的危险中，98并导致更成功的勒索企图。 93https://www.lawfareblog.com/outsource-disinformation 94https://fsi.stanford.edu/content/ira-takedown-20201215 95https://www.huffingtonpost.co.uk/entry/deepfake-porn_uk_5bf2c126e4b0f32bd58ba316 96https:/ /www.wsj.com/articles/fraudsters-use-ai-to-mimic-ceos-voice-in-unusual-cybercrime-case-11567157402 97https://www.nytimes.com/2020/09/02/technology/ Peacedata-writer- Russian-misinformation.html 98https://www.dw.com/en/social-media-uptick-in-honor-crime-in-middle-east/a-56370773138 基础模型研究中心 (CRFM ) 

### 5.2.2 基础模型将成为有害内容的强大检测器

虽然基础模型的生成能力将提供充足的滥用机会，但这些相同的能力可能使它们成为有害内容的强大检测器。虽然这些功能对于检测人类和模型生成的内容同样相关，但我们在本节中重点关注模型生成的内容的检测。首先，我们概述了当前的手动检测方法在发现基础模型的有害滥用方面将面临的挑战。然后，我们提出基础模型的交互式和多模态表示功能如何使其成为自动检测有害内容的强大工具。最后，我们讨论了在在线环境中部署自动检测模型以防止潜在的基础模型滥用的相关风险。重新思考人类干预。目前，恶意行为经常被人们通过搜索互联网来发现内容来源而发现（在社交媒体上，有时会被删除）。99例如，虚假的社交媒体个人资料通常会从约会网站窃取个人资料照片，这些照片可以通过反向图像搜索发现。同样，虚假信息网站经常使用抄袭内容来掩盖欺骗性内容 [DiResta 和 Grossman 2019]，这些内容可以通过进行互联网短语搜索轻松识别。基础模型将限制这些检测策略的功效。相对简单的虚假信息活动已经利用人工智能生成的照片100消除了通过反向图像搜索发现的可能性。评估这些照片是否是人工智能生成的工具是可用的，但基础模型将使这项工作变得复杂——对于文本和视频也是如此——挑战人类手动发现技术[Ippolito et al.，2017]。 2020；克拉克等人。 2021]。作为探测器的基础模型。基础模型具有相同的能力，使它们成为创意内容的强大生成器，也可能使它们成为模型生成内容的强大检测器。现有的工作表明，基础模型可以适应于检测来自文本生成器的虚假信息[Zellers et al.2019b]——生成统计文本工件[Holtzman et al.2019b]。 2020] - 并且它们可以用于通过提示问题来评估自己这一代人的毒性水平[Schick et al.2021]。下面，我们将描述未来的基础模型将如何启用更强大的机器生成有害内容检测系统。基础模型的交互式和多模式界面的改进将为改进对基础模型滥用以生成有害内容的检测提供新的机会。当前的统计检测器必须重新训练和重新部署，以整合有关滥用策略文本内容的新知识[Dinan et al.2019]。基础模型的快速学习能力（第 4.3 节：适应）可能使它们能够根据人类反馈适应新的误用策略，而基础模型最初并未经过训练来识别这些策略 [Lee et al.2021a]。同时，基础模型的多模态能力将使滥用生态系统能够更具表现力的表示。之前的工作已经探索了错误信息如何比真实内容更快地在社交网络中传播[Starbird et al.2018； Vosoughi et al.2018]，回顾性分析时产生可识别的签名。基础模型的多模态功能可以使它们共同学习有害内容的表示及其在社交网络上的典型传播特征。这些联合表示可以提供强大的工具来预测某些类型的自动生成的内容是否表明滥用行为。 99https://www.theatlantic.com/ideas/archive/2020/09/future-propaganda-will-be-computer- generated/616400/ 100有关中东活动示例，请参阅 https://www.thedailybeast.com/右翼媒体被中东宣传活动欺骗。有关古巴的示例，请参阅 https://raw.githubusercontent.com/stanfordio/publications/main/twitter-CU-202009.pdfOn the Opportunities and Risks of Foundation Models 139 基础模型作为自动检测器的风险。针对模型生成和人类生成的有害内容的自动检测系统的改进将使这些系统在网上更加普遍，从而产生潜在的负面后果。任何检测系统都会出现误报情况，其中人类生成的公平内容将被标记为有害[Sap et al.2019; Xu等人.2021]。算法误报影响用户（或用户组）的速度可能会导致下游损害（§5.1：公平性）。基础模型的自适应能力应该使系统性误报更容易解决，因为可以对模型进行本地编辑以重新分类这些示例（第 4.3 节：自适应）。然而，极端情况可能不会被优先考虑，并且在这些情况下追索权将具有挑战性。更广泛地说，滥用检测系统的大规模部署可能会引发有害内容生成器和检测器之间的“军备竞赛”。大多数使用基础模型的内容生成器将缺乏单独开发它们的资源，并且将使用较大实体部署的系统。虽然使用条款政策应概述这些系统的可接受用途（第 5.6 节：道德），但基础模型的部署者还需要内部检测系统来识别其产品的滥用101并减轻它们（第 5.4 节：合法性）。然而，对滥用资源的行为者的控制将会减少，他们有资源来开发自己的基于模型的内容生成器，这给平台带来了压力，要求它们管理通过其分发渠道共享的内容。乐观的是，内容平台涵盖了世界上一些资本最雄厚的公司。他们的资源可能使探测器的开发超出了大多数个人滥用代理的能力。由于大规模重复训练这些系统的成本很高，这种资源优势可能会抑制个体基础模型的开发。然而，即使没有最大的基础模型来支持，许多基础模型滥用的实例仍然可能成功，特别是当攻击者可能利用基础模型的交互功能来快速生成可以逃避检测的内容时。 101https://www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/140 基础模型研究中心 (CRFM) 

## 5.3 环境

作者：Peter Henderson、Lauren Gillespie、Dan Jurafsky

部署基础模型的成本效益分析的可视化。模型的总价值可以通过首先考虑模型的净正社会效益以及任何环境效益来估算。然后，我们减去训练和部署模型的负能源成本、训练模型所排放的碳的社会成本以及二次环境影响。如果净成本超过收益，那么基础模型开发人员和大规模部署者应该考虑减少危害的策略。这可能包括部署更高效的模型或根本不部署该模型。基础模型可能会带来许多社会和环境效益，例如在法律领域（§3.2：法律）、医疗保健（§3.1：医疗保健），甚至应对气候变化 [Rolnick et al.2019]。但由于它们的规模，如果模型创建者不小心，它们本身可能会通过增加碳排放对环境产生负面影响[Strubell et al.2019；洛蒂克等人，2019；施瓦茨等人，2019；拉科斯特等人，2019；曹等人.2020;亨德森等人2020；本德等人，2021；帕特森等人，2021； Lannelongue 等人，2021； Parcollet 和 Ravanelli 2021]。解决此类排放势在必行：当前的预测表明，气候变化的发生速度比之前想象的要快[Masson-Delmotte 等，2017]。 2021]。为了了解基础模型中哪些位置可能发生此类排放，我们考虑了它们的生命周期。首先，它们接受大量数据的训练，可能长达数月的时间，并且通常分布在数百到数千个 GPU 上。之后，它们可能会适应新的领域，或者可能被提炼成更小的模型。所有这些都可以被视为训练制度的一部分。纯粹用于研究的模型可能不会超出这些步骤。模型经过调整和/或提炼后，可能会继续部署到生产中。此时，模型将进行多轮推理，直到训练出新模型并重复该循环。这些步骤中的每一步都有可能利用大量能源，并可能导致碳排放。基础模型在初始训练阶段可能会产生大量的一次性能源成本和碳排放。例如，在某些情况下，训练一个 BERT 基础模型所产生的排放量只能通过种植 10 年的 40 棵树来抵消。102 如果大规模部署，基础模型可能需要大量能源来服务数百万个请求103—如果使用不可再生资源，就会产生大量碳排放。 102Strubell 等人[2019]计算了在美国平均能源网格上训练 BERT 的碳排放量，我们使用 https://www.epa.gov/energy/greenhouse-gas-equivalcies-calculator 将其转换为等效排放量其他域。我们注意到，这个数字可能会根据能源网和其他考虑因素而变化[Henderson et al.2020；帕特森等人。 2021]。 103例如，变压器已经在微软和谷歌的搜索中大规模使用。请参阅 https://www.blog.google/products/search/search-language-understanding-bert/ 和 https://azure.microsoft.com/en-us/blog/microsoft-makes-it-easy-to-构建流行语言表示模型-bert-at-large-scale/。关于基础模型的机遇和风险 141 因此，训练和部署基础模型的某些设计决策对环境的影响可能很大。即使是看似微不足道的决定，例如减少模型的层数，也可能会导致大规模的环境成本显着降低。例如，根据 Henderson 等人[2020]的计算，在商业翻译服务规模上部署的稍微更节能的翻译模型每天可以节省 78 kgCO2eq 到 12,768 kgCO2eq 的碳排放量，具体取决于所使用的能源网络。这大致相当于 1 至 211 棵树生长 10 年所固碳，或 0.35 至 57.4 英亩森林一年固碳。 104因此，基础模型的设计、部署和部署后监测应充分考虑反映这些风险。当然，计算任何给定模型所使用的能源量或碳排放量都存在不确定性[Henderson et al.2020；曹等人.2020; Patterson et al.2021]，而其他排放源目前可能比基础模型产生的排放量大得多[Mora et al.2021]。 2018]。但如果基础模型继续扩大规模并越来越受欢迎，它们很可能成为碳排放的重要贡献者。我们的目标是为基础模型开发者和大规模部署者105提供一个框架，以考虑如何减少不必要的碳排放并保持这些模型的积极净社会影响。我们建议：（1）在许多情况下可以而且应该减轻碳影响。这可以通过在低碳强度地区训练模型或使用更高效的模型和硬件来实现（第 5.3.1 节：环境缓解）。 (2) 当所有缓解机制都已用尽并且不再可能缓解时，应评估社会的成本和效益，以确定是否以及何时应在更小、更高效的模型上部署更大的基础模型——了解大型基础模型的前期成本可能会在模型的整个生命周期内摊销（第 5.3.2 节：环境成本）。 (3) 应明确报告能源、计算和碳成本以及为减轻负面影响而采取的任何努力，以便为政策制定和研究提供信息（§5.3.3：环境报告）。 

### 5.3.1 在许多情况下，碳影响可以而且应该得到缓解

训练基础模型的碳影响不同于部署它们进行推理的影响。模型训练没有延迟要求，因此可以在云环境中相对轻松地跨能源网格移动训练。每个能源网都有自己的碳强度——每千瓦时能源所排放的碳量。例如，魁北克由于依赖水力发电而具有极低的碳强度，而爱沙尼亚的能源网由于依赖页岩油而具有极高的碳强度（尽管这种情况正在迅速改变）[Henderson et al.2020]。最近的研究甚至表明，前 5% 的污染发电厂贡献了所有电力排放的 73% [Grant et al.2021]。因此，虽然训练基础模型可能相当耗能，但研究人员已经证明，通过选择碳排放量最少的能源网可以部分减轻这些模型的碳影响[Henderson et al.，2017]。 2020；拉科斯特等人。 2019；帕特森等人。 2021]。 104 封存量通过 https://www.epa.gov/energy/greenhouse-gas-equivalcies-calculator 估算，但可能会更大，具体取决于其他估算方法。更高效的能源网络将排放更少的碳，从而产生广泛的影响范围。 105我们专注于模型开发者和大规模部署者，例如那些在基础模型之上构建生产系统的人，因为他们最有能力做出有意义的改变以减少能源使用和碳排放。这些参与者的一个单一改变——比如使用更有效的模型——就可以大规模节省碳，否则需要开展大规模活动来覆盖所有下游模型用户。 142 基础模型研究中心 (CRFM) 碳抵消也已被纳入提议作为权宜之计，直到所有数据中心都提供无碳可再生电力。该战略涉及减少一项活动的碳排放量，以抵消另一项活动的碳排放量。但大多数（如果不是全部）碳补偿绝对是比不排放 CO 2 更糟糕的解决方案 [Holl 和 Brancalion 2020]。一些碳抵消计划甚至会产生负面影响。例如，对森林种植活动（通常是碳抵消的来源）的研究表明，它们弊大于利。它们可以产生单一栽培（使用一种特定树种），从而减少该地区的生物多样性并减少森林土壤中的碳储存[Heilmayr et al.2020； Hong 等人.2020b]。与最初从未排放过的碳相比，使用碳补偿可能会导致更多的碳排放。因此，在训练或部署基础模型时，我们建议预先设计尽可能少的碳排放，而不是简单地依靠碳抵消来取消排放。当无法在低碳地区运行时，应利用其他缓解策略，减少不必要的能源使用。这包括：使用更高效的硬件，106 使用混合精度训练 [Micikevicius 等人 2017] 或量化 [Gholami 等人 2017]。 2021]，使用更高效的架构（例如，在普通变压器架构上使用进化变压器；或使用稀疏模型）[So et al.2019；帕特森等人，2021； Mostafa and Wang 2019]、蒸馏模型和使用蒸馏模型（例如，[Sanh et al. 2019]），以及利用其他可降低能源成本的优化策略（请参阅第 4.5 节：系统中的更多讨论）。开源项目和云计算的维护者应努力将其默认设置设置为最有效的，因为“绿色默认”被认为是最有效的缓解策略（请参阅[Henderson et al.2020]中的讨论）。其他缓解策略可以在最近的文献中找到[Strubell et al.2019;拉科斯特等人，2019；施瓦茨等人，2019；亨德森等人。 2020]。我们还注意到，减少和减少能源使用还有一个额外的好处，即使计算访问权限有限的人更容易访问模型（请参阅第 5.6 节：更多讨论的道德规范）。然而，当模型主要用于推理时，例如部署在生产应用程序中，它通常无法移动到碳密集度较低的能源网格以实现低延迟应用程序。除了使用上面指定的缓解策略之外，在这种情况下，权衡所提出的基础模型与更节能的替代方案的好处也很重要。我们将在后续部分进一步讨论这一点。 

### 5.3.2 在使用基础模型之前应评估成本和收益

在采取尽可能多的缓解措施（或在不可能缓解的情况下）之后，评估基础模型所需的规模或是否应该使用基础模型至关重要。这种成本效益分析应考虑：
- （1）部署基础模型的社会成本和环境成本是否大于模型的社会效益？ 
- （2）另一种计算更简单、成本更低的方法是否能实现可比的社会效益（例如，更有效的基础模型，或者可能是简单的基线）？

评估这种权衡的简化方案考虑了模型𝑀的整体影响：106值得注意的是，加利福尼亚州现在因此对具有低效 GPU 的计算机进行监管，要求它们保持在每年 30-100 kWh 以下，具体取决于制造日期和计算机类型。请参阅加利福尼亚州电器效率法规（第 20 篇）第 1601-1608 节。关于基础模型的机会和风险 143 𝑉(𝑀)=𝑆(𝑀)−𝐶(𝑀)−𝐸(𝑀)−𝑂(𝑀)。 (7) 图 26 表示该方程以及可能进入每个变量的成本和收益。这里，𝑀 是模型，𝑆 是净社会效益以及环境效益（以美元为单位）。 𝑆可以通过改善医疗保健、诉诸司法、减少贫困、改善环境监测、协助生态系统保护工作等来增加。 𝐶是能源使用产生的碳的社会成本。这代表了以当前货币价值形式释放的碳对未来社会的危害。美国环境保护署 (EPA) 对 2017 年碳社会成本的估计上限为每吨 CO 2 排放 105 美元（按 2007 年美元计算）。107 𝐸是模型的能源成本。例如，2021 年 4 月，美国平均住宅能源成本约为每千瓦时 0.1376 美元。108这个变量中还可能包括因电网压力增加而产生的成本。例如，最近的一项研究表明，每次电网中断事件的成本（按平均需求标准化）可能高达每千瓦 15.9 美元 [Sullivan 等人，2017 年]。 2015].109 𝑂是其他二阶环境影响的社会成本。这可能包括： 芯片需求和芯片生产增加带来的复合碳影响 [Gupta 等人，2017]。 2021a]。芯片制造的其他环境影响，例如在硅谷建立有毒废物场，其健康影响不均匀地分配给社会弱势群体[Stewart et al.2014]，或者台湾制造业的污染与慢性病有关健康问题 [Tu 和 Lee 2009；林等人。 2016]。 SCC 模型中尚未包含气候变化的复合效应。例如，这些影响可能包括加速荒漠化[Huang et al.2016]、使许多物种面临灭绝风险的快速生态系统变化[Urban 2015]以及由于永久冻土融化而增加的碳排放[Schuur et al.2015]。 2015]。对芯片产能造成不必要的压力。最近的芯片短缺导致汽车制造业停工。110没有证据表明对机器学习优化芯片的需求增加导致了这种短缺。111但这种考虑属于二阶效应，研究人员可能会权衡风险是否轻微，无论风险多么轻微造成此类负面影响的因素值得使用或部署大型模型。 112 在本分析中，重要的是要考虑到碳的经济效益和社会成本可能在社区之间分配不均，较贫困的社区受气候影响更严重变革和富裕社区受益于模型 [Bender 等人。 2021].113因此，在进行公式 7 分析时，应考虑其好处，107请参阅 https://19january2017snapshot.epa.gov/climatechange/social-cost-carbon_.html。但请注意，碳的社会成本可能是一个有争议的指标 [Stern 和 Stiglitz 2021]。通过使用有利的折扣系数，可以降低碳成本。因此，该指标的计算可能因方法而异。 108https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=epmt_5_6_a 109与碳的社会成本一样，这些成本的计算可能会因建模方法的不同而波动。 110https://www.reuters.com/business/autos-transportation/ford-shut-some-n-american-plants-few-weeks-chip-shortage-2021-06-30/ 111尽管最近的报告表明，数据中心芯片已经超越了游戏领域。请参阅 https://www.nextplatform.com/2020/08/21/the-local-maxima-ascension-of-datacenter-at-nvidia/。 112与之前描述的其他指标一样，如何计算这些影响并将其归因于模型存在不确定性。 113 另见 https://www.un.org/sustainabledevelopment/blog/2016/10/report-inequalities-exacerbate-climate-impacts-on-poor/ 和 https://blogs.imf.org/2020/12/ 02/人工智能如何扩大贫富国家之间的差距/.144 基础模型研究中心 (CRFM) 图 27. 摊销微调的假设示例显示基础模型（在本例中为 BERT Base）比从头开始训练的 Transformer 模型具有更低的能源成本。我们估计了 Strubell 等人 [2019] 训练 BERT 的前期能源成本，以及 Chaudhary 等人 [2020] 微调下游任务的成本。我们与 Strubell 等人[2019]从头开始训练 Transformer 的线性增加成本进行了比较。如果 BERT 用于少于 ∼80 个任务，则无法收回前期能源成本。在那之后，BERT 比从头开始训练的模型更加节能。对更广泛的社会造成伤害，而不是对特定组织或国家造成伤害。在这种情况下，𝑉(𝑀) 可以被视为一个分布，并且理想情况下应该均匀分布在整个群体中。在分布高度不均匀的情况下（例如，所有好处都落在模型设计者身上，而所有危害都落在永远不会从模型中受益的人群身上），设计者应该在部署模型之前花更多的精力来缓解风险。当然，在评估公式 7 的每个组成部分时使用哪种方法存在一些不确定性。其中许多项的经验估计可能有多个量级，具体取决于数据源和现象的建模选择，例如不同的机制评估碳的社会成本。当然，还需要继续考虑可能难以用货币量化的额外外部性。然而，这种成本效益分析的关键要点不是等式中每一项的美元估值，而是每一项影响的存在及其相对重要性。我们的目标是提供一个高级框架来开始考虑这些权衡。未来的研究可能会为如何量化这些值提供更多指导。最后，我们注意到这些因素也应该在模型的整个生命周期内进行评估，而不是在每次运行的基础上进行评估。考虑一个替代的基线模型，必须为每个新任务从头开始训练。基线很可能需要昂贵的超参数搜索才能在下游任务上实现同等性能。相比之下，基础模型将首当其冲的成本放在初始预训练过程上，微调可能更简单、更节能。在基础模型的生命周期内，它的碳效率可能比基线更高（图 27）。更有效的适应机制可以进一步改善这种摊销（参见§4.3：适应）。关于基础模型的机遇和风险 145 然而，适应的效率并不能得到保证。某些基础模型可能永远不会比特定基线更有效，即使分摊到许多任务上也是如此。例如，不能假设参数较少的较小模型将转化为能源效率的提高。由于超参数调整成本或其他优化的增加，在某些情况下参数的数量已被证明与能源效率不相关[Zhou et al.2020；亨德森等人。2020]。因此，基础模型开发人员应该在开始大规模训练工作之前严格评估其模型和适应机制的效率。本节中的框架旨在引导读者思考训练和部署模型时的环境和社会权衡，但部署基础模型还涉及其他实质性的社会正义考虑因素，如第 5.6 节：伦理中所述。 §5.5：经济学还更详细地讨论了算法部署带来的社会福利动态。 

### 5.3.3 应系统地报告碳/能源影响

除非研究基础模型的研究人员和工程师报告其模型的计算、能源和碳成本，否则无法进行成本效益分析。我们鼓励基础模型开发者、提供者和策展人报告这些指标，以及在制作基础模型时使用了哪些碳减排策略。参见[Henderson et al.2020；洛蒂克等人，2019；拉科斯特等人，2019；施密特等人，2021； Anthony et al.2020]了解碳影响声明的示例以及可以促进此报告的工具。对于研究人员来说，此类报告可以在发布时进行，但我们也鼓励行业参与者采用透明度机制来报告其部署模型的这些指标。 114这将有助于在行业和学术界制定政策建议，并帮助下游用户识别碳排放友好的使用模式。标准化报告还将有助于确定计算访问权限有限的人可以访问哪些模型（有关可访问性的更多讨论，请参阅第 5.6 节：道德规范）。为了鼓励更多地报告能源和碳影响，我们建议采取以下策略：在会议上授予绿色徽章，要求报告相关指标以提交给会议场地，游说基础模型的大规模部署者提供更高的透明度，以及通常将学术界和工业界的专业规范转向这些指标的标准报告（请参阅第 5.6 节中关于专业规范的更多讨论：道德规范以及 Henderson 等人 [2020] 的关于报告机制的更多讨论）。一些云计算提供商已经朝着这个目标迈出了一小步，他们确定了最碳友好的云区域。例如，请参见 https://cloud.google.com/blog/topics/sustainability/pick-the-google-cloud-region-with-the-lowest-co2.146 基础模型研究中心 (CRFM) 

## 5.4 合法性 

作者：Neel Guha、Peter Henderson、Lucia Cheng、Mark Krass、Daniel E. Ho 

在本节中，我们将描述美国法律如何影响、约束或促进基础模型的创建和使用。 我们注意到，围绕算法工具仍然不确定。我们强调与（1）模型训练、（2）模型预测的责任和（3）模型输出保护有关的问题。尽管了解法律如何影响基础模型至关重要，但重要的是要认识到法律不能成为我们评估基础模型的构建、维护和使用的唯一透镜。道德框架对于理解法律允许的基础模型应用可能仍然不明智而造成的危害是必要的，并且在第 5.6 节：道德和第 5.1 节：公平中进行了更深入的讨论。研究滥用的可能性和可能的安全问题（参见§5.2：滥用和§4.7：安全）对于事前预防有害结果至关重要，这与法律机制通常提供的事后处理相反。 

### 5.4.1 训练

训练基础模型将需要积累大量的多模式数据，从而引发有关数据收集和数据使用的问题。首先，模型创建者通过网络抓取增长数据集的能力将受到法院解释服务条款条款的方式的约束，特别是美国《计算机欺诈和滥用法案》(CFAA)，该法案将“未经授权访问服务器”定为犯罪行为。授权”[Wajert 和 Rottman 2019]。法院在这些问题上存在冲突，最近的案例试图澄清在什么情况下可能会禁止网络抓取。116数据访问的限制性将从根本上影响从业者可用于训练基础模型的数据的多样性[Levendowski 2018]。其次，训练集中包含的大部分数据将受版权保护，并可能受到知识产权法的保护。然而，版权法承认允许个人使用受版权保护的材料的例外情况。 117一些学者认为，训练数据集的法律许可性很大程度上取决于法院是否将模型训练过程解释为合理使用原则下的“变革性”[Lemley 和 Casey] 2020]。尽管什么才算是变革性的问题在很大程度上取决于具体情况，但一般规则是变革性用途是“添加新的东西，具有进一步的目的或不同的特征，并且不取代作品的原始用途”[办公室2021]。最近发布的 Github Copilot 工具已经将这些论点带到了前台 [Gershgorn 2021]。最后，一些训练数据集可能违反隐私法。例如，伊利诺伊州允许个人起诉不当收集或使用生物识别数据（例如，视网膜或虹膜扫描、指纹、声纹或手部或面部几何形状扫描）。118欧盟《通用数据保护条例》(GDPR) 等外国隐私法 — 如果数据集包含以下内容，这将影响美国模型创建者来自欧盟公民的信息——将要求数据主体了解数据收集的目的。《加州消费者保护隐私法案》(CCPA) 等法律可能会出现进一步的问题，该法案为个人提供了“被遗忘的权利”，引发了质疑115 我们的观点集中于美国法律和法律框架。因此，讨论基础模型对其他国家的影响可能会采取不同的观点。 116Van Buren 诉美国，141 S.Ct. 1648（2021）。 117参见，例如, , 17 USC §107 至 112。 118IBM 是当前集体诉讼中的被告，指控 IBM 收集和使用此数据（包括用于机器视觉目的）违反了该法规。请参阅集体诉讼投诉，网址为 2，Vance 诉 Int'l Bus。 Machines Corp.，编号 20 C 577（伊利诺伊州北达科他州，2020 年 1 月 24 日提交）。关于基础模型的机遇和风险 147 模型创建者是否需要从模型中“删除”训练数据 [Villaronga et al.2018;吉纳特等人。 2019]。 

### 5.4.2 产出责任

尽管基础模型本身与任务无关，但微调模型——或者基础模型本身学习到的表示——可以用于传统的预测任务。当这些任务构成更大决策系统的组成部分时，基础模型将影响行动、决策或政策。当这些行为造成伤害时，模型创建者以及操作模型的个人可能要承担法律责任。将基础模型嵌入物理系统（例如自动驾驶汽车、电网管理、医疗诊断等）可能会对个人造成身体伤害。在这里，法院可能会解决侵权原则下的责任问题 [Lemley and Casey 2019;自2020年]。关键的开放性问题包括用户、基础模型提供商和应用程序开发人员的责任之间的相互作用，以及法院用于评估基础模型风险状况的标准。在特别敏感的领域（例如医学）的部署将需要监管部门的批准，并需要开发标准化流程来评估安全性[Wu等人。 2021克]。以与受保护属性（例如种族、性别）相关的方式对个人进行分类的微调基础模型可能会面临民权法的挑战。学者们指出，对基础模型造成的差别待遇的主张可能会在雇用、住房或信贷贷款的背景下提出[Gillis 和 Spiess 2019； Scherer 等人，2019]。法院将如何裁决这些问题尚不清楚。例如，学者们指出，法院对“歧视”的传统观点实际上会阻止机器学习从业者实施许多算法公平技术[Xiang 2021； Ho and Xiang 2020].119 美国法律承认政府实体的特权和限制。因此，除了平等保护主张之外，地方、州或联邦层面的政府实体使用基础模型将意味着特殊的考虑。使用模型进行风险评估——或在其他导致生命、自由或财产被剥夺的情况下——将引发程序性正当程序索赔。120例如，当行政机构（例如环境保护局）使用模型时，原告可能声称此类使用违反了正当程序、合理性/非任意性和透明度的基本标准。 

### 5.4.3 输出的法律保护

模型输出——以及负责模型的模型创建者——也可能受到一定的法律保护。首先，生成模型生成的内容可能涉及言论自由问题。目前尚不清楚法院会在多大程度上认定第一修正案对机器生成内容的保护。学者们讨论了许多悬而未决的问题，包括“人工智能语音”是否受到保护[Massaro et al.2016]，或者模型输出是否实际上是人类程序员的语音[Kajbaf 2019]。其他人指出了披露要求的可能性（类似于药品或其他物质的安全披露），也涉及言论原则，在这种原则下，模型将被迫与听众分享其内容是机器生成的[Lamo 和 Calo 2019]。这些问题可能会产生广泛的后果，影响个人是否可以使用基础模型大规模产生语音，或者模型创建者是否应对基础模型生成的内容负责。 119有关模型如何嵌入某些偏见的更多信息，请参阅§5.1：公平性。 120 程序正当程序承认，原告在任何会剥夺其生命、自由或财产的审议过程中通常拥有某些基本权利（例如，盘问不利证人的权利）。148 基础模型研究中心 (CRFM) 其次，谁可以主张模型输出的所有权存在不确定性。现有的版权法不承认计算机程序为作者，因此不为计算机程序创建的“作品”提供版权保护 [Grimmelmann 2015]。因此，学者们提倡采取多种方法。一些人认为，根据具体情况，程序的人类创建者及其人类用户都可以声称自己是该程序输出的“作者”[Ginsburg 和 Budiardjo 2019]。随着模型越来越多地在“创造”过程中使用——从艺术创作到新闻报道等更平凡的环境——关于机器生成内容的所有权的争议将变得更加普遍。虽然我们上面的分析只涉及了基础模型所涉及的法律问题的表面，但这些问题的解决对于基础模型的构建、使用和部署至关重要，或者借用 Larry Lessig 的话来说，“代码就是法律” ” [Lessig 2000]。关于基础模型的机遇和风险 149 

## 5.5 经济学

作者：Zanele Munyikwa、Mina Lee、Erik Brynjolfsson

基础模型有潜力通过提高生产力和创新来大幅提高整体生活水平。这些模型可以用来替代人类劳动力、增强人类能力，或者帮助发现新的任务和机会，这可能导致所有权和权力更加集中，或者更加去中心化。在更广泛的层面上，结果可能是由于潜在的中心化而加剧不平等（§5.1：公平，§5.6：道德），或者由于基础模型更容易适应广泛的应用而导致更广泛的共享繁荣（§1 ： 介绍 ）。所有这些维度的最终结果不仅仅取决于技术或经济，还取决于技术专家、政策制定者、管理者、工人和其他社会成员的选择和行动。基础模型可以被认为是经济学家所说的通用技术 [Bresnahan 和 Trajtenberg 1995]。通用技术是指蒸汽机和电力等技术，由于其普遍性、随着时间的推移而不断改进以及产生互补创新的能力（围绕一个核心产品的一系列产品和服务）推动了转型和生产力增长的浪潮）。虽然基础模型目前可能并不普遍，但它们似乎有望成为广泛技术创新的基础，并且具有通用技术的关键特征。因此，这些模型可能具有重要的经济意义。在考虑基础模型对经济的影响时，我们将重点关注三个广泛的影响领域：生产率、工资不平等和所有权。 

### 5.5.1 生产力和创新

基础模型可能会大幅提高生产力和创新能力。生产力增长是提高生活水平的主要因素之一，因为它增加了国家财富，并解决了从贫困、医疗保健到环境和教育等一系列挑战。生产率被定义为每单位投入的产出。提高生产率的一种方法是减少分母。例如，使公司的广告能够由更少的文案撰写者或每个文案撰写者更少的劳动时间来撰写，从而减少了输入单位的数量。生产力也可以通过增加分子来提高，例如使软件开发人员能够在给定时间内编写更多代码。如果分子的增长足够大，这可能会导致更多的人开发软件，而不是更少[Autor 2015]。在许多任务中，我们已经观察到机器学习系统提高了生产力。例如，临床文档自动完成系统将临床概念的击键负担减少了 67% [Gopinath 等人。 2020]。同样，基础模型影响生产力的潜力几乎涵盖所有行业和许多职业。仅考虑语言，使用美国劳工部 O*NET 数据库对美国职业的分析表明，许多职业涉及可能受基础模型影响的语言相关工作类型。大约 13% 的职业的主要任务与写作有关，这些职业的工资总额（年薪乘以该职业就业人数）超过 6750 亿美元。然而，基础模型的潜在影响超出了语言范围。它们还将对医学、平面设计、音乐中的诊断成像产生影响。

请注意，如果测量得当，生产率不仅仅是计算生产的单位或工作时间的问题，而且还考虑到质量变化。因此，一定量劳动的质量提高，例如更有趣的小说，也算作生产率的提高。 122https://www.landr.com/150 基础模型研究中心 (CRFM) 和许多其他任务，人们在这些任务中创建与已经存在的其他东西类似的东西 [Winkler 等人。 2019；拉梅什等人。 2021]。也许基础模型最深远的影响（如果仍然是推测性的）是它们增强创造力和提高创新率本身的潜力。例如，DALL·E [Ramesh 等人。 2021] 可能会改变插图市场，就像廉价相机彻底改变摄影一样。如果这些模型使人类能够开发新的方法来编写新的歌曲和小说（§2.5：交互），发现药物分子的变体（§3.1：医疗保健），扩展专利（§3.2：法律），构建创新的软件应用程序，或开发新的业务流程不仅会提高生产力水平，而且会提高生产力的增长率。这样，基础模型具有保罗·罗默增长模型[Romer 1990]中的想法或蓝图的一些特征，甚至是元想法（关于想法的想法），与大多数其他商品不同，它是非竞争性的，从而加速生长。值得注意的是，生产率的变化并不总是在官方统计数据中可见，因为投入和产出的许多方面都难以衡量[Brynjolfsson and Collis 2019]。因此，基础模型的收益和成本将无法通过传统的生产力指标或国内生产总值 (GDP) 或价格水平（整个商品和服务范围内当前价格的平均值）等相关指标来充分体现。 对于历史上的通用技术来说尤其如此，因为它们是一系列二次创新的催化剂，这些二次创新往往会在数年甚至数十年的时间内改变经济中的商品和服务，甚至改变生产和创新的性质。 

### 5.5.2 工资不平等

即使基础模型提高了平均生产率或收入，也没有经济规律可以保证每个人都受益。在某种程度上，这是因为并非所有任务都会受到相同程度的影响。更重要的是，无论生产率增长如何，基础模型对劳动力需求（以及就业和工资）的影响可能是积极的，也可能是消极的[Brynjolfsson and McAfee 2011; Brynjolfsson 和 Mitchell 2017]。当一项技术替代人类完成任务时，它往往会减少对执行这些任务的工人的需求。这抑制了就业和工资。然而，当一项技术补充劳动力，或促进创造新的机会或任务时，它往往会增加劳动力需求 [Acemoglu 和 Restrepo 2019]。即使生产力提高，就业也可以（而且经常）增加。例如，飞机的发明创造了对全新职业——航空公司飞行员的需求。反过来，喷气发动机的发展与人类飞行员形成了互补，进一步增加了对飞行员的需求。同样，基础模型对就业、工资和收入不平等的影响也会根据其使用方式的不同而有所不同。虽然工业革命主要改变了体力工作，但基础模型可能会改变涉及认知工作的任务，例如内容创建和交流。一般来说，由于基础模型是中介资产，通常具有强大的生成能力，我们预计它们将能够在许多创造性环境中增强人类，而不是取代人类，因为单独使用这些模型进行开放仍然存在很大的限制。 -结束的生成任务[参见 et al.2019]。正如我们在§2.5：交互中所描述的，基础模型还可以为用户可以利用的系统提供动力，以共同构建新颖的艺术形式或更有效地构建新应用程序的原型。流畅的人机和人机交互将需要界面设计（§2.5：交互）方面的进步，以及这些模型的可解释性（§4.11：可解释性）和鲁棒性（§4.8：鲁棒性）的根本改进，以便人类能够理解模型行为并期望模型在不同的环境中表现良好。

### 5.5.3 中心化

基础模型经济影响的另一个关键决定因素是谁拥有数据和模型。特别是，迄今为止，推动基础模型的前沿主要是大型企业实体的职责范围。因此，数据和模型的所有权往往高度集中，导致市场集中（§5.6：道德）。反过来，这可能导致决策权和权力的严重集中，从而减少那些没有所有权的人的收入和机会。这种权力集中可以导致一种均衡，即更少的人拥有社会和经济流动性和机会，这种情况被 Brynjolfsson [2022] 称为“图灵陷阱”。为了平衡这种集中化，基层开始努力开源 AI 研究，例如 Masakhane、EleutherAI 和 HuggingFace，或者通过分布式训练构建基础模型。然而，由于基础模型依赖于大量数据和计算资源（§5.3：环境），行业可以训练的私有模型与向社区开放的模型之间的差距可能仍然很大。

### 5.5.4 其他考虑因素

这一简短的章节并不意味着全面涵盖基础模型的所有经济影响。除了影响生产率、工资不平等和所有权之外，基础模型还可能对工作质量和工作满意度产生重大影响。例如，他们可能通过自动化重复、无趣的工作部分来提高工作满意度，或者通过加快工作节奏来降低满意度，从而导致更频繁的倦怠。正如第 5.1 节：公平和第 5.6 节：道德中所讨论的，它们还经常以意想不到的方式放大和延续偏见，或者被用作减少偏见的工具。基础模型可以促进全球贸易和远程工作，就像机器翻译系统的早期使用在这些领域产生了重大影响一样[例如，Brynjolfsson et al.2019]。还可能产生重大的环境影响（第 5.3 节：环境），以及对经济中职业变革和业务转型的速度和方向产生意外和意料之外的影响。更广泛地说，鉴于基础模型的新兴能力，我们应该预期会出现难以预测的新的未知未知数，并且可能会产生重大的后续影响。 124 总之，基础模型有望成为重要的通用目的我们这个时代的技术。它们有可能大幅提高生活水平，但也带来了不平等加剧和权力集中的风险。这些技术的经济影响不是预先确定的，而是取决于技术专家、政策制定者、管理者、工人和其他利益相关者如何应对以下挑战：我们如何利用基础模型的潜力来提高生产力？我们能否开发出增强创造力并提高创新率的模型？利益和控制权会仅限于少数人还是广泛共享？了解这些系统的经济潜力是引导它们朝着符合我们价值观的方向发展的第一步。 

Lambda Lab 估计 GPT-3 训练成本超过 460 万美元，研发成本在 1140 万美元到 2760 万美元之间，运行 GPT-3 所需的硬件成本在 10 万美元到 15 万美元之间，不考虑其他成本（电力、冷却、备份、等），每年的运行成本至少为 87,000 美元。 (https://bdtechtalks.com/2020/09/21/gpt-3-economy-business-model)

作为次生效应的一个例子，考虑汽车的发明影响了郊区的发展和扩张。基础模型研究中心 (CRFM) 

## 5.6 规模伦理 

作者：Kathleen Creel、Dallas Card、Rose E. Wang、Isabelle Levent、Alex Tamkin、Armin W. Thomas、Lauren Gillespie、Rishi Bommasani、Rob Reich

基金会的广泛采用除了对不平等加剧的担忧（§5.1 的主题：公平）之外，模型还带来了伦理、社会和政治挑战。在本节中，我们讨论与基础模型应用规模相关的社会、政治和伦理风险，例如同质化和权力集中、适合解决这些问题的规范和释放策略，以及对更广泛的政治经济的担忧。开发和部署了哪些基础模型。 

### 5.6.1 同质化和规模化

如果同一模型以最小的适应性跨多个领域使用，则原始模型的优点、缺点、偏差和特质将被放大（第 5.1 节：公平性）。对于任何标准化技术的广泛采用和依赖都是如此。与许多汽车或飞机中使用的零件制造失败可能在各个行业产生广泛而严重的后果类似，基础模型固有的偏差或服务失败可能会向外蔓延。然而，当前基础模型的不可解释性（§4.11：可解释性）及其与任务无关的训练使得预测、理解和解决这些弱点变得具有挑战性。如果基础模型（似乎有可能）被广泛采用，那么基础模型开发人员比标准模型开发人员承担更大的谨慎责任，因为他们在设计和部署方面的选择具有广泛的影响[Arendt 1987]。基础模型的定义特征——它们能够有效适应多种任务的能力——使得它们有可能被广泛应用于各种具有社会意义的任务。与当前分布式且多样化的决策模型相比，对多个自动化决策任务采用同一基础模型的许多改编意味着决策主体可能面临植根于底层基础模型的一组更加同质的判断。这种算法单一文化 [Kleinberg and Raghavan 2021] 可能会导致一致且任意的拒绝、错误分类或虐待个人决策主体 [Gandy 2021]。我们将这种现象称为同质化 [Creel and Hellman 2021]。例如，第 4.6.2 节：数据解决方案讨论了导致数据子群体出现不良行为的数据质量问题，其中子群体可以由任何对数据进行分层的过滤器（包括社会群体）生成（请参阅第 4.11 节中的相关讨论）。 1：可解释性行为和§4.8.1：稳健性优势）。直到数据质量工具（§4.6.2：数据解决方案）和识别模型表现不佳的数据切片的能力得到改进[Chung et al.2019； Goel et al .2021]，基础模型可能始终无法向一小群人提供准确的信息或服务（另请参见§4.8：稳健性。同质化有可能放大偏见；标准化偏见，加剧不公正而不是分散它们；并放大任意排除 [Creel and Hellman 2021; Gandy 2021]。例如，Zhou 等人[2021a] 认为 BERT 默认编码一种以盎格鲁中心为中心的相似性度量，如果跨基础模型的上下文应用该度量，则可能是有害的跨领域的基础模型应用有可能成为一种认识论和文化同质化力量，在多个应用领域传播一种隐含的观点，通常是社会主导的观点。训练语料库标准化的现有趋势可能会由于需要大量的未标记和标记数据，基础模型中的问题会加剧。就基础模型 153 的机会和风险模型而言，它们很可能会获得类似的行为模式、偏差（第 5.1 节） .3：公平性-来源）和错误。之前的大量数据管理和标记工作（例如 ImageNet）已经标准化了训练语料库。在此过程中，他们还标准化了错误：在 ImageNet 上训练的模型通常依赖于相同的“虚假线索”和“捷径”，例如使用绿草等背景纹理来预测牛等前景对象类别[Geirhos et al.2020 ; Hendrycks 等人.2021e]。尽管基础模型和其他大型模型对许多类型的分布变化的鲁棒性有所提高（§4.8.1：鲁棒性优势），但它们学习虚假相关性的可能性并不低（§4.8.2：鲁棒性挑战），因此如果在相同的数据集上进行训练，可能会学到类似的错误。由于选择公开可用的未标记数据，可能会产生类似的效果。许多基础模型都是在未标记的语料库上进行训练的，这些语料库是根据其便利性和可访问性而选择的，例如公共互联网数据 [Caswell 等人。 2021]，而不是他们的质量。然而，公开访问的数据，无论是标记的还是未标记的，通常都被许多专有基础模型的训练语料库中的专有数据所压倒，如 [Marr 2017] 和 §4.6.1: data-desiderata 中所讨论的。因此，需要更多的研究来了解相似数据的训练在多大程度上使基础模型内的相关性均质化，以及这种均质化可能在多大程度上导致模型的适应导数一致失败（除非应用约束来消除每次适应期间的行为，如§4.3.2 中讨论：适应用例）。同质化并非不可避免。随着模型开发人员有意扩大数据集中代表的视角范围（第 5.1.3 节：公平来源），需要对基础模型在用于生成任务时提供多种视角的能力进行更多研究。例如，Sheng 等人[2021] 证明，采用特定人口群体“角色”的对话系统在衡量社会偏见方面表现不同。除了为了避免偏见而在“角色”之间进行选择之外，沿着各种认知和人口统计轴的多样化“角色”也可以用于为生成任务生成更广泛的连贯输出。关于如何平衡输出的多样性与个人用户的相关性和效用，仍然存在许多悬而未决的问题。

### 5.6.2 监视、排斥和权力

基础模型的一个关键前提是，大量未标记的数据集可以与大量的计算资源相结合，以创建一个基础，从中可以为各种应用派生出大量产品。这种范式转变有可能改变社会结构和权力转移，建立或巩固模型创建者的影响力 [Zimmerman 2020]。我们在下面讨论三个潜在的影响。海量数据收集和监视。虽然收集标记数据集通常需要与领域专家合作并了解此类数据的问题和局限性，但训练基础模型对大量数据的需求促使一些研究人员强调数量而不是质量。 126尽管预处理可以帮助改进这些数据的质量[例如，Brown et al.2020]，所涉及的规模需要自动化方法，这些方法可能是生硬的或记录不足的[Dodge et al.2020]。 2021]。尽管数据保护立法不断发展（例如欧洲的 GDPR），但在获取数据时仍然使用各种可疑的做法，包括不透明的政策 [Obar 和 Oeldorf-Hirsch 2020] 以及“黑暗模式”的使用（即操纵接口[Narayanan 125有关可能的实现方法，请参阅[Keskar et al.2019]中对可控生成的讨论和[Dinan et al.2021]中的§4.3.2：适应用例和一般讨论。126例如，Ding 等人。[2021] 收集了 3000 万个文本图像对，选择不处理水印和白边等伪影，尽管它们对模型质量有影响。154 基础模型研究中心 (CRFM) 等人，2020] ）彻底违反服务条款。事实上，这本质上是 Clearview AI 所采取的策略，该公司未经用户同意，违反平台服务条款，从社交媒体上抓取照片，以开发面部分类软件。尽管如此，该公司仍能够将这项技术出售给警察部门和其他组织，在许多情况下，州立法者或部门负责人并不知情[Mac et al.2021]。在某种程度上，基础模型的范式增加了首先拥有任何特定领域的最大可能数据集的价值，这可能会进一步鼓励参与者追求积极的数据收集，即使这种追求在法律上有问题或与用户的期望相反[尼森鲍姆 2009；祖博夫 2018]。数据对基础模型的重要性也意味着已经从事广泛数据收集的组织将在开发此类模型方面处于有利地位，并且可能有动力保持这种优势。如果衍生产品本身可以用于收集额外的数据（例如，在监视或健康诊断应用中），基础模型的开发人员可能会寻求确保他们获得此类数据的所有权。因此，尽管基础模型范式的一个关键优势是能够生成适应衍生品，但基础模型的开发人员可能会寻求以确保数据从所有适应衍生品流回他们的方式许可他们的工作。 127力量。尽管随着时间的推移，计算的绝对成本已经变得非常便宜，但最大的基础模型的训练目前需要计算资源，这使得它们的开发超出了除少数机构和组织之外的所有人的能力范围（§5.3：环境）。因此，谁有权访问相关计算资源和数据的问题可能会决定谁能够在未来几年产生尖端的基础模型（另见§5.5.3：经济集中化）。 GPT-3 至少在一定程度上是一个规模实验，表明可以通过扩大模型大小、数据量和训练时间来实现重大收益，而无需重大建模创新。尽管正在进行大量研究来减少训练此类模型所需的资源量（参见第 4.2 节：训练），但 OpenAI 的工作表明，更大规模的努力仍然可以获得收益 [Kaplan et al.2020]，并且其他组织可能会在其他领域寻求遵循这条道路，这似乎是合理的（例如，参见 [Lieber et al.2021]）。如果规模确实对成功至关重要，那么最有能力产生有竞争力的基础模型的组织将是资源最充足的：风险投资的初创企业、已经占据主导地位的科技巨头和州政府。这引发了对市场集中度的潜在担忧，并可能表明国防和半导体制造等极端资本密集型行业目前存在的现有垄断或寡头垄断[Carril and Duggan 2020]。此外，这种权力集中引发了人们对当前边缘化个人和社区参与基础模型开发过程的能力的担忧 [Kalluri 2020]。特别是在政府服务领域，采用基础模型可能会进一步将决策权从政府转移到企业服务提供商，并给正当程序和问责制带来额外的障碍[Citron 2008]。尽管如此，更多的基层努力（例如 Masakhane、EleutherAI、HuggingFace）提供了令人鼓舞的替代方案，并且在如何纳入参与式或价值敏感设计方面开展了大量工作 [Friedman 和 Hendry 2019；普拉巴卡兰和唐纳德·马丁 2020]。 127 作为一个不太复杂的例子，考虑信用评分行业，该行业已经能够将自己定位为当人们使用其产品时信息流回中央数据经纪人（如审查贷款申请），并且个人别无选择，只能参与[劳尔 2017]。关于基础模型的机遇和风险 155 推动广泛的自动化决策。近年来，自动化决策系统在工业和政府中的使用急剧扩展[O'Neil 2016； Engstrom 等人，2020]。尽管对此类自动化的许多担忧并非特定于基础模型，但 GPT-3 等模型的生成能力以及在基准任务上令人印象深刻的性能（例如，Devlin 等人[2019]）具有潜力促使行政机构等机构不太谨慎地采用这项技术，其中许多机构缺乏理解复杂机器学习系统所需的专业知识 [Calo 和 Citron 2021]。因此，清楚地传达基础模型的现实能力和局限性尤为重要。大多数自动化决策系统将作为更广泛的社会技术系统的一部分存在，其中人类发挥着关键作用[Selbst et al.2018]。128因此，即使标准化评估性能的大幅提高也不能保证转化为期望的结果在现实世界中（特别是在没有仔细考虑或持续评估的情况下部署系统时）。例如，研究表明，法官在解释风险评估系统的输出时可能会重新施加种族偏见[Albright 2019]，或者以其他方式施加自己的偏见[Stevenson and Doleac 2021]。具有适当生态有效性的持续评估[de Vries et al.2020]在这方面至关重要，但可能无法阻止在没有足够证据的情况下采用潜在危险或成本高昂的系统[Ferguson 2017]。拒绝方法的研究正在进行中：个人选择不参与基础模型及其改编衍生品的方法，无论是作为数据还是决策主体，而不会产生影响[Benjamin 2016]。简而言之，算法决策的现有问题将在基础模型部署后的功能中体现出来。就采用基础模型加速从人类决策向机器决策的转变而言，基础模型强调了对自动化的担忧。尽管这些挑战没有明显的解决方案，但重要的是要对基础模型将如何影响有关其创建的对话中的权力部分提出问题；与民间社会组织、政策制定者和公民就此类系统的功能和局限性进行沟通；并努力在社会各阶层之间就采用此类模式进行更广泛的对话。

### 5.6.3 规范

公共政策和法律的正式监管（§5.4：合法性）在创建技术创新基础设施以及减轻广泛传播技术的潜在有害影响方面发挥着至关重要的作用。正如塔斯基吉梅毒实验与 IRB 等研究方案和机构的发展之间长达数十年的差距所表明的那样，保护人类受试者和利益相关者的公共政策往往落后于公众意识和对他们造成伤害的证据 [Grady 2015;斯塔克2012；卫生和福利部 1979]。因此，社会依赖专业规范进行负责任的开发和部署以及最佳实践的建立。规范存在于建议和要求之间的连续体中。作为一项新兴技术，负责任的基础模型开发和部署的规范尚未在任何推荐强度下得到很好的建立[Crootof 2019]。接下来，我们将讨论部署模型的规范，因为研究模型具有更广泛的自由度。那些希望基础模型开发者采用某些规范的人可能会以身作则，让他们自己的行为和声明来推荐该规范。正如第 1.2 节：生态系统中所讨论的，我们认为大学和其他非营利机构在为基础模型建模规范方面发挥着重要作用。作为教育机下一代理论家和实践者考虑本报告中提出的问题，并促进研究人员和学生之间的跨学科对话[Rogers 2021]。大学和学院还可以通过审核现有的基础模型并发布其发现、建立道德审查委员会来为规范的建立做出贡献[Bernstein 等人，2017]。 2021]，并开发自己的基础模型。要创建和采用规范，需要在资金结构、模型存储库、发布实践、会议提交和拨款提案要求方面实现制度化。 129例如，HuggingFace 的界面目前鼓励发布数据和模型卡，包括关于偏见和社会影响的讨论.130由于它不是必需的，并且可能是因为数据质量工作相对于其重要性被低估了[Sambasivan et al.2021]，因此很少有人填写。偏见和社会影响包含在会议的道德声明和某些形式的标准评估中（如第 4.4 节：评估中所述），但也被一些研究人员视为可选考虑因素。这必须改变。对于一些具有社会影响的用例，我们建议建立法律标准，要求改编的衍生品可证明表现出某些属性（§5.4：合法性）。特别关注的领域应以民主方式决定，但可能包括政府服务的分配和分配、医疗诊断和监测、雇用和贷款：在所有情况下，人们的机会甚至生命都取决于适应性衍生品的正常运作。我们应该提倡、制度化或要求哪些规范？我们在这里推荐一些，但主要目的是鼓励就基础模型的开发和使用的适当规范进行对话。之前的工作通常侧重于提倡文档化的规范[Gebru et al.2018；本德和弗里德曼 2018；米切尔等人，2019；道奇等人，2019]。由于下游环境中出现的许多负面社会后果最初可能看起来是外在的或特定于用例（第 5.1 节：公平性），因此文档和透明度对于基础模型尤其重要。目前，那些采用记录其改编衍生品的偏差或其他负面特征的基础模型的人没有自动机制将他们的发现报告给基础模型的开发人员。编译适应衍生品中相关问题的多个报告可以让模型开发团队发现跨越多个用例的模型的内在属性。由于改编衍生品的创建者通常代表与基础模型开发商或提供商不同的实体，因此需要额外的报告结构和规范或法规才能将此类反馈传达给基础模型开发商。此类反馈也可以提供给模型审计师的一般受众，从而使审计和追索变得更加容易。对规范、标准和报告机制创建的公开承诺也可以允许下游用户向基础模型提供商提交反馈。为了实现这一点，改编后的衍生品应以一致的方式进行标记，以便受影响的各方能够追踪问题的根源。重大的技术和社会障碍可能会阻碍实践中的这种追踪，例如隐私考虑和许多基础模型的专有性质，但如果没有标签，这是不可能的。模型开发者和提供者创建此类报告的机制非常重要。报告机制可以通过当前平台上的类似结构来通知，例如 GitHub 上开源项目的问题跟踪。特别是，提交的问题应该公开，以便其他用户即使尚未做出更改也可以识别趋势，并且开发人员和提供商可以对未解决的问题负责。需要额外的机制129有关部分遵守“非强制性公平意识政策”（例如此处讨论的规范）的有益讨论，请参阅戴等人。 [2021b]。 130https://huggingface.co/docs/datasets/master/关于基础模型的机遇和风险 157 将趋势向上升级到基础模型提供商。 Dinan 等人讨论了有关训练数据中跟踪问题的类似建议。 [2021] 和§4.6：数据。 Holland 等人 [2018] 借鉴消费者隐私中的标签讨论，建议将营养标签作为一种有用的模式 [Kelley 等人 2009]。营养标签包括“原始”成分列表和加工食品的完整营养信息。因此，适应衍生物的模型卡[Mitchell et al.2019]或营养标签也可以包括“原材料”列表，例如所使用的训练数据和基础模型，以及适应衍生物的完整“营养成分”例如其已知的能力、弱点和偏见。为了使数据主体和受影响的各方能够追踪其来源，有必要报告整个管道。然而，如果没有能力将损害责任归咎于改编后的衍生品、基础模型或两者，并且在损害发生后没有追索框架，即使成功追踪损害也不太可能导致变化在模型中（另请参见§5.1.4：公平追索权）。因此，需要开展大量的技术、政策和法律工作，以开发向其他专家并最终向公众传达数据、模型和衍生内容的框架；确定损害的责任；并创造追索途径。 

### 5.6.4 发布和审核

2019年2月，OpenAI开始了一项实验。通过发布减少了 124M 参数、无数据集的 GPT-2，他们希望赢得时间：测试偏差的时间、为滥用做好准备的时间以及社会适应大型语言模型的存在的时间 [Solaiman et al.2019 ]。八个月后，当 OpenAI 发布完整的约 15 亿参数版本时，测试暴露了该模型的部分功能和局限性，但绝不是全部。今天考虑类似问题时，发布的可能危害（主要集中在滥用（第 5.2 节：滥用）131）必须与闭门测试无法复制的透明度的好处（即更广泛和独立的审核和访问）进行权衡。审计 审计员探究当前模型的局限性，并提出解决这些问题的路径，并在各种自然环境中测试模型的适应性衍生品。审计开放访问政策允许更多、更多样化的研究人员调查任何模型的偏差、局限性和安全漏洞，更好地告知模型的可接受用途并校准对模型的适当信任 [Danks 2019; Baier 1986].132为了支持基础模型的独立审计，模型开发人员或第三方中介机构可以为审计人员提供开放 API 访问，包括梯度访问，并允许访问训练数据 [Raji 和 Buolamwini 2019；拉吉等人。 2020]。在工业专有数据上训练的基础模型不太可能被发布，而在私人数据（如在医疗环境中）上训练的模型也不应该被发布。为了使专有模型从独立审计中受益，并使模型主体从审计过程提示的改进中受益，我们建议在分阶段发布期间进行审计。虽然分阶段发布可能无法阐明所有可能的模型用例，但扩大未覆盖用例范围的一种方法是聘请中立的第三方来决定哪些个人或组织应该在分阶段发布计划中获得早期访问权。当模型开发人员决定谁应该获得分阶段访问权限时，他们就会面临偏袒、选择性分发和操纵公众对其产品看法的指控。中立的“分阶段发布委员会”或联邦审计员可以针对这些故障模式提供支持，并确保广泛的131有关滥用的危害的分析，请参阅关于假新闻的[Rini 2017]和关于深度伪造的[Rini 2020] 。 132 校准信任可能需要能够阐明与信任相关的模型特征的解释，例如“敏感特征的歧视性使用”[Dimanov 等人，2017]。 2020].158 为基础模型研究中心 (CRFM) 审计员和用户提供访问权限，以获取一系列学科专业知识和社会部门的知识。分阶段发布委员会还可以减轻人们的看法，即如果审计师共享不讨人喜欢的输出，他们将面临失去对模型的早期访问的风险，因为他们可能处于标准的分阶段发布流程中。访问和适应。如果基础模型具有社会效益，模型的发布就有可能进一步分发它们。 BERT 和 M-BERT 等大型语言模型能够进行跨语言迁移，当模型开源时，这可能允许适应那些可用文本太少的语言 [Wu 和 Dredze 2019； Wang 等人.2020a]。考虑到商业提供商目前无法很好地服务的语言数量，仅这样的好处就可能是巨大的。发布不足以使基础模型的访问民主化，因为计算能力的障碍仍然阻止许多人修改甚至加载基础模型，更不用说开发自己的模型了。然而，在这些方面，我们最近都看到了显着的技术改进。零冗余优化器 (ZeRO) 等内存技术允许研究人员在简单的设置上运行和训练非常大的模型 [Rasley 等人，2017]。 2020；拉杰班达里等人。 2021]。蒸馏等技术可以允许发布更小、更容易处理的模型，这些模型可以恢复其父模型的大部分性能，同时更容易训练[Li et al.2020d]。如第 5.3 节：环境中所述，开发能源密集度较低的训练方法可以进一步扩展使用已发布模型的能力。训练更大的模型需要提高效率，例如硬件和软件的协同设计，如第 4.5 节：系统中所述，但也可用于降低访问当前模型的价格。相比之下，最严重的危害并不是由释放明显加剧的。有能力实施大规模虚假信息、网络战或有针对性的网络钓鱼的老练或机构行为者也可能有能力创建类似的模型（如果没有发布的话）。尽管可能很严重，但这些危害不应因此在释放计算中占据很大的比重 [Solaiman et al.2019;谢夫兰和达福 2020]。需要权衡收益的危害是来自资源匮乏的参与者的危害，他们无法创建自己的基础模型，但可能会产生垃圾邮件或滥用行为、虚假评论或在测试中作弊。发布的好处是否超过了参与者的潜在危害，这些参与者足够成熟，可以使用已发布的模型或 API，但不够成熟，无法创建自己的模型或 API？我们相信答案是肯定的。拥有开发基础模型所需资源和人脉的研究团队数量很少。即使作为一个整体，我们也不太可能有足够多或多样化的数量来想象所有可能的有益用例或所有可能的可以阐明基础模型能力表面的探索。 

### 5.6.5 何时不构建

强大技术的开发和部署不像重力那样作用于我们的外力。技术反映了人类做出的一系列选择；人类能动性塑造了技术前沿。因此，技术人员可以选择何时不构建、设计或部署基础模型 [Zimmermann 2021]。这个决定不必是二元的；相反，人们可以通过颠覆内在价值观、挑战假设和制定研究议程来拒绝采用默认方式[Simpson 2007]。技术制品（包括基础模型）本质上是政治性的，因此对它们的研究具有社会政治背景，而不仅仅是技术背景。开发人员和研究人员应该认识到他们寻求解决哪些问题，例如，如何扩展基础模型以及如何使其更易于计算访问；这些问题是如何表述的；以及他们的解决方案最终为谁提供支持 [Rogaway 2016； 1980年获胜者；帕西和巴罗卡斯 2019]。我们应该重视旨在使基础模型更加可解释、可访问、可持续和公平的研究。关于基础模型 159 的机遇和风险（参见§4.11：可解释性、§5.3：环境、§5.1：公平性）。通过询问何时不构建基础模型或改编衍生品，我们隐含地不仅在问“我们应该构建或不构建什么？”而且，“应该在什么条件下建立模型？”以及“管理建筑的标准和原则是什么？”第一个问题源于模型视图；从生态系统的角度提出以下问题（§1：引言）。邀请考虑拒绝建设并不等于说“什么也不做”。它邀请人们对什么值得花费时间、财政资源、专业知识和能源来构建、设计和部署做出深思熟虑和明智的选择。归根结底，这是一个根植于背景和价值观的困难道德问题。在某些情况下，自适应衍生品（以及更广泛的算法和机器学习）的应用是不合适的，因为社区影响了抗议活动，或者因为自适应衍生品天真地加剧了系统性问题，而这些问题可以通过公共政策、额外资金或跨学科合作来更好地解决[安格温等人。 2016]。Floridi 等人[2018] 中将贝尔蒙特报告应用于机器学习，为这个问题提供了一个可能的框架。根据“仁慈”原则[Department of Health and Welfare 1979]，当适应性衍生品或研究途径可能弊大于利，甚至根本没有任何好处时，我们可以确定重新考虑建设的案例。或者，在某些情况下，自适应导数更适合在效率、性能和泛化指标方面的任务，这些指标是机器学习社区优先考虑的价值观 [Birhane et al.2020]，但个人、社区或组织可能会选择优先考虑强调其他价值观的现有解决方案，例如人与人之间的联系和可解释性[Benjamin 2016]。 133这样做时，他们行使了自主权——正如贝尔蒙特报告“尊重人”中所解释的那样——决定这不是一个建立适当的环境[卫生和福利部 1979]。回答何时不建造的问题是个人责任以及更广泛的职业责任的问题。如果决定不由一个人、一个团队或一家公司构建某个东西，就会招致这样的答复：“但如果我们不构建这个，其他人就会构建，而且他们可能会做得更糟。”对两种模型结果的相对危害进行简单的功利主义权衡忽略了诚信的重要性。是否是我们构建了糟糕的模型，或者是否是其他人，这非常重要[Williams 1973]。个人有理由不建造一些违背他们价值观的东西，或者他们不能认可建造的东西[Korsgaard 2009]。然而，这样创建的结构环境是不同的。即使有一家公司决定建立一种道德上可疑的模型的最有效版本，他们也会为其他公司考虑类似的研究途径打开大门；不进行研究会使人处于竞争劣势[Askell et al.2019]。何时不建设既是一个集体问题，也是一个个人问题，要求社区遵守职业道德和责任准则。与医疗领域等其他领域相比，人工智能/机器学习社区的基础设施尚不发达。尽管像计算机协会（ACM）这样的专业机构有道德声明，但工业界和学术界都缺乏广泛使用和接受的职业誓言（例如希波克拉底誓言或工程师义务），以及参与部署和部署的监管机构。研究（例如，FDA 负责药物）和官方伦理审查协议（例如，IRB 负责涉及人类受试者的研究；[Bernstein et al.2021]）。选择退出的能力可以在许多阶段纳入基础模型生态系统，包括在数据生产、适应和部署期间。随着规范转向收集更大的数据，133参见§4.11.4：可解释性影响，以进行有关不可解释性影响的相关讨论。160基础模型研究中心（CRFM）更大的训练数据（§4.6：数据），我们应该努力保持“对人的尊重”[卫生和福利部 1979] 强调隐私和同意作为数据生命周期的一部分。这需要数据管理方面的创新，以及对在线知情同意、记录和确保同意得到尊重的方式以及隐私有更具体的理解（技术上和哲学上）（参见§4.6：特定数据管理提案的数据；[欧姆 2014]）。尽管数据和基础模型的应用各不相同，但数据参与者应该能够表明他们不希望如何使用他们的数据。选择退出同意模式有利于开发人员，因为它不需要他们为每个新的、意外的用例获得同意。那么重要的是，对于现在正在寻求但最初给予同意时并未给予的申请，有权撤销空洞的同意。 

### 5.6.6 结论

在本节中，我们调查了伴随基础模型广泛采用而带来的一些社会风险，例如结果的同质化和权力的集中化。基础模型的开发者应采用有关基础模型的开发、审计和发布的规范，以在立法要求的帮助下解决这些风险，并且个人应该能够拒绝成为基础模型的数据或决策主体而不会产生影响。基础模型的生成和交互能力的许多含义尚未得到调查。例如，§5.5：经济学讨论了创意和设计工作自动化对经济生产力的潜在收益。然而，由于其生成性，基础模型可能会取代许多人认为有意义且有成就感的工作，例如图形设计和写作。我们希望本报告的范围能够帮助其他人解决此处未解决的道德和社会问题。

# 6 结论

在本报告中，我们努力全面讨论许多最关键的问题基础模型的各个方面，从技术基础到社会后果。通过这种方式，我们承认所采取的不寻常的方法：我们试图澄清可能才刚刚开始的范式的本质，而不是等待更多的展开或尘埃落定。因此，尽管我们付出了努力，但仍有许多事情不清楚，我们重申，这只是范式转变的开始：基础模型才刚刚开始改变人工智能系统在世界上构建和部署的方式。展望未来，我们认为这份文件在指导和框架有关这些模型和人工智能新范式的对话方面发挥着重要作用。也就是说，为了确保在持久的基础上负责任地开发和部署这些模型，我们认为不同部门、机构和学科之间的合作从一开始就尤为重要。