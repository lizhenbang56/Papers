---
Url: https://arxiv.org/pdf/2212.02081.pdf
Publish: CVPR2024
Year: "2022"
Month: "12"
Title: "YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection"
创新点: 训练一个多标签分类器之后，设置阈值进行二分类。
---
## 摘要

近年来，由于在部署系统中的重要性，Out-of-distribution (OOD) 检测引起了机器学习研究界的广泛关注。大多数先前的研究集中于在**多类别分类**任务中检测OOD样本。然而，在**多标签分类**任务中进行OOD检测，这是一个更常见的实际应用案例，仍然是一个未充分探索的领域。在这项研究中，我们提出了YolOOD - 一种利用目标检测领域的概念来执行**多标签分类**任务中的OOD检测的方法。目标检测模型具有区分感兴趣对象（内部分布）和无关对象（例如，OOD对象）的固有能力，在包含属于不同类别的多个对象的图像中。这些能力使我们能够仅通过进行微小更改将常规目标检测模型转换为具有固有OOD检测能力的图像分类器。我们将我们的方法与最先进的OOD检测方法进行了比较，并展示了YolOOD在一系列全面的内部分布和OOD基准数据集上优于这些方法的能力。

## 引言 

机器学习，特别是基于深度学习的网络，已成为计算机视觉任务的最先进解决方案，例如图像分类、目标检测和图像分割。然而，已经表明这些模型可能会对不属于它们训练的分布内的样本产生过度自信的预测，即OOD样本。在过去几年中，已经提出了许多解决方案来解决这个问题，其中大多数集中于区分内部分布和OOD数据。然而，这些研究仅提出了**多类别分类**任务的解决方案，在该任务中，一个输入与单个类别相关联。**多标签分类**领域中OOD检测的问题被忽视，并且仍然未被充分探索。尽管其重要性，但仅有两项研究专门解决了这个问题。**多标签图像分类**和目标检测是两个密切相关的任务。前者涉及为图像分配多个标签，而后者则更进一步，不仅识别图像中存在的对象，还通过边界框对它们进行定位。因此，目标检测器具有区分感兴趣对象和无关对象的固有能力。这种能力以及目标检测与多标签分类的相似性可以用来创建多标签设置中的OOD检测机制。

在本文中，我们提出了YolOOD，一种**多标签分类器**，它利用了最先进的目标检测器的主要概念，特别是YOLO目标检测器。为了将YOLO的网络转换为图像分类器，我们仅仅用一个简化的最后一层替换了每个检测头。YolOOD基于常用于目标检测器的对象得分概念来实现，该概念用于理解图像不同部分的相关性。这个概念通过训练网络来预测背景区域或包含无关对象的区域的低得分来实现。在OOD检测方面，这可以解释为将低分配给OOD数据，并将高分分配给内部分布数据。

与现有的OOD方法不同，这些方法是通过从外部OOD数据源积极训练网络负面数据来实现的，YolOOD提供了一个重大优势，因为它利用了图像的所有部分来建模原始数据分布和可能是OOD的对象，而无需外部数据源。为了解决我们方法对边界框注释的要求，我们利用了大型语言模型（LLM）最近取得的巨大进步，以执行完全自动化的数据集标注。

使用Grounding DINO，一个可以通过文本输入检测任意对象的多模式开放式对象检测模型，我们仅使用标准图像分类注释（即类别名称）自动生成每个图像的边界框。我们进行了广泛的评估（超过500个训练模型），证明了YolOOD在大规模基准图像数据集（例如MS-COCO）上的最先进性能和优越性。此外，我们提出了新的内部分布和OOD数据集基准，用于**多标签领域的OOD检测**。为了创建内部分布数据集，我们从Objects365数据集中提取了20个类别的子集。关于OOD数据集，我们旨在创建更好地捕获多标签设置复杂性的数据集，其中图像可能包含属于不同类别的多个对象。新数据集包括：（a）一个Objects365数据集的子集，在该子集中类别与上述内部分布子集中的类别不同，以及（b）NUS-WIDE数据集的一个子集。我们的结果表明，与其他OOD检测方法相比（例如，使用Objects365子集作为内部分布数据集和NUS-WIDE子集作为OOD数据集时），YolOOD显著提高了FPR95值（假正率95%）。

我们的贡献总结如下：

- 我们提出了YolOOD，一种新颖的**多标签图像分类**领域的OOD检测技术，它由YOLO目标检测系统驱动，并且优于最先进的技术。
- 我们是第一个采用边界框注释进行OOD检测的，这是一种独特的技术，利用输入图像的所有部分来建模原始数据分布和OOD数据，而不依赖于外部数据源。
- 我们介绍了新的基准数据集：（a）一个大规模的内部分布数据集，和（b）两个更好地反映多标签领域OOD检测复杂性的OOD数据集，并将其提供给科学界。

## 2. 背景 

### 2.1. 多标签分类 

**多标签分类**问题定义如下： 设 X 为输入空间，Y 为分类器 f：X → R|Y| 的输出空间，这个分类器是在从分布 D(X, Y) 中抽取的样本上训练的。每个输入 x ∈ X 可以与一个标签子集 Y = {1, 2, ..., Nc} 相关联，该子集由一个二进制指示向量 y = {0, 1}^Nc 表示，其中如果输入与类别 n 相关联，则 yn = 1。 目标检测是**多标签分类**任务的一个变体，其中模型还确定现有对象的位置，即边界框坐标。

### 2.2. 外部分布检测 

让 Din 表示 D 在 X 上的边缘分布，表示内部分布数据的分布。在推断时，系统可能会遇到从不同分布 Dout 上抽取的输入 x。对于多标签设置中的OOD检测，定义一个决策函数 G 如下： G(x; f) = ( 1 如果 x ∼ Din 0 如果 x ∼ Dout 其中如果它不包含内部分布对象，则 x 被认为是OOD。

### 2.3. YOLO目标检测器 

在本文中，我们专注于最先进的单阶段YOLO目标检测器，并利用其能力，我们提出的方法使用其最新版本之一，YOLOv5。 YOLO的架构。 YOLO的架构由两个组件组成：（a）用于从输入图像中提取特征的主干网络，以及（b）三个检测头，它们在三个不同的尺度上处理图像的特征。检测头的大小由输入图像的大小和网络的步幅（降采样因子）确定 - 32、16和8。这使得网络能够检测不同大小的对象：第一个检测头（具有最大步幅）具有更广泛的上下文，专门用于检测大对象，而最小的检测头具有更好的分辨率，并专门用于检测小对象。 每个检测头的最后一层预测一个大小为 W × H ×（4 + 1 + Nc）的3D张量，其中 W × H 是网格大小（W 和 H 分别是宽度和高度），而4 + 1 + Nc（在本文的其余部分将被称为候选项）编码三部分：边界框偏移量、物体性得分和类得分。 训练YOLO。每个地面实物对象与每个检测头中的单个单元格相关联。输入图像被分成一个 W × H 网格，并且负责的单元格是由对象的中心落入的网格单元格确定的。YOLO不假设类别是相互排斥的；因此，类得分向量在多标签配置下训练（对每个输出神经元应用sigmoid函数）。 后处理。YOLO输出固定数量的候选项（数量取决于输入图像的大小），然后经过三个连续步骤进行过滤：物体性得分过滤、类得分过滤和非极大值抑制（NMS）。 有关YOLO的架构、训练和推断的其他细节可以在补充材料中找到。

让 cˆ 表示预测的候选项 c ∈ Ck 的相应地面真值。在位置 (i, j) 处的 cˆ 的物体性得分值定义如下： φu = i ≥ xk,center − pk · Wr / 2 , φl = j ≥ yk,center − pk · Hr / 2 φd = i ≤ xk,center + pk · Wr / 2 , φr = j ≤ yk,center + pk · Hr / 2 (2) cˆobj(i, j) = ( 1 φu ∧ φd ∧ φl ∧ φr 0 其他 (3)

不同 pk 值的效果在第 4 节中讨论。类别得分。每个预测的候选项 c ∈ Ck 包含 Nc 个类别得分，代表模型对每个特定类别存在的置信度。由于多个单元格可能负责预测一个对象，因此一个单元格可以被分配给多个类别。与原始的 YOLO 训练过程一样，我们以多标签的方式训练每个候选项，即类别不是相互排斥的。形式上，位置 (i, j) 处的 cˆ 中类别 n ∈ {1, .., Nc} 的类别得分为： cˆcls n(i, j) = ( 1 类别 n 在单元格 (i, j) 中 0 其他 (4)

## 方法

目标检测器为OOD检测提供了自然解决方案，因为它们具有区分感兴趣对象（内部分布数据）和无关对象（OOD数据）的固有能力。此外，目标检测器在训练过程中利用未标记的数据（即，未包含在带标签的边界框内的区域）进行被动负学习，使它们能够更好地泛化到丢弃无关对象的子任务。在YOLO中，通过使用目标性分数来实现这一点。总的来说，这些因素的结合使目标检测器成为OOD检测任务的理想候选者。

在本节中，我们介绍了YolOOD，我们的新型多标签领域OOD检测方法，受YOLO目标检测器的启发。通过微小的更改，我们将YOLO网络转换为多标签图像分类器。

### 3.1. YolOOD检测层

由于我们提出了一种用于图像分类领域的方法，因此我们的模型不需要预测边界框坐标。因此，我们将每个检测头的最后一层替换为大小为Wk×Hk×（1 + Nc）的三维张量，其中k表示第k个检测头（k∈{1,2,3}）。大小为Wk×Hk的网格与原始YOLO架构中的网格保持相同，在该网格中，每个单元格仅预测目标性分数和Nc个类分数。

形式上，让fYolOOD：X→C是一个YolOOD图像分类器，它接收一个输入图像x∈X，并输出一个候选集fYolOOD(x)=C，其中C=∑kCk，其中Ck表示第k个检测头的候选集。每个候选集Ck由Wk×Hk个候选项组成（根据第2节中提到的下采样因子），因此|C|=∑3k=1|Ck|=∑3k=1Wk·Hk。

【目标性分数】如第2节中所述，在原始YOLO模型中，只有一个单元格网格负责预测一个对象。但是，在图像分类中，我们不限于特定单元格预测对象的边界框的情况。因此，我们建议扩大“责任”区域，使包含对象任何部分的多个网格单元格负责其检测，从而允许模型捕获更广泛的对象表示。

然而，需要考虑的一个重要方面是，边界框不能准确地分割对象（即，对象的形状不一定是矩形），这可能导致不属于边界框内对象的背景区域被包含在内。这可能导致将对象边界框内的无关区域与对象错误地关联起来。因此，我们建议仅使用覆盖对象区域的部分网格单元格。

更形式化地说，让（Wo，Ho）∈[0，1]2⊂R2是对象的归一化宽度和高度。特定网格中对象的相对宽度和高度定义为：（Wr，Hr）=（Wo·Wk，Ho·Hk）。此外，让（xk，center，yk，center）表示对象中心落入第k个网格的单元格的索引。我们将pk定义为相对于网格大小的网格单元格的部分，关于对象的中心，即，负责网格单元格随着pk的函数从对象中心扩展到对象边界。如图2所示，当pk = 0时，负责单元格与原始YOLO模型的单元格相同（单个单元格），而当pk = 1时，负责单元格覆盖整个对象的区域。

让 cˆ 表示预测候选项 c ∈ Ck 的相应的地面真实值。在位置 (i, j) 处的 cˆ 的目标性分数值定义如下：φu = i ≥ xk,center − pk · Wr / 2 , φl = j ≥ yk,center − pk · Hr / 2 φd = i ≤ xk,center + pk · Wr / 2 , φr = j ≤ yk,center + pk · Hr / 2 （2） cˆobj(i, j) = （ 1 φu ∧ φd ∧ φl ∧ φr 0 else）（3）。 不同 pk 值的影响在第 4 节中进行了讨论。

【类分数】每个预测的候选项 c ∈ Ck 包含 Nc 个类分数，代表模型对每个特定类别存在的信心。由于可能有多个单元格负责预测一个对象，因此一个单元格可以被分配给多个类别。与原始YOLO训练过程一样，我们以多标签的方式训练每个候选项，即，类别不是相互排斥的。形式上，类别 $n \in \{1, \cdots, N_c\}$ 在位置 $(i, j)$ 处的 cˆ 的类分数为：cˆcls n(i, j) = （ 1 如果类别 n 在单元格（i，j）中 0 否则）（4）。

### 3.2. YolOOD 损失函数 

为了训练 YolOOD，我们设计了一个定制的损失函数，由两个部分组成： • 物体性得分损失 - Lobj = Σ c∈C LBCE(cobj, cˆobj) (5) 其中 LBCE 表示二元交叉熵损失。 • 类别得分损失 - Lcls = Σ c∈C Σ n∈{1,..,Nc} cˆobj · LBCE(ccls n, cˆcls n) (6)

最后，总损失函数为： Ltotal = Lobj + Lcls (7)

### 3.3. YolOOD 作为多标签分类器 

由于**多标签分类器**输出的核心定义是一个**包含类别概率的向量**，我们将 YolOOD 的输出聚合，使得每个类别 $n$ 的分数是所有候选项中的最高分，并形式上定义为：
$$y_n = \max_{c\in \mathcal{C}} \{ \sigma(c_{\text{obj}}) \cdot \sigma(c_{\text{cls}~n})\}$$其中 $\sigma$ 表示 sigmoid 函数。

### 3.4. 用于多标签OOD检测的 YolOOD 

类似于 YOLO 候选项的后处理（见第 2.3 节），我们利用物体性和类别得分，并提出以下方式使用 YolOOD 进行OOD检测（在图 3 中可视化）： YolOOD(x) = max n∈{1,..,Nc} Σ Ck∈fYolOOD(x) max c∈Ck {σ(cobj) · σ(ccls n)} (9) G(x, τ ) = ( 1 如果 YolOOD(x) ≥ τ 0 如果 YolOOD(x) < τ (10)

其中 τ 表示阈值，可以根据 G(x, τ ) 正确分类高百分比的内部分布数据的值（例如，95%）进行选择。

## 4. 评估 

### 4.1. 实验设置 

【内部分布数据集】我们考虑以下内部分布数据集，最初在 [11] 中提出：
- PASCAL VOC [7] - 包含 20 个类别的 5,717 个训练、5,823 个验证和 10,991 个测试图像。
- MS-COCO [19]（2017 版本）- 包含 80 个类别的 117,266 个训练、4,952 个验证和 40,670 个测试图像。

此外，我们提出了一个新的基准，即 Objects365 数据集的子集： • Objects365in - 原始数据集的子集，由不与OOD数据集中的类别重叠的最频繁的 20 个类别组成（下文介绍的）。包含 68,723 个训练、5,000 个验证和 10,000 个测试图像。更多详细信息请参阅补充材料。 

训练和验证图像在训练过程中使用，而测试集用作OOD检测评估中的内部分布集。此外，为了证明我们的方法不仅限于包含边界框注释的数据集，我们利用了 Grounding DINO [21]，一个多模态开放式目标检测模型。Grounding DINO 接受 ⟨IMAGE，CAPTION⟩ 对并基于提供的标题返回图像中对象的位置。在我们的研究中，我们使用 Grounding DINO 自动标注训练集图像，其中标题简单地是图像中存在的类别名称的串联（即，标准图像分类注释）。

【OOD数据集】在之前的研究中 [11, 34]，提出了解决多标签设置下OOD检测的解决方案，并评估了来自 ImageNet-22K [30] 和 Textures [6] 数据集的子集的效果。然而，这些数据集仅包含一个单一类别的图像，导致了一个过于简化的设置。

因此，我们提出了两个新的基准，构建自包含与多个类别和实例相关联的图像的数据集，从而反映了多标签设置的复杂性：
- 【Objects365out】Objects365 数据集的一个子集，包含∼200 个类别，不与内部分布数据集（例如，lamp，tomato）（特别是 Objects365in 子集）中的任何类别重叠。该子集包含 11,669 个图像。
- 【NUS-WIDE】原始 NUS-WIDE 数据集 [5] 的一个子集。我们移除了重叠的类别类别，留下了一个包含 54 个类别（例如，toy，tree，whales）的子集。该子集包含 13,149 个图像。有关数据集的进一步详细信息，请参见补充材料。 指标。在我们的评估中，性能使用OOD检测领域常用的指标进行衡量：(a) FPR95 - 当真正阳性率达到 95% 时，OOD样本的假阳性率；(b) AUROC - 接收器操作特性曲线下的面积；和(c) AUPR - 精确率-召回率曲线下的面积。 

网络架构。我们使用最新版本的YOLO目标检测器，YOLOv5 [14]，在 MS-COCO 数据集上进行了预训练。正如第 2.3 节中所解释的，网络由主干和三个检测头组成。YOLOv5 提供了几种模型大小：nano、small、medium 等，每种模型包含不同数量的主干和检测头的可学习参数。我们使用 YOLOv5 small 版本（YOLOv5s），其中主干和检测头分别包含约 4.1M 和 3M 个可学习参数，总共约 7.1M 个参数。为了应用我们的OOD检测方法，我们将每个检测头的最后一层替换为第 3.1 节中描述的检测层。 为了在提出的方法和其他最先进的OOD检测方法之间进行公平比较，我们基于YOLOv5s的主干训练了一个多标签图像分类器。为此，我们用三个全连接层替换了检测头，从而得到一个大小相似的网络（总共约 7.1M 个参数）。这个网络在评估中被称为YOLO-cls。 训练细节。对于每个内部分布数据集，我们使用主干的预训练权重微调了一对YolOOD和YOLO-cls模型。更具体地说，我们微调了五对，每对使用不同的种子初始化。本文中呈现的结果是它们的平均值（包括补充材料中的标准偏差值）。我们使用 Adam 优化器 [15]，初始学习率分别为 10^-5 和 10^-4。如果验证集上的 mAP 连续两个时期没有改进，学习率将降低 10 倍。对于 YOLO-cls 模型，我们对分类层（即，对数）的输出应用逻辑 sigmoid 函数进行多标签训练。对于这两个模型，图像被调整为 640 × 640 像素，并应用基于颜色的增强和几何变换。YolOOD 的 mAP 与在所有评估的内部分布数据集上的 YOLO-cls 大致相当（∼1-2% 的差异）。详细结果见补充材料。 

### 4.2. 结果 

负责网格单元百分比 pk 的影响。我们表征了负责单元格的百分比 {pk|k ∈ {1, 2, 3}} 的影响（在第 3.1 节中描述），其中 p1（resp. p3）代表最小（resp. 最大）检测头。