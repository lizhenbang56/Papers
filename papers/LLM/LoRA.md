LORA：大型语言模型的低阶自适应

# 摘要

自然语言处理的一个重要范例包括对通用领域数据的大规模预训练以及**对特定任务或领域的适应**。当我们预训练更大的模型时，重新训练所有模型参数的完全微调变得不太可行。以 GPT-3 175B 为例，部署微调模型的独立实例（每个实例都有 175B 参数）的成本极其昂贵。我们提出了低秩适应（LoRA），它冻结了预训练的模型权重，并将可训练的秩分解矩阵注入到 Transformer 架构的每一层中，大大减少了下游任务的可训练参数的数量。与使用 Adam 微调的 GPT-3 175B 相比，LoRA 可以将可训练参数数量减少 10,000 倍，GPU 内存需求减少 3 倍。 LoRA 在 RoBERTa、DeBERTa、GPT-2 和 GPT-3 上的模型质量上表现与微调相当或更好，尽管可训练参数更少、训练吞吐量更高，并且与适配器不同，没有额外的推理延迟。我们还对语言模型适应中的排名缺陷进行了实证研究，这揭示了 LoRA 的功效。我们发布了一个软件包，可促进 LoRA 与 PyTorch 模型的集成，并在[https://github.com/microsoft/LoRA上提供 RoBERTa、DeBERTa 和 GPT-2 的实现和模型检查点](https://github.com/microsoft/LoRA)。

# 1 简介

自然语言处理中的许多应用程序依赖于将一种大规模的、预先训练的语言模型适应多个下游应用程序。这种适应通常是通过微调来完成的，微调会更新预训练模型的所有参数。**微调的主要缺点是新模型包含与原始模型一样多的参数**。随着每隔几个月训练一次更大的模型，这从 GPT-2（Radford 等人，b）或 RoBERTa Large（Liu 等人，2019）的单纯“不便”转变为 GPT-3 的关键部署挑战（ Brown 等人，2020）拥有 1750 亿个可训练参数。许多人试图通过仅调整某些参数或学习外部模块来完成新任务来缓解这一问题。这样，除了每个任务的预训练模型之外，我们只需要存储和加载少量特定于任务的参数，大大提高了部署时的运行效率。然而，现有技术 通常通过扩展模型深度或减少模型的可用序列长度来引入推理延迟（Houlsby et al., 2019; Rebuffi et al., 2017）（Li＆Liang, 2021; Lester et al., 2021; Hambardzumyan et al., 2020） ；Liu 等人，2021）（第 3 节）。更重要的是，这些方法通常无法匹配微调基线，从而在效率和模型质量之间进行权衡。我们从 Li 等人那里汲取灵感。 （2018a）；阿加詹扬等人。 （2020）表明，学习的过参数化模型实际上存在于较低的内在维度上。我们假设模型适应期间权重的变化也具有较低的“内在等级”，从而导致我们提出的低等级适应（LoRA）方法。 LoRA 允许我们通过优化适应期间密集层变化的秩分解矩阵来间接训练神经网络中的一些密集层，同时保持预训练权重冻结，如图1 所示。使用 GPT-3 175B 作为例如，我们表明，即使满秩（即 d）高达 12,288，非常低的秩（即图 1 中的 r 可以是 1 或 2）也足够了，从而使 LoRA 具有存储和计算效率。 LoRA 拥有几个关键优势。 
- 可以共享预训练模型并用于构建许多小型LoRA 模块来执行不同的任务。我们可以通过替换图1中的矩阵A和B来冻结共享模型并有效地切换任务，从而显着降低存储需求和任务切换开销。
- 使用自适应优化器时，LoRA 使训练更加高效，并将硬件进入门槛降低了多达 3 倍，因为我们不需要计算梯度或维护大多数参数的优化器状态。相反，我们只优化注入的、小得多的低秩矩阵。
- 我们简单的线性设计使我们能够在部署时将可训练矩阵与冻结权重合并，通过构造，与完全微调的模型相比，不会引入推理延迟。
- LoRA 与许多现有方法正交，并且可以与其中许多方法相结合，例如前缀调整。我们在附录 E 中提供了一个示例。 术语和约定 我们经常引用 Transformer 架构，并对其维度使用常规术语。我们将 Transformer 层的输入和输出维度大小称为 dmodel。我们使用 Wq、Wk、Wv 和 Wo 来指代自注意力模块中的查询/键/值/输出投影矩阵。 W 或 W0 指预训练的权重矩阵， ΔW指适应过程中累积的梯度更新。我们使用 r 来表示 LoRA 模块的等级。我们遵循 (Vaswani et al., 2017; Brown et al., 2020) 制定的约定，并使用 Adam (Loshchilov & Hutter, 2019; Kingma & Ba, 2017) 进行模型优化，并使用 Transformer MLP 前馈维度 df fn = 4 × d 模型。

# 2 问题陈述

虽然我们的建议与训练目标无关，但我们专注于语言建模作为我们的激励用例。下面是对语言建模问题的简要描述，特别是在给定特定任务提示的情况下条件概率的最大化。假设我们有一个预训练的自回归语言模型 PΦ(y|x)，由 Φ 参数化。例如，PΦ(y|x) 可以是基于 Transformer 架构（Vaswani 等人，2017）的通用多任务学习器，例如 GPT（Radford 等人，b；Brown 等人，2020）。考虑使此预训练模型适应下游条件文本生成任务，例如摘要、机器阅读理解 (MRC) 和自然语言到 SQL (NL2SQL)。每个下游任务都由上下文目标对的训练数据集表示：Z = {(xi , yi)}i=1,..,N ，其中 xi 和 yi 都是标记序列。例如，在NL2SQL中，xi是自然语言查询，yi是对应的SQL命令； xi 是文章的内容，yi 是摘要。 2 在完全微调过程中，模型被初始化为预训练权重 Φ0，并通过重复遵循梯度更新为 Φ0 + Δ Φ ，以最大化条件语言建模目标：max Φ X (x,y) ∈ ZX |y| t=1 log (P Φ (yt|x, y<t)) (1) 完全微调的主要缺点之一是，对于每个下游任务，我们学习一组不同的参数Δ Φ ，其维度 | Δ Φ |等于 | Φ0 |。因此，如果预训练模型很大（例如 | Φ 0| ≈ 1750 亿的 GPT-3），那么存储和部署微调模型的许多独立实例可能会具有挑战性（如果可行的话）。在本文中，我们采用了一种参数效率更高的方法，其中特定于任务的参数增量Δ Φ = Δ Φ ( θ ) 由一组更小的参数θ进一步编码，其中 | θ | | Φ0 |。因此，找到Δ Φ的任务就变成了对θ进行优化： max θ X (x,y) ∈ ZX |y| t=1 对数 在后续部分中，我们建议使用低秩表示来编码Δ Φ ，这既具有计算效率又具有内存效率。当预训练模型为GPT-3 175B时，可训练参数个数| θ |可小至 | 的 0.01% Φ0 |。

# 3 现有解决方案还不够好吗？

**我们要解决的问题**绝不是新问题。自迁移学习诞生以来，已有数十项工作试图**提高模型适应的参数和计算效率**。请参阅第 6 节对一些著名作品的调查。以语言建模为例，在高效适应方面有两种突出的策略：
- 添加适配器层（Houlsby et al., 2019; Rebuffi et al., 2017; Pfeiffer et al., 2021; Ruckl œ et al., 2021）。 ，2020）或
- 优化输入层激活的某些形式（Li＆Liang，2021；Lester等人，2021；Hambardzumyan等人，2020；Liu等人，2021）。
  
然而，这两种策略都有其局限性，特别是在大规模且对延迟敏感的生产场景中。适配器层引入推理延迟 适配器有许多变体。我们专注于 Houlsby 等人的原创设计。 (2019)，每个 Transformer 块有两个适配器层，Lin 等人最近有一个适配器层。 (2020)，每个块只有一个，但有一个额外的 LayerNorm (Ba et al., 2016)。虽然可以通过修剪层或利用多任务设置来减少总体延迟（Ruckl et al., 2020；Pfeiffer et al., 2021），但没有直接的方法可以绕过适配器层中的额外计算。这似乎不是问题，因为适配器层被设计为具有很少的参数（有时<原始模型的 1%），并且具有小的瓶颈尺寸，这限制了它们可以添加的 FLOP。然而，大型神经网络依赖硬件并行性来保持低延迟，并且适配器层必须按顺序处理。这对在线推理设置产生了影响，其中批量大小通常小至 1。在没有模型并行性的一般场景中，例如在单个 GPU 上的 GPT-2（Radford 等人，b）介质上运行推理，我们发现使用适配器时延迟会显着增加，即使瓶颈维度非常小（表1）。当我们需要像 Shoeybi 等人所做的那样对模型进行分片时，这个问题会变得更糟。 （2020）；莱皮欣等人。 (2020)，因为额外的深度需要更多的同步GPU操作，例如AllReduce和Broadcast，除非我们多次冗余存储适配器参数。直接优化提示很困难另一个方向，例如前缀调整（Li＆Liang，2021），面临着不同的挑战。我们观察到前缀调整很难优化，并且其性能在可训练参数中非单调变化，证实了原始论文中的类似观察结果。更根本的是，保留一部分序列长度用于适应必然会减少可用于处理下游任务的序列长度，我们怀疑与其他方法相比，这使得调整提示的性能较差。我们将任务绩效的研究推迟到第 5 节。

# 4 我们的方法

我们描述了 LoRA 的简单设计及其实际好处。这里概述的原则适用于深度学习模型中的任何密集层，尽管我们在实验中只关注 Transformer 语言模型中的某些权重作为激励用例。

## 4.1 低阶参数化更新矩阵

神经网络包含许多执行矩阵乘法的密集层。这些层中的权重矩阵通常具有满秩。当适应特定任务时，Aghajanyan 等人。 （2020）表明，预训练的语言模型具有较低的“固有维度”，尽管随机投影到较小的子空间，但仍然可以有效地学习。受此启发，我们假设权重的更新在适应过程中也具有较低的“内在等级”。对于预训练的权重矩阵 W0 ∈ R d×k ，我们通过用低秩分解 W0 + Δ W = W0 + BA 表示后者来约束其更新，其中 B ∈ R d × r ，A ∈ R r × k ，以及秩 r min(d, k)。训练期间，W0 被冻结，不接收梯度更新，而 A 和 B 包含可训练参数。请注意，W0 和Δ W = BA 都与相同的输入相乘，并且它们各自的输出向量按坐标求和。对于 h = W0x，我们修改后的前向传递得到： h = W0x + Δ W x = W0x + BAx (3) 我们在图 1 中说明了我们的重新参数化。我们对 A 使用随机高斯初始化，对 B 使用零，因此Δ W =训练开始时BA为零。然后我们将Δ W x 按α r 缩放，其中α是 r 中的常数。当使用 Adam 进行优化时，如果我们适当缩放初始化，调整α与调整学习率大致相同。因此，我们只是将 α 设置为我们尝试的第一个 r，而不对其进行调整。当我们改变 r 时，这种缩放有助于减少重新调整超参数的需要（Yang & Hu，2021）。全面微调的推广。更通用的微调形式允许训练预训练参数的子集。 LoRA 更进一步，不需要对权重矩阵进行累积梯度更新来在适应过程中获得满秩。这意味着，当将LoRA应用于所有权重矩阵并训练所有偏差2时，我们通过将LoRA等级r设置为预训练权重矩阵的等级来粗略地恢复完全微调的表达能力。换句话说，当我们增加可训练参数 3 的数量时，训练 LoRA 大致收敛到训练原始模型，而基于适配器的方法收敛到 MLP，基于前缀的方法收敛到不能接受长输入序列的模型。无额外推理延迟。当部署在生产中时，我们可以显式计算和存储 W = W0 + BA 并像往常一样执行推理。请注意，W0 和 BA 都在 R d×k 中。当我们需要切换到另一个下游任务时，我们可以通过减去 BA 然后添加不同的 B0A0 来恢复 W0，这是一个快速操作，内存开销非常小。至关重要的是，与权重相比，这 2 个参数的数量可以忽略不计。 3适应艰巨任务时的必然性。 4 保证与通过构造微调的模型相比，我们在推理过程中不会引入任何额外的延迟。

## 4.2 将LORA应用于Transformer

原则上，我们可以将 LoRA 应用于神经网络中权重矩阵的任何子集，以减少可训练参数的数量。在 Transformer 架构中，自注意力模块中有四个权重矩阵（Wq、Wk、Wv、Wo），MLP 模块中有两个。我们将 Wq （或 Wk，Wv）视为维度 dmodel ×dmodel 的单个矩阵，即使输出维度通常被分割为注意力头。为了简单性和参数效率，我们将我们的研究限制为仅调整下游任务的注意力权重，并冻结 MLP 模块（因此它们不在下游任务中进行训练）。我们进一步研究了调整不同类型的注意力权重矩阵的效果第 7.1 节中的变压器。我们将调整 MLP 层、LayerNorm 层和偏差的实证研究留给未来的工作。实际的好处和局限性。最显着的好处来自内存和存储使用量的减少。对于使用 Adam 训练的大型 Transformer，如果使用 r dmodel，我们可以将 VRAM 使用量减少多达 2/3，因为我们不需要存储冻结参数的优化器状态。在 GPT-3 175B 上，我们将训练期间的 VRAM 消耗从 1.2TB 减少到 350GB。当 r = 4 并且仅调整查询和值投影矩阵时，检查点大小减少了大约 10,000×（从 350GB 到 35MB）4 。这使我们能够使用更少的 GPU 进行训练并避免 I/O 瓶颈。另一个好处是，我们可以通过仅交换 LoRA 权重而不是所有参数来以更低的成本在部署时切换任务。这允许创建许多定制模型，这些模型可以在将预先训练的权重存储在 VRAM 中的机器上动态地换入和换出。与完全微调相比，我们还观察到 GPT-3 175B 训练期间的速度提高了 25%，因为我们不需要计算绝大多数参数的梯度。 LoRA 也有其局限性。例如，如果选择将 A 和 B 吸收到 W 中以消除额外的推理延迟，那么在一次前向传递中将不同 A 和 B 的不同任务的输入批量输入并不简单。尽管在延迟不重要的情况下，可以不合并权重并动态选择用于批量样本的 LoRA 模块。

# 5 实证实验

我们评估了 LoRA 在 RoBERTa（Liu 等人，2019）、DeBERTa（He 等人，2021）和 GPT-2（Radford 等人，b）上的下游任务性能，然后扩展到 GPT-3 175B （布朗等人，2020）。我们的实验涵盖了从自然语言理解（NLU）到生成（NLG）的广泛任务。具体来说，我们评估 RoBERTa 和 DeBERTa 的 GLUE（Wang 等人，2019）基准。我们遵循 Li & Liang (2021) 在 GPT-2 上的设置进行直接比较，并添加 WikiSQL (Zhong et al., 2017)（NL 到 SQL 查询）和 SAMSum (Gliwa et al., 2019)（对话摘要）用于 GPT-3 的大规模实验。有关我们使用的数据集的更多详细信息，请参阅附录 C。**我们使用 NVIDIA Tesla V100 进行所有实验**。 

## 5.1 基线 

为了与其他基线进行广泛比较，我们复制了先前工作中使用的设置，并尽可能重复使用其报告的数字。然而，这意味着某些基线可能只出现在某些实验中。微调（FT）是一种常见的适应方法。在微调过程中，模型被初始化为预先训练的权重和偏差，并且**所有模型参数都进行梯度更新**。一个简单的变体是仅更新某些层，同时冻结其他层。我们在 GPT-2 的先前工作（Li＆Liang，2021）中报告了一个这样的基线，它**仅适应最后两层**（FTTop2）。 仅偏差或 BitFit 是一个基线，我们仅训练偏差向量，同时冻结其他所有内容。与此同时，BitFit 也研究了这一基线（Zaken 等人，2021）。前缀嵌入调整 (PreEmbed) 在输入标记中插入特殊标记。这些特殊标记具有可训练的词嵌入，并且通常不在模型的词汇表中。放置此类令牌的位置可能会对性能产生影响。我们重点关注“前缀”（将此类标记添加到提示符之前）和“中缀”（附加到提示符后）； Li＆Liang（2021）对两者进行了讨论。我们使用 lp（或 li）表示前缀（或中缀）标记的数量。可训练参数的数量为|θ| = d 模型 × (lp + li)。前缀层调整（PreLayer）是前缀嵌入调整的扩展。我们不只是学习一些特殊标记的词嵌入（或者等效地，嵌入层之后的激活），而是学习每个 Transformer 层之后的激活。从先前层计算的激活被简单地替换为可训练的激活。可训练参数的最终数量为|θ| = L × dmodel × (lp + li)，其中 L 是 Transformer 层数。 Houlsby 等人提出的适配器调整。 (2019) 在自注意力模块（和 MLP 模块）和后续残差连接之间插入适配器层。有两个完全连接的层，其中适配器层存在偏差，中间存在非线性。我们将这个原始设计称为 AdapterH 。最近，林等人。 (2020) 提出了一种更高效的设计，仅在 MLP 模块和 LayerNorm 之后应用适配器层。我们称之为 AdapterL 。这与 Pfeiffer 等人提出的另一种设计非常相似。 (2021)，我们称之为 AdapterP 。我们还包括另一个名为 AdapterDrop 的基线（Ruckl et al., 2020），它删除了一些适配器层以提高效率（AdapterD）。我们尽可能引用先前作品中的数字，以最大限度地增加我们比较的基线数量；它们位于第一列带有星号 (*) 的行中。在所有情况下，我们都有|θ| = LˆAdpt ×(2×dmodel ×r+r+dmodel)+ 2×LˆLN ×dmodel 其中 LˆAdpt 是适配器层的数量，LˆLN 是可训练 LayerNorm 的数量（例如，在 AdapterL 中）。 LoRA 与现有权重矩阵并行添加可训练的秩分解矩阵对。正如 4.2 节中提到的，为了简单起见，我们在大多数实验中仅将 LoRA 应用于 Wq 和 Wv。可训练参数的数量由等级 r 和原始权重的形状决定：|θ| = 2 × LˆLoRA × dmodel × r，其中 LˆLoRA 是我们应用 LoRA 的权重矩阵的数量。

## 5.2 ROBERTA 

BASE/LARGE RoBERTa（Liu et al., 2019）优化了 BERT 中最初提出的预训练方案（Devlin et al., 2019a），并在不引入更多可训练参数的情况下提升了后者的任务性能。尽管 RoBERTa 近年来在 NLP 排行榜上已被 GLUE 基准（Wang 等人，2019）等更大的模型超越，但就其规模而言，它在从业者中仍然是一个有竞争力且受欢迎的预训练模型。我们从HuggingFace Transformers 库（Wolf 等人，2020）中获取预训练的 RoBERTa base (125M) 和 RoBERTa Large (355M)，并根据 GLUE 基准评估不同高效适应方法在任务上的性能。我们还复制了Houlsby 等人的研究。 （2019）和 Pfeiffer 等人。 （2021）根据他们的设置。为了确保公平比较，我们在与适配器进行比较时对 LoRA 的评估方式做出了两项重要改变。首先，我们对所有任务使用相同的批量大小，并使用序列长度 128 来匹配适配器基线。其次，我们将模型初始化为 MRPC、RTE 和 STS-B 的预训练模型，而不是像微调基线那样已经适应 MNLI 的模型。按照 Houlsby 等人提供的更受限制的设置运行。 (2019) 标有 †。结果如表 2 所示（前三部分）。有关所使用的超参数的详细信息，请参阅第 D.1 节。 

## 5.3 DEBERTA XXL

DeBERTa（He et al., 2021）是 BERT 的最新变体，它经过更大规模的训练，并且在 GLUE（Wang et al., 2019）和 Su perGLUE（Wang et al., 2019）等基准测试上表现非常有竞争力。等，2020）。我们评估 LoRA 是否仍然可以与 GLUE 上完全微调的 DeBERTa XXL (1.5B) 的性能相匹配。结果如表 2（底部）所示。有关所使用的超参数的详细信息，请参阅第 D.2 节。 5.4 GPT-2 MEDIUM/LARGE 已经证明 LoRA 可以成为 NLU 上完全微调的有竞争力的替代方案，我们希望回答 LoRA 是否仍然在 NLG 模型上占主导地位，例如 GPT-2 中型和大型（Radford 等人， b).我们使我们的设置尽可能接近 Li & Liang (2021)，以便进行直接比较。由于篇幅限制，本节我们仅展示 E2E NLG Challenge 的结果（表 3）。有关 WebNLG（Gardent 等人，2017）和 DART（Nan 等人，2020）的结果，请参阅 F.1 节。我们提供了 D.3 节中使用的超参数列表。

## 5.5 扩展到 GPT-3 175B 

作为 **LoRA 的最终压力测试**，我们扩展到具有 1750 亿个参数的 GPT-3。由于训练成本较高，我们仅报告给定任务在随机种子上的典型标准差，而不是为每个条目提供一个标准差。有关所使用的超参数的详细信息，请参阅第 D.4 节。如表 4 所示，LoRA 在所有三个数据集上都匹配或超过了微调基线。请注意，并非所有方法都能从拥有更多可训练参数中单调受益，如图 2 所示。当我们使用超过 256 个特殊标记进行前缀嵌入调整或使用超过 32 个特殊标记进行前缀层调整时，我们观察到性能显着下降。这证实了 Li＆Liang (2021) 中的类似观察。虽然对这种现象的彻底调查超出了这项工作的范围，但我们怀疑拥有更多特殊标记会导致输入分布进一步远离预训练数据分布。另外，我们在 F.3 节中研究了低数据情况下不同适应方法的性能。

# 6 相关作品 

Transformer 语言模型。 Transformer（Vaswani 等人，2017）是一种序列到序列的架构，大量使用自注意力。雷德福等人。 (a) 通过使用一堆 Transformer 解码器将其应用于自回归语言建模。从那时起，基于 Transformer 的语言模型就主导了 NLP，在许多任务中实现了最先进的技术。 BERT（Devlin 等人，2019b）和 GPT-2（Radford 等人，b）出现了一种新的范式——两者都是在大量文本上训练的大型 Transformer 语言模型——对特定于任务的数据进行微调与直接对特定任务数据进行训练相比，对通用领域数据进行预训练可提供显着的性能提升。训练更大的 Transformer 通常会带来更好的性能，并且仍然是一个活跃的研究方向。 GPT-3（Brown 等人，2020）是迄今为止使用 175B 个参数训练的最大的单一 Transformer 语言模型。及时的工程和微调。虽然 GPT-3 175B 只需几个额外的训练示例即可调整其行为，但结果在很大程度上取决于输入提示（Brown 等人，2020）。这就需要一种编写和格式化提示的经验艺术，以最大限度地提高模型在所需任务上的性能，这被称为提示工程或提示黑客。微调将在一般领域预训练的模型重新训练到特定任务 Devlin 等人。 （2019b）；雷德福等人。 （A）。它的变体包括仅学习 Devlin 等人的参数的子集。 （2019b）； Collobert & Weston (2008)，但从业者经常对所有这些人员进行重新培训，以最大限度地提高下游绩效。然而，GPT-3 175B 的巨大性使得以通常的方式执行微调变得具有挑战性，因为它产生的检查点很大，而且由于它具有与预训练相同的内存占用，进入的硬件门槛很高。参数有效的适应。许多人建议在神经网络中的现有层之间插入适配器层（Houlsby 等人，2019；Rebuffi 等人，2017；Lin 等人，2020）。我们的方法使用类似的瓶颈结构对权重更新施加低秩约束。关键的功能区别在于，我们学习的权重可以在推理过程中与主要权重合并，因此不会引入任何延迟，而适配器层则不是这种情况（第 3 节）。适配器的一个临时扩展是 COMPACTER（Mahabadi 等人，2021），它本质上是使用 Kronecker 产品和一些预定的权重共享方案来参数化适配器层。同样，将 LoRA 与其他基于张量积的方法相结合可能会提高其参数效率，我们将其留给未来的工作。最近，许多人提出优化输入词嵌入来代替微调，类似于提示工程的连续且可微的推广（Li＆Liang，2021；Lester 等人，2021；Hambardzumyan 等人，2020；Liu 等人）等，2021）。我们在实验部分中包含了与 Li & Liang (2021) 的比较。然而，这一系列工作只能通过在提示中使用更多特殊标记来扩展，当学习位置嵌入时，这些标记会占用任务标记的可用序列长度。深度学习中的低阶结构。低秩结构在机器学习中很常见。许多机器学习问题都具有一定的内在低秩结构（Li et al., 2016; Cai et al., 2010; Li et al., 2018b; Grasedyck et al., 2013）。此外，众所周知，对于许多深度学习任务，特别是那些具有严重过度参数化神经网络的任务，学习的神经网络在训练后将具有低秩属性（Oymak et al., 2019）。一些先前的工作甚至在训练原始神经网络时明确地施加低秩约束（Sainath et al., 2013; Povey et al., 2018; Zhuang et al., 2014; Jaderberg et al., 2014; Chao et al., 2014）。 ，2016 年；Khodak 等人，2021 年；Denil 等人，2014 年）；然而，据我们所知，这些工作都没有考虑对冻结模型进行低秩更新以适应下游任务。在理论文献中，众所周知，当底层概念类具有一定的特征时，神经网络的性能优于其他经典学习方法，包括相应的（有限宽度）神经切线核（Allen-Zhu et al., 2019；Li＆Liang，2018）低阶结构（Ghorbani et al., 2020；Allen-Zhu & Li, 2019；Allen-Zhu & Li, 2020a）。 Allen-Zhu & Li (2020b) 的另一个理论结果表明，低秩适应对于对抗训练很有用。总之，我们相信我们提出的低秩适应更新受到文献的充分启发。

# 7 了解低级别更新

鉴于 LoRA 的经验优势，我们希望进一步解释从下游任务中学习到的低秩适应的特性。请注意，低秩结构不仅降低了硬件进入门槛，使我们能够并行运行多个实验，而且还可以更好地解释更新权重与预训练权重的相关性。我们的研究重点是 GPT-3 175B，其中我们实现了可训练参数的最大减少（高达 10,000×），而不会对任务性能产生不利影响。我们进行一系列实证研究来回答以下问题：1）给定参数预算约束，我们应该适应预训练 Transformer 中的权重矩阵的哪个子集 最大化下游性能？ 2）“最优”适应矩阵Δ W 真的是秩亏的吗？如果是这样，在实践中使用什么等级比较好？ 3） ΔW和W之间有什么关系？ Δ W 与 W 高度相关吗？与 W 相比，ΔW有多大？我们相信，我们对问题（2）和（3）的回答揭示了使用预训练语言模型进行下游任务的基本原理，这是 NLP 的一个关键主题。 7.1 我们应该在 Transformer 中的哪些权重矩阵上应用 LORA？给定有限的参数预算，我们应该使用 LoRA 调整哪些类型的权重才能在下游任务上获得最佳性能？正如4.2节中提到的，我们只考虑自注意力模块中的权重矩阵。我们在 GPT-3 175B 上设置了 18M 的参数预算（如果存储在 FP16 中，则约为 35MB），对于所有 96 层，如果我们采用一种类型的注意力权重，则对应于 r = 8；如果我们采用两种类型，则对应于 r = 4。结果如表 5 所示。

请注意，将所有参数放入Δ Wq 或Δ Wk 中会导致性能显着降低，而同时调整 Wq 和 Wv 会产生最佳结果。这表明，即使是四阶也能捕获足够的ΔW信息，因此适应更多的权重矩阵比适应具有更大阶数的单一类型权重更可取。

7.2 LORA 的最佳排名 r 是多少？我们将注意力转向等级 r 对模型性能的影响。我们采用 {Wq, Wv}、{Wq, Wk, Wv, Wc}，并且仅采用 Wq 进行比较。

令人惊讶的是，表 6 表明，LoRA 在 r 非常小的情况下已经具有竞争力（对于 {Wq, Wv} 来说更是如此，而不仅仅是 Wq）。这表明更新矩阵Δ W 可能具有非常小的“内在秩” 。6 为了进一步支持这一发现，我们检查通过不同的 r 选择和不同的随机种子学习到的子空间的重叠。我们认为增加 r 并不能覆盖更有意义的子空间，这表明低秩适应矩阵就足够了。

不同r之间的子空间相似度。给定 Ar=8 和 Ar=64（使用相同的预训练模型学习到的秩 r = 8 和 64 的适应矩阵），我们执行奇异值分解并获得右奇异酉矩阵 UAr=8 和 UAr=64 。 7 我们希望回答：UAr=8（对于 1 ≤ i ≤ 8）中前 i 个奇异向量所跨越的子空间有多少包含在 UAr=64（对于 1 ≤ j 时）中前 j 个奇异向量所跨越的子空间中≤ 64)?我们使用基于格拉斯曼距离的归一化子空间相似度来测量这个量（更正式的讨论请参阅附录 G） φ(Ar=8, Ar=64, i, j) = ||U i> Ar=8 U j Ar =64 ||2 F min(i, j) ε [0, 1] (4) 其中 U i Ar=8 表示对应于前 i 个奇异向量的 UAr=8 的列。 φ(·) 的范围为 [0, 1]，其中 1 表示子空间完全重叠，0 表示子空间完全分离。请参见图 3，了解当我们改变 i 和 j 时 φ 如何变化。由于空间限制，我们只查看第 48 层（共 96 层），但结论也适用于其他层，如 H.1 节所示。

我们从图 3 中得到了一个重要的观察结果。与顶部奇异向量相对应的方向在 Ar=8 和 Ar=64 之间显着重叠，而其他方向则不然。具体来说， Ar=8 的Δ Wv（分别为Δ Wq）和 Ar=64 的Δ Wv（分别为Δ Wq）共享一个 1 维子空间，归一化相似度 > 0.5，这解释了为什么 r = 1 表现得很好在 GPT-3 的下游任务中。由于 Ar=8 和 Ar=64 都是使用相同的预训练模型学习的，因此图 3 表明 Ar=8 和 Ar=64 的顶部奇异向量方向是最有用的，而其他方向可能主要包含随机噪声训练期间积累的。因此，适应矩阵确实可以具有非常低的秩。不同随机种子之间的子空间相似性。我们通过绘制 r = 64 的两个随机种子运行之间的归一化子空间相似性来进一步证实这一点，如图 4 所示。 Δ Wq 似乎比Δ Wv 具有更高的“内在等级” ，因为两者都学习了更常见的奇异值方向运行Δ Wq，这与我们在表 6 中的经验观察一致。作为比较，我们还绘制了两个随机高斯矩阵，它们彼此之间没有任何共同的奇异值方向。 7.3 适应矩阵Δ W 与 W 相比如何？我们进一步研究了ΔW和 W之间的关系。特别是， ΔW与 W 高度相关吗？ （或者从数学上讲， Δ W 是否主要包含在 W 的顶部奇异方向中？）此外， 与 W 中相应的方向相比， Δ W有多大？这可以揭示适应预训练语言模型的底层机制。为了回答这些问题，我们通过计算 U >WV > 将 W 投影到Δ W的 r 维子空间上，其中 U/V 是Δ W的左/右奇异向量矩阵。然后，我们比较 kU 之间的 Frobenius 范数>WV >kF 和 kWkF 。作为比较，我们还通过将 U、V 替换为 W 的前 r 个奇异向量或随机矩阵来计算 kU >WV >kF。

我们从表7中得出几个结论。首先，与随机矩阵相比，ΔW与W具有更强的相关性，这表明ΔW放大了W中已经存在的一些特征。其次， ΔW不是重复W的顶部奇异方向，而是重复W只放大W中没有强调的方向。第三，放大系数相当大：对于r = 4，21.5 ≈ 6.91/0.32。参见H.4节，了解为什么r = 64放大系数较小。我们还在 H.3 节中提供了可视化，说明当我们包含更多来自 Wq 的顶部奇异方向时相关性如何变化。这表明低秩适应矩阵可能会放大特定下游任务的重要特征，这些特征是在一般预训练模型中学习但未强调的。

8 结论和未来工作

就所需的硬件以及托管不同任务的独立实例的存储/切换成本而言，微调巨大的语言模型成本高昂。我们提出了 LoRA，一种有效的适应策略，既不会引入推理延迟，也不会减少输入序列长度，同时保持高模型质量。重要的是，当它部署为服务时，通过共享绝大多数模型参数，可以实现快速任务切换。虽然我们专注于 Transformer 语言模型，但所提出的原理通常适用于任何具有密集层的神经网络。未来的工作有很多方向。 1）LoRA可以与其他有效的适应方法相结合，有可能提供正交改进。 2）微调或 LoRA 背后的机制还远未明确——如何将预训练过程中学到的特征转化为在下游任务中表现出色？我们相信 LoRA 比完全微调更容易回答这个问题。 3）我们主要依靠启发式方法来选择应用 LoRA 的权重矩阵。有没有更有原则性的方法来做？ 4）最后， ΔW的等级不足表明W也可能是等级不足的，这也可以成为未来工作的灵感来源。

# C 数据集详细信息 

**GLUE Benchmark** 是自然语言理解任务的广泛集合。 它包括 MNLI（推论，Williams 等人（2018））、SST-2（情感分析，Socher 等人（2013））、MRPC（释义检测，Dolan & Brockett（2005））、CoLA（语言可接受性，Warstadt） 等人（2018）），QNLI（推理，Rajpurkar 等人（2018）），QQP8（问答），RTE（推理）和 STS-B（文本相似性，Cer 等人（2017））。 广泛的覆盖范围使 GLUE 基准测试成为评估 RoBERTa 和 DeBERTa 等 NLU 模型的标准指标。 各个数据集根据不同的许可协议发布。 

**WikiSQL** 是由Zhong 等人介绍的。 (2017) 并包含 56、355/8、421 个训练/验证示例。 任务是从自然语言问题和表模式生成 SQL 查询。 我们将上下文编码为 x = {表架构，查询}，将目标编码为 y = {SQL}。 该数据集根据 BSD 3-Clause License 发布。 Gliwa 等人介绍了 SAMSum。 (2019)，包含 14 个 732/819 个训练/测试示例。 它由两个人之间的分阶段聊天对话以及语言学家撰写的相应抽象摘要组成。 我们将上下文编码为“\n”串联话语，后跟“\n\n”，目标为 y = {summary}。 该数据集根据非商业许可发布：Creative Commons BY-NC-ND 4.0。 

**E2E NLG 挑战赛**首次由 Novikova 等人提出。 （2017）作为训练端到端、数据驱动的自然语言生成系统的数据集，通常用于数据到文本的评估。 E2E 数据集包含来自餐厅领域的大约 42, 000 个训练示例、4, 600 个验证示例和 4, 600 个测试示例。 每个用作输入的源表可以有多个引用。 每个样本输入 (x, y) 由一系列槽值对以及相应的自然语言参考文本组成。 该数据集根据 Creative Commons BY-NC-SA 4.0 发布。 

**DART** 是 Nan 等人描述的开放域数据到文本数据集。 （2020）。 DART 输入的结构为 ENTITY — RELATION — ENTITY 三元组的序列。 与 E2E 相比，DART 总共有 82K 个示例，是一个更大、更复杂的数据到文本任务。 该数据集是在 MIT 许可下发布的。 

**WebNLG** 是另一个用于数据到文本评估的常用数据集（Gardent et al., 2017）。 WebNLG 总共有 22K 个示例，包含 14 个不同的类别，其中 9 个是在训练期间看到的。 由于 14 个类别中的 5 个类别在训练期间未见，但在测试集中有所体现，因此评估通常按“已见”类别 (S)、“未见”类别 (U) 和“全部”(A) 进行细分。 每个输入示例都由一系列 SUBJECT — PROPERTY — OBJECT 三元组表示。 该数据集根据 Creative Commons BY-NC-SA 4.0 发布。