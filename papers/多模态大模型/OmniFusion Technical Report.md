---
Url: https://arxiv.org/pdf/2404.06212
Year: "2024"
Month: "04"
Code: https://github.com/AIRI-Institute/OmniFusion
Stars: "195"
---
## 摘要

去年，多模态架构在基于人工智能的方法和解决方案中引发了一场革命，扩展了大型语言模型（LLM）的能力。我们提出了一种基于预训练LLM和用于视觉模态的适配器的OmniFusion模型。我们评估并比较了几种架构设计原则，以实现更好的文本和视觉数据耦合：MLP和变压器适配器，各种基于CLIP ViT的编码器（如SigLIP、InternVIT等），以及它们的融合方法、图像编码方法（整个图像或瓦片编码）和两个7B LLMs（专有和开源的Mistral）。在8个视觉-语言基准上的实验显示，在不同的VQA任务方面，最佳OmniFusion设置的得分高于开源LLaVA类似解决方案：VizWiz、Pope、MM-Vet、ScienceQA、MMBench、TextVQA、VQAv2、MMMU。我们还提出了多种情况，在这些情况下，OmniFusion提供了不同领域的高度详细的答案：家政、观光、文化、医学、手写和扫描的方程识别等。基于Mistral的OmniFusion模型是一个开源解决方案，其权重、训练和推理脚本可在[https://github.com/AIRI-Institute/OmniFusion](https://github.com/AIRI-Institute/OmniFusion) 上找到。

## 引言

近年来，多模态架构作为增强人工智能系统的强大范式而出现，使其能够同时处理和理解多种类型的数据。不同数据模态的整合，如文本和图像，显着提升了大型语言模型（LLMs）在各种任务中的能力，从视觉问答（VQA）到复杂的决策过程。然而，有效耦合各种数据类型的挑战仍然是真正整合的AI模型发展中的重要障碍。此外，这样的多模态多任务架构被解释为人工通用智能（AGI）发展的第一步，扩大了世界认知中的挑战数量。

本文介绍了OmniFusion模型，这是一种新颖的多模态架构，利用了预训练的LLMs的优势，并引入了用于处理视觉信息的专门适配器。我们评估了多种融合文本和视觉数据的架构设计，例如MLP和变压器适配器。同时，我们集中比较了各种图像编码器，如CLIP-ViT和SigLIP，以及视觉编码器融合技术。评估的方法为我们提供了广泛的范围，以改进更好地提取和检索视觉信息的视觉背景。

OmniFusion的一个关键创新是其对图像编码的灵活方法，探索了整个图像和瓦片图像编码策略，这使得对视觉内容与文本数据的更加细致的理解成为可能。这种适应性对于解决视觉-语言基准的多样化需求至关重要，其中OmniFusion在一系列VQA任务中展示出卓越的性能，超过了现有的开源解决方案。

我们在包括VizWiz、POPE、MM-Vet、ScienceQA、MMBench、TextVQA、VQAv2和MMMU等八个视觉-语言基准上进行了广泛评估，确认了OmniFusion模型的有效性。该模型不仅在各种VQA任务中取得了与领先方法竞争性的性能，而且在提供跨多个领域的详细答案方面表现出色，如家政、观光、文化和医学，展示了其多功能性和广泛适用性。

## 2 OmniFusion 

### 2.1 模型架构 

OmniFusion模型将预训练的LLM与用于图像嵌入的特殊适配器相结合，促进了文本和视觉模态的融合。与端到端训练流程相比，这种方法已经在各种VLM开发中证明了其适用性，因为适配器方法对计算资源的要求不高。此外，它不需要大量交错的文本-图像数据集进行训练，而对于端到端方法，这种数据是极其必要的。总体而言，我们在本报告中进一步调查并涵盖了两个主要点：适配器技术选择和编码视觉数据策略确定。

我们使用可训练嵌入的特殊标记来表示来自非文本模态的令牌序列的开头和结尾。这种方法增强了模型区分文本和视觉数据流的能力。视觉编码器的输出嵌入通过一个变压器适配器处理，该适配器由一个具有四个头的单个变压器层组成，或通过一个两层MLP。

我们的实验显示，采用合并策略最有效，该策略结合了两个编码器的特征：CLIP-ViT-L和DINO-v2。合并适配器本质上是一个两层MLP，其中第一层对每个视觉令牌独立，输出层在所有令牌上共享。合并策略意味着在第一层之后对输出进行求和，优化文本和视觉信息的整合。

2.2 训练流程 当整体架构设置固定后，我们继续进行整体训练，包括确定阶段和使用的数据集。 OmniFusion模型经历了一个设计高效利用其多模态能力的两阶段训练过程。

第1阶段：预训练。在第一阶段中，适配器和特殊标记在大量的图像-文本对数据集上进行预训练。此阶段旨在微调适配器以转换视觉嵌入，并训练特殊标记以标记文本和图像数据之间的转换。

在此阶段，我们利用了专门用于图像字幕用途的图像-文本对。我们使用的图像字幕数据来自于ShareGPT4V-PT，该数据集受到了现有字幕数据集（如COCO和TextCaps）的支持。此外，我们还将通过光学字符识别（OCR）获取的文档描述包含在预训练集中。

在预训练期间，模型接收的指令包括各种提示的变体，例如“对图像进行简要描述”，“详细描述图像”和“提供图像的简要描述”。

值得注意的是，我们目前在预训练中不使用交错数据，这可能会增强多模态模型的少样本学习能力。然而，我们计划在未来的工作中探索这个途径。

我们使用以下来源的图像-文本字幕：ShareGPT4V-PT（695K对）、具有BLIP字幕的LAION-CC-SBU（558K对）。总体而言，我们在适配器对齐的预训练阶段利用了120万个图像字幕。

第2阶段：微调。第二阶段涉及使用一系列指导性对话对多模态模型进行微调。此过程旨在增强模型理解和回答需要对文本和视觉信息进行综合分析的复杂查询的能力。此外，它被结构化为根据自然指令便利地进行多任务微调。

最近的研究强调了用于视觉指导调整的数据的合成性质以及任务多样性缺乏可能导致多模态模型输出中的重大幻觉的潜在缺陷。为了缓解这些挑战，我们通过将学术基准与相应的指令和合成对话数据相结合来构建监督微调数据集。这种方法确保了模型更多样化和更健壮的训练环境。表1提供了微调过程中使用的数据集。