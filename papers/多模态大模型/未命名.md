---
Publish: CVPR2024
---
- 单类 VAD 方法：仅对正常视频进行训练

**利用大型语言模型进行无需训练的视频异常检测**

#视频异常检测 #CVPR2024

Luca Zanella1 Willi Menapace1 Massimiliano Mancini1 Yiming Wang2 Elisa Ricci1,2

特伦托大学1 布鲁诺·凯斯勒基金会2

[https://lucazanella.github.io/lavad/](https://lucazanella.github.io/lavad/)

**摘要**

视频异常检测 (VAD) 旨在定位视频中的异常事件。现有工作主要依赖于训练深度模型来学习正常情况的分布，方法包括视频级监督、单类监督或无监督设置。基于训练的方法往往是领域特定的，因此在实际部署中成本很高，因为任何领域的变化都将涉及数据收集和模型训练。在本文中，我们从之前的研究中彻底脱颖而出，提出了一种基于语言的 VAD 方法 (LAVAD)，该方法利用预训练的大型语言模型 (LLM) 和现有的视觉语言模型 (VLM) 的能力，以一种新颖的、无需训练的范例来解决 VAD 问题。我们利用基于 VLM 的字幕模型为任何测试视频的每一帧生成文本描述。然后，利用文本场景描述，我们设计了一种提示机制，以释放 LLM 在时间聚合和异常分数估计方面的能力，将 LLM 变成一个有效的视频异常检测器。我们进一步利用模态对齐的 VLM，并提出基于跨模态相似性的有效技术，用于清理噪声字幕和细化基于 LLM 的异常分数。我们在两个具有现实世界监控场景（UCF-Crime 和 XD-Violence）的大型数据集上评估了 LAVAD，结果表明它优于无监督和单类方法，无需任何训练或数据收集。

**1. 引言**

视频异常检测 (VAD) 旨在定位给定视频中与正常模式明显不同的事件，即异常事件。VAD 具有挑战性，因为异常事件通常是未定义的和上下文相关的，并且它们在现实世界中很少发生。文献 [10] 通常将 VAD 视为一个分布外检测问题，并使用具有不同监督级别（参见图 1）的训练数据来学习正常分布，包括完全监督（即正常和异常视频的帧级监督）[1，32]、弱监督（即正常和异常视频的视频级监督）[11，13，15，24，28，35]、单类监督（即仅正常视频）[18，20，21，25，37，38] 和无监督（即未标记视频）[30，31，40]。虽然更多的监督会导致更好的结果，但人工标注的成本过高。另一方面，无监督方法假设异常视频构成训练数据的特定部分，这在实践中如果没有人工干预，则是一个脆弱的假设。至关重要的是，每种现有方法都需要一个训练过程来建立一个准确的 VAD 系统，这带来了一些限制。一个主要问题是泛化性：在特定数据集上训练的 VAD 模型在不同设置（例如，白天与夜间场景）录制的视频中往往表现不佳。另一个方面，尤其是与 VAD 相关的方面，是数据收集的挑战，特别是在某些应用领域（例如视频监控），隐私问题可能会阻碍数据获取。这些考虑因素促使我们探索一个新的研究问题：我们能否开发一种无需训练的 VAD 方法？

在本文中，我们旨在回答这个具有挑战性的问题。由于缺乏对目标设置的明确视觉先验知识，开发无需训练的 VAD 模型非常困难。然而，这种先验知识可以使用大型基础模型来绘制，这些模型以其泛化能力和广泛的知识封装而闻名。因此，我们研究了将现有的视觉语言模型 (VLM) 与大型语言模型 (LLM) 结合起来解决无需训练的 VAD 问题的潜力。在我们初步发现的基础上，我们提出了第一个无需训练的基于语言的 VAD 方法 (LAVAD)，该方法联合利用预训练的 VLM 和 LLM 进行 VAD。LAVAD 首先利用现成的字幕模型为每个视频帧生成文本描述。我们通过引入基于字幕和视频中帧之间的跨模态相似性的清理过程来解决字幕中潜在的噪声。为了捕捉场景的动态，我们使用 LLM 汇总时间窗口内的字幕。该摘要用于提示 LLM 为每一帧提供异常分数，通过聚合语义相似的时间摘要之间的帧的异常分数，可以进一步细化异常分数。我们在两个基准数据集上评估了 LAVAD：UCF-Crime [24] 和 XD-Violence [36]，并通过实证表明，我们的无需训练的提案在两个数据集上都优于无监督和单类 VAD 方法，证明了在没有训练和没有数据收集的情况下解决 VAD 问题是可能的。

**贡献。** 总结起来，我们的贡献是：

- 我们首次研究了无需训练的 VAD 问题，主张其在无法进行数据收集的实际设置中部署 VAD 系统的重要性。
    
- 我们提出了 LAVAD，这是第一个基于语言的无需训练的 VAD 方法，它使用 LLM 仅根据场景描述来检测异常。
    
- 我们引入了基于跨模态相似性的新技术，并结合预训练的 VLM 来减少噪声字幕并细化基于 LLM 的异常评分，从而有效地提高 VAD 性能。
    
- 实验表明，在不使用特定任务监督和不进行训练的情况下，LAVAD 相对于无监督和单类 VAD 方法取得了具有竞争力的结果，为未来的 VAD 研究开辟了新的视角。
    

**2. 相关工作**

**视频异常检测。** 基于训练的 VAD 方法的现有文献可以根据监督级别分为四类：
- 监督、
- 弱监督、
- 单类分类和
- 无监督。

监督 VAD 依赖于帧级标签来区分正常帧和异常帧 [1，32]。然而，由于其高昂的标注成本，这种情况很少受到关注。弱监督 VAD 方法可以访问视频级标签（如果至少有一帧异常，则整个视频被标记为异常，否则被视为正常）[11，13，15，24，28，35]。大多数这些方法利用 3D 卷积神经网络进行特征学习，并采用多实例学习 (MIL) 损失进行训练。

单类 VAD 方法仅对正常视频进行训练，尽管需要人工验证以确保收集数据的正常性。已经提出了几种方法 [18，20，21，25，37，38]，例如考虑生成模型 [37] 或伪监督方法，其中伪异常实例是从正常训练数据中合成的 [38]。

最后，无监督 VAD 方法不依赖于预定义的标签，利用正常和异常视频，并假设大多数视频包含正常事件 [26，27，30，31，40]。这一类中的大多数方法利用生成模型来捕获视频中的正常数据模式。特别是，生成式协作学习 (GCL) [40] 采用交替训练：自动编码器重建输入特征，而来自重建误差的伪标签指导判别器。Tur 等人 [30，31] 使用扩散模型从噪声特征中重建原始数据分布，并根据去噪样本和原始样本之间的重建误差计算异常分数。其他方法 [26，27] 从使用 OneClassSVM 和 iForest [16] 生成的一组伪标签中训练回归器网络。相反，我们通过利用现有的 大型基础模型来设计一个无需训练的 VAD 管道，完全避免了收集数据和训练模型的需要。

**LLM 用于 VAD。** 近年来，人们已经探索了 LLM 在跨不同应用领域检测视觉异常方面的应用。Kim 等人 [12] 提出了一种主要利用 VLM 检测异常的无监督方法，其中 ChatGPT 仅用于生成表征正常和异常元素的文本描述。然而，该方法需要人工参与来根据特定的应用上下文细化 LLM 的输出，并且需要进一步训练以适应 VLM。其他例子包括利用 LLM 进行图像中的空间异常检测，以解决机器人 [4] 或工业 [7] 中的特定应用。与之不同的是，我们将 LLM 与 VLM 结合起来，以解决视频中的时间异常检测问题，并提出了第一个无需训练的 VAD 方法，该方法不需要训练和数据收集。

**3. 无需训练的 VAD**

在本节中，我们首先形式化 VAD 问题和提出的无需训练的设置（第 3.1 节）。然后，我们分析了 LLM 在对视频帧中的异常进行评分方面的能力（第 3.2 节）。最后，我们描述了 LAVAD，即我们提出的 VAD 方法（第 3.3 节）。

**3.1. 问题公式**

给定一个包含 M 帧的测试视频 V = [I1, ..., IM]，传统的 VAD 方法旨在学习一个模型 f，该模型可以将每一帧 I ∈ V 分类为正常（分数 0）或异常（分数 1），即 f: IM → [0, 1]M，其中 I 是图像空间。f 通常在一个由 (V, y) 形式的元组组成的数据集 D 上训练。根据监督级别，y 可以是带有帧级标签的二进制向量（完全监督）、二进制视频级标签（弱监督）、默认值为 1（单类）或不存在（无监督）。然而，在实践中，由于异常很少见，收集 y 以及 V 本身可能会很昂贵，因为存在潜在的隐私问题。此外，由于应用上下文的不断变化，标签和视频数据可能需要定期更新。与之不同的是，在本文中，我们介绍了一种新的 VAD 设置，称为无需训练的 VAD。在此设置下，我们旨在仅使用预训练模型在推理时估计每个 I ∈ V 的异常分数，即无需任何涉及训练数据集 D 的训练或微调。

**3.2. LLM 是否适合 VAD？**

我们建议利用 LLM 的最新进展来解决无需训练的 VAD 问题。由于 LLM 在 VAD 中的使用仍处于起步阶段 [12]，我们首先分析 LLM 根据视频帧的文本描述生成异常分数的能力。为了实现这一点，我们首先利用最先进的字幕模型 ΦC，即 BLIP-2 [14]，为每一帧 I ∈ V 生成文本描述。然后，我们将异常分数估计视为一个分类任务，要求 LLM ΦLLM 从区间 [0, 1] 中 11 个均匀采样的值列表中仅选择一个分数，其中 0 表示正常，1 表示异常。我们得到异常分数为：

$\netllm (\prompt _C \circ \prompt _F \circ \netcaptioner (\image ))$ (1)

其中 PC 是一个上下文提示，它为 LLM 提供关于 VAD 的先验知识，PF 指示 LLM 所需的输出格式，以促进自动文本解析1，◦ 是文本连接操作。我们将 PC 设计为模拟 VAD 系统的潜在最终用户，例如执法机构，因为我们根据经验观察到，模拟可以有效地指导 LLM 的输出生成。例如，我们可以将 PC 形成如下：“如果您是一个执法机构，您将如何对所描述的场景进行评分，评分范围为 0 到 1，其中 0 表示标准场景，1 表示存在可疑活动的场景？”。请注意，PC 并不对异常类型本身进行编码，而只是对上下文进行编码。最后，利用公式 (1) 中估计的异常分数，我们使用接收器操作特征曲线 (AUC ROC) 下的标准面积来衡量 VAD 性能。

图 2 报告了在 UCF-Crime 数据集 [24] 的测试集上获得的结果，其中使用了 BLIP-2 的不同变体来获取帧字幕，并使用了不同的 LLM（包括 Llama [29] 和 Mistral [9]）来计算帧级异常分数。作为参考，我们还提供了无监督设置下最先进的性能（最接近我们的设置）[27]，以及随机评分作为下限。该图表明，最先进的 LLM 具有异常检测能力，在很大程度上优于随机评分。然而，即使在无监督设置中，这种性能也远低于训练过的最先进方法。我们观察到，有两个方面可能是 LLM 性能的限制因素。首先，帧级字幕可能非常嘈杂：字幕可能不完整或不能完全反映视觉内容（参见图 3）。尽管使用了 BLIP-2 [14]（最先进的现成字幕模型），但有些字幕仍然出现损坏，从而导致不可靠的异常分数。其次，帧级字幕缺乏关于全局上下文和场景动态的详细信息，而这些是建模视频时的关键要素。接下来，我们将解决这两个限制，并提出 LAVAD，这是第一个无需训练的 VAD 方法，该方法利用 LLM 进行异常评分，并结合模态对齐的 VLM。

**3.3. LAVAD：基于语言的 VAD**

LAVAD 将 VAD 函数 f 分解为五个元素（参见图 4）。与初步研究中一样，前两个元素是将图像映射到语言空间 T 中的文本描述的字幕模块 ΦC，即 ΦC: I → T，以及从语言查询生成文本的 LLM ΦLLM，即 ΦLLM: T → T。其他元素涉及将输入表示映射到共享潜空间 Z 的三个编码器。具体来说，我们有图像编码器 EI: I → Z，文本编码器 ET: T → Z，以及视频编码器 EV: V → Z。请注意，所有五个元素都只涉及现成的冻结模型。

根据初步分析的积极发现，LAVAD 利用 ΦLLM 和 ΦC 来估计每一帧的异常分数。我们设计 LAVAD 通过引入三个组件来解决与帧级字幕中的噪声和缺乏场景动态相关的限制：i) 通过 EI 和 ET 的视觉语言表示进行图像-文本字幕清理，ii) 基于 LLM 的异常评分，通过 ΦLLM 编码时间信息，以及 iii) 通过视频-文本相似性（使用 EV 和 ET）进行视频-文本分数细化。接下来，我们将详细描述每个组件。

**图像-文本字幕清理。** 对于每个测试视频 V，我们首先使用 ΦC 为每一帧 Ii ∈ V 生成一个字幕 Ci。具体来说，我们将字幕序列表示为 C = [C1, ..., CM]，其中 Ci = ΦC(Ii)。然而，如第 3.2 节所示，原始字幕可能很嘈杂，存在不完整的句子或不正确的描述。为了缓解这个问题，我们依赖于整个视频的字幕 C，假设在这个集合中存在不完整且更好地捕获其各自帧内容的字幕，这个假设在实践中经常得到验证，因为视频的特点是由静态相机以高帧率拍摄的场景。因此，帧之间的语义内容可以重叠，无论它们的时间距离如何。从这个角度来看，我们将字幕清理视为在 C 中找到语义上最接近目标帧 Ii 的字幕。形式上，我们利用视觉语言编码器，并通过 ET 对 C 中的每个字幕进行编码，形成一组字幕嵌入，即 {ET(C1), ..., ET(CM)}。对于每一帧 Ii ∈ V，我们计算其语义上最接近的字幕为：

$\retrievedcaption _i = \arg \max _{\rawcaption \in \rawcaptionset } \langle \netvlmimage (\image _i) \cdot \netvlmtext (\rawcaption )\rangle ,$ (2)

其中 ⟨·, ·⟩ 是余弦相似度，EI 是 VLM 的图像编码器。然后，我们构建清理后的字幕集为 Cˆ = [Cˆ 1, ..., CˆM]，将每个初始字幕 Ci 替换为从 C 中检索到的对应字幕 Cˆ i。通过执行字幕清理过程，我们可以传播语义上更符合视觉内容的帧的字幕，无论其时间位置如何，以改进或纠正噪声描述。

**基于 LLM 的异常评分。** 获得的字幕序列 Cˆ 虽然比初始集更干净，但缺乏时间信息。为了克服这个问题，我们利用 LLM 提供时间摘要。具体来说，我们在以 Ii 为中心的 T 秒的时间窗口内均匀采样 N 帧，形成一个视频片段 Vi 和一个字幕子序列 Cˆ i = {Cˆ n} N n=1。然后，我们可以使用 Cˆ i 和提示 PS 查询 LLM，以获得以帧 Ii 为中心的时间摘要 Si：

$\summarycaption _i = \netllm (\prompt _S \circ \rawcaptionsetclean _i)$ (3)

其中提示 PS 的形式为“请根据以下场景的时间描述，用几句话总结发生了什么。不要包含任何不必要的细节或描述。”2。将公式 (3) 与公式 (2) 的细化过程相结合，我们得到了帧的文本描述 (Si)，它在语义和时间上都比 Ci 更丰富。利用 Si，我们可以查询 LLM 来估计异常分数。按照第 3.2 节中描述的相同提示策略，我们要求 ΦLLM 为每个时间摘要 Si 分配一个区间为 [0, 1] 的分数 ai。我们得到分数为：

$\scoreabn _i = \netllm (\prompt _C \circ \prompt _F \circ S_i)$ (4)

其中，与第 3.2 节中一样，PC 是一个包含 VAD 上下文先验知识的上下文提示，PF 提供关于所需输出格式的信息。

**视频-文本分数细化。** 通过使用公式 (4) 查询视频中每一帧的 LLM，我们得到了视频的初始异常分数 a = [a1, ..., aM]。然而，a 完全基于其摘要中编码的语言信息，而没有考虑到整个分数集。因此，我们通过利用视觉信息来聚合来自语义相似帧的分数，进一步细化它们。具体来说，我们使用 EV 编码以 Ii 为中心的视频片段 Vi，并使用 ET 编码所有时间摘要。让我们将 Ki 定义为 {S1, ..., SM} 中 K 个与 Vi 最接近的时间摘要的索引集，其中 Vi 和字幕 Sj 之间的相似度是余弦相似度，即 ⟨EV(Vi), ET(Sj)⟩。我们得到细化的异常分数 a˜i：

$\refinedscoreabn _i = \sum _{k \in \mathbf {K}_i}{\scoreabn _k} \cdot \frac {e^{\langle \netvlmvideo (\video _i),\netvlmtext (\summarycaption _k)\rangle }}{\sum _{k \in \mathbf {K}_i}e^{\langle \netvlmvideo (\video _i),\netvlmtext (\summarycaption _k)\rangle }}$ (5)

其中 ⟨·, ·⟩ 是余弦相似度。请注意，公式 (5) 利用了与公式 (2) 相同的原理，使用帧级估计（即分数/字幕）与视频中其他帧的视觉语言相似度（即视频/图像）来细化它们。

最后，利用测试视频 ˜a = [˜a1, ..., a˜M] 的细化异常分数，我们通过阈值化来识别异常时间窗口。

**4. 实验**

我们在两个数据集上验证了我们的无需训练的提案 LAVAD，并与使用不同监督级别训练的最先进的 VAD 方法以及无需训练的基线进行了比较。我们进行了广泛的消融研究，以证明我们关于提出的组件、提示设计和分数细化的主要设计选择是合理的。接下来，我们将首先描述我们在数据集和性能指标方面的实验设置。然后，我们在第 4.1 节中介绍和讨论结果，然后在第 4.2 节中进行消融研究。我们在补充材料中展示了更多定性结果和对次要设计的消融。

**数据集。** 我们使用两个常用的 VAD 数据集来评估我们的方法，这些数据集包含现实世界的监控场景，即 UCF-Crime [24] 和 XD-Violence [36]。UCF-Crime 是一个大型数据集，由 1900 个长的未剪辑的现实世界监控视频组成，涵盖 13 种现实世界异常。训练集由 800 个正常视频和 810 个异常视频组成，而测试集包括 150 个正常视频和 140 个异常视频。XD-Violence 是另一个用于暴力检测的大型数据集，包含 4754 个带有音频信号和弱标签的未剪辑视频，这些视频是从电影和 YouTube 收集的。XD-Violence 捕获了 6 类异常，并被分为 3954 个视频的训练集和 800 个视频的测试集。