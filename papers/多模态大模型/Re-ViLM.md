---
Publish: arXiv2023
Year: "2023"
Month: "02"
Url: https://arxiv.org/pdf/2302.04858
Title: "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning"
---
![[Pasted image 20240514141000.png]]

#VLM #RAG
## 摘要

将预训练语言模型（LM）与视觉编码器（例如，Flamingo）结合，可以在图像到文本生成任务中取得最先进的成果。然而，这些模型将所有知识存储在其参数中，因此往往需要庞大的模型参数来处理丰富的视觉概念和文本描述。此外，它们在整合新数据时效率较低，需要耗费大量计算资源进行微调。在本研究中，我们介绍了一种增强检索的视觉语言模型（Re-ViLM），它基于Flamingo构建，支持**从外部数据库中检索相关知识**，以实现零样本和上下文少样本的图像到文本生成。通过在外部数据库中显式存储某些知识，我们的方法减少了模型参数的数量，并且在评估期间只需更新数据库即可轻松整合新数据。我们还构建了一种交错的图像和文本数据，促进了上下文少样本学习能力。我们证明了Re-ViLM在图像到文本生成任务中显著提升了性能，特别是在域外的零样本和少样本生成任务中，与基线方法相比，其参数数量减少了4倍。

## 引言

图像到文本生成，也称为图像描述（例如，Karpathy 和 Fei-Fei，2015），在理解视觉信息和增强人机交互中起着至关重要的作用。该任务有广泛的实际应用，例如游戏、虚拟现实和机器人技术（Luo 等，2019；Zhao 等，2021；Liu 等，2021）。为了应对这一问题，近年来提出了许多方法并取得了很大成功（例如，Alayrac 等，2022；Wang 等，2022c；Hu 等，2022；Chen 等，2022b,a；Wang 等，2022b）。

其中，视觉语言模型（LM）基于预训练的自回归语言模型（例如，GPT-3，Brown 等，2020），并继承了其强大的文本生成能力。特别是，预训练的语言模型参数通常被冻结，并在多模态预训练期间在大型语言模型中添加一些可训练层（例如适配器）（Eichenberg 等，2022；Mokady 等，2021；Tsimpoukelli 等，2021；Alayrac 等，2022）。这种冻结语言模型策略可以避免在视觉语言模型训练时发生灾难性遗忘，因为文本质量通常低于用于预训练语言模型的纯文本语料库。此外，它还赋予了预训练语言模型强大的零样本或少样本能力（例如，Flamingo，Alayrac 等，2022）。

然而，这些方法有两个主要限制：1）它们**将所有获取的知识存储在模型参数中**，使得在建模丰富的视觉概念（例如罕见的物体）和丰富的文本描述（例如对同一场景的不同描述）时参数效率低下；2）它们在**整合新数据时效率低下**，通常需要计算量巨大的微调（Chen 等，2022a）或在越来越多的参数和交错的⟨图像，文本⟩数据上进行再训练（Alayrac 等，2022）。

在过去的几年里，增强检索的语言模型（Guu 等，2020；Lewis 等，2020；Karpukhin 等，2020；Borgeaud 等，2022）在提高准确性和减少模型参数方面取得了显著成功，通过检索大规模文本语料库。然而，在将检索技术应用于图像到文本生成的视觉语言模型之前，还有几个问题需要解决：
- i）视觉语言模型需要在多模态预训练开始时无缝检索和编码外部知识，否则强大的预训练自回归语言模型会忽略编码不佳的外部知识；
- ii）在多模态数据集中，有些情况下会有多个描述同一图像的标题（例如，不同标注者提供的描述（Lin 等，2014）），以及多个图像具有相同的标题（例如，参见附录中的图3）。因此，简单地执行标准的最近邻检索往往会使模型在训练期间通过复制粘贴检索示例来取巧；
- iii）在大规模交错的⟨图像，文本⟩数据集上训练模型促进了少样本学习能力（例如，Alayrac 等（2022）中的M3W），但收集这种数据集非常昂贵。

在这项工作中，我们提出了一种增强检索的视觉语言模型（Re-ViLM），它通过无缝集成多模态检索器和检索增强的语言模型层（交叉关注文本编码器，参见图1中的示例和图2中的模型框架），提升了最先进的视觉语言模型Flamingo在零样本和上下文少样本图像描述中的性能。具体来说，我们做出了以下贡献：

1. 与以往的工作不同，我们用预训练的检索增强语言模型（RETRO，Borgeaud 等，2022）初始化Re-ViLM，因此它可以在多模态预训练开始时无缝集成检索能力，从而提高性能。
2. 我们研究了构建多模态检索器的检索策略。在多模态预训练中，我们发现基于图像CLIP（Radford 等，2021）嵌入的余弦相似度检索k最近邻标题可以获得最佳性能，同时通过过滤掉与训练实例标题相同的检索候选项来避免训练中的“复制粘贴”行为。
3. 我们使用现有的公共数据集构建了由交错的⟨图像，文本⟩对组成的预训练和评估数据集。这促进了在给定少样本示例作为交错的⟨图像，文本⟩对的情况下的上下文学习。
4. 我们在包括MSCOCO（Lin 等，2014）、Flickr30k（Plummer 等，2015）和NoCaps（Agrawal 等，2019）在内的各种基准上进行了广泛的零样本、少样本和微调图像到文本生成实验。Re-ViLM在所有设置下都一致优于基线Flamingo模型。尤其在零样本和少样本设置中，改进尤为显著，例如我们的Re-ViLM在零样本评估中可以超过包含4倍参数的Flamingo模型。

本文其余部分的组织如下：在第2节，我们讨论相关工作。在第3节介绍Re-ViLM模型及第4节介绍多模态预训练和检索的数据集。在第5节展示实验结果并在第6节总结本文。

## 2 相关工作

### 视觉语言模型

许多近期的研究致力于为给定图像生成文本描述（例如，Wang 等，2022c；Alayrac 等，2022；Aghajanyan 等，2022；Wang 等，2022b；Hu 等，2022；Li 等，2022；Chen 等，2022b；Yu 等，2022）。在这些研究中，视觉语言模型（Tsimpoukelli 等，2021；Alayrac 等，2022）通过添加视觉组件直接增强预训练的语言模型，在零样本和少样本生成中取得了强劲的结果。

### 增强检索模型

检索技术已成功应用于各种自然语言处理任务，包括问答（Guu 等，2020；Karpukhin 等，2020）、自回归语言建模（Borgeaud 等，2022）及其他知识密集型任务（例如，Lewis 等，2020）。在计算机视觉领域，检索也被用于处理具有长尾分布类别的图像识别任务（Long 等，2022）。在本研究中，我们将检索应用于图像描述生成。

RA-CM3（Yasunaga 等，2022）通过检索增强了CM3骨干模型（Aghajanyan 等，2022），能够执行图像到文本生成和文本到图像合成。与我们的Re-ViLM相比，有以下不同点：

1. 我们研究了增强检索模型在训练过程中的“复制粘贴”行为，并提出了一种简单的检索过滤策略。而RA-CM3提出了一种在检索中丢弃查询标题部分token的查询弃权策略。在我们的消融研究中，我们发现我们的简单策略比丢弃正则化效果更好，如第5.5.1节所示。
2. 我们的Re-ViLM使用增强检索的语言模型解码层，其中包含交叉注意模块以关注检索到的相似标题。相比之下，RA-CM3将检索到的标题附加为解码器侧的前缀上下文。在我们的消融研究中，我们发现我们的方法更有效，如第5.5.2节所示。
3. 对于图像到文本生成，RA-CM3仅提供了在MSCOCO（Lin 等，2014）上进行的2-shot上下文图像描述的结果。相比之下，我们在各种基准下对Re-ViLM进行了广泛的评估，包括零样本、少样本和微调设置。

## 3 Re-ViLM 架构

在本节中，我们首先概述 Re-ViLM 的框架。之后，我们将深入探讨每个组件的详细信息。

### 3.1 框架

图 2 展示了我们用于图像字幕生成的 Re-ViLM 框架。它由三个基本组件组成：

- **图像编码器**：从预训练的 CLIP（Radford 等，2021）视觉变压器开始，从输入图像中提取视觉特征。这些特征随后被传送到可训练的 Perceiver Resampler（Jaegle 等，2021），以将图像特征统一为检索增强语言模型中使用的文本表示。
    
- **检索增强的语言模型**：用预训练的 RETRO 模型（Borgeaud 等，2022）初始化，根据图像特征和检索到的证据生成相应的字幕。在某些语言模型层中，该模块交叉关注来自图像编码器或共享文本编码器的隐藏表示。双向文本编码器对从多模态检索器获得的检索字幕进行编码。
    
- **多模态检索器**：包含一个使用 Faiss（Johnson 等，2019）进行索引的存储 ⟨图像，文本⟩ 对的检索数据库。这些对是使用它们的 CLIP 嵌入进行索引的。给定查询图像，检索器使用图像编码器中的 CLIP-ViT 模块提取其嵌入，然后返回按查询图像与检索图像之间嵌入的余弦相似度度量的前 k 个 ⟨图像，文本⟩ 对。随后，检索到的字幕由语言模型编码以生成相关的字幕。
    

在以下小节中，我们将提供关于模型每个组件的更多详细信息。

### 3.2 图像编码器

为了充分利用现有的预训练模型，我们使用预训练的 CLIP-ViT（Radford 等，2021）初始化图像编码器，这是一种通过将图像划分为网格块并用变压器编码器处理每个块来处理图像的视觉变压器。在我们的实验中，我们使用了两种不同尺寸的 CLIP-ViT 模型：ViT-B/32 和 ViT-L/14。在多模态预训练期间，我们冻结 CLIP-ViT 部分以避免灾难性遗忘，而在微调时使其可训练以获得更好的结果。然后，我们使用 Perceiver Resampler 从 CLIP-ViT 令牌嵌入中获取固定长度的隐藏表示。Perceiver 在多模态预训练和未来微调期间都是可训练的，允许它为文本解码器调整视觉表示并连接两种模态。

### 3.3 多模态检索器

**构建数据库**：检索数据库建立在图像-文本配对数据集之上，并被结构化为一个键值映射，其中键是 CLIP ViT-B/32 图像嵌入，值是相应的文本描述。数据库由 Faiss 库（Johnson 等，2019）索引。

**检索**：给定输入查询图像 I，我们使用查询图像与数据库图像之间嵌入的余弦相似度进行 k 近邻检索。检索结果表示为 R(I) = {(i1, c1), ···, (ik, ck)}，其中 ij 和 cj（j ∈ [1, k]）分别代表检索到的图像和字幕。

**过滤策略**：在某些数据集中（例如 MSCOCO（Lin 等，2014）），相同图像可能有来自不同注释者的多个字幕。在这种情况下，这些检索到的字幕与真实字幕高度相关（甚至接近重复），可能会给模型带来一种错误的检索证据质量的感觉，导致由于训练和评估设置之间的差异而可能降低测试性能。为避免这种情况，如果检索到的图像与查询图像 I 相同（例如 i1 = I），我们在训练和推理期间过滤掉检索到的图像-文本对。

此外，在图像-文本数据集中（例如 Conceptual Captions（Sharma 等，2018；Changpinyo 等，2021）），多个图像可能有相同的字幕。例如，一个注释者可能为相似的图像提供相同的字幕或替代文本。如果在训练期间检索到的字幕与真实字幕相同，它将鼓励模型走捷径，简单地将检索到的字幕复制粘贴到模型输出中，阻碍 Re-ViLM 的训练。为了解决这个问题，我们采用过滤策略，如果其文本与用作语言模型解码器教师强制输入的训练图像的字幕相同，则过滤掉检索到的图像-文本对。因此，如果 I 的相应真实字幕为 C，则过滤后的检索结果 R(I) = {(i1, c1), ···, (ik, ck) | cj ≠ C，ij ≠ I，∀j ∈ [1, k]）。在 § 5.5.1 中，我们显示了 Re-ViLM 可以通过这种简单的过滤策略得到显著改进。

### 3.4 检索增强的语言模型

为了便于模型在多模态预训练开始时无缝检索和编码外部知识，我们用预训练的 RETRO 模型（Borgeaud 等，2022）初始化文本编码器和语言模型解码器层，这是一种最先进的检索增强语言模型。在我们的实验中，我们使用了三种不同大小的 RETRO 模型：RETRObase（148M 参数）、RETROmedium（405M 参数）和 RETROlarge（1.5B 参数）。为了生成基于视觉输入的字幕，我们将语言模型解码器层与门控交叉注意稠密层（gated xttn-dense）交替，这些层将 Perceiver Resampler 的输出作为交叉注意的键和值。为了将检索到的字幕作为证据，我们将语言模型解码器层与检索增强层交替，这些层将文本编码器输出作为交叉注意的键和值。注意，文本编码器在检索增强层之间是共享的。我们在多模态预训练时冻结检索增强的语言模型，并在微调时使其可训练。

**文本编码器**：给定检索到的 k 近邻字幕，我们使用基于变压器的双向编码器模型来获取隐藏表示。具体来说，我们的文本编码器与语言模型解码器共享子词嵌入表。我们沿长度维度连接 k 个嵌入以形成检索原始嵌入张量 E ∈ R (k×m)×d，其中 m 是序列长度，d 是隐藏维度。在应用变压器层后，编码器输出被语言模型解码器层交叉关注。我们用预训练的 RETRO 模型（Borgeaud 等，2022）初始化文本编码器，而不是从头开始训练，以提高模型性能。

### 3.5 在预训练和微调时可训练的模块

与 Flamingo 类似，我们的训练目标是最大化给定图像的字幕的条件似然。在预训练时，我们按照 Flamingo 的策略冻结预训练的组件，包括 CLIP-ViT 和检索增强语言模型，仅从头开始训练 Perceiver Resampler。在微调时，我们解冻所有预训练的组件，并将图像分辨率从 224×224 提高到 480×480，如（Alayrac 等，2022）所建议的。这被证明可以提高整体性能。

## 4. 多模态预训练和检索数据

### 4.1 图像-文本对数据

在本研究中，我们使用两个多模态数据集对模型进行预训练：

1. CC3M + CC12M + SBU，这些数据集包括来自 Conceptual 数据集（Sharma 等，2018；Changpinyo 等，2021）和 SBU Captions（Ordonez 等，2011）的总计 1500 万个高质量图像-文本对；
2. COYO-700M（Byeon 等，2022），该数据集在从 100 亿个网页图像-文本资源中过滤掉低质量样本后，包含 7.47 亿个图像-文本对。

在实验中，我们发现高质量的字幕对于图像字幕生成任务的预训练至关重要。因此，我们进一步过滤掉文本中包含不规则标记或图像与文本之间 CLIP 相似度得分低的实例，最终获得了 1.04 亿个高质量图像-文本对。为简化起见，我们将 CC3M + CC12M + SBU 数据集称为 CCS，将 COYO-104M 数据集称为 COYO。

在检索时，我们使用 CCS 和 COYO 作为主要的检索数据库来源，并利用 Faiss 库（Johnson 等，2019）来支持快速的基于相似度的检索。存储 Faiss 索引文件需要 241GB，每次查询在我们的数据库上进行检索大约需要 50 毫秒。

### 4.2 交错的图像-文本数据

在本小节中，我们讨论我们提出的用于增强模型上下文少样本能力的预训练方法。在这种情况下，模型需要高度依赖之前的少样本（图像-文本对）来有效生成测试图像的字幕。然而，现有的多模态模型通常在预训练中不使用多个图像-文本对作为输入（Tsimpoukelli 等，2021；Yasunaga 等，2022）。这使得在推理时的上下文少样本学习具有挑战性，因为在预训练期间没有这样的监督。尽管 Alayrac 等（2022）构建了一个内部的大规模多模态语料库，其中包含交错的图像和文本，但收集这样的数据集成本高昂。

我们使用公开可用的图像-文本对数据集构建我们的图像-文本交错数据集。我们使每个交错样本中的图像-文本对相关，以显式地教模型如何依赖之前的数据样本生成当前图像的字幕。我们的交错数据集是使用 CCS 构建的。对于 CCS 中的每个图像-文本对（查询），我们从同一语料库中选择四个相关的数据对来构建每个交错样本，从而每个交错样本包含五个数据对。每个查询的数据选择过程包括两个步骤：

**步骤 1**：我们使用 L2 度量来衡量查询图像与 CCS 中其余图像之间的 CLIP 嵌入距离，选择图像与查询图像的归一化距离得分在 0.4 到 0.6 之间的数据对。

**步骤 2**：为了确保交错样本中的字幕相似，我们进一步使用 CLIP 嵌入来计算查询字幕与所选数据对的字幕之间的距离。我们选择字幕与查询字幕最相似的前四个数据对。

## 5 实验

在本节中，我们在不同的图像描述基准上评估 Re-ViLM 在零样本、少样本和微调三种不同设置下的性能。我们的目标是通过从外部数据库检索相关知识，证明我们的检索增强技术在提高生成描述的质量和相关性方面的优越性。我们将结果与几种广泛使用的图像描述模型进行比较，如 EncDec（Changpinyo 等，2021）、SimVLM（Wang 等，2022c）和 Flamingo（Alayrac 等，2022）。通过对 Re-ViLM 的广泛评估，我们得出结论，Re-ViLM 在零样本和少样本设置下表现出色。

### 5.1 实验设置

**评估数据集**  
我们在三个多模态数据集上进行图像描述评估：

1. **MSCOCO**（Lin 等，2014）：用于图像描述、目标检测和分割。我们使用 Karpathy 分割（Karpathy 和 Fei-Fei，2015），其中包括 82k/5k/5k 张图像分别用于训练、验证和测试。每张图像最多有 5 个人工生成的描述。
2. **Flickr30k**（Plummer 等，2015）：这是一个基于句子的图像描述标准基准，Karpathy 分割中包括 29k/1k/1k 张图像。
3. **NoCaps** 包含 15k 张图像，包含近 400 个 MSCOCO 原有类别之外的新类，可用于微调后的新对象描述性能评估。

对于零样本设置，我们重点评估 MSCOCO 和 Flickr30k 数据集。在微调设置下，我们在 MSCOCO、Flickr30k 和 NoCaps 数据集上进行评估。我们仅在 MSCOCO 数据集上进行少样本实验，以评估 Re-ViLM 的泛化和适应性。在整个实验中，我们报告 BLEU@4、CIDEr 和 SPICE 分数（Lin 等，2014）来衡量生成描述在给定输入图像情况下的质量和相关性。

**实现**  
我们基于不同规模的 CLIP-ViT 和 RETRO 开发了不同规模的 Re-ViLM，分别命名为 Re-ViLMbase（ViT-B/32，RETRO-148M）、Re-ViLMmedium（ViT-L/14，RETRO-410M）和 Re-ViLMlarge（ViT-B/32，RETRO-1.5B）。与具有相同 CLIP-ViT 和可比 GPT-3 配置（Brown 等，2020）的 Flamingo 模型相比，Re-ViLM 引入了最多 16% 的额外参数，同时大幅提升了性能。我们使用 Megatron-LM 基础设施构建我们的模型，以支持大型视觉语言模型的训练和评估。全局批次大小设置为 256，训练时使用 Adam 优化器。推理时使用束搜索，束宽为 3，最大生成长度为 10。在实验中，我们设置检索的描述数量 k = 2。我们还评估了 k = 5 和 10 时 Re-ViLM 的性能，结果表明提升不明显。详情见附录 B。

### 5.2 零样本评估

我们在 MSCOCO 和 Flickr30k 数据集上进行零样本评估。预训练期间，我们将 CCS 和 COYO 作为我们的检索数据库，并报告所有不同设置中的最佳结果。结果如表 1 所示。我们发现，通过引入最多 16% 的额外参数，Re-ViLM 与 Flamingo 模型相比，在 CIDEr 分数上显著提升（约 10.0）。即使是 Re-ViLMbase 也远远超过最大的 SimVLM。完整结果，包括 Re-ViLM 在不同预训练和检索数据库组合下的表现，见附录 A。

### 5.3 少样本评估

我们通过在构建的交错 CCS 数据集上预训练 Re-ViLM，并在交错 MSCOCO 数据集上进行评估，来评估 Re-ViLM 的少样本学习能力。交错 MSCOCO 数据集的构建过程如 § 4.2 所述，包含 {2, 4, 8} 个样本。结果如表 2 所示。与相似规模的 Flamingo 模型相比，在 {2, 4} 样本设置下的显著改进清晰可见，但我们注意到当样本数量增加（即 8 样本）时，检索增强的收益减少。这并不意外，因为来自 MSCOCO 的少样本域内示例比我们的检索数据库 CCS 和 COYO 的域外示例包含更多有用信息，以提高模型在 MSCOCO 上的性能。随着域内示例数量的增加，从域外示例中检索的收益变得边际。

### 5.4 微调评估

我们在 MSCOCO、Flickr30K 和 NoCaps 基准上对 Re-ViLM 进行了微调评估。为了评估 MSCOCO 和 Flickr30k，我们分别在 MSCOCO 和 Flickr30k 数据集上使用较小的学习率和提前停止策略对预训练的 Re-ViLM 进行微调。对于 NoCaps 评估，我们在 MSCOCO 数据集上对模型进行微调，参考之前的研究（Li 等，2022）。结果如表 3 所示。我们观察到，虽然 Re-ViLM 仍然稳定地优于 Flamingo，但相对于零样本和少样本设置，改进的相对幅度变小了。包含 Re-ViLM 在不同预训练和检索数据库组合下表现的完整结果见附录 A。

### 5.5 消融研究

#### 5.5.1 检索过程中的过滤

在多模态数据集中，可能存在两种不同类型的重复场景：同一图像有多个描述（这在 MSCOCO、Flickr30k 和 NoCaps 数据集中很常见），可能会导致训练期间的标签泄漏；多张图像具有相同的描述（这在多模态数据集中很常见，如 Conceptual Captions，见附录 C 中的图 3）。这两种重复情况都会导致一个严重的问题，即 Re-ViLM 可以简单地复制粘贴检索到的描述来实现与真实描述的 100% 匹配。更深入的讨论见附录 C。

为了缓解上述问题，我们开发了一种简单的过滤策略，丢弃在训练时与查询图像 I 或其描述 C 匹配的检索样本（即 R(I) = {(i1, c1), ··· ,(ik, ck) | cj ≠ C, ij ≠ I, ∀j ∈ [1, k]}）。我们注意到另一个同时进行的工作 RA-CM3 也提出了查询丢弃策略，通过根据与查询图像和文本的相似性随机丢弃检索到的描述词来减轻这种重复问题。我们进行消融研究，将我们的简单过滤方法与查询丢弃方法进行比较。结果如表 4 所示，表明我们的简单过滤策略在提高 Re-ViLM 性能方面取得了持续改进，而查询丢弃策略虽然竞争力强，但结果略逊于简单过滤策略。

#### 5.5.2 检索增强作为上下文前置

我们的 Re-ViLM 通过双向文本编码器和语言模型解码器层之间的交叉注意力来整合检索到的描述。而同时进行的一项研究 RA-CM3 提出了另一种检索增强方法，即将检索到的描述作为前缀上下文添加到解码器端，以一种更简单的方法利用检索到的证据，而无需引入额外的参数。我们研究了这种类似提示的增强方法，并通过在 Flamingo 模型的预训练和推理过程中将前两个检索到的证据作为前缀进行复制。我们将这种检索增强方法与我们的检索增强型语言模型层方法进行比较。结果如表 5 所示。我们可以观察到，我们的检索增强设计在零样本描述生成性能方面优于 Yasunaga 等人 (2022) 的检索增强方法。这揭示了我们基于检索的架构设计的重要性。

## 6 结论

在这项工作中，我们提出了 Re-ViLM，一个带有检索增强的图像到文本模型，具有强大的零样本和少样本图像描述生成能力。Re-ViLM 基于 Flamingo 构建，显著减少了参数数量，同时在不同设置中获得了出色的结果，因为它不需要将所有知识存储在参数中。我们还提出了一种简单但有效的检索过滤策略，以避免检索增强模型的“复制粘贴”行为。此外，我们构建了一个用于预训练的交错图像-文本数据集，这对于上下文少样本学习至关重要。对多种图像描述生成数据集的广泛实验表明，Re-ViLM 在所有设置中始终优于基线 Flamingo 模型。此外，我们在微调设置中进行了实验，并显示出令人鼓舞的结果。

## 7 限制

在本文中，我们专注于探索新兴的零样本和上下文少样本图像描述生成。为了实现这一目标，我们主要基于 Flamingo 框架（Alayrac et al., 2022）设计了我们的检索增强模型，并将我们的检索设计应用于其他图像到文本框架（Bao et al., 2021；Chen et al., 2022b；Wang et al., 2022a）作为未来工作。此外，由于 Flamingo 及其训练数据集没有官方实现，我们的框架基于我们重新实现的 Flamingo，训练在公开可用的数据集和手动制作的交错图像-文本数据集上。从扩展的角度来看，与仅文本领域的 GPT 相比，Flamingo 类模型的一个最重要的优势是扩展性。在这项研究中，我们未能将 Re-ViLM 进一步扩展到 80B，以解决在大规模视觉语言模型上从检索中获益的问题。