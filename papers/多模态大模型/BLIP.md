---
Title: "BLIP: Bootstrapping Language-Image Pre-training for\rUnified Vision-Language Understanding and Generation"
---
#VLM  #arXiv2022 

### 摘要

视觉-语言预训练（VLP）已显著提升了许多视觉-语言任务的性能。然而，大多数现有的预训练模型仅在理解型任务或生成型任务中表现出色。此外，性能提升主要通过扩大包含来自网络的噪声图文对的数据集来实现，这是一种次优的监督源。在本文中，我们提出了一种新的VLP框架——BLIP，该框架能够灵活地转移到视觉-语言理解和生成任务中。BLIP通过引导生成和过滤噪声图文对有效利用了网络数据，其中一个生成器生成合成的描述，过滤器则移除噪声描述。我们在许多视觉-语言任务上取得了最先进的结果，例如图文检索（平均召回率@1提高2.7%）、图像描述（CIDEr提高2.8%）和视觉问答（VQA得分提高1.6%）。BLIP还展示了在零样本情况下直接转移到视频-语言任务中的强大泛化能力。代码、模型和数据集已发布。

### 1. 引言

视觉-语言预训练在各种多模态下游任务上最近取得了巨大成功。然而，现有方法存在两个主要局限性：

1. **模型视角**：大多数方法要么采用编码器模型（Radford et al., 2021; Li et al., 2021a），要么采用编码器-解码器模型（Cho et al., 2021; Wang et al., 2021）。但编码器模型不易直接转移到文本生成任务（如图像描述），而编码器-解码器模型在图文检索任务中未能成功采用。
2. **数据视角**：大多数最先进的方法（如CLIP (Radford et al., 2021)、ALBEF (Li et al., 2021a)、SimVLM (Wang et al., 2021)）在网络收集的图文对上进行预训练。尽管通过扩大数据集规模获得了性能提升，但我们的论文表明，噪声网络文本对视觉-语言学习来说是次优的。

为此，我们提出BLIP：引导语言-图像预训练，用于统一的视觉-语言理解和生成。BLIP是一个新的VLP框架，能够比现有方法涵盖更广泛的下游任务。它在模型和数据视角上分别引入了两个创新：

- **多模态混合编码器-解码器（MED）**：一种新模型架构，用于有效的多任务预训练和灵活的迁移学习。MED可以作为单模态编码器、图像引导的文本编码器或图像引导的文本解码器操作。该模型通过图文对比学习、图文匹配和图像条件语言建模三种视觉-语言目标进行联合预训练。
- **描述生成和过滤（CapFilt）**：一种新的数据引导方法，用于从噪声图文对中学习。我们将预训练的MED微调为两个模块：一个生成器生成给定网络图像的合成描述，另一个过滤器移除来自原始网络文本和合成文本中的噪声描述。

我们进行了广泛的实验和分析，得出了以下关键结论：

- 我们证明了生成器和过滤器通过引导描述共同工作，显著提升了各种下游任务的性能。我们还发现，更多样化的描述带来了更大的提升。
- BLIP在多种视觉-语言任务上取得了最先进的性能，包括图文检索、图像描述、视觉问答、视觉推理和视觉对话。在直接转移到两个视频-语言任务（文本到视频检索和视频问答）时也取得了最先进的零样本性能。

### 2. 相关工作

#### 2.1 视觉-语言预训练

视觉-语言预训练（VLP）旨在通过在大规模图文对上预训练模型，提升下游视觉和语言任务的性能。由于获取人工标注文本的成本高昂，大多数方法（Chen et al., 2020; Li et al., 2020; 2021a; Wang et al., 2021; Radford et al., 2021）使用从网络上抓取的图像和替代文本对（Sharma et al., 2018; Changpinyo et al., 2021; Jia et al., 2021）。尽管使用了简单的基于规则的过滤器，网络文本中的噪声仍然普遍存在。然而，这些噪声的负面影响在很大程度上被忽视了，掩盖在通过扩大数据集规模获得的性能提升之下。我们的论文表明，噪声网络文本对视觉-语言学习而言是次优的，并提出了CapFilt，以更有效地利用网络数据集。

有许多尝试将各种视觉和语言任务统一到一个框架中（Zhou et al., 2020; Cho et al., 2021; Wang et al., 2021）。最大挑战在于设计能够同时执行理解型任务（如图文检索）和生成型任务（如图像描述）的模型架构。无论是基于编码器的模型（Li et al., 2021a;b; Radford et al., 2021），还是编码器-解码器模型（Cho et al., 2021; Wang et al., 2021），都不能在这两类任务中都表现出色，而单一的统一编码器-解码器（Zhou et al., 2020）也限制了模型的能力。我们提出的多模态混合编码器-解码器模型提供了更多的灵活性和更好的下游任务性能，同时保持了预训练的简单和高效。

#### 2.2 知识蒸馏

知识蒸馏（KD）（Hinton et al., 2015）旨在通过从教师模型中提取知识来提升学生模型的性能。自蒸馏是KD的一种特殊情况，其中教师和学生的规模相同。它已被证明对图像分类（Xie et al., 2020）有效，最近也对VLP有效（Li et al., 2021a）。不同于大多数现有的KD方法简单地要求学生具有与教师相同的分类预测，我们提出的CapFilt可以解释为在VLP背景下进行KD的更有效方法，其中描述生成器通过语义丰富的合成描述提炼其知识，过滤器则通过去除噪声描述提炼其知识。

#### 2.3 数据增强

虽然数据增强（DA）在计算机视觉中被广泛采用（Shorten & Khoshgoftaar, 2019），但对于语言任务的数据增强不太直观。最近，生成语言模型被用于合成各种NLP任务的例子（Kumar et al., 2020; Anaby-Tavor et al., 2020; Puri et al., 2020; Yang et al., 2020）。与这些专注于低资源语言任务的方法不同，我们的方法展示了合成描述在大规模视觉-语言预训练中的优势。