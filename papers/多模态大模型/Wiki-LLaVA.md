#arXiv2024 #RAG #VLM 

![[Pasted image 20240514150014.png]]
## 摘要 

多模式LLM是LLM的自然演变，扩展了它们的能力，使其可以超越纯文本模态。随着研究不断进行，设计新颖的架构和视觉-语言适配器，本文集中于赋予这些模型回答需要外部知识的问题的能力。我们的方法，称为Wiki-LLaVA，旨在整合一个外部知识源的多模态文档，通过分层检索管道访问该源。使用这种方法，可以从外部知识源中检索相关段落，并将其作为LLM的附加上下文，增强生成的对话的有效性和精度。我们对专门用于视觉问答的数据集进行了大量实验，并展示了我们方法的适用性。

## 引言 

最近，大型语言模型（LLMs）在零-shot文本任务中表现出了令人印象深刻的性能。具体来说，最近的文献已经设计出了能够处理多样化任务的模型，如用户指示的[6, 30, 41]。在这种情况下，经典方法是在各种通过自然语言描述的任务上对模型进行微调[7, 34]，从而赋予模型吸收外部提供的指示并促进跨多个领域的强大泛化能力。在这些进步之后，计算机视觉社区开始研究将这些模型扩展到视觉-语言上下文，从而生成多模式大型语言模型（MLLMs）。在这一领域，通过视觉到语言适配器将视觉特征融合到LLM骨干中[1, 21, 23, 48]，已经引起了显著的性能改善，使其能够广泛泛化到需要详细视觉描述的视觉-语言任务。 在这种情况下，MLLMs通过简单地包含一个小模块（即适配器）将视觉特征与文本特征对齐而表现出色。然而，尽管这些模型是在大规模数据上训练的LLMs构建的，但当面对高度特定的用户查询或需要某种程度的组合推理来构思响应时，它们表现出明显的局限性。此外，由于训练数据中长尾信息的稀缺性，某些知识证明在MLLM的参数内编码是具有挑战性的。作为对这一挑战的回应，最近引入了不同的基准用于评估MLLM解决与外部数据相关的查询的能力，例如InfoSeek [5]和Encyclopedic-VQA [28]。虽然不同的作品[8, 20, 21, 32]已经在这些基准上进行了测试，强调了这一领域的重要性，但它们没有一个是专门为解决外部知识而设计的架构。 基于这些考虑，本文提出了第一个通过检索模块增强的MLLM，从而将重点转向教会模型在其响应中利用各种信息并学会区分每个信息的相对重要性。特别地，我们的模型从外部文档的知识库中检索出适当的信息，并采用分层检索方法来识别相关段落。然后，这些额外的知识被馈送到MLLM中，而不改变其结构但提高了其回答能力。据我们所知，我们的工作代表了第一个利用外部来源检索能力的MLLM。我们通过进行大量实验和与最近的MLLMs [8, 21, 24] 的比较来评估所提出方法的质量，并展示了我们设计选择的有效性。实验结果表明了从外部来源检索的优势以及我们模型设计的适当性。总的来说，我们认为我们的工作是检索增强MLLMs方向的第一步，这可能会促进该领域未来的研究。

## 相关工作 

多模态LLM。LLM已经显著改变了人工智能研究和应用的格局，其中引人注目的例子包括OpenAI的ChatGPT和GPT-4。这些模型利用了诸如指令调整[30]和来自人类反馈的强化学习[39]等对齐技术，并在语言理解和推理方面取得了显著的能力。像Flan-T5 [7]、Vicuna [6]、LLaMA [41]和Alpaca [40]等开源LLM进一步加速了研究社区的发展。这股LLM的发展潮流随后导致了MLLM的出现[3]，它可以将对视觉输入的理解与自然语言生成相结合。 早期构建MLLM的尝试，例如Visual-GPT [4]和Frozen [42]，使用了预训练的语言模型来增强视觉-语言模型，特别是用于任务如图像字幕和视觉问答。这项最初的研究为该领域的后续研究铺平了道路，随后引入了解决方案，如Flamingo [1]或BLIP-2 [21]，它们分别允许通过在LLM内直接使用可训练的交叉注意力层或通过可学习的查询结合图像和文本特征的Q-Former块将图像特征集成到LLM中。在这些进步的基础上，随后引入了一些模型，如FROMAGe [19]、Kosmos-1 [14]和MiniGPT-4 [48]，以进一步完善LLM体系结构中视觉和语言模态之间的相互作用。 与此同时，LLaVA系列模型 [23–25]引入了在多模态领域中使用指令调整的方法，通过在GPT-4收集的策划数据集上进行训练。这种策略现在是构建MLLM的最有希望的方法之一。 检索增强语言模型。近年来，通过从外部来源提取相关文本段落扩展语言模型的输入空间[10]，或最终直接从网络检索[29]，将检索增强应用于语言模型。这些技术在知识密集型任务中表现出了显著的改进，并在模型大小方面节省了大量资源。 传统上，将外部知识整合到文本生成中一直局限于初始阶段。不同的解决方案 [17]建议在专有LLM之上自适应地检索用于生成的段落。一些工作 [10]则专注于以更模块化和可解释的方式捕获知识，通过在语言模型的预训练中增加一个潜在知识检索器。这使得模型能够从大型语料库（如维基百科）中检索和关注文档。 尽管在文本增强方面受到了很多关注，但最近在视觉-语言任务的上下文中也有类似的研究努力[2, 13, 31, 37]。沿着这个方向，[13]中提出的工作提出了一种检索增强的视觉语言模型，将世界知识编码到一个大规模的内存中。其他方法 [35, 36] 也将检索应用于特定的下游任务，如图像字幕。与所有前述方法不同，我们的工作是第一个将检索增强应用于MLLM的工作。我们通过在由多模态文档组成的知识库上应用分层检索策略来实现这一点。 基于知识的视觉问答。最近出现的新基准，如Encyclopedic-VQA [28]和InfoSeek [5]，提高了标准知识型VQA [16, 27, 38] 的难度，因为这些问题需要对特定实体的知识进行深入了解，以至于即使是基于LLM的模型在不从外部来源检索信息的情况下也表现不佳。通常情况下，会使用对比图像-文本编码器来检索给定查询图像的目标实体[44, 46]。然后，实体名称被用作访问外部知识库的关键，该知识库通常由几个包含正确答案的文本段落组成。在本工作中，我们设计了一个基于CLIP [33]和Contriever模型 [15]的分层检索方案，以推断出相关的段落，并将它们馈送给MLLM来帮助生成答案。

## 3. 提议的方法 

我们的目标是为多模态LLM（MLLMs）配备能够回答复杂且具体问题的能力，这些问题不能仅通过图像内容和预训练知识来解决。为了实现这一目标，我们提出了Wiki-LLaVA，它将来自外部内存的外部知识整合到LLaVA模型中，而不会显著改变其设计。相反，我们通过将检索信息作为额外的输入上下文来增强模型的能力。总体而言，Wiki-LLaVA包括三个组件，如图2所示：视觉编码器，用于为MLLM提供视觉上下文并作为从外部知识库检索的查询，知识库本身（例如维基百科），以及一个分层检索模块，用于从外部知识库中检索相关文档和段落，以作为MLLM的附加上下文。

### 3.1. 基于知识的增强 

多模态整合和自回归生成。一个MLLM通常将一个多模态输入查询作为输入，包括图像和文本，并以自回归方式生成文本输出。形式上，该架构被训练来建模概率分布 𝑝(𝑤𝑡∣𝐼,𝑤0,𝑤1,...,𝑤𝑡−1,𝜃)p(wt​∣I,w0​,w1​,...,wt−1​,θ)，其中 𝜃θ 表示模型的参数， 𝐼I 表示输入图像， 𝑤0,..,𝑤𝑡−1w0​,..,wt−1​ 表示文本提示。文本提示通常包括预定义的系统级提示和由用户给出的与输入图像相关的问题。显然，标准MLLM只能依赖于用户提示、输入图像和存储在其内部参数中的知识（即 𝜃θ ）来满足请求，从而限制了其依赖外部知识回答问题的能力。在本文的其余部分，我们将LLaVA [24]作为我们的参考MLLM。LLaVA利用了预训练的LLM（即Vicuna [6]）和预训练的视觉模型（即基于CLIP的视觉编码器 [33]），它们通过一个MLP适配器相互连接，负责将CLIP特征转换为密集输入标记。因此，对于输入图像 𝐼I，LLaVA利用预训练的CLIP视觉编码器 𝐸𝑣Ev​，提取出密集的视觉特征网格 𝑍𝑣=𝐸𝑣(𝐼)Zv​=Ev​(I)，然后通过可学习的MLP将其投影到产生一系列密集的嵌入标记 𝑣0,𝑣1,...,𝑣𝑁v0​,v1​,...,vN​。最后，这些标记被预置到系统提示之前，并且整个视觉和文本标记序列被作为模型的LLM组件的输入。 使用外部知识进行增强。为了增强MLLM的外部知识，我们通过将来自由文件组成的外部内存中的相关文本数据注入输入上下文来丰富输入。形式上，MLLM的分布是在额外的文本检索知识标记的条件下进行的，导致 𝑝(𝑤𝑡∣\overbracket𝑣𝑜,𝑣1,...,𝑣𝑁视觉标记,    \underbracket𝑤0,𝑤1,...,𝑤𝑡−1系统 + 用户提示,\overbracket𝑒0,𝑒1,...,𝑒𝜏外部内存标记),p(wt​∣\overbracketvo​,v1​,...,vN​视觉标记,    \underbracketw0​,w1​,...,wt−1​系统 + 用户提示​,\overbrackete0​,e1​,...,eτ​外部内存标记), (1) 其中 𝑒0,...,𝑒𝜏e0​,...,eτ​ 表示从外部内存中检索到的添加的标记。与MLLM的标准公式不同，通过丰富输入上下文，我们允许模型利用从内存中检索到的标记来生成更具体的答案。 来自外部内存的分层检索。外部内存包括从文件中提取的（文档、图像、文本标题）三元组的集合，表示为 𝐷={(𝑑𝑖,𝑡𝑖)}𝑖D={(di​,ti​)}i​。在该内存中，我们进行分层两步搜索来检索相关信息。首先，我们定位最相关的文档，然后识别特定文档内的相关段落，随后将其作为MLLM的附加输入上下文利用。 在第一阶段，给定输入查询图像 𝐼I，我们通过使用文档标题作为可检索键，在外部内存中进行近似的 𝑘k 近邻搜索。查询图像与文本标题之间的相似度被建模为它们各自嵌入的内积，这些内积是通过视觉和文本CLIP编码器（即 𝐸𝑣Ev​ 和 𝐸𝑡Et​）计算得出的，如下所示： sim(𝐼𝑖,𝑡𝑖)=𝐸𝑣(𝐼)⋅𝐸𝑡(𝑡𝑖)𝑇.sim(Ii​,ti​)=Ev​(I)⋅Et​(ti​)T. 然后，知识检索器返回与使用上述过程检索到的最相关项目相关联的前 𝑘k 个文档。 检索文档段落。在第二步中，我们分析每个检索到的文档，以识别与用户问题最相关的段落。每个文档被定义为一系列块，表示为 𝑑𝑖=[𝑐𝑖0,..,𝑐𝑖𝑇]di​=[ci0​,..,ciT​]，给定输入问题，我们检索与问题最相似的块。我们使用Contriever架构 [15]来嵌入所选文档的每个块，以及查询（即用户提供的问题），并将相似性计算为嵌入之间的内积。通过在每个检索到的文档内检索 𝑛n 最合适的段落，总体上我们获得 𝑘⋅𝑛k⋅n 个段落。 上下文丰富。一旦我们找到最相关的块，我们将它们的原始内容作为MLLM的附加输入。具体来说，我们采用的最终提示包括图像标记、检索到的原始块、系统级提示和用户问题。形式上，考虑到三个检索到的段落，最终提示定义如下： ...

### 3.2. 训练 

虽然上述方法可以以零样本的方式工作，使用预训练的MLLM的原始权重 𝜃θ，但我们还研究了微调模型以增强其利用检索到的段落的能力的情况。特别是，在这种情况下，模型是根据需要外部知识的问题和真实答案的成对进行训练的。由于这可能会降低MLLM在不需要外部知识的任务上的能力（即，模型最初训练的所有其他任务），因此我们采用数据混合方法，其中需要外部知识的真实对与不需要外部知识的真实对在同一小批次中混合。

## 4. 实验

在本节中，我们首先介绍实验设置，描述所使用的数据集、评估协议以及执行实验所使用的实现和训练细节。然后，我们展示我们的实验结果，分析了CLIP微调的有效性，并评估了如何将检索到的知识合并到MLLM中。最后，报告了所提方法的局限性和可能的未来工作。

### 4.1. 数据集

维基百科问答（Encyclopedic-VQA）[28]。该数据集包含约221k个问题-答案对，涉及16.7k个不同的精细实体，最多有5张代表同一实体的图像。总体而言，数据集包含超过1M个由图像、问题和相应答案组成的三元组。精细实体和相关图像是从iNaturalist 2021 [43]和Google Landmarks Dataset V2 [45]中提取的，并与相应的维基百科文章相关联。问题分为四个不同的类别，即单跳、自动生成、多答案和双跳。特别是，单跳问题已经手动注释，并且需要一个维基百科文章来回答它们。自动生成的问题与单跳问题类似，但是由自动模型生成。相反，多答案问题可以用一个术语列表来回答，但总是指向一个单个精细实体。最后，双跳问题需要两个检索步骤来回答。数据集还配有一个由200万篇维基百科文章组成的知识库，适用于回答数据集问题。数据集三元组分为训练、验证和测试集，分别由100万、13.6k和5.8k个样本组成。在我们的实验中，我们使用训练集对LLaVA模型进行微调，并在数据集的测试集上报告结果。在测试期间，我们过滤掉了双跳问题，得到了4,750个测试三元组。

InfoSeek [5]。该数据集包含大约130万个图像-问题-答案三元组，对应大约11k个不同的实体（即维基百科文章）。绝大多数问题是通过几乎完全自动的过程获得的，方法是使用Wikidata中的知识三元组填充由人类编写的模板。在这种情况下，图像来自OVEN数据集 [12]。三元组分为训练、验证和测试集，分别包含约934k、73k和348k个样本。在提交时，测试集的真实答案尚不可用。因此，我们在验证集上报告结果。验证集和测试集都包含与训练集中不包括的新实体相关的问题，以及在训练期间未见过的问题。

除了图像-问题-答案三元组之外，还提供了一个由600万个维基百科实体组成的知识库。在我们的实验中，我们考虑了一个随机抽取的包含100k个实体的子集，其中我们保证了数据集问题相关的11k个实体的存在。

### 4.2. 实现细节

LLaVA微调。我们采用两种不同的微调方法，每种方法分别应用于一个数据集。为了保持LLaVA模型在已建立的MLLM数据集上的性能，我们将微调数据与LLaVA-Instruct数据集 [24]中的样本相结合。具体而言，考虑到其大小为158k，我们在每个小批次中加倍了来自该数据集的示例的概率。为了减少可训练参数的数量，我们使用总批次大小为512个样本进行低秩适配器的训练。

检索。从维基百科内容中获取的文本文档使用Contriever架构 [15]进行嵌入，将文本分成每个600个字符的块。此外，为了简化效率，该过程涉及使用单个视觉编码器。具体而言，根据LLaVA架构 [24]，我们使用CLIP ViT-L/14@336骨干来嵌入图像以作为输入提供给MLLM，同时利用它来提取查询视觉特征，在初始的层次化检索步骤中，促进外部记忆组件的集成。为了进行实体检索，我们采用了近似kNN搜索，而不是精确kNN搜索，因为这显著提高了整个管道的计算速度。为此，我们使用Faiss库 [18] 和一个基于图的HNSW索引，每个顶点具有32个链接。

### 4.3. 评估协议

我们在两种设置下评估我们的模型：不带外部知识库和带外部知识库。前者意味着我们要求模型直接回答视觉问题，仅依靠在预训练和/或微调过程中学到的能力。另一方面，在后者的设置中，我们利用所提出的层次化检索方法在外部知识库中搜索额外信息。实际上，这由两个维基百科转储组成，分别包括2M和100k页。关于评估指标，我们报告了Encyclopedic-VQA测试集和InfoSeek验证集上的准确性，遵循随数据集一起提供的官方评估脚本。

### 4.4. 实验结果

分析CLIP性能。我们首先评估使用CLIP的实体检索结果。在这种情况下，我们考虑了Encyclopedic-VQA测试集和InfoSeek验证集中的图像，并测量CLIP找到各自数据集的知识库中正确实体的能力（即Encyclopedic-VQA有2M个条目，InfoSeek有100k个条目）。如前所述，我们使用图像作为查询，使用维基百科标题作为可检索项。

结果如表1所示，以R@k（其中k=1, 10, 20, 50）表示，该值衡量了在前k个检索到的元素中正确实体出现的百分比。值得注意的是，正确检索与输入图像相关的维基百科实体很大程度上取决于所使用的知识库的大小。事实上，当使用100k个条目时，如InfoSeek的情况下，正确实体在第一项中被检索出的概率为36.9%，在前10项中为66.1%。相反，当使用像Encyclopedic-VQA这样包含2M个条目的显著更大的知识库时，检索结果显著较低，分别为3.3%和9.9%，即R@1和R@10。

Encyclopedic-VQA和InfoSeek的结果。接下来，我们在表2中报告了视觉问答结果。我们包括了零次微调模型（例如BLIP-2 [21]、InstructBLIP [8]和LLaVA-1.5基线模型 [24]），它们未在考虑的数据集上进行微调，并且没有利用外部知识库。此外，我们考虑了LLaVA-1.5在Encyclopedic-VQA和InfoSeek的训练集上进行微调但未增加检索上下文的精度结果。我们的方法（即Wiki-LLaVA）的结果分别报告了标准设置，其中CLIP用于从知识库中检索最具代表性的实体，并且其oracle版本，后者使用与输入图像-问题对应的实体。对于这两种情况，我们考虑了不同数量的检索文本块n，所有这些块都对应于前1个（或真实值）实体。在使用CLIP时，我们还通过使用n=1当k大于1时，来改变检索到的实体数量k（即k=1, 2, 3）。这个选择是由Vicuna所接受的最大上下文长度所决定，它被设置为2048个标记。

