## 摘要

视觉识别技术近年来通过两种方式进行学习：一种是在人工标注的图像-标签数据上进行监督学习，另一种是利用网络爬取的图像-文本对进行语言-图像对比学习。虽然监督学习可以产生更具辨别力的表示，但语言-图像预训练展现出了前所未有的零样本识别能力，这主要归因于数据来源和学习目标的不同特性。在这项工作中，我们引入了一个新的公式，将两种数据源组合成一个共同的图像-文本-标签空间。在这个空间中，我们提出了一种新的学习范式，称为统一对比学习 (UniCL)，它使用单一的学习目标来无缝地促进两种数据类型的协同作用。大量实验表明，我们的 UniCL 是一种有效的学习语义丰富且具有辨别力的表示方法，适用于零样本、线性探测、完全微调和迁移学习场景下的图像识别。特别是在零样本识别基准测试中，与语言-图像对比学习和监督学习方法相比，它分别获得了高达 9.2% 和 14.5% 的平均增益。在线性探测设置中，它也分别比这两种方法提高了 7.3% 和 3.4% 的性能。我们的研究还表明，UniCL 单独作为纯图像-标签数据的学习器，在三个图像分类数据集和两种类型的视觉骨干网络（ResNet 和 Swin Transformer）上都可与监督学习方法相媲美。

## 引言

学习识别图像中的视觉概念是一个基础且长期的研究问题。通常，这可以通过两种方式解决：一种是在人工标注的图像-标签对上进行监督学习[10]，另一种是对网络爬取的图像-文本对进行对比学习 [29, 48]。

当使用干净且大规模的人工标注图像-标签数据（例如 ImageNet [10]）时，监督学习可以在给定的类别上获得不错的视觉识别能力 [23, 35, 55]，并具备强大的迁移学习能力 [14, 33]。然而，收集精确的图像-标签数据可能是一个费力且昂贵的过程，更不用说将其扩展到大量视觉概念的难度了1。

【引出图文对比学习】另一方面，语言-图像对比学习最近成为一种很有前景的方法，它利用了大量网络爬取的图像-文本对。这些对通常包含噪声，形式自由，但涵盖了许多视觉概念。正如 CLIP [48] 和 ALIGN [29] 中所展示的，从数亿个图像-文本对中学习的模型可以获得令人印象深刻的低样本识别性能，适用于广泛的视觉理解场景。

尽管这些图像-文本模型显示出对视觉概念的广泛覆盖，但我们在实验中发现，它们通常缺乏迁移学习所需的强大辨别能力。一个自然的问题是：我们能否拥有一个模型，既能获得具有辨别力的表示，又能获得广泛的视觉概念覆盖？

在这项工作中，我们迈出了回答这个问题的第一步。我们从一个新的视角开始，如图 1 所示。我们没有孤立图像-标签和图像-文本数据，而是定义了一个图像-文本-标签空间，并展示了如何消除两种数据类型之间的界限。

如图 1 左侧所示，图像-标签数据的监督学习 [30] 通常旨在将图像映射到离散标签，并且在训练过程中完全忽略与每个标签相关的文本概念。相反，语言-图像对比学习 [48] 旨在学习一对视觉和文本编码器，以对齐图像和文本，如图 1 右侧所示。这种学习方法隐含地假设每个图像-文本对都有一个唯一的标签。将这两种学习范式并排比较，我们可以看到它们实际上都存在于共同的图像-文本-标签空间中，该空间是通过将每个标签映射到一个文本概念来进行监督学习，并为每个文本描述分配一个唯一的标签来进行语言-图像预训练，如图 1 底部所示。

基于这种新视角，我们可以简单地使用视觉编码器和语言编码器对图像和文本进行编码，并在标签的指导下对齐视觉和文本特征（图像-文本对的唯一标签和图像-标签数据的人工标签）。然而，现有的监督学习和语言-图像对比学习范式无法支持从这些组合标签中学习。为此，我们提出了一种统一的对比学习方法，称为 UniCL，以无缝地适应两种数据类型进行视觉-语义表示学习。它接受图像、文本作为输入，并使用从标签导出的软目标计算损失。

通过 UniCL，我们将图像-标签和图像-文本数据结合在一起，学习具有辨别力和语义丰富的表示，这对各种下游任务都有益。

总而言之，我们的主要贡献是：

- 我们引入了图像-文本-标签空间的新视角，它可以无缝地统一常用的图像-标签和图像-文本数据。
    
- 我们提出了在图像-文本-标签空间中称为 UniCL 的统一对比学习方法，可以从图像-标签和图像-文本数据中的任何一种或两者中学习。
    
- 大量实验表明，我们的 UniCL 可以有效地利用两种类型的数据，并在标准零样本、线性探测、完全微调和迁移学习设置上普遍实现优越的性能。
    

最后，我们将 UniCL 扩展到 Florence [72] 中的数十亿个图像-文本-标签数据，并证明了其在数十个基准测试中优于 CLIP [48] 和 ALIGN [29]。因此，我们强烈推荐 UniCL 作为一种通用的视觉多模态学习范式。

## 2. 相关工作

### 2.1 监督学习

用于图像分类的监督学习历史悠久。如前所述，监督学习的典型方法是将图像映射到人工标签。为了实现这一目标，大量工作从不同的方向推动了图像识别性能的发展，例如：数据规模从 MNIST [37] 到 ImageNet-1K [10]，模型架构从卷积神经网络 (CNN) [23, 26, 35, 36, 41, 54, 55] 到 Transformers [15, 44, 59, 64, 67, 71, 76]，学习目标从最初的交叉熵 [47] 到边缘损失 [11, 43, 52]，以及最近的监督对比损失 [30]。

在本文中，我们将图像-标签视为图像-文本-标签数据，以学习通用的视觉-语义空间，并开发了一种统一的对比学习方法。它回调了标签背后的文本概念，并将其用作一种特殊的语言格式。从这个意义上说，我们的工作也与传统的零样本分类 [9, 28, 46, 65, 69, 70] 相关。这些工作中的大多数都关注于识别小规模的细粒度类别。我们的工作超越了这种受限的设置，旨在从组合的图像-标签和图像-文本对中学习良好且丰富的视觉-语义表示。

### 2.2 语言-图像对比学习

视觉和语言是一个快速发展的领域。现有的工作可以大致分为两类：

(i) 受 BERT [13] 成功启发，第一类研究侧重于学习通用的多模态融合层，基于掩码标记预测和/或图像-文本匹配，给定从视觉和文本编码器中预先提取的特征 [17, 31, 39, 40, 45, 53, 66, 77]。它们旨在改进下游任务，例如视觉问答 [2, 27]、图像字幕 [1, 42] 和视觉常识推理 [74]。

(ii) 另一类工作侧重于从自然语言监督中学习可迁移的视觉表示，包括生成方法 [12, 50] 和对比方法 [16, 29, 48, 62, 63, 78]。近年来，对比学习在代表性工作（例如 CLIP [48] 和 ALIGN [29]）中得到了扩展，通过在数亿个网络爬取的图像-文本对上进行预训练。我们的工作与这些工作相近，因为我们也使用图像-文本数据作为主要数据源之一。然而，这些工作忽略了图像-标签数据。我们的工作提出了第一个统一的对比学习方法，可以无缝地利用两者。

### 2.3 自监督学习

用于视觉的自监督学习 (SSL) 旨在从原始像素中学习通用视觉表示，而无需标签或文本的监督 [19]。对比学习为性能最佳的 SSL 模型奠定了基础 [3, 6, 8, 21, 24, 57, 58]。它最大化了同一图像的不同增强视图之间学习表示的一致性，并最小化了不同图像视图之间的一致性。这种基于增强视图的范式也已扩展到非对比方法 [4, 7, 20, 38]，其中在学习中只考虑正图像视图对。尽管图像 SSL 在训练中利用几乎无限量的未标记图像数据方面具有很大的潜力 [18]，但缺乏语言关联使其难以应用于零样本识别。然而，对比学习在 SSL 中的成功激发了将这种方法推广到更广泛的领域，例如图像-文本设置中的 CLIP [48] 和图像-文本-标签设置中的我们的 UniCL，其中图像和语言描述可以被视为同一底层概念的多模态视图。

## 3. 方法

### 3.1 预备知识

#### 问题设置

我们定义一个三元组数据格式 S = {(xn, tn, yn)}Nn=1，其中 x ∈ X 是图像，t ∈ T 是其对应的语言描述（范围从简单的标记（如类别名称）到自由形式的文本序列），y ∈ Y 是一个标签，表示数据集中分组或唯一语言描述的索引。如前所述，这种三元组数据表示是广泛存在的图像数据的通用格式，包括常用的图像-文本和图像-标签数据。一方面，来自网络的图像-文本对 {(xn, tn)}Nn=1 通常具有一对一映射，因此每个图像-文本对都有唯一的标签，S 减少为 {(xn, tn, yn ≡ n)}Nn=1。另一方面，尽管图像分类问题通常使用简单的类别标签或索引，但每个标签都是由其任务定义中概念的相似性引起的 [10]。因此，对于图像-标签数据，S 减少为 {(xn, tn ≡ C[yn], yn)}Nn=1，其中 C 是由 yn 索引的概念名称集合。基于此定义，我们可以将图像-标签对表示为带标签的图像-文本对，而将图像-文本对表示为具有唯一标签的图像-文本对。图 2 说明了它们是如何统一的。这项工作的目标是从联合数据 S 中学习，相信语言描述 t 中丰富的语义和标签 y 的结构化组织一起有利于学习图像 x 的语义丰富且具有辨别力的视觉表示。

### 3.2 统一的图像-文本-标签对比

对于每个图像 x，由 θ 参数化的图像编码器模型 fθ 首先将 x 表示为视觉特征向量 ṽ ∈ Rd×1：ṽ = fθ(x)。对于每个语言描述 t ∈ T，我们使用由 φ 参数化的文本编码器 fφ(t) 对其进行编码，以获得其特征向量 ũ ∈ Rd×1：ũ = fφ(t)。对于批处理 B 中的第 i 个图像 xi 和第 j 个语言描述 tj，我们使用 ui = fθ(xi)/‖fθ(xi)‖ 和 vj = fφ(tj)/‖fφ(tj)‖ 将其特征向量归一化到超球面上，并将其相似度计算为 sij = uT i vj。我们考虑图像和语言之间的双向学习目标：

min {θ,φ} LBiC = Li2t + Lt2i, (1)

包括两个对比项（温度超参数 τ 控制对难负样本的惩罚强度）：

- 图像到文本的对比损失，用于将批处理中匹配的图像与给定文本对齐
    

Li2t = - ∑ i∈B 1/|P(i)| ∑ k∈P(i) log exp(τuT i vk)/∑ j∈B exp(τuT i vj) (2)

其中 k ∈ P(i) = {k|k ∈ B, yk = yi}。

- 文本到图像的对比损失，用于将匹配的文本与给定图像对齐
    

Lt2i = - ∑ j∈B 1/|P(j)| ∑ k∈P(j) log exp(τuT k vj)/∑ i∈B exp(τuT i vj) (3)

其中 k ∈ P(j) = {k|k ∈ B, yk = yj}。

以图 2 右侧为例，Li2t 是为每一行计算的，而 Lt2i 是为每一列计算的。红色瓷砖表示正对，而空白瓷砖表示负对，所有瓷砖都是根据标签分配的。