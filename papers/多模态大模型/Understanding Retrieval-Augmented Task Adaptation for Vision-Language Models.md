#VLM #RAG #arXiv2024 #ICML 

预训练的对比视觉-语言模型在各种任务中展示了出色的性能。然而，在处理在预训练阶段没有充分代表的类别的精细训练数据集时，它们常常表现不佳，因此需要进行适应。最近的研究通过利用来自大规模网页数据库的样本进行检索增强适应，特别是在数据量较少的情况下，取得了令人鼓舞的成果。尽管在实践中取得了成功，但检索如何影响视觉-语言模型的适应仍然是一个未解的研究问题。在这项工作中，我们采用了反思性视角，系统地研究了检索增强适应中的关键组件的作用。我们揭示了关于单模态和跨模态检索的新见解，并强调了logit集成在有效适应中的关键作用。此外，我们提出了直接支持我们经验观察的理论基础。

### 引言

对比视觉-语言预训练已经成为自然语言处理和计算机视觉广泛任务的基础（Radford et al., 2021; Jia et al., 2021; Yang et al., 2022; Li et al., 2022b; Mu et al., 2022; Yu et al., 2022; Sun et al., 2023; Xu et al., 2024）。这些模型在捕捉视觉和文本数据中存在的复杂关系方面表现出色，使它们能够整体理解上下文、语义和关联。如今，使用来自大规模网络预训练的对齐多模态特征已成为一种常见做法。然而，当这些预训练模型遇到真实世界的下游数据集时，特别是在数据量少（few-shot）的情况下，常常面临挑战。这些数据集通常包含在初始预训练阶段未充分代表的细粒度类别，这对模型适应这些细微区别提出了显著挑战。

在低数据量环境中，检索增强适应表现出了希望，因为可以有效检索到大量的互联网外部资源来增强适应。最近的研究（Udandarao et al., 2023; Zhang et al., 2023）通过利用大规模的文本和图像数据库（Schuhmann et al., 2022）展示了令人鼓舞的结果。检索增强适应包括两个主要步骤：首先从外部来源检索最相关的数据，然后基于检索到的样本进行下游任务的适应。虽然现有工作主要集中于开发新的适应算法或整合不同的知识来源，但在理解检索增强如何影响视觉-语言模型的适应方面仍然存在显著差距。理解这一点对于指导未来开发有效的算法至关重要。

在这项工作中，我们采用反思性视角，通过系统研究来理解检索增强适应，并建立新的理论基础。我们的实证分析揭示了两个方面的关键见解：（1）检索方法的影响，和（2）检索到的样本如何帮助适应。首先，我们显示了图像到图像（I2I）检索在各种下游任务中始终优于文本到图像（T2I）检索。在相同的检索预算下，这两种检索方法在使用的查询样本上有所不同：I2I使用目标数据分布中的一些种子图像，而T2I使用每个类别标签的文本描述。虽然I2I和T2I检索相对于目标数据都引入了分布偏移，但我们表明，I2I在直接从目标分布中检索（即无分布偏移）时表现更为出色。其次，我们显示，将零样本预测与I2I检索到的样本进行集成是提高适应性能的关键。对于给定的测试样本，集成通过对从检索特征缓存中获得的logit和零样本推理的logit进行加权平均来实现。我们实验证明，没有集成，检索增强适应的性能显著下降。这一新发现补充了先前研究，后者通常将检索的成功归因于样本的多样性和质量。

除了实证分析外，我们还提供了直接支持上述经验观察的理论见解。我们通过表征每种检索方案的多模态特征空间来形式化T2I和I2I检索。在现实假设下，我们分析了检索如何影响模态间隙和检索与目标分布之间的偏移。特别是，我们证明了I2I检索优于T2I检索（定理4.1），并且logit集成对于通过更好地利用不同模态中编码的知识来改进基于CLIP的适应至关重要（定理4.2）。我们的理论结果揭示了设计有效的检索增强适应算法的关键因素。

我们的主要贡献总结如下：

- 我们对视觉-语言模型的检索增强适应进行了及时且系统的调查，强调了检索方法和logit集成等关键组件。
- 我们提供了更细致的实证研究和深入分析，揭示了单模态检索和logit集成在低数据量环境中有效的CLIP-based适应中的关键作用。
- 我们开发了一个新的检索增强适应的理论框架，并提出直接支持我们经验观察的理论结果。
- 我们进一步提供了全面的消融研究，并讨论了模型架构的影响、使用微调特征缓存进行适应以及数据混合适应等替代设计选择。

## 2. 检索增强任务适应

在本节中，我们首先讨论对比视觉-语言模型的基础知识以及用于检索的外部数据库（第2.1节）。接着，我们展示检索增强任务适应的两个主要步骤：通过从外部数据库中检索相关样本构建特征缓存（第2.2节），以及基于检索到的样本执行任务适应（第2.3节）。图1展示了整个流程。

### 2.1. 基础知识

流行的对比视觉-语言模型，如CLIP（Radford等，2021），采用双流架构，包含一个文本编码器T：t → Rd和一个图像编码器I：x → Rd。模型在大规模网页图像-标题数据集上预训练，使用多模态对比损失对齐不同模态的特征。这种多模态嵌入的对齐为当代大规模多模态向量数据库（Schuhmann等，2022）提供了显著优势，使基于语义相似性的高效检索成为可能。

**零样本推理**。在推理时，给定一个测试输入x，我们可以得到视觉嵌入I(x)与每个标签c∈{1, 2, ..., C}的上下文化表示T(tc)之间的余弦相似度fZOC(x) = sim(I(x), T(tc))。这里，tc可以是一个通用模板，如“a photo of CLASS”，或者是该类别的文本描述。我们将零样本模型的logit向量表示为fZOC(x)∈RC，它由C个余弦相似度组成。类别预测可以基于C个类别中最大余弦相似度进行。

**外部大规模知识库**。预训练的CLIP模型在处理预训练数据集中未充分代表的细粒度类别的下游数据集时常常表现不佳。为了在低数据量方案中将CLIP模型适应于细粒度数据集，最近的研究（Liu等，2023）通过利用诸如LAION（Schuhmann等，2022）等外部资源展示了有希望的性能。LAION是一个包含数十亿图像-文本对的网页规模知识库，涵盖了现实世界中各种概念。在给定的预算下，我们可以通过近似KNN搜索（Johnson等，2019）从知识库中高效地检索相关样本，构建少样本缓存。具体细节如下。
### 2.2. 通过检索构建特征缓存

给定一个包含C个类别的下游数据集：Y = {1, 2, ..., C}和大小为KC的预算，我们可以为每个类别检索K个样本，构建一个大小为KC的缓存。对于视觉-语言模型，检索方法可以分为单模态检索和跨模态检索，具体如下：

**单模态检索**。我们主要考虑图像到图像（I2I）检索，因为它很流行。对于I2I检索，我们假设可以访问来自下游数据集的一小部分查询图像。查询集QI = ∪C_{c=1} QcI，其中QcI = {xc,1, xc,2, ..., xc,nc}包含每个类别c的nc个种子图像。然后我们从SL中为每个类别检索前K个相似图像： 𝑅𝐼2𝐼(𝑐)=𝑡𝑜𝑝𝐾{𝑥∈𝑆𝐿:𝑠𝑖𝑚(𝐼(𝑥),𝐼(𝑥𝑐,𝑖)),𝑥𝑐,𝑖∈𝑄𝐼}RI2I(c)=topK{x∈SL:sim(I(x),I(xc,i)),xc,i∈QI} 其中，sim(I(x), I(xc,i))是检索数据库中图像x的图像嵌入与查询图像xc,i的余弦相似度，topK表示选择前K项的操作。我们可以通过将这些集合在所有类别上取并来构建I2I检索的K-shot缓存： 𝑆𝐼2𝐼𝑅=⋃𝑐∈𝐶{(𝑥,𝑡)∈𝑆𝐿:𝑥∈𝑅𝐼2𝐼(𝑐)}SI2IR​=⋃c∈C​{(x,t)∈SL:x∈RI2I(c)}

**跨模态检索**。我们主要考虑文本到图像（T2I）检索。我们假设可以访问目标数据集中的类别名称，这也称为“仅名称转移”（Udandarao等，2023）。查询集QT = {tc}C_{c=1}，其中tc是类别c的通用文本描述。类别c的检索K个样本为： 𝑅𝑇2𝐼(𝑐)=𝑡𝑜𝑝𝐾{𝑥∈𝑆𝐿:𝑠𝑖𝑚(𝐼(𝑥),𝑇(𝑡𝑐)),𝑡𝑐∈𝑄𝑇}RT2I(c)=topK{x∈SL:sim(I(x),T(tc)),tc∈QT} 其中，sim(I(x), T(tc))是图像x的图像嵌入与类别c的文本嵌入之间的余弦相似度。T2I检索的K-shot缓存表示为： 𝑆𝑇2𝐼𝑅=⋃𝑐∈𝐶{(𝑥,𝑡)∈𝑆𝐿:𝑥∈𝑅𝑇2𝐼(𝑐)}ST2IR​=⋃c∈C​{(x,t)∈SL:x∈RT2I(c)}

### 2.3. 使用检索到的样本进行任务适应

给定K-shot缓存（SI2I_R或ST2I_R）和预训练的CLIP图像和文本编码器I和T，我们可以对细粒度目标数据集进行适应。为了更好地理解检索样本的影响，我们在第3节中考虑零样本适应，其中缓存仅包含检索样本。在第5节中，我们讨论少样本适应，其中缓存包含目标训练集中的样本和检索样本的混合。

**基于检索的适应**。最近提出了各种基于缓存的适应方法（Zhang等，2022a; 2023; Udandarao等，2023）。这些方法的核心通常是基于两个来源为每个测试输入获取logit集成：（1）零样本CLIP模型的logit，以及（2）缓存的logit。为了通用性，我们考虑一个代表性的适应框架TipAdaptor（Zhang等，2022a）。具体来说，给定大小为CK的缓存（由C类每类K个检索样本组成），我们将视觉特征的集合表示为K = [k1,1, k1,2, ..., kC,K] ∈ Rd×CK，其中kc,i = I(xc,i)。对于每个测试输入x，我们可以获得CK个余弦相似度sc,i(x) = sim(I(x), kc,i)。余弦相似度然后通过一个指数函数s˜：s 7→ exp(-ω + ωs)进行缩放，超参数ω调节锐度。因此，我们可以根据视觉特征为每个类别获得一个平均相似度向量，fRET_c(x) = 1/K ∑K_{i=1} s˜c,i(x)。测试样本的最终logit是来自特征缓存和零样本CLIP预测的logit的集成： 𝑓𝐸𝑁(𝑥)=𝛼𝑓𝑍𝑂𝐶(𝑥)+𝛾𝑓𝑅𝐸𝑇(𝑥)fEN(x)=αfZOC(x)+γfRET(x) 其中α和γ权衡两个logit的相对重要性。这种logit集成方案在最近的工作中也被广泛采用（Zhang等，2023）。为了完整性，我们还通过将K中的视觉特征设置为可学习参数来讨论基于学习的适应。我们将这种方法表示为Ensemble(F)，其中F代表微调。

## 3. 检索增强适应的细粒度分析

不同于最近在算法设计和新知识源整合方面的工作（Zhang等，2023；Iscen等，2023；Udandarao等，2023），我们工作的目标是提供一个系统的分析，并在理论上洞察检索增强如何影响视觉-语言模型的适应。在本节中，我们进行经验分析，主要关注两个方面的影响：检索方法（第3.2节）和使用检索样本进行的logit集成（第3.3节）。在第4节中，我们将提供理论分析以支持这些经验发现。在第5节中，我们讨论替代设计选择和消融研究。

### 3.1. 设置

**数据集**。按照先前的工作（Zhang等，2022a），我们考虑了涵盖常见和细粒度类别的广泛现实世界数据集：Caltech101（Fei-Fei等，2004），Birds200（Wah等，2011），Food101（Bossard等，2014），OxfordPets（Parkhi等，2012），Flowers102（Nilsback和Zisserman，2008），Textures（Cimpoi等，2014）和UCF101（Soomro等，2012）。

**实施细节**。我们使用LAION-5B（Schuhmann等，2022）作为检索数据库，该数据库包含5.85亿对图像-文本对。对于T2I检索，默认查询集包含使用提示模板的类别描述。对于I2I检索，默认情况下，我们使用每个类别8个种子图像作为查询集。基于查询集，我们使用clip-retrieval工具从LAION-5B进行高效检索。我们变化每个类别的检索样本数K ∈ {1, 2, 4, 8, 16}。对于适应，我们使用预训练的带有RN50骨干的CLIP作为默认模型。除非特别说明，每个报告结果都是三个独立运行的平均值。两个logit的集成权重α、γ在验证集上调整。种子图像数量和替代骨干的消融研究在第5节中进行。更多实施细节见附录A。

### 3.2. 检索方法的影响

**I2I检索始终优于T2I检索**。为了更好地理解检索方法的影响，我们比较了使用I2I和T2I检索的适应性能（准确率）。结果如图2所示，横轴表示每个类别的检索样本数（shot）。由于I2I和T2I检索相对于目标分布引入了分布偏移，我们还绘制了从目标训练集中检索样本时的oracle性能，记为ID检索（绿色）。直接从目标训练集中检索可以视为性能的上限。

我们观察到几个显著趋势：（1）I2I检索在所有shots和数据集上始终优于T2I检索。特别是，当增加shots时，I2I和T2I之间的差距扩大。（2）与没有知识增强的零样本推理（紫色星标）相比，I2I检索显著提高了性能。值得注意的是，I2I检索与ID检索（理想）的差距平均可以小到1%（12个shots），这突显了在没有目标数据集训练数据的极低数据方案中利用检索样本的潜力。（3）虽然T2I检索获得了与类别语义对应的多样化样本集合，但与零样本CLIP相比，多数据集的性能提升可能有限。我们接下来通过详细检查检索样本来调查原因，并在第4节中提供理论理解（定理4.1）。类似趋势也适用于基于训练的适应，我们对缓存特征进行微调，如Zhang等（2022a）所示（见附录E的图10）。

### 3.3. 检索样本如何帮助适应？

**与零样本预测的集成是关键**。我们展示了将零样本预测与I2I检索样本集成在一起是提高适应性能的关键。结果如图4所示，其中ensemble表示使用fEN = αfZOC + γfRET，α、γ ∈ (0, 1)，RET表示仅使用fRET（α = 0, γ = 1），ZOCLIP表示仅使用fZOC（α = 1, γ = 0）。这一有趣现象突显了logit集成在将视觉-语言模型适应于下游任务中的重要性。通过检查RET和ensemble的类别性能（见附录B的图8），也可以看到这种益处。类似趋势也适用于基于训练的适应，记为Ensemble (F)，我们微调缓存特征，如Zhang等（2022a）所示。接下来，我们将提供进一步的理论解释（定理4.2）。

## 4. 理论理解

我们现在提供理论支持以解释我们的实证观察，并正式理解检索增强任务的适应性。概述如下，定理4.1解释了为什么I2I检索优于T2I检索。我们进一步证明了在定理4.2中，logit集成是检索增强适应的关键。这两个定理证实了我们在第3节中的实证结果。完整的证明见附录D。

### 4.1 问题设定

给定一个具有C类的下游任务，记[C] := {1, 2, ···, C}。T = [t1, ..., tC] ∈ R^d×C表示所有类别的文本嵌入矩阵，其中tc := T(tc) ∈ R^d且tc是类别c的通用文本描述。回顾一下，K = [k1,1, k1,2 ... , kC,K] ∈ R^d×CK表示检索到的图像的嵌入矩阵，其中kc,i := I(xc,i) ∈ R^d。为简化记号，我们假设文本和图像特征都是ℓ2归一化的。令K¯ = KV^⊤K ∈ R^d×C包含每个类别的平均检索特征。V ∈ R^C×CK是一个稀疏矩阵，包含检索样本的单热标签，其条目为Vi,j = 1{i = ˜j}，其中i ∈ [C]，j ∈ [CK]，且˜j := ⌊j/K⌋ (Zhang et al., 2022a)。例如，当K = 2，C = 3时，我们有：

𝑉=[110000001100000011]V=⎣⎡​100​100​010​010​001​001​⎦⎤​

在推理时，令(x, y) ∼ D_T为目标分布D_T中的一个测试样本，其标签为y ∈ [C]，其视觉特征为z := I(x)。测试样本的最终logit可以表示为零样本CLIP和检索特征缓存的logit的加权和（集成）：

𝑓(𝑥)=(𝛼𝑇+𝛾𝐾¯)⊤𝑧,f(x)=(αT+γK¯)⊤z,

其中0 ≤ α, γ ≤ 1。

给定一个损失函数ℓ（例如交叉熵），下游分布上的风险为L(f) := E_(x,y)∼D_T[ℓ(f(x), y)]。为了简化记号，我们表示风险为R(Q) := E[ℓ(Q^⊤z, y)]，其中Q ∈ R^d×C。例如，logit集成的风险为R(αT + γK¯)。

模态差距和检索分布偏移。为了理解检索的影响，我们表征了特征空间中检索数据和下游数据之间的分布偏移。我们定义s¯_c := E_(x,y)∼D_T[I(x)|y = c]为基于下游分布的类别c ∈ [C]的图像表示。令S¯ := [s¯_1, ..., s¯_C]。我们定义了T2I和I2I检索的检索数据和目标数据之间的分布偏移为ξ_T2I_c和ξ_I2I_c。令ξ_t := max_c∈[C] ξ_T2I_c和ξ_s := max_c∈[C] ξ_I2I_c（定义D.4）。我们可以通过引理D.10得到ξ_s的上界和ξ_t的下界。

### 4.2 主要结果

在T2I和I2I检索的预训练特征空间的现实假设下，我们提出了以下两个关键结果。详细版本及完整证明见附录D。

**定理4.1（单模态检索的优势）**。以至少1 − δ的概率，集成风险的上界如下：

𝑅(𝛼𝑇+𝛾𝐾¯)−𝑅(𝑆¯)≤𝐿(𝛼∥(𝑇−𝑆¯)⊤𝑧∥2+𝛾𝜅𝑟8𝐶𝐾log⁡𝐶𝛿+𝛾2𝐶𝜉),R(αT+γK¯)−R(S¯)≤L(α∥(T−S¯)⊤z∥2+γκ8CKr​logδC​+γ2Cξ​),

其中L ≤ √exp(2) + 1，κ表征类内特征浓度（定义D.1），ξ是I2I检索的ξ_s或T2I检索的ξ_t。

解释：上述上界由三部分组成：文本和视觉模态之间的差距，检索特征的样本复杂度（随着K的增加而减少），以及由检索方法引起的分布偏移项。通过引理D.10，我们可以进一步证明I2I由于更小的ξ而明显优于T2I检索。

此外，为了理解logit集成的好处，我们定义了以下三个事件：

- E1：{(x, y) ∼ D_T : y ≠ argmax_c∈[C] t^⊤_c z 且 y ≠ argmax_c∈[C] k¯^⊤_c z}
- E2：{(x, y) ∼ D_T : y = argmax_c∈[C] t^⊤_c z 且 y ≠ argmax_c∈[C] k¯^⊤_c z}
- E3：{(x, y) ∼ D_T : y ≠ argmax_c∈[C] t^⊤_c z 且 y = argmax_c∈[C] k¯^⊤_c z}

其中E1表示f_ZOC和f_RET都错误分类测试样本，而E2和E3表示其中一个正确预测的事件。我们可以看到R0-1(f_ZOC) = Pr(E1) + Pr(E3)和R0-1(f_RET) = Pr(E1) + Pr(E2)。

**定理4.2（logit集成的好处）**。在I2I检索的现实假设下，当α = γ = 1/2时，我们可以对logit集成的0-1风险进行上界：

𝑅0−1(𝑓)≤𝑃𝑟(𝐸1)+𝐶1(𝑃𝑟(𝐸2)+𝑃𝑟(𝐸3))+𝜌𝑐R0−1(f)≤Pr(E1)+C1(Pr(E2)+Pr(E3))+ρc​

其中C1 := ρ_d max{6κ − ν, 2κ + τ}是一个与模态差距、类内特征浓度和类间分离相关的项。ρ_c表征异常值的比例。详细定义见附录D中κ、τ、ν、ρ_c和ρ_d。

解释：上述定理通过模态差距和检索与目标分布的关键属性对0-1风险进行了上界。此外，logit集成利用不同模态编码的知识相互受益。我们可以进一步证明在某些条件下（详见附录D），logit集成相比零样本模型具有更低的0-1风险（即更高的准确率）。

## 5. 设计选择讨论

在本节中，我们讨论了检索增强适应性的其他设计选择的影响。

### 模型架构的影响

我们进行了模型架构影响的消融研究。我们考虑了使用ResNet（RN50）和ViT（Dosovitskiy等，2021）骨干（CLIP-B/32、CLIP-B/16、CLIP-L/14）的CLIP模型，其中视觉编码器分别基于ViT-B/32和ViT-L/14。结果如图5所示。我们观察到对于不同的骨干，CLIP都呈现出类似的趋势，其中I2I检索始终优于T2I检索。特别是，较大的骨干（如CLIP-L/14）相对于较小的骨干在每类检索样本数量方面导致了整体性能的提升。

### 种子图像数量的影响

为了调查种子图像数量对I2I检索的影响，我们将每类的种子图像数量从2个调整到8个。基于Textures（K = 16）的结果如表1所示。我们可以看到增加种子图像的数量可以改善适应性性能，因为这样可以减少对有限检索样本的过拟合。类似的趋势也适用于测试套件中的其他数据集。

|种子 #|方法|ZOCLIP|RET|Ensemble|Ensemble (F)|
|---|---|---|---|---|---|
|2|42.79|38.48|51.77|57.98||
|4|42.79|44.09|52.96|58.57||
|8|42.79|45.86|55.32|62.94||

表1. 种子图像数量（每类）对I2I检索的影响。结果基于RN50骨干，K = 16。

### 使用ID和检索样本的混合适应

先前，我们仅考虑使用特征缓存中的检索样本来更好地理解检索的影响。当我们可以访问少样本（ID）训练集时，另一个实际场景是使用检索样本和ID样本的混合。结果如图6所示。我们报告了I2I检索的平均性能（跨7个数据集）（K = 16）。EN表示仅使用检索样本的logit集成。MIX表示使用ID样本和检索样本混合的logit集成。EN（F）和MIX（F）代表微调的变体。混合比例为1:1。我们观察到将ID和检索样本混合进一步提高了性能，与仅使用少样本ID样本相比。我们的观察与先前的研究结果一致（Udandarao等，2023；Zhang等，2023），不同的是在不同的logit集成方案下，强调了检索增强的少样本适应性的潜力。

图6. 使用ID数据和检索样本的混合影响。我们报告了I2I检索的平均性能（跨所有数据集）（K = 16）。EN表示仅使用检索样本的logit集成。MIX表示使用ID样本和检索样本混合的logit集成。混合比例为1:1。

### 微调特征缓存进行适应

为了完整起见，我们讨论了通过将缓存中的视觉特征K设置为经过预训练的CLIP模型初始化后的可学习参数来进行基于学习的适应性。我们将这种变体称为Ensemble(F)，其中F表示微调。我们遵循Zhang等（2022a）的超参数调整方案，并在图7中展示结果（跨所有数据集的平均值）。我们可以看到在训练为基础的适应性方面呈现出类似的趋势，其中I2I检索显著优于零样本CLIP和T2I检索。在低样本设置中（K = 1或2），性能接近理想情况（ID检索）。个别数据集的完整结果可以在附录E中看到。

图7. 使用微调特征缓存进行适应。我们观察到与无训练适应性相似的趋势。

由于空间限制，我们在附录中提供了额外的消融研究。

## 6. 相关工作

### 视觉语言模型的少样本任务适应

近年来，对于视觉语言模型的少样本任务适应变得越来越受欢迎。最近，对比语言-图像预训练（CLIP）（Radford等，2021；Jia等，2021；Yang等，2022；Li等，2022b；Mu等，2022；Yu等，2022；Zhai等，2022；Sun等，2023；Zhai等，2023；Xu等，2024）等方法备受关注。尽管类似CLIP的模型学习了对齐的多模态特征，但它们在对预训练期间不充分表示的细粒度数据集中往往表现不佳，这使得适应性变得必要。最近的研究提出各种有前途的解决方案来适应少样本的视觉语言模型，如调整文本提示（Zhou等，2022a;b）、视觉提示（Bahng等，2022；Chen等，2023a）、多模态提示（Khattak等，2023）。Zhang等（2022b）利用神经架构搜索来优化提示模块。Lu等（2022）通过学习提示分布来优化提示。另一方面，Yu等（2023）调整了额外的任务残差层。另一种方法利用适配器（Zhang等，2022a；Gao等，2023；Zhang等，2023；Udandarao等，2023），通过维护存储少样本数据特征的记忆缓存。Zhang等（2022a）使用来自目标训练集的特征缓存的加法logit集成。与此相反，我们关注检索的影响，并构建了由检索样本而不是下游数据集构建的缓存。

### CLIP的知识增强适应

任务适应的自然想法是利用外部知识源通过检索或合成。从外部数据集采样在将视觉模型适应到细粒度数据集方面表现出有希望的性能（Liu等，2022；Kim等，2023）。对于基于CLIP的适应，现有方法可根据使用的外部数据量分为两种情况。在高数据量情况下，Liu等（2023）通过首先构建一个包含从Web规模数据库中检索的相关样本的大规模数据集（10M），然后在检索的数据集上对CLIP模型进行微调，展示了有前景的零样本性能。Xie等（2023）提出了一种检索增强模块，以增强1.6M个检索样本上的CLIP预训练。最近，Iscen等（2023）提倡了单模态搜索但跨模态融合的CLIP适应，其中融合模型在1000万个样本上进行了训练。Long等（2022）展示了检索对长尾视觉识别任务的潜力。在低数据量情况下，最近的工作还利用预训练生成模型合成样本来增强检索增强流水线（Udandarao等，2023；Zhang等，2023）。除了增强视觉模态外，Shen等（2022）还利用外部文本知识源（如WordNet（Fellbaum，1998）和Wiktionary（Meyer＆Gurevych，2012））来增强具有类别特定描述的标题，而Pratt等（2023）通过查询大型语言模型来执行增强。El Banani等（2023）利用语言指导来找到类似的视觉最近邻居。Li等（2022a）建立了一个评估语言增强视觉模型的迁移学习性能的基准。在这项工作中，我们采用了一种反思性的视角，并提供了一个系统的研究来理解在低数据情况下检索增强适应的原理，并建立了新的理论见解。
### 多模态学习的理论理解

一些工作为多模态学习提供了理论解释（Zadeh等，2020；Huang等，2021；Furst等，2022；Chen等，2023b）。对于CLIP模型，Liang等（2022）展示并系统分析了两种模态特征之间的模态差距。Nakada等（2023）建立了CLIP与奇异值分解（SVD）之间的联系，在线性表示下。Chen等（2023b）开发了一个理论框架来理解CLIP的零样本转移机制。与先前的工作不同，我们关注理解检索增强任务适应的理论理解。

## 7. 结论

在本工作中，我们对低数据情况下的视觉语言模型的检索增强适应进行了及时和系统的调查。我们的工作提供了更细粒度的实证研究，揭示了跨模态和单模态检索的影响。此外，我们强调了logit集成的必要性。我们还开发了一个支持我们实证发现的新的理论框架，并提供了对检索增强适应的更深入理解。此外，我们的全面消融研究探讨了检索增强流水线中的各种设计选择。我们希望我们的工作能够成为未来关于算法设计和理论理解的有效适应视觉语言模型的研究的跳板。