---
Publish: arXiv2024
Year: "2024"
Month: "04"
Url: https://arxiv.org/pdf/2404.02905.pdf
Code: https://github.com/FoundationVision/VAR
Stars: "2900"
---
- 可执行 in-painting, out-painting, class-conditional image editing 任务
	- in-painting: 填充矩形框内的图像。GT 为矩形框内的原始图像。
	- out-painting: 填充矩形框外的图像。GT 为矩形框外的原始图像。
	- class-conditional image editing: 填充矩形框内的图像。填充某个类别的内容。

##  视觉自回归建模：通过下一尺度预测实现可扩展图像生成

**摘要**

我们提出了一种新的图像生成范式——视觉自回归建模  (VAR)。它将图像上的自回归学习重新定义为从粗到细的“下一尺度预测”或“下一分辨率预测”，区别于标准的逐行扫描“下一标记预测”。这种简单直观的学习方法让自回归  (AR)  变换器可以快速学习视觉分布并良好泛化：VAR  首次使  GPT  风格的  AR  模型在图像生成方面超越了扩散变换器。在  ImageNet  256×256  基准测试中，VAR  显着改善了  AR  基线，将  Fréchet  初始距离  (FID)  从  18.65  提升到  1.80，将初始分数  (IS)  从  80.4  提升到  356.4，推理速度提升了  20  倍。实验证明，VAR  在图像质量、推理速度、数据效率和可扩展性等多个方面都优于扩散变换器  (DiT)。扩大  VAR  模型展现出清晰的幂律缩放规律，类似于大型语言模型  (LLM)  中观察到的规律，线性相关系数接近  -0.998，提供了强有力的证据。VAR  进一步展示了在图像修复、图像扩展和图像编辑等下游任务中的零样本泛化能力。这些结果表明  VAR  初步模拟了  LLM  的两个重要属性：缩放规律和零样本泛化。我们已发布所有模型和代码，以促进  AR/VAR  模型在视觉生成和统一学习方面的探索。

**1  引言**

GPT  系列  [48,  49,  9,  45,  1]  和其他自回归  (AR)  大型语言模型  (LLM)  [13,  3,  27,  63,  64,  70,  60,  4,  61]  的出现预示着人工智能领域新时代的到来。尽管存在诸如幻觉  [28]  之类的问题，但这些模型在通用性和多功能性方面展现出有希望的智能，仍然被认为是朝着通用人工智能  (AGI)  迈出了坚实的一步。这些大型模型背后的关键是一种自监督学习策略——预测序列中的下一个标记，这是一种简单而深刻的方法。对这些大型  AR  模型成功原因的研究突出了它们的可扩展性和泛化性：前者以缩放规律  [30,  22]  为例，允许我们从较小的模型预测大型模型的性能，从而指导更好的资源分配；而后者以零样本和少样本学习  [49,  9]  为例，强调了无监督训练模型对多样化、未见过任务的适应性。这些属性揭示了  AR  模型在从大量未标记数据中学习方面的潜力，encapsulating  the  essence  of  “AGI”。

与此同时，计算机视觉领域一直在努力开发大型自回归或世界模型  [42,  68,  5]，旨在模拟其令人印象深刻的可扩展性和泛化性。VQGAN  和  DALL-E  [19,  51]  及其后续模型  [52,  71,  37,  77]  等开创性工作展示了  AR  模型在图像生成方面的潜力。这些模型利用视觉标记器将连续图像离散化为  2D  标记网格，然后将其展平为  1D  序列用于  AR  学习（图  2  b），这反映了顺序语言建模的过程（图  2  a）。然而，这些模型的缩放规律仍未得到充分探索，更令人沮丧的是，它们的性能远远落后于扩散模型  [46,  2,  38]，如图  3  所示。与  LLM  取得的显著成就相比，自回归模型在计算机视觉中的力量似乎有些受限。

自回归建模需要定义数据的顺序。我们的工作重新考虑了如何“排序”图像。人类通常以分层的方式感知或创建图像，首先捕捉全局结构，然后捕捉局部细节。这种多尺度、从粗到细的方法自然地暗示了图像的“顺序”。也受到广泛的多尺度设计  [40,  39,  62,  31,  33]  的启发，我们在图  2  (c)  中将图像的自回归学习定义为“下一尺度预测”，区别于图  2  (b)  中传统的“下一标记预测”。我们的方法首先将图像编码为多尺度标记图。然后从  1×1  标记图开始自回归过程，并逐步扩展分辨率：在每个步骤中，变换器根据所有先前的标记图预测下一个更高分辨率的标记图。我们将此方法称为视觉自回归  (VAR)  建模。

VAR  直接利用  GPT-2  类似的变换器架构  [49]  进行视觉自回归学习。在  ImageNet  256×256  基准测试中，VAR  显着改善了其  AR  基线，实现了  1.80  的  Fréchet  初始距离  (FID)  和  356.4  的初始分数  (IS)，推理速度提高了  20  倍（详见第  4.4  节）。值得注意的是，VAR  在  FID/IS、数据效率、推理速度和可扩展性方面都优于扩散变换器  (DiT)——Stable  Diffusion  3.0  和  SORA  [18,  8]  等领先扩散系统的基础。VAR  模型还展现出类似于  LLM  中观察到的缩放规律。最后，我们展示了  VAR  在图像修复、图像扩展和图像编辑等任务中的零样本泛化能力。总之，我们对社区的贡献包括：

1.  使用具有下一尺度预测的多尺度自回归范式的新视觉生成框架，为计算机视觉的自回归算法设计提供了新的见解。
        
2.  对  VAR  模型缩放规律和零样本泛化潜力的经验验证，这初步模拟了大型语言模型  (LLM)  的吸引力属性。
        
3.  视觉自回归模型性能的突破，使  GPT  风格的自回归方法首次在图像合成方面超越了强大的扩散模型2。
        
4.  全面的开源代码套件，包括  VQ  标记器和自回归模型训练流水线，以帮助推动视觉自回归学习的进步。
        

**2  相关工作**

**2.1  大型自回归语言模型的属性**

**缩放规律。**  幂律缩放规律  [22,  30]  以数学方式描述了模型参数、数据集大小、计算资源的增长与机器学习模型性能提升之间的关系，并带来了几个独特的优势。首先，它们便于通过增加模型大小、数据大小和计算成本来推断更大模型的性能。这有助于节省不必要的成本，并提供分配培训预算的原则。其次，缩放规律证明了性能的持续且非饱和的增长，证实了它们在增强模型能力方面的持续优势。在神经语言模型  [30]  中缩放规律原理的推动下，已经提出了几个大型语言模型  [9,  76,  70,  27,  63,  64]，体现了增加模型规模往往会产生更好的性能结果的原则。基于变换器解码器架构的  GPT  [49,  9]  经历了生成式预训练，并将模型大小扩展到前所未有的  175B  个参数。Llama  [63,  64]  发布了一系列预训练和微调的大型语言模型  (LLM)，规模从  70  亿到  700  亿个参数不等。将缩放规律应用于语言模型的明显功效让我们得以一窥视觉模型  [5]  中放大规模的巨大潜力。

**零样本泛化。**  零样本泛化  [55]  是指模型（尤其是大型语言模型）执行其未经明确训练的任务的能力。在视觉界，人们对基础模型、CLIP  [47]、SAM  [35]、Dinov2  [44]  的零样本和上下文学习能力越来越感兴趣。Painter  [69]  和  LVM  [5]  等创新利用视觉提示设计上下文学习范式，从而促进对下游未见过任务的泛化。

**2.2  视觉生成**

**图像标记器和自回归模型。**  语言模型  [15,  49,  63]  依赖于字节对编码  (BPE  [20])  或  WordPiece  算法进行文本标记化。基于语言模型的视觉生成模型也需要将  2D  图像编码为  1D  标记序列。早期的  VQVAE  [65]  已经证明了将图像表示为离散标记的能力，尽管重建质量相对一般。VQGAN  [19]  通过结合对抗性损失和感知损失来提高图像保真度，并采用仅解码器变换器以标准的逐行扫描自回归方式生成图像标记，从而改进了  VQVAE。VQVAE-2  [52]  和  RQ-Transformer  [37]  也遵循  VQGAN  的逐行扫描自回归方法，但通过额外的尺度或堆叠代码进一步改进了  VQVAE。Parti  [72]  利用  ViT-VQGAN  [71]  的基础架构将变换器模型大小扩展到  200  亿个参数，在文本到图像合成方面取得了显著成果。

**掩码预测模型。**  MaskGIT  [11]  采用掩码预测框架  [15,  6,  21]  以及  VQ  自动编码器，通过“贪婪”算法生成图像标记。MagViT  [73]  将这种方法应用于视频数据，而  MagViT-2  [74]  通过引入改进的  VQVAE  增强了  MaskGIT。MUSE  [10]  将  MaskGIT  的架构扩展到  30  亿个参数，并将其与  T5  语言模型  [50]  合并，在文本到图像合成方面树立了新的基准。

**扩散模型**  [23,  59,  16,  53]  被认为是视觉合成的前沿，因为它们具有卓越的生成质量和多样性。扩散模型的进展主要集中在改进的采样技术  [26]、更快的采样  [58,  41]  和架构增强  [53,  24,  54,  46]。Imagen  [54]  结合了  T5  语言模型  [50]  进行文本条件设置，并通过多个独立的扩散模型构建图像生成系统，用于级联生成和超分辨率。潜在扩散模型  (LDM)  [53]  在潜在空间中应用扩散，提高了训练和推理的效率。DiT  [46]  用基于变换器的架构  [66,  17]  替代了传统的  U-Net，并用于最近的图像或视频合成系统，如  Stable  Diffusion  3.0  [18]  和  SORA  [8]。

**3  方法**

**3.1  预备知识：通过下一标记预测进行自回归建模**

**公式。**  考虑一个离散标记序列  $x  =  (x_1,  x_2,  ...,  x_T)$，其中每个标记  $x_t  ∈  [V]$  都是来自大小为  $V$  的词汇表的整数。下一标记自回归模型假设观察当前标记  $x_t$  的概率仅取决于其前缀  $(x_1,  x_2,  ...,  x_{t-1})$。这种单向标记依赖假设允许我们将序列  $x$  的似然分解为  $T$  个条件概率的乘积：

(1)

训练一个由  $θ$  参数化的自回归模型  $p_θ$  涉及优化数据集上的  $p_θ(x_t  |  x_1,  x_2,  ...,  x_{t-1})$。这个优化过程称为“下一标记预测”，训练好的  $p_θ$  可以生成新的序列。

**标记化。**  图像本质上是  2D  连续信号。要通过下一标记预测将自回归建模应用于图像，我们必须：1)  将图像标记为几个离散标记，以及  2)  为单向建模定义  1D  标记顺序。对于  1)，通常使用量化自动编码器（例如  [19]）将图像特征图  $f  ∈  R^{h×w×C}$  转换为离散标记  $q  ∈  [V]^{h×w}$：

(2)

其中  $im$  表示原始图像，$E(·)$  表示编码器，$Q(·)$  表示量化器。量化器通常包含一个可学习的码本  $Z  ∈  R^{V×C}$，其中包含  $V$  个向量。量化过程  $q  =  Q(f)$  会将每个特征向量  $f_{(i,j)}$  映射到其欧几里得距离最近的码的码索引  $q_{(i,j)}$：

(3)

其中  $lookup(Z,  v)$  表示在码本  $Z$  中取第  $v$  个向量。为了训练量化自动编码器，每个  $q_{(i,j)}$  都通过  $lookup$  获取  $Z$，得到  $\hat{f}$，即原始  $f$  的近似值。然后使用给定  $\hat{f}$  的解码器  $D(·)$  重建新图像  $\hat{im}$，并最小化复合损失  $L$：

(4)

(5)

其中  $L_P(·)$  是感知损失（例如  LPIPS  [75]），$L_G(·)$  是判别损失（例如  StyleGAN  的判别器损失  [33]），$λ_P,  λ_G$  是损失权重。一旦自动编码器  ${E,  Q,  D}$  完全训练好，它将用于对图像进行标记化，以便随后训练单向自回归模型。

$q  ∈  [V]^{h×w}$  中的图像标记排列在  2D  网格中。与具有固有从左到右排序的自然语言句子不同，图像标记的顺序必须明确定义用于单向自回归学习。以前的  AR  方法  [19,  71,  37]  使用某种策略（例如行优先的逐行扫描、螺旋或  z  曲线顺序）将  $q$  的  2D  网格展平为  1D  序列  $x  =  (x_1,  ...,  x_{h×w})$。一旦展平，他们就可以从数据集中提取一组序列  $x$，然后训练自回归模型通过下一标记预测最大化  (1)  中的似然。

**讨论。**  上述标记化和展平操作可以通过下一标记自回归学习来实现图像生成，但它们也引入了一些问题：

1.  **违反数学前提。**  图像编码器通常会产生图像特征图  $f$，其中包含所有  $i,  j$  的相互依赖的特征向量  $f_{(i,j)}$。因此，在量化和展平之后，标记序列  $(x_1,  x_2,  ...,  x_{h×w})$  呈现出双向相关性。这与自回归模型的单向依赖假设相矛盾，后者规定每个标记  $x_t$  只能依赖于其前缀  $(x_1,  x_2,  ...,  x_{t-1})$。
        
2.  **结构退化。**  展平操作破坏了图像特征图中固有的空间局部性。例如，标记  $q_{(i,j)}$  及其  4  个紧邻  $q_{(i±1,j)},  q_{(i,j±1)}$  由于其接近性而密切相关。这种空间关系在线性序列  $x$  中被破坏，其中单向约束减少了这些相关性。
        
3.  **低效。**  使用传统的自注意力变换器生成图像标记序列  $x  =  (x_1,  x_2,  ...,  x_{n×n})$  会导致  $O(n^2)$  个自回归步骤和  $O(n^6)$  的计算成本。
        

空间局部性的破坏（问题  2）是显而易见的。关于问题  1，我们在附录中提供了经验证据，分析了流行的量化自动编码器  [19]  中的标记依赖性，并揭示了显着的双向相关性。问题  3  的计算复杂度证明在附录中有详细说明。这些理论和实践上的限制要求重新思考自回归模型在图像生成中的应用。

**3.2  通过下一尺度预测进行视觉自回归建模**

**重新公式化。**  我们通过将“下一标记预测”策略转变为“下一尺度预测”策略来重新概念化图像上的自回归建模。在这里，自回归单元是整个标记图，而不是单个标记。我们首先将特征图  $f  ∈  R^{h×w×C}$  量化为  $K$  个多尺度标记图  $(r_1,  r_2,  ...,  r_K)$，每个标记图的分辨率  $h_k  ×  w_k$  逐渐提高，最终  $r_K$  与原始特征图的分辨率  $h  ×  w$  匹配。自回归似然公式如下：

(6)

其中每个自回归单元  $r_k  ∈  [V]^{h_k×w_k}$  是尺度  $k$  上的标记图，序列  $(r_1,  r_2,  ...,  r_{k-1})$  充当  $r_k$  的“前缀”。在第  $k$  个自回归步骤中，$r_k$  中  $h_k  ×  w_k$  个标记上的所有分布都是相互依赖的，并将根据  $r_k$  的前缀和相关的第  $k$  个位置嵌入图并行生成。这种“下一尺度预测”方法就是我们定义的视觉自回归  (VAR)  建模，如图  4  右侧所示。

**讨论。**  VAR  解决了前面提到的三个问题，如下所示：

1.  如果我们限制每个  $r_k$  仅依赖于其前缀，即获取  $r_k$  的过程仅与  $(r_1,  r_2,  ...,  r_{k-1})$  相关，则满足数学前提。这个约束是可以接受的，因为它与自然、从粗到细的进程特征（如人类视觉感知和艺术绘画）一致。更多细节将在下面的标记化部分提供。
        
2.  保留了空间局部性，因为  (i)  VAR  中没有展平操作，以及  (ii)  每个  $r_k$  中的标记完全相关。多尺度设计进一步加强了空间结构。
        
3.  生成具有  $n  ×  n$  潜在空间的图像的复杂度显着降低到  $O(n^4)$，证明见附录。这种效率的提高得益于每个  $r_k$  中的并行标记生成。
        

**标记化。**  我们开发了一种新的多尺度量化自动编码器，将图像编码为  VAR  学习  (6)  所需的  $K$  个多尺度离散标记图  $R  =  (r_1,  r_2,  ...,  r_K)$。我们采用与  VQGAN  [19]  相同的架构，但修改了多尺度量化层。算法  1  和  2  详细介绍了对  $f$  或  $\hat{f}$  进行残差设计的编码和解码过程。我们根据经验发现这种类似于  [37]  的残差风格设计可以比独立插值表现更好。算法  1  显示每个  $r_k$  仅依赖于其前缀  $(r_1,  r_2,  ...,  r_{k-1})$。请注意，所有尺度都使用共享码本  $Z$，以确保每个  $r_k$  的标记都属于相同的词汇表  $[V]$。为了解决将  $z_k$  上采样到  $h_K  ×  w_K$  时信息丢失的问题，我们使用  $K$  个额外的卷积层  ${ϕ_k}_{k=1}^K$。将  $f$  下采样到  $h_k  ×  w_k$  后，不使用卷积。

**算法  1：多尺度  VQVAE  编码**

1.  输入：原始图像  $im$；
        
2.  超参数：步骤  $K$、分辨率  $(h_k,  w_k)_{k=1}^K$；
        
3.  $f  =  E(im),  R  =  []$；
        
4.  对于  $k  =  1,  ·  ·  ·  ,  K$  执行：
        
5.  $r_k  =  Q(interpolate(f,  h_k,  w_k))$；
        
6.  $R  =  queue_push(R,  r_k)$；
        
7.  $z_k  =  lookup(Z,  r_k)$；
        
8.  $z_k  =  interpolate(z_k,  h_K,  w_K)$；
        
9.  $f  =  f  -  ϕ_k(z_k)$；
        
10.  返回：多尺度标记  $R$；
        

**算法  2：多尺度  VQVAE  重建**

1.  输入：多尺度标记图  $R$；
        
2.  超参数：步骤  $K$、分辨率  $(h_k,  w_k)_{k=1}^K$；
        
3.  $\hat{f}  =  0$；
        
4.  对于  $k  =  1,  ·  ·  ·  ,  K$  执行：
        
5.  $r_k  =  queue_pop(R)$；
        
6.  $z_k  =  lookup(Z,  r_k)$；
        
7.  $z_k  =  interpolate(z_k,  h_K,  w_K)$；
        
8.  $\hat{f}  =  \hat{f}  +  ϕ_k(z_k)$；
        
9.  $\hat{im}  =  D(\hat{f})$；
        
10.  返回：重建图像  $\hat{im}$；
        

**3.3  实现细节**

**VAR  标记器。**  如前所述，我们使用具有多尺度量化方案的  vanilla  VQVAE  架构  [19]，并带有  $K$  个额外的卷积（0.03M  个额外参数）。我们在所有尺度上使用共享码本，其中  $V  =  4096$，潜在维度为  32。遵循基线  [19]，我们的标记器也使用复合损失  (5)  在  OpenImages  [36]  上进行训练。有关更多详细信息，请参阅附录。

**VAR  变换器。**  我们的主要重点是  VAR  算法，因此我们保持简单的模型架构设计。我们采用类似于  GPT-2  和  VQ-GAN  [49,  19]  的标准仅解码器变换器的架构，唯一的修改是用自适应归一化  (AdaLN)  替换传统的层归一化——这种选择是由于其广泛采用以及在视觉生成模型  [33,  34,  32,  57,  56,  29,  46,  12]  中的有效性而得到证实的。对于类条件合成，我们使用类嵌入作为起始标记  $[s]$  以及  AdaLN  的条件。我们不使用现代大型语言模型中的高级技术，例如旋转位置嵌入  (RoPE)、SwiGLU  MLP  或  RMS  归一化  [63,  64]。我们的模型形状超参数遵循一个简单的规则  [30]，即宽度  $w$、头部数量  $h$  和丢弃率  $dr$  与深度  $d$  线性缩放，如下所示：

(7)

因此，深度为  $d$  的  VAR  变换器的主要参数数量  $N$  由下式给出3：

(8)

所有模型都使用类似的设置进行训练：每个  256  批量大小的基本学习率为  $10^{-4}$，AdamW  优化器，$β_1  =  0.9,  β_2  =  0.95,  decay  =  0.05$，批量大小为  512  到  1024，训练时期为  200  到  350（取决于模型大小）。我们在第  4  节中的后续评估将表明，这种简单的模型设计能够很好地扩展和泛化。

**4  实验结果**

本节首先在第  4.1  节中比较  VAR  与其他图像生成模型系列4  在性能和效率方面的表现。VAR  模型的可扩展性和泛化性的评估将在第  4.2  节和第  4.3  节中介绍。最后，我们将进行一些消融和可视化。

**4.1  最先进的图像生成**

**设置。**  我们在  ImageNet  256×256  和  512×512  条件生成基准测试中测试了深度为  16、20、24  和  30  的  VAR  模型，并将其与最先进的图像生成模型系列进行比较。在所有基于  VQVAE  的  AR  或  VAR  模型中，VQGAN  [19]  和我们的模型使用相同的架构  (CNN)  和训练数据  (OpenImages  [36])  用于  VQVAE，而  ViT-VQGAN  [71]  使用  ViT  自动编码器，它和  RQTransformer  [37]  都直接在  ImageNet  上训练  VQVAE。结果总结在表  1  和表  2  中。

**总体比较。**  与现有的生成方法（包括生成对抗网络  (GAN)、扩散模型  (Diff.)、BERT  风格的掩码预测模型  (Mask.)  和  GPT  风格的自回归模型  (AR)）相比，我们的视觉自回归  (VAR)  建立了一个新的模型类别。如表  1  所示，VAR  不仅获得了最佳的  FID/IS，而且在图像生成速度方面也表现出色。VAR  还保持了良好的精度和召回率，证实了其语义一致性。这些优势在  512×512  合成基准测试中也成立，如表  2  所示。值得注意的是，VAR  显着提升了传统  AR  的能力。据我们所知，这是自回归模型首次在性能上超越扩散变换器，这是  VAR  解决第  3  节中讨论的  AR  限制所取得的里程碑。

**效率比较。**  传统的自回归  (AR)  模型  [19,  52,  71,  37]  受限于高计算成本，因为图像标记的数量与图像分辨率成二次方关系。完整自回归生成  $n^2$  个标记需要  $O(n^2)$  次解码迭代和  $O(n^6)$  次总计算。相比之下，VAR  仅需要  $O(\log(n))$  次迭代和  $O(n^4)$  次总计算。表  1  中报告的挂钟时间也提供了经验证据，表明即使有更多模型参数，VAR  也比  VQGAN  和  ViT-VQGAN  快大约  20  倍，达到了仅需  1  步即可生成图像的高效  GAN  模型的速度。

**与流行的扩散变换器比较。5**  VAR  模型在多个方面都超越了最近流行的扩散模型扩散变换器  (DiT)，后者是最新  Stable-Diffusion  3  [18]  和  SORA  [8]  的前身：1)  在图像生成多样性和质量  (FID  和  IS)  方面，具有  2B  参数的  VAR  始终优于  DiT-XL/2  [46]、L-DiT-3B  和  L-DiT-7B  [2]。VAR  还保持了相当的精度和召回率。2)  对于推理速度，DiT-XL/2  需要  VAR  45  倍的挂钟时间，而  3B  和  7B  模型  [2]  的成本会更高。3)  VAR  被认为更具数据效率，因为它只需要  350  个训练时期，而  DiT-XL/2  需要  1400  个训练时期。4)  对于可扩展性，图  3  和表  1  显示  DiT  在超过  675M  参数后仅获得边际收益甚至负收益。相比之下，VAR  的  FID  和  IS  持续改进，这与第  4.2  节中的缩放规律研究一致。这些结果表明，与  DiT  等模型相比，VAR  是一种更高效、更可扩展的图像生成模型。

**4.2  幂律缩放规律**

**背景。**  先前的研究  [30,  22,  27,  1]  已经确定，扩大自回归  (AR)  大型语言模型  (LLM)  会导致测试损失  $L$  的可预测减少。这种趋势与参数数量  $N$、训练标记  $T$  和最佳训练计算  $C_{min}$  相关，遵循幂律：

(9)

其中  $X$  可以是  $N$、$T$  或  $C_{min}$  中的任何一个。指数  $α$  反映了幂律的平滑度，$L$  表示由不可约损失  $L_∞$  [22]6  归一化的可约损失。对  $L$  和  $X$  进行对数变换将揭示  $\log(L)$  和  $\log(X)$  之间的线性关系：

(10)

这些观察到的缩放规律  [30,  22,  27,  1]  不仅验证了  LLM  的可扩展性，而且还为  AR  建模提供了预测工具，这便于根据较小的  AR  模型估计较大  AR  模型的性能，从而通过大型模型性能预测来节省资源使用。鉴于  LLM  带来的缩放规律的这些吸引人的属性，因此在计算机视觉中复制它们具有重要意义。

**缩放  VAR  模型的设置。**  遵循  [30,  22,  27,  1]  中的协议，我们研究了我们的  VAR  模型是否符合类似的缩放规律。我们在包含  128  万张图像（或在我们的  VQVAE  下为  870B  个图像标记）的  ImageNet  训练集  [14]  上训练了  12  种不同大小的模型，参数范围从  18M  到  2B。对于不同大小的模型，训练跨度为  200  到  350  个时期，最大标记数量达到  3050  亿。下面我们将重点关注模型参数  $N$  和最佳训练计算  $C_{min}$（给定足够的标记数量  $T$）的缩放规律。

**模型参数  $N$  的缩放规律。**  我们首先研究  VAR  模型大小增加时测试损失的趋势。深度为  $d$  的  VAR  变换器的参数数量  $N(d)  =  73728d^3$  在  (8)  中指定。我们将  $d$  从  6  变化到  30，产生了  12  个模型，参数范围为  18.5M  到  2.0B。我们评估了  ImageNet  50,000  张图像  [14]  的验证集上的最终测试交叉熵损失  $L$  和标记预测错误率  $Err$。我们计算了最后一个尺度（在最后一个下一尺度自回归步骤）以及全局平均值的  $L$  和  $Err$。结果绘制在图  5  中，我们观察到  $L$  作为  $N$  的函数存在明显的幂律缩放趋势，这与  [30,  22,  27,  1]  一致。幂律缩放规律可以表示为：

(11)

虽然缩放规律主要研究的是测试损失，但我们也根据经验观察到标记错误率  $Err$  也有类似的幂律趋势：

(12)

这些结果验证了  VAR  的强可扩展性，通过扩大  VAR  变换器可以不断提高模型的测试性能。

**最佳训练计算  $C_{min}$  的缩放规律。**  然后，我们检查  VAR  变换器在增加训练计算  $C$  时的缩放行为。对于  12  个模型中的每一个，我们跟踪了测试损失  $L$  和标记错误率  $Err$  作为  $C$  的函数，以  PFlops（每秒  $10^{15}$  次浮点运算）为单位。结果绘制在图  6  中。这里，我们绘制了  $L$  和  $Err$  的帕累托前沿，以突出显示达到一定损失或错误值所需的最佳训练计算  $C_{min}$。

$L$  和  $Err$  作为  $C_{min}$  函数的拟合幂律缩放规律为：

(13)

(14)

这些关系  (13,  14)  在  $C_{min}$  的  6  个数量级上成立，我们的发现与  [30,  22]  中的发现一致：当使用足够的数据进行训练时，更大的  VAR  变换器更具计算效率，因为它们可以使用更少的计算达到相同的性能水平。

**可视化。**  为了更好地理解  VAR  模型在放大时是如何学习的，我们在图  7  中比较了来自  4  种不同大小（深度  6、16、26、30）和  3  个不同训练阶段（总训练标记的  20%、60%、100%）的  VAR  模型生成的一些  256×256  样本。为了保持内容一致，使用了相同的随机种子和教师强制的初始标记。观察到的视觉保真度和合理性的改进与缩放规律一致，因为更大的变换器被认为能够学习更复杂和更细粒度的图像分布。

**4.3  零样本任务泛化**

**图像修复（image in-painting）和图像扩展（image out-painting）。**  测试了  VAR-d30。对于修复和扩展，我们在掩码外强制使用真实标记，并让模型仅生成掩码内的标记。没有将类标签信息注入模型。结果可视化在图  8  中。在没有修改网络架构或调整参数的情况下，VAR  在这些下游任务上取得了不错的成绩，证实了  VAR  的泛化能力。

**类条件图像编辑。**  继  MaskGIT  [11]  之后，我们还测试了  VAR  在类条件图像编辑任务上的表现。与修复的情况类似，模型被强制仅在边界框内生成标记，并以某个类标签为条件。图  8  显示模型可以生成合理的内容，并很好地融合到周围的上下文中，再次验证了  VAR  的通用性。

**4.4  消融研究**

在本研究中，我们旨在验证我们提出的  VAR  框架的有效性和效率。结果报告在表  3  中。