---
Url: https://arxiv.org/pdf/2312.14733v1.pdf
Year: "2023"
Month: "12"
代码: https://github.com/fudan-zvg/meta-prompts
Stars: "58"
---


## 摘要

生成式预训练在视觉模型中的问题一直存在着长期的难题。目前，文本到图像（T2I）扩散模型展现出了在生成与文本输入匹配的高清晰度图像方面的非凡熟练度，这一成就是通过其在大规模图像文本对上的预训练实现的。这引发了一个自然的疑问：扩散模型是否可以用于处理视觉感知任务？在本文中，我们提出了一种简单而有效的方案，利用扩散模型进行视觉感知任务。我们的关键洞察是向预训练的扩散模型引入可学习的嵌入（元提示），以提取适合感知的特征。元提示的效果是双重的。首先，作为T2I模型中文本嵌入的直接替代，它可以在特征提取过程中激活与任务相关的特征。其次，它将被用于重新排列提取的特征，以确保模型专注于任务中最相关的特征。此外，我们设计了一个全面利用扩散模型特性的递归细化训练策略，从而产生更强大的视觉特征。对各种基准测试的广泛实验验证了我们方法的有效性。我们的方法在NYU深度V2和KITTI的深度估计任务中实现了新的性能记录，在CityScapes的语义分割任务中也取得了成绩。同时，所提出的方法在ADE20K的语义分割和COCO数据集上的姿势估计方面达到了与当前最先进方法相媲美的结果，进一步证明了其鲁棒性和多功能性。

## 引言

在不断发展的计算机视觉领域中，从文本描述生成图像已经取得了显著进展，这要归功于创新型模型，比如文本到图像扩散模型。这些模型能够仅仅依靠文本信息生成详细的图像，这一壮举得以实现归功于对大量图像文本对数据集的严格训练。尽管这种生成任务可能令人印象深刻，但这些模型的潜力远不止于此。一些开创性的工作，比如[3, 9, 17, 20]，已经有效地利用了扩散模型方法来处理视觉感知任务。这些创新性方法将密集预测任务转化为一个从扩散开始并以连续去噪结束的过程。值得注意的是，这些方法通常依赖于视觉感知任务中普遍存在的传统骨干网络，从输入图像中提取特征以增强后续去噪过程。

然而，在各种方法的范畴中，另一种新兴趋势显著突出：将文本到图像扩散模型用作特征提取器。然而，将文本到图像扩散模型调整为视觉感知任务仍然是一个重大挑战，特别是当提示界面需要复杂的适应技术时。现有方法涉及在文本格式中利用数据集类别标签[50]或诉诸于额外模型如BLIP[21]来生成图像标题[19]，将类别标签或图像标题输入文本编码器以获得文本嵌入。然而，这些优秀的工作可能也存在某些缺点。由于依赖数据集标签，使用类别标签作为提示在基准评估中存在不足，因此对于无标签的数据集不适用。使用图像标题作为提示的局限性在于生成的标题可能与下游任务不一致。此外，这种标题生成方法依赖于外部训练的标题生成模型，增加了训练和推断成本。尽管这些方法有效，但仍引发了一个问题：是否可以有一种更简洁、更高效的适应方法？

在本文中，我们介绍了具有元提示的感知扩散模型，如图2所示，这是一种简洁有效的利用扩散模型的原始力量进行视觉感知的方法。我们不依赖于额外的多模态模型生成图像标题，也不使用包含在数据集中的类别标签，而是采用可学习的嵌入，命名为元提示。不同提示界面之间的主要区别如图1所示。这些元提示可以根据目标任务和数据集进行端到端训练，为去噪UNet创建一个适应性条件，以此来激活与特定感知任务相关的特征。此外，元提示还被用于进一步通过点乘对去噪UNet生成的分层金字塔特征进行重新排列。这种重新排列的过滤确保模型专注于任务中最相关的特征，增强了各种视觉感知应用中的准确性和有效性。在这些提示引导的特征重新排列之后，然后使用特定的解码器产生最终的预测结果。此外，与传统感知模型骨干类似的方式不同，我们受到了扩散模型在生成任务中的应用的启发，并探索了去噪UNet逐步细化训练策略。随着模型通过细化周期的进展，输入特征的分布会发生变化，但UNet的参数保持不变。我们通过在每个步骤引入独特的时间步嵌入来调节UNet的参数，以解决这种不一致性。跨多个基准测试的实验结果验证了我们具有元提示的感知扩散模型的有效性。

我们做出了以下贡献：（i）我们提出了一种简化且具有影响力的适应策略，利用有限的元提示对扩散模型进行视觉感知任务的微调。（ii）我们开发了一种递归细化训练方法，充分利用了UNet具有相同输入和输出形状的特点，从而生成增强的视觉特征。（iii）我们的方法在NYU深度V2和KITTI数据集上建立了深度估计的新性能基准，在CityScapes上实现了语义分割的新成绩。它还在ADE20K的语义分割和COCO数据集上取得了与当前最先进技术相媲美的结果，进一步展示了其鲁棒性和多功能性。

## 相关工作

【扩散模型】扩散模型已经在图像合成中确立了它们的重要地位。它们架构的核心是有意向数据引入噪声，然后通过迭代过程逆向恢复原始数据。这种去噪过程为各种应用提供了一种新的范式，最突出的是生成高质量样本。它们的优势在于能够建模复杂的数据分布，但它们经常因其计算强度和漫长的训练过程而受到批评。在传统扩散模型的基础上，潜在扩散模型通过在潜在空间中操作引入了更高的复杂性和灵活性。这种配置允许更大程度地操纵数据，通常产生了更好的样本质量和更精确的重构。通过在潜在空间中工作，这些模型可以更有效地利用数据中的潜在模式和关系。此外，它们还通过交叉注意机制创新地引入了条件。这些进展使得可以在像LAION-5B这样的大规模图像文本对数据集上开发文本到图像扩散模型。利用文本到图像扩散模型中的内在知识来增强视觉感知任务具有重要的前景。

视觉感知扩散模型已经做出了许多创新和有影响力的努力，旨在将扩散模型在生成任务中的显著成就转化为视觉感知任务的复杂领域。DDP提出了一个利用条件扩散过程进行视觉感知任务的框架，并提供了动态的、具有不确定性感知的推断，无需进行特定于任务的架构修改。在DiffusionDet中，他们将目标检测视为一个去噪过程，增强了在框检测和迭代细化中的灵活性。DiffusionDepth通过潜在空间中的去噪扩散过程靠近单目深度估计，通过迭代地精炼深度预测来获得更高的准确性和细节。这项研究通过扩散模型增强了语义分割，突出了对何处引入噪声以及简化去噪方法的好处。这些技术重新解释了某些感知挑战，通过扩散和去噪过程塑造了对分割遮罩、深度图和边界框等预测目标的建模。尽管将扩散模型调整为感知任务已经取得了令人鼓舞的结果，但值得注意的是，它们显著依赖于来自传统感知任务的预训练判别模型，这些模型被用作图像编码器来促进建模过程。另一种新兴方法是直接将文本到图像扩散模型用作图像编码器。为了更好地将文本到图像扩散模型调整到下游感知任务，这些方法利用数据集类别标签或图像标题作为文本提示。然后，它们利用文本编码器生成文本嵌入，赋予它们适应于当前感知任务和数据集的能力。与这些依赖于额外多模态模型生成图像标题或使用包含在数据集中的类别标签的创新性工作不同，我们采用了可学习的嵌入，命名为元提示，来利用扩散模型的原始力量进行视觉感知。
