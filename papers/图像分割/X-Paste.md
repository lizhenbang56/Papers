---
Title: "X-Paste: Revisiting Scalable Copy-Paste for Instance Segmentation using CLIP and StableDiffusion"
Url: https://arxiv.org/pdf/2212.03863.pdf
Year: "2022"
Month: "12"
---
## X-Paste：使用 CLIP 和 StableDiffusion 重温可扩展的复制粘贴实例分割

赵汉青 1 盛典谟 1 包建民 2 陈冬冬 2 陈栋 2 文方 2 袁路 2**

**刘策 2 周文博 1 褚琦 1 张伟明 1 余能海 1**

## 摘要

复制粘贴是一种简单有效的实例分割数据增强策略。通过将对象实例随机粘贴到新的背景图像上，它可以免费创建新的训练数据，并显着提高分割性能，尤其是对于罕见的对象类别。虽然多样化、高质量的对象实例在复制粘贴中可以带来更多的性能提升，但以前的工作要么利用人工标注的实例分割数据集中的对象实例，要么从 3D 对象模型中渲染，这两种方法的扩展成本都太高，无法获得良好的多样性。在本文中，我们借助新兴的零样本识别模型（例如 CLIP）和文本到图像模型（例如 StableDiffusion）的力量，重新审视了可扩展的复制粘贴。我们首次证明，使用文本到图像模型生成图像或使用零样本识别模型过滤不同对象类别的大规模爬取图像，是使复制粘贴真正可扩展的可行方法。为了实现这样的成功，我们设计了一个数据获取和处理框架，称为“X-Paste”，并在此基础上进行了系统研究。在 LVIS 数据集上，X-Paste 比以 Swin-L 为骨干网络的强大的基线 CenterNet2 提供了令人印象深刻的改进。具体来说，它在所有类别上实现了 +2.6 的框 AP 和 +2.1 的掩码 AP 增益，在长尾类别上甚至实现了 +6.8 的框 AP 和 +6.5 的掩码 AP 的更显着增益。我们的代码和模型可在 [https://github.com/yoctta/XPaste](https://github.com/yoctta/XPaste) 获得。

*贡献相同 1中国科学技术大学 2微软。通信作者：包建民 [jianmin.bao@microsoft.com](mailto:jianmin.bao@microsoft.com)，周文博 [wel-beckz@ustc.edu.cn](mailto:wel-beckz@ustc.edu.cn)。

## 1. 引言

实例分割（Dai et al., 2016; He et al., 2017; Hafiz & Bhat, 2020）是计算机视觉中一项基础任务，具有非常广泛的应用。为了获得特定类别合理的性能，大多数现有方法（Li et al., 2022; Dong et al., 2022a; Liu et al., 2022; He et al., 2017）依赖于大量为此类别标注的图像，这不仅昂贵而且耗时。这也使得扩展对象类别覆盖范围变得极其困难。事实上，现实世界的图像通常遵循长尾分布，因此仅收集一些稀有类别的足够图像本身就已经非常困难。因此，研究如何高效且可扩展地增强和创建训练数据具有重要意义。

作为一种简单而有效的数据增强策略，复制粘贴（Dwibedi et al., 2017; Ghiasi et al., 2021a; Dvornik et al., 2018）已被广泛研究以提高数据效率。通过将对象实例随机粘贴到背景图像上，它可以免费生成组合数量的训练数据，并提高实例分割模型的性能，尤其是对于稀有类别。直观地说，如果我们能够在复制粘贴中利用更多样化的对象实例，则可以实现更多的性能提升。然而，现有方法中使用的对象实例要么来自实例分割数据集本身（Ghiasi et al., 2021a; Dvornik et al., 2018），要么从外部 3D 模型中渲染（Dwibedi et al., 2017）。在本文中，我们认为这两种方式都不可扩展，并且没有充分发挥复制粘贴的潜力。更具体地说，对于前一种方式，一方面，收集人工标注数据是不可扩展的。另一方面，稀有类别的有限实例数量也导致复制粘贴数据的有限多样性。在后一种方式中，构建/收集 3D 模型本身非常困难且不可扩展。

在本文中，我们提出了一种新的对象实例获取和处理框架，称为“X-Paste”。X-Paste 建立在复制粘贴（Ghiasi et al., 2021a）的基础上，用于训练实例分割模型，但旨在使复制粘贴更具可扩展性，即以高效和自动的方式为无限类别获取具有高质量掩码（“复制粘贴的氧气”）的大规模对象实例。X-Paste 由四个主要模块组成：对象实例获取、实例掩码生成、实例过滤和实例与背景图像合成。

为了以可扩展的方式获取不同类别的大规模图像，对象实例获取模块的核心思想是充分利用新兴的强大的零样本文本到图像生成模型，如 DALLE（Ramesh et al., 2022）和 StableDiffusion（Rombach et al., 2022）（由于其可访问性而使用），或零样本识别模型，如 CLIP（Radford et al., 2021）和 Florence（Yuan et al., 2021），这两者都使用网络规模的图像-文本对进行训练。对于文本到图像生成模型，通过输入特定类别的不同文本提示，它们可以生成具有不同外观、视角和样式的非常多样化的图像。相比之下，以前的方法（Dwibedi et al., 2017; Ghiasi et al., 2021a; Dvornik et al., 2018）从训练数据集中“复制”图像或从有限的 3D 模型中渲染图像，其多样性较差。对于零样本识别模型，即使它无法像文本到图像模型那样生成图像，它也使我们能够从大规模的网络检索图像中过滤出任何类别的优质数据，这是以前的封闭集（有限类别覆盖范围）图像识别模型无法实现的。此外，我们的研究表明，这两种数据获取方法可以结合起来以获得更好的性能。

给定生成的或检索的图像，以下实例掩码生成模块旨在获取实例掩码，这是复制粘贴合成所需的。我们观察到，为不同类别生成的或网络检索的图像通常是以对象为中心的。特别是对于生成的图像，它们通常具有简单的背景。因此，使用现成的显着对象分割模型（Qin et al., 2020; Yun & Lin, 2022; Su et al., 2022）可以相对容易地对其进行分割。这些模型的一个优点是它们可以估计任何类别的掩码，即与类别无关，这使得在开放词汇表设置下生成实例成为可能。请注意，为了获得网络检索图像的精确分割掩码，我们进行了背景分析并去除了具有复杂背景的图像。

在获得实例掩码后，X-Paste 执行实例过滤以删除类别错误或掩码分割不精确的样本。我们进一步利用 CLIP 模型计算分割结果与给定类别之间的相似度，然后删除相似度低的样本。之后，我们可以为不同的类别提供大量的对象实例。最后，实例合成模块通过将对象实例粘贴到不同的背景图像上，生成用于训练分割模型的训练数据。我们研究了各种合成策略，发现为这些实例分配随机的比例和位置可以显着提高性能。我们还发现，诸如泊松混合之类的先进混合方法并没有显示出更多收益。

我们的 X-Paste 提出了一种重要的尝试，即利用高质量的零样本文本到图像模型和识别模型来帮助图像理解任务，即使它看起来非常直观和简单。通过减轻实例分割的人工标注压力，其强大的可扩展性可以帮助实现大型甚至开放词汇表的实例分割。它可以作为任何实例分割框架的“即插即用”组件，无需任何架构更改或推理开销。据我们所知，这是第一项展示如何大规模进行复制粘贴并进一步探索其潜力的工作。

我们进行了大量实验以验证 X-Paste 的优越性。在 COCO 数据集上，X-Paste 实现了 57.0 的框 mAP 和 48.6 的掩码 mAP，比基线高出 +1.7 mAP 和 +0.9 mAP。在 LVIS 数据集上，我们为所有对象实现了 44.4 的掩码 mAP，为稀有对象实现了 43.3 的掩码 mAP，比基线高出 +2.1 和 +6.5。我们的方法也可以使开放词汇表设置受益，我们的模型为所有对象实现了 31.8 的掩码 mAP，为稀有对象实现了 21.4 的掩码 mAP，比基线高出 +1.6 和 +5.0 mAP。同时，我们对我们的框架 X-Paste 进行了全面的分析，以证明每个步骤的有效性。

## 2. 相关工作

### 实例检测和分割

实例检测和分割（He et al., 2017; Dai et al., 2021; Meng et al., 2022; Li et al., 2022）在过去几十年中得到了广泛的研究。给定一张图像，实例检测是为了定位对象实例的精确位置并确定实例的类别，而实例分割则需要进一步获取实例的细粒度掩码。自 Mask-RCNN（He et al., 2017）以来，这两项任务通常被集成到一个统一的两阶段框架中，因此我们使用实例分割来表示这两项任务。最近，长尾和开放词汇表实例分割引起了更多关注，因为它们是实际应用中更现实的设置。

LVIS 数据集（Gupta et al., 2019）是最具挑战性的长尾实例分割数据集之一。由于高度不平衡和实例数量有限，当前的实例分割模型往往难以很好地处理这些稀有类别。因此，人们提出了不同的技术，例如数据重采样和损失重新加权（Wang et al., 2020; Mahajan et al., 2018; Zang et al., 2021; Tan et al., 2021; Wang et al., 2021）、分数归一化（Pan et al., 2021）和数据增强（Ghiasi et al., 2021a）。在本文中，我们专注于基于复制粘贴的增强，并研究如何使其可扩展以进一步探索其潜力。从这个意义上说，我们的研究是对其他非复制粘贴技术的补充。

开放词汇表实例检测 (OVD) 旨在检测推理时训练/基础类别（已见）词汇表中不存在的目标/新类别（未见）对象。因此，它可以被视为长尾实例检测的一个难题，其中某些类别不存在实例。为了提高开放词汇表性能，一些现有方法利用大规模的预训练视觉语言模型（例如 CLIP）将丰富的知识转移到分类器中，包括 OVR-CNN（Zareian et al., 2021）、ViLD（Gu et al., 2021）、OpenSeg（Ghiasi et al., 2021b）、Region-CLIP（Zhong et al., 2022）、DetPro（Du et al., 2022）和 PromptDet（Feng et al., 2022）。最新的工作 Detic（Zhou et al., 2022）建议利用图像分类数据（即 ImageNet-22k（Deng et al., 2009））通过图像级监督来帮助训练分类器。在下面的实验中，我们将展示我们的 X-Paste，作为一种简单的“即插即用”增强策略，无需任何模型架构或训练策略更改，就已经可以使 vanilla 实例分割模型具有合理的开放词汇表能力。同样，它是对上述开放词汇表技术的补充。

### 实例分割的数据增强

由于大多数现有的实例分割模型都非常耗费数据，因此人们已经付出了很多努力来从数据增强角度提高性能。根据增强的数据类型，它们可以大致分为基于合成和基于检索。

对于基于合成的方法，早期的工作使用基于图形的渲染（Hinterstoisser et al., 2018; Su et al., 2015; Hodaň et al., 2019）或电脑游戏（Richter et al., 2016; 2017）来生成高质量的标注数据。然而，这些方法通常存在真实数据和纯合成数据之间巨大的领域差距。然后，在以下工作中引入了合成真实图像，包括复制粘贴（Dwibedi et al., 2017）、上下文复制粘贴（Dvornik et al., 2018）和 Instaboost（Fang et al., 2019）。简单的复制粘贴（Ghiasi et al., 2021a）进一步表明，简单地将真实分割的对象随机粘贴到背景图像上就已经效果很好，而无需像上下文建模那样复杂的策略。现有复制粘贴方法的主要问题是它们不可扩展并且实例多样性有限，因为它们要么从实例数据集本身获取对象实例，要么从 3D 模型中渲染。

检索是收集大规模数据的另一种方式。随着大规模图像-文本数据集 LAION（Schuhmann et al., 2022）和图像分类数据集 ImageNet-22k（Deng et al., 2009）的发展，一些方法（Feng et al., 2022; Zhou et al., 2022; Hong et al., 2017; Jin et al., 2017; Shen et al., 2018; Wei et al., 2016）研究了如何利用这些数据来帮助实例分割。但是，由于这些数据本身没有精确的实例标注，因此需要一些专门的设计，例如伪标签（Zhong et al., 2022）或冻结位置分支，仅使用 (Zhou et al., 2022) 中的图像级监督来训练分类器。

我们的 X-Paste 与上述工作之间的主要区别在于，我们尝试在相同的简单复制粘贴框架下利用文本到图像模型合成的数据和检索数据，并使其可扩展，而无需任何算法更改。此外，它与利用预训练的 CLIP 模型进行知识蒸馏或图像级监督等工作是正交的。

### 零样本识别和文本到图像生成

视觉语言预训练 (VLP) 最近取得了令人鼓舞的突破。通过对网络规模的图像-文本数据进行对比学习，包括 CLIP（Radford et al., 2021）、MaskCLIP（Dong et al., 2022b）、Florence（Yuan et al., 2021）、ALIGN（Jia et al., 2021）和 OmniVL（Wang et al., 2022）在内的代表性工作，在将视觉特征与文本特征对齐方面展现出了强大的力量，实现了真正的零样本识别，而不是传统的封闭集识别。同样在网络规模的图像-文本数据上进行预训练，另一个突破是关于零样本文本到图像生成模型。代表性工作包括 DALL-E（Ramesh et al., 2021; Nichol et al., 2021; Ramesh et al., 2022）、CogView（Ding et al., 2021）、Parti（Yu et al., 2022）、Imagen（Saharia et al., 2022）、VQ-Diffusion（Gu et al., 2022）、StableDiffusion（Rombach et al., 2022）和 Frido（Fan et al., 2023）。通过输入自由格式的文本，这些模型可以生成与文本输入条件匹配的高保真图像。直观地说，这种零样本文本到图像模型是自然的数据生成器，但很少有先前的工作使用它们来帮助图像理解训练。在本文中，我们首次尝试利用强大的零样本识别和文本到图像模型来重新审视可扩展的复制粘贴并帮助实例分割。

## 3. X-Paste

如上所述，以前的复制粘贴方法（Dwibedi et al., 2017; Ghiasi et al., 2021a; Dvornik et al., 2018）利用来自内部实例分割数据集本身的实例或从 3D CG 模型渲染的实例，这使得它们难以扩展和充分发挥复制粘贴的潜力。在本文中，我们研究了如何通过以自动和高效的方式获取外部对象实例来使复制粘贴可扩展。我们建议利用零样本文本到图像模型（即 StableDiffusion（Rombach et al., 2022））或零样本图像分类模型（例如 CLIP（Radford et al., 2021））生成的图像。为了在相同的简单复制粘贴框架下使用这些图像，我们提出了一个完整的数据获取和处理框架“X-Paste”。如图 1 所示，整体框架非常简单明了，由四个主要模块组成：对象实例获取、实例掩码生成、实例过滤和实例与背景图像合成。在以下部分中，我们将详细阐述每个模块。

### 3.1. 对象实例获取

为了收集具有多样化外观、视角和样式的特定类别的图像，我们基于最近兴起的大规模视觉语言模型（即文本到图像模型和图像-文本对比模型）提供了两种高效且有效的解决方案，这两种模型都显示出强大的零样本能力，并使任何类别的 data 采集都具有可扩展性。具体而言，作为第一个解决方案，我们建议直接应用强大的零样本文本到图像生成模型（Rombach et al., 2022）为感兴趣的类别生成图像。我们通过输入像“一张单个 [类别名称] 的照片”这样的文本提示来获取特定类别的图像。这里的“单个”一词是为了鼓励模型在图像中生成单个实例，因为生成模型可能会为许多类别生成多个实例。我们发现这些生成的图像非常逼真、多样化，并且在语义上与给定类别相匹配，因为此类文本到图像模型使用网络规模的数据进行训练，并显示出强大的组合能力。在本文中，我们默认使用 PLMS 采样器的 StableDiffusion V1.4，因为它是开源的。

作为第二个可扩展的解决方案，我们建议从互联网上抓取真实图像作为对象实例源。由于网络抓取的图像通常非常嘈杂，因此直接使用所有图像而不进行过滤会破坏模型。在最新的零样本图像-文本对比模型出现之前，过滤不同类别的图像并不是一项简单的任务，因为大多数识别模型都使用封闭集类别进行训练。为了使此类识别模型对任何类别都表现良好，需要收集大规模的特定类别数据，因此这是一个先有鸡还是先有蛋的问题。通过对网络规模的图像-文本数据进行对比学习，最新的视觉语言对齐模型，如 CLIP（Radford et al., 2021）、Florence（Yuan et al., 2021），使零样本图像-文本相似度匹配成为可能。因此，我们应用 CLIP 模型计算每个类别与其对应的爬取图像之间的语义相似度，并仅保留语义相似度高的图像。此外，为了帮助以下实例掩码生成模块生成高质量的实例掩码，我们进一步进行了背景分析，并仅保留具有简单背景的图像。具体而言，我们简单地计算每张图像的颜色直方图并找到主导颜色。然后，我们仅保留超过 40% 的图像像素接近主导颜色（色差小于 5）的图像。

### 3.2. 实例掩码生成

由于复制粘贴在合成过程中需要实例掩码，因此给定生成的或爬取的图像，另一个挑战是获取精确的对象实例掩码。通过浏览获取的图像，我们观察到其中大多数是以对象为中心的。这促使我们使用与类别无关的前景分割模型来生成实例掩码。具体而言，我们研究了三种类型的前景分割算法：显着对象分割、共显着性分割和文本引导分割。我们使用流行的方法 U2Net（Qin et al., 2020）和 SelfReformer（缩写为 SRF）（Yun & Lin, 2022）进行显着对象分割，使用 UFO（Su et al., 2022）对来自每个类别的一批图像进行共显着性分割，并使用 CLIPseg（Lüddecke & Ecker, 2022）以类别名称作为文本提示来分割对象。由于 CLIPseg（Lüddecke & Ecker, 2022）生成的分割掩码非常粗糙，我们将生成的掩码转换为 tri-map，并使用图像抠图方法（Park et al., 2022）进一步细化掩码。

我们观察到所有这些分割方法在某些情况下都会失败，如图 2 所示。因此，我们提出了一种 CLIP 引导的选择策略，从这四种方法中为每个图像选择一个实例掩码作为伪标注。具体来说，我们计算了空白背景中分割对象图像与其类别名称之间的跨模态相似度。我们的动机是更好的分割将获得更高的语义相似度。为了说明这一动机，我们在图 2 中可视化了 CLIP 分数。

### 3.3. 实例过滤

为了进一步删除具有低质量掩码的对象实例，我们在实例过滤模块中应用了多种策略。首先，我们过滤掉面积小于图像 5% 或大于图像 95% 的掩码，因为这些实例很可能是错误分割的。其次，我们根据预先计算的 CLIP 分数选择与类别具有高语义相关性的实例。考虑到 CLIP 分数对每个类别的不同敏感度，我们将类别特定的阈值 thresi 设置为 min(t, max(Ci) - d)，其中 t 是预定义的 CLIP 分数阈值，d 是针对那些图像-文本相似度较低的类别的减法阈值，默认为 0.01，Ci 是来自类别 i 的实例的 CLIP 分数集。由于复制粘贴在长尾数据集中对稀有类别特别有帮助，因此为稀有类别生成足够多样化的实例更为重要。因此，我们很好奇生成模型是否可以为这些稀有类别生成高质量的实例。为了验证这一点，我们在图 3 中可视化了 LVIS 数据集中不同类别生成的图像的 CLIP 分数分布。我们发现 StableDiffusion 在生成稀有类别时实现了类似的 CLIP 分数分布，表明它没有遇到严重的数据不平衡问题，并且对稀有类别的表现同样良好。

### 3.4. 实例合成

当应用 X-Paste 合成训练图像时，我们使用类别平衡的采样策略对实例进行采样，并将它们粘贴到背景图像的随机位置。根据背景图像的分辨率和取决于数据分布的比例因子（对象区域在背景图像中的百分比），将实例调整为适当的比例。背景图像可以是带标注的图像，也可以是纯背景图像，为了通用性，我们仅使用训练数据。当实例遮挡背景图像中的对象时，我们删除完全被遮挡的对象，并相应地更新掩码和边界框标注。

**类别平衡采样：** 我们为每张训练图像随机选择实例数量 Ni ∈ [1, Nmax]（Nmax 默认值为 20），然后我们以可重复的方式为每个类别采样 Ni 个类别和一个实例。

**实例合成：** 我们计算训练集中每个类别 C 的对象比例（掩码面积除以图像面积的平方根）的均值 µC 和标准差 σC。当粘贴具有类别 C 的实例 I 时，我们从高斯分布 N(µC, σ2C) 中采样比例 Sr，并以比例 S2rHW 将实例粘贴到背景图像上（H、W 表示图像的高度和宽度），其中边界框中心点的坐标是在整个图像中随机选择的。在图 4 中，我们展示了一些由 X-Paste 生成的训练样本。这些生成的图像与以前方法（Fang et al., 2019; Dvornik et al., 2018）合成的图像不同，后者旨在生成逼真的图像进行训练。我们生成的图像并不真实，因为对象的比例和相对关系可能不合理。尽管如此，这些生成的图像仍然可以作为良好的训练数据来提高性能。

## 4. 实验

### 4.1. 设置

**数据集。** 我们在 LVIS（Gupta et al., 2019）和 MS-COCO（Lin et al., 2014）数据集上进行了对象检测和实例分割实验。LVIS 数据集包含 100k 张训练图像和 20k 张验证图像。它有 1203 个类别，每个类别的实例数量呈长尾分布。根据每个类别的实例数量，可以将这些类别分为常见、频繁和稀有类别。常见、频繁和稀有类别的数量分别为 461、405 和 337。在开放词汇表实例检测设置中，常见和频繁类别用于训练，稀有类别用作测试的新类别。MS-COCO 数据集包含 118K 张训练图像、5K 张验证图像和 20K 张测试开发图像。我们使用官方拆分进行训练。

**学习框架。** 我们在 Detectron2（Wu et al., 2019）中利用典型的对象检测和实例分割框架 Center-Net2（Zhou et al., 2021）。我们的基本设置使用在 ImageNet-22k 上预训练的 Resnet 50（He et al., 2016）作为骨干网络。训练配置设置为如下：训练分辨率设置为 640，批量大小为 32，以及 4× 时间表（48 个 epoch）。我们使用单尺度策略测试性能。我们报告了所有类别（表示为 APbox 和 APmask）以及稀有类别（表示为 APbox_r 和 APmask_r）的框 AP 和掩码 AP。

**X-Paste 的基线设置。** 我们设计了一个基线设置来消融 X-Paste。为了确保每个类别都包含足够的实例，我们使用 CLIP 模型（Radford et al., 2021）为每个类别过滤 1k 个真实图像。此外，我们应用 StableDiffusion 模型（Rombach et al., 2022）为每个类别生成 1k 个图像。对于 StableDiffusion，扩散步骤设置为 200，无分类器比例设置为 5.0。我们采用第 3.2 节中描述的方法（表示为 max CLIP）作为实例掩码生成的默认设置。对于实例过滤，我们将 CLIP 阈值设置为 0.21 以过滤所有获得的实例。我们保留了来自 StableDiffusion 的 150k 个生成的实例和来自 CLIP 模型的 150k 个真实实例。在实例合成模块中，粘贴到每个背景图像的实例数量设置为 20 以进行训练。

### 4.2. X-Paste 的分析

我们进行了消融研究来分析我们提出的 X-Paste，并发现了一些有趣的特性。

**更多实例，更好的结果。** 在表 1 中，我们比较了在 X-Paste 中不使用和使用不同数量实例的结果。我们发现，仅使用 X-Paste 中 100k 个生成的实例或过滤后的真实实例就可以显着提高性能。具体来说，它实现了 36.3 的框 AP 和 32.3 的掩码 AP，比基线高出 +1.9 mAP 和 +1.5 mAP。它在稀有类别上实现了更大的增益，框 AP 为 +4.0，掩码 AP 为 +3.6。更重要的是，如果我们继续将实例的规模从 100k 增加到 300k，我们可以获得更好的结果。这也证明了使用文本到图像生成的图像的巨大价值，因为从搜索引擎中检索大量以对象为中心的真实图像非常具有挑战性。为了进一步说明这一点，我们在表 3 中使用了 LVIS 数据集上的所有一百万个检索图像，并将结果与使用一百万个检索图像 + 一百万个生成图像的基线进行了比较。结果表明，即使有大量检索图像，添加文本到图像生成的图像仍然可以将掩码 AP 提高 0.7 ∼ 0.9。

**多样化的实例来源很重要。** 我们在表 2 中展示了使用不同实例来源进行 X-Paste 的结果。在相同的实例规模下，使用 CLIP 过滤的真实图像可以获得更好的结果。此外，我们观察到使用来自生成模型和过滤后的真实图像的实例可以获得比使用单个实例来源更好的性能。这证实了这些实例的多样性对于提高性能非常重要。

**实例掩码生成的效果。** 准确的实例掩码在训练对象检测和实例分割中起着重要作用。因为不精确的掩码可能会导致模型学习错误的知识。我们在这里研究了几种与类别无关的分割方法。具体来说，我们实验了由 U2Net（Qin et al., 2020）和 SelfReformer（Yun & Lin, 2022）、UFO（Su et al., 2022）、CLIPseg（Lüddecke & Ecker, 2022）以及我们提出的选择具有最大 CLIP 分数的掩码分割的方法生成的实例掩码。表 4 报告了使用不同实例掩码生成方法的结果。我们发现这些分割方法可以实现相当的结果。我们提出的 max CLIP 在 APbox_r、APmask_r、APbox 和 APmask 方面可以产生稍微更好的结果。

**使用不同的 CLIP 分数阈值进行实例过滤的效果。** 使用不同的 CLIP 分数阈值，我们选择固定数量的实例来训练框架。我们在表 5 中报告了结果。我们发现 0.21 是最佳阈值，与非过滤（阈值设置为 0）相比，它为所有类别提供了 0.6 的掩码 AP 增益，为稀有类别提供了 1.4 的掩码 AP 增益。我们还观察到，更高的 CLIP 分数阈值可能不会产生更好的性能。我们认为原因是更高的 CLIP 分数意味着生成的样本与给定文本的相关性更高，但它们在外观、视角和风格方面可能表现出有限的多样性。缺乏多样性会导致性能下降。这再次证明了实例的质量和多样性对于训练实例分割都很重要。

**研究不同的实例放置策略。** 我们将我们的随机放置策略与基于参考的放置策略进行了比较。基于参考的放置策略将生成的实例粘贴到背景图像中原始边界框的位置。我们还研究了粘贴到单个背景图像的最大实例数 Nmax 如何影响性能。表 6 报告了结果。我们观察到随机放置策略比基于参考的方法实现了更好的性能。此外，我们发现 20 是粘贴到背景图像的最大实例数的合适数字。

### 4.3. 与先前方法的比较

**与其他数据增强方法的比较。** 在表 7 中，我们与先前与数据增强相关的方法进行了公平比较。我们使用相同的框架 CenterNet2（Zhou et al., 2021），并以 Resnet50 作为骨干网络。我们在 LVIS 数据集上将我们的方法与复制粘贴（Ghiasi et al., 2021a）和 Detic（Zhou et al., 2022）进行了比较。表 7 报告了 LVIS 数据集上的结果。作为参考，我们还展示了另一个直接将我们收集的数据添加到训练数据中而无需增强的基线（“基线 + 外部数据”）。我们使用与 X-Paste 相同的实例训练 Detic，以研究 X-Paste 是否比使用图像级标注的弱监督学习更有效。与强大的基线复制粘贴（Ghiasi et al., 2021a）相比，我们的方法取得了显着更好的结果。在所有类别中，X-Paste 比复制粘贴的框 AP 高出 +1.2，掩码 AP 高出 +1.2。在稀有类别中，X-Paste 实现了更大的增益，框 AP 为 +3.2，掩码 AP 为 +4.4。这证实了大规模实例的有效性对于获得更好的结果至关重要。我们的 X-Paste 也优于最近提出的 Detic（Zhou et al., 2022）。与 Detic 相比，X-Paste 不涉及复杂的机制，也不需要更大的批量大小。这证实了提出的 X-Paste 的有效性。

为了验证 X-Paste 是否适用于大型模型，我们进一步使用最近提出的大型模型 Swin-Large（Liu et al., 2022）（简称 Swin-L）作为骨干网络进行了实验。我们将输入分辨率增加到 896，并使用指数移动平均值训练模型 4 × 时间表（48 个 epoch）以更新模型。我们使用大约一百万个生成的和检索的实例，并将 CLIP 分数阈值设置为 0.21 进行 X-Paste。表 8 列出了结果。我们观察到，在所有类别中，X-Paste 比复制粘贴的框 AP 高出 +0.6，掩码 AP 高出 +0.7，在稀有类别中，框 AP 高出 +4.8，掩码 AP 高出 +4.8。