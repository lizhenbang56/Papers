<!DOCTYPE html>
<!-- saved from url=(0055)https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers -->
<html lang="en" style="scroll-padding-top: 70px;"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="./2024_files/virtual.js.下载"></script>
    <meta name="google-site-verification" content="0jwPnVXIAk4FvFdT37dwMmd-kjHF86e5DKwvqlStUW0">

    <title>CVPR 2024</title>
    
    <link rel="stylesheet" href="./2024_files/core.css" type="text/css">
    <link rel="stylesheet" href="./2024_files/virtual.css" type="text/css">
     <link href="./2024_files/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

    <link rel="stylesheet" href="./2024_files/custom.css" type="text/css">
    
    <link rel="stylesheet" href="./2024_files/bootstrap-select.min.css">
    <link href="./2024_files/css2" rel="stylesheet">
    <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
      "tex2jax": {
        "inlineMath": [["$","$"], ["\\(","\\)"]],
        "displayMath": [["\\[","\\]"]],
        "processEscapes": true
      }
    }
    );
    </script>

    <script type="text/javascript" async="" src="./2024_files/MathJax.js.下载">
    </script>

    <!--This script keeps local links inside the web app rather than opening them
in Safari, and has nothing to do with editing or Aloha.-->

<script type="text/javascript">
	(function(document,navigator,standalone) {
		// prevents links from apps from opening in mobile safari
		// this javascript must be the first script in your <head>
		if ((standalone in navigator) && navigator[standalone]) {
			var curnode, location=document.location, stop=/^(a|html)$/i;
			document.addEventListener('click', function(e) {
				curnode=e.target;
				while (!(stop).test(curnode.nodeName)) {
					curnode=curnode.parentNode;
				}
				// Conditions to do this only on links to your own app
				// if you want all links, use if('href' in curnode) instead.
				if(
					'href' in curnode && // is a link
					(chref=curnode.href).replace(location.href,'').indexOf('#') && // is not an anchor
					(	!(/^[a-z\+\.\-]+:/i).test(chref) ||                       // either does not have a proper scheme (relative links)
						chref.indexOf(location.protocol+'//'+location.host)===0 ) // or is in the same protocol and domain
				) {
					e.preventDefault();
					location.href = curnode.href;
				}
			},false);
		}
	})(document,window.navigator,'standalone');
</script>        

<!-- This style sets the minimum size of a blurb to 260 px unless there is a
template context variable blurb_min_height that sets it otherwise. If blurbs
aren't all about the same size, they don't flow well when the window is
resized.-->


<style>
/*This is here rather that in a .css file for a reason.*/
    @media screen and (min-width: 767px) {
        .blurb {
            min-height:260px;
        }
    }
</style>
    

<script src="./2024_files/jquery-3.6.1.min.js.下载" integrity="sha256-o88AwQnZB+VDvE9tvIXrMQaPlFFSUTR+nldQm1LuPXQ=" crossorigin="anonymous"></script>


<script>
    if (typeof jQuery === 'undefined') {
        var script = document.createElement('script');
        script.type = 'text/javascript';
        script.src = "/static/core/js/jquery-3.6.1.min.js";
        document.head.appendChild(script);
    }
</script>


    <script>
        var $ = jQuery;
        /*Store a pointer to jquery2, so I can reference it later.  Aloha loads jquery 1.7 and much
        of bootstrap 3 is not compatible. This comment is deprecated. */
    </script>

    
    <script src="./2024_files/bootstrap.bundle.min.js.下载" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>

    <script src="./2024_files/ajax-csrf-snippet.js.下载" type="text/javascript"></script>
    <script src="./2024_files/be44b7e05d.js.下载" crossorigin="anonymous"></script><style media="all" id="fa-kit-upload"></style><style media="all" id="fa-v5-font-face">/*!
 * Font Awesome Pro 6.5.2 by @fontawesome - https://fontawesome.com
 * License - https://fontawesome.com/license (Commercial License)
 * Copyright 2024 Fonticons, Inc.
 */@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.ttf) format("truetype");unicode-range:u+e007,u+e013,u+e01a,u+e01e,u+e049,u+e052,u+e055-e057,u+e077-e084,u+e087-e088,u+f081-f082,u+f08c,u+f092,u+f099-f09b,u+f0d2-f0d5,u+f0e1,u+f113,u+f136,u+f13b-f13c,u+f15a,u+f167-f169,u+f16b-f16e,u+f170-f171,u+f173-f174,u+f179-f17e,u+f180-f181,u+f184,u+f189-f18d,u+f194,u+f198,u+f19a-f19b,u+f19e,u+f1a0-f1a9,u+f1b4,u+f1bc,u+f1be,u+f1e8,u+f1ed,u+f1f0-f1f1,u+f20e,u+f210,u+f213-f214,u+f232,u+f23a,u+f26b,u+f270,u+f288,u+f299,u+f2a6,u+f2b0,u+f2c5-f2c6,u+f2e0,u+f368,u+f379,u+f392-f393,u+f39f,u+f3a9,u+f3ab-f3ac,u+f3c0,u+f3c7,u+f3ca,u+f3e2,u+f3eb-f3ec,u+f3ef,u+f3f8,u+f3fe,u+f419,u+f41b,u+f4d5,u+f4e4,u+f4f8-f4f9,u+f514,u+f5b5,u+f6c9,u+f731,u+f77b,u+f7af,u+f7e1,u+f83b}@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.ttf) format("truetype");unicode-range:u+f1aa,u+f1b5-f1b7,u+f1bd,u+f1ca-f1cc,u+f1d0-f1d7,u+f1e7,u+f1e9,u+f1ee,u+f1f2-f1f5,u+f202-f203,u+f208-f209,u+f20d,u+f211-f212,u+f215-f216,u+f231,u+f237,u+f23b-f23e,u+f24b-f24c,u+f25e,u+f260-f261,u+f263-f26a,u+f26d-f26e,u+f27c-f27e,u+f280-f282,u+f284-f287,u+f289-f28a,u+f293-f294,u+f296-f298,u+f2a5,u+f2a9-f2ae,u+f2b1-f2b4,u+f2b8,u+f2c4,u+f2d5-f2da,u+f2dd-f2de,u+f35c,u+f369-f375,u+f378,u+f37a-f37d,u+f37f-f380,u+f383-f385,u+f388,u+f38b-f38f,u+f391,u+f394-f397,u+f399-f39a,u+f39d-f39e,u+f3a1-f3a4,u+f3a6-f3a8,u+f3aa,u+f3ad-f3b2,u+f3b4-f3bd,u+f3c3-f3c4,u+f3c6,u+f3c8,u+f3cb-f3cc,u+f3d0,u+f3d2-f3dc,u+f3df,u+f3e1,u+f3e3-f3e4,u+f3e6-f3e7,u+f425,u+f4e6}@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.ttf) format("truetype");unicode-range:u+f3e8-f3ea,u+f3ee,u+f3f3,u+f3f5-f3f7,u+f3f9,u+f402-f405,u+f407-f40d,u+f411-f417,u+f41a,u+f41c-f421,u+f423,u+f426-f431,u+f44d,u+f452,u+f457,u+f459,u+f4e5,u+f4e7-f4f7,u+f50a-f513,u+f592,u+f59e,u+f5a3,u+f5a8,u+f5b2,u+f5be,u+f5c6,u+f5cc,u+f5cf,u+f5f1,u+f5f7,u+f5fa,u+f60f,u+f612,u+f63f,u+f642,u+f69d,u+f6ca,u+f6cc,u+f6dc,u+f730,u+f75d,u+f77a,u+f785,u+f789,u+f78d,u+f790-f791,u+f797-f799,u+f7b0-f7b1,u+f7b3,u+f7bb-f7bc,u+f7c6,u+f7d3,u+f7d6,u+f7df-f7e0,u+f7e3,u+f834-f83a,u+f83c-f83d,u+f83f-f842,u+f89e,u+f8a6,u+f8ca,u+f8d2,u+f8e1,u+f8e8}@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400.ttf) format("truetype");unicode-range:u+a}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.ttf) format("truetype");unicode-range:u+f040,u+f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f108-f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-2.ttf) format("truetype");unicode-range:u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047-e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-4.ttf) format("truetype");unicode-range:u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-8.ttf) format("truetype");unicode-range:u+f80b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-11.ttf) format("truetype");unicode-range:u+f8bc}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.ttf) format("truetype");unicode-range:u+f0d9-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221,u+f381-f382}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.ttf) format("truetype");unicode-range:u+f222-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f319,u+f31c,u+f4e6,u+f8e5}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-15.ttf) format("truetype");unicode-range:u+f31d-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-16.ttf) format("truetype");unicode-range:u+f45a-f47c,u+f47e-f480,u+f482-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f507,u+f509,u+f515-f51a,u+f51c,u+f51f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-17.ttf) format("truetype");unicode-range:u+f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-18.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f5c3-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f66f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-19.ttf) format("truetype");unicode-range:u+f670-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f710}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-20.ttf) format("truetype");unicode-range:u+f711-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-21.ttf) format("truetype");unicode-range:u+f7c3-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f87b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-22.ttf) format("truetype");unicode-range:u+f87c-f892,u+f895-f896,u+f898-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.ttf) format("truetype");unicode-range:u+f040,u+f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f108-f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-2.ttf) format("truetype");unicode-range:u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-3.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047-e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-4.ttf) format("truetype");unicode-range:u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-8.ttf) format("truetype");unicode-range:u+f80b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-11.ttf) format("truetype");unicode-range:u+f8bc}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.ttf) format("truetype");unicode-range:u+f0d9-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221,u+f381-f382}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.ttf) format("truetype");unicode-range:u+f222-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f319,u+f31c,u+f4e6,u+f8e5}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.ttf) format("truetype");unicode-range:u+f31d-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.ttf) format("truetype");unicode-range:u+f45a-f47c,u+f47e-f480,u+f482-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f507,u+f509,u+f515-f51a,u+f51c,u+f51f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-17.ttf) format("truetype");unicode-range:u+f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-18.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f5c3-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f66f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-19.ttf) format("truetype");unicode-range:u+f670-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f710}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-20.ttf) format("truetype");unicode-range:u+f711-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-21.ttf) format("truetype");unicode-range:u+f7c3-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f87b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-22.ttf) format("truetype");unicode-range:u+f87c-f892,u+f895-f896,u+f898-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-1.ttf) format("truetype");unicode-range:u+f040,u+f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f108-f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-2.ttf) format("truetype");unicode-range:u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-3.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047-e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-4.ttf) format("truetype");unicode-range:u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-8.ttf) format("truetype");unicode-range:u+f80b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-11.ttf) format("truetype");unicode-range:u+f8bc}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-12.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-13.ttf) format("truetype");unicode-range:u+f0d9-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221,u+f381-f382}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-14.ttf) format("truetype");unicode-range:u+f222-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f319,u+f31c,u+f4e6,u+f8e5}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-15.ttf) format("truetype");unicode-range:u+f31d-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-16.ttf) format("truetype");unicode-range:u+f45a-f47c,u+f47e-f480,u+f482-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f507,u+f509,u+f515-f51a,u+f51c,u+f51f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-17.ttf) format("truetype");unicode-range:u+f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-18.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f5c3-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f66f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-19.ttf) format("truetype");unicode-range:u+f670-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f710}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-20.ttf) format("truetype");unicode-range:u+f711-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-21.ttf) format("truetype");unicode-range:u+f7c3-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f87b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-22.ttf) format("truetype");unicode-range:u+f87c-f892,u+f895-f896,u+f898-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c,u+10e010,u+10e017,u+10e01b,u+10e01f-10e021,u+10e024,u+10e02f,u+10e03a,u+10e042,u+10e045-10e046,u+10e060,u+10e068,u+10e06e,u+10e074,u+10e076,u+10f001,u+10f004-10f005,u+10f007-10f008,u+10f00c,u+10f011-10f012,u+10f015,u+10f017-10f019,u+10f01c,u+10f023-10f025,u+10f02a,u+10f02c-10f031,u+10f03a,u+10f03d-10f03e,u+10f041,u+10f04a-10f04e,u+10f05b,u+10f060-10f065,u+10f067-10f068,u+10f06b-10f06e,u+10f072,u+10f075,u+10f077-10f078,u+10f07b,u+10f084,u+10f086,u+10f091,u+10f093,u+10f095,u+10f09c-10f09d,u+10f0a3,u+10f128,u+10f12a,u+10f155,u+10f292,u+10f295,u+10f332,u+10f541,u+10f80a,u+10f80c}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-1.ttf) format("truetype");unicode-range:u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8-f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f47d,u+10f0a6,u+10f0ac-10f0ad,u+10f0b0-10f0b1,u+10f0c0-10f0c2,u+10f0c5-10f0c6,u+10f0c8-10f0c9,u+10f0cc,u+10f0ce,u+10f0d1,u+10f0d6-10f0d7,u+10f0dc,u+10f0e0,u+10f0e7-10f0e8,u+10f0eb,u+10f0f3,u+10f0f8,u+10f106,u+10f109,u+10f10e,u+10f110-10f111,u+10f11c,u+10f11e,u+10f121,u+10f126,u+10f129,u+10f12c-10f12e,u+10f130-10f133,u+10f135,u+10f13d,u+10f140,u+10f145,u+10f14e,u+10f15b,u+10f164,u+10f186,u+10f188,u+10f1ab,u+10f1ad-10f1ae,u+10f1b2,u+10f1b8,u+10f1bb,u+10f1c0-10f1c3,u+10f1ce,u+10f1d8,u+10f1dc,u+10f1e4-10f1e6,u+10f1ea-10f1ec,u+10f1f8-10f1f9,u+10f205,u+10f20a,u+10f217,u+10f219-10f21d,u+10f22d,u+10f233-10f234,u+10f238,u+10f246,u+10f24d,u+10f251,u+10f25d,u+10f275,u+10f29e,u+10f47d}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-2.ttf) format("truetype");unicode-range:u+f040,u+f108,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+10f040,u+10f108,u+10f2a0,u+10f2a7,u+10f2b5,u+10f2bb,u+10f2cc-10f2cd,u+10f2d2,u+10f2db,u+10f2e1,u+10f2ec,u+10f2f7,u+10f2fc,u+10f302-10f303,u+10f316,u+10f31a,u+10f328,u+10f335,u+10f363,u+10f37e,u+10f390,u+10f3c5,u+10f3ce,u+10f3e5,u+10f3f4,u+10f3fb,u+10f40e,u+10f435,u+10f44b,u+10f481,u+10f48a,u+10f48f-10f490,u+10f493-10f494,u+10f498,u+10f4b7,u+10f4ca,u+10f4cc,u+10f4d1,u+10f4d7-10f4d8,u+10f4e2,u+10f503,u+10f508,u+10f51b,u+10f51d-10f51e,u+10f521-10f522,u+10f52b,u+10f530,u+10f535,u+10f53e,u+10f543-10f545,u+10f548-10f549,u+10f54e,u+10f555,u+10f559,u+10f55d,u+10f564,u+10f56c,u+10f56e-10f570,u+10f577-10f578,u+10f590,u+10f594-10f595,u+10f5a1-10f5a2,u+10f5aa-10f5ab,u+10f5b0,u+10f5b7,u+10f5ba,u+10f5bf,u+10f5ca,u+10f5db-10f5dc,u+10f5ef,u+10f5f2,u+10f5f6,u+10f5fb}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-3.ttf) format("truetype");unicode-range:u+f069,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+10f069,u+10f5fd,u+10f621,u+10f63b,u+10f649-10f64a,u+10f64f,u+10f653-10f654,u+10f656,u+10f65b,u+10f664,u+10f673,u+10f675,u+10f67d,u+10f67f,u+10f695,u+10f69c,u+10f6a8,u+10f6bf-10f6c0,u+10f6d5,u+10f6e3,u+10f6e9,u+10f6f5,u+10f6fa,u+10f6ff-10f700,u+10f70b,u+10f70e,u+10f715,u+10f71b,u+10f72e-10f72f,u+10f733-10f734,u+10f747,u+10f755,u+10f757,u+10f75c,u+10f762,u+10f773,u+10f77c,u+10f781,u+10f784,u+10f788,u+10f7b2,u+10f7b6,u+10f7bd,u+10f7d5,u+10f7ee,u+10f7ff,u+10f801,u+10f804,u+10f813-10f814,u+10f82f-10f830,u+10f845-10f846,u+10f850,u+10f855,u+10f858-10f859,u+10f85c,u+10f866,u+10f86d,u+10f871,u+10f875,u+10f893-10f894,u+10f897,u+10f89f,u+10f8a9,u+10f8b1-10f8b2,u+10f8bb,u+10f8c7,u+10f8d6-10f8d7,u+10f8d9,u+10f8df-10f8e0,u+10f8e7,u+10f8ee-10f8ef,u+10f8fd}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-4.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+10e000-10e006,u+10e008-10e00f,u+10e011-10e012,u+10e014-10e016,u+10e018-10e019,u+10e01c-10e01d,u+10e022-10e023,u+10e025-10e02e,u+10e030-10e039,u+10e03b-10e041,u+10e043-10e044,u+10e047,u+10f1fa,u+10f52c,u+10f531,u+10f536,u+10f69f}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-5.ttf) format("truetype");unicode-range:u+e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086,u+10e048,u+10e04a-10e051,u+10e053-10e054,u+10e058-10e05f,u+10e061-10e067,u+10e069-10e06d,u+10e06f-10e073,u+10e075,u+10e085-10e086}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-9.ttf) format("truetype");unicode-range:u+f80b,u+10f80b}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-12.ttf) format("truetype");unicode-range:u+f8bc,u+10f8bc}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-13.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+10f000,u+10f002,u+10f009-10f00b,u+10f00d-10f00e,u+10f010,u+10f013,u+10f01e,u+10f021-10f022,u+10f026-10f029,u+10f02b,u+10f032-10f039,u+10f03b-10f03c,u+10f042-10f044,u+10f047-10f049,u+10f050-10f05a,u+10f05e,u+10f066,u+10f06a,u+10f070-10f071,u+10f073-10f074,u+10f076,u+10f079-10f07a,u+10f07c-10f07e,u+10f080,u+10f083,u+10f085,u+10f089,u+10f08b,u+10f08d-10f08e,u+10f090}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-14.ttf) format("truetype");unicode-range:u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f157,u+f381-f382,u+10f094,u+10f098,u+10f09e,u+10f0a0-10f0a1,u+10f0a4-10f0a5,u+10f0a7-10f0ab,u+10f0ae,u+10f0b2,u+10f0c3-10f0c4,u+10f0c7,u+10f0ca-10f0cb,u+10f0cd,u+10f0d0,u+10f0d8-10f0db,u+10f0dd-10f0de,u+10f0e2-10f0e3,u+10f0e9-10f0ea,u+10f0ec-10f0ee,u+10f0f0-10f0f2,u+10f0f4,u+10f0f9-10f0fe,u+10f100-10f105,u+10f107,u+10f10a-10f10b,u+10f10d,u+10f118-10f11b,u+10f120,u+10f122,u+10f124-10f125,u+10f127,u+10f12b,u+10f134,u+10f137-10f13a,u+10f13e,u+10f141-10f144,u+10f146,u+10f148-10f14d,u+10f150-10f154,u+10f156-10f157,u+10f381-10f382}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-15.ttf) format("truetype");unicode-range:u+f158-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f258,u+10f158-10f159,u+10f15c-10f15e,u+10f160-10f163,u+10f165,u+10f175-10f178,u+10f182-10f183,u+10f185,u+10f187,u+10f191-10f193,u+10f195,u+10f197,u+10f199,u+10f19c-10f19d,u+10f1ac,u+10f1b0,u+10f1b3,u+10f1b9-10f1ba,u+10f1c4-10f1c9,u+10f1cd,u+10f1da,u+10f1dd-10f1de,u+10f1e0-10f1e3,u+10f1f6,u+10f1fb-10f1fe,u+10f200-10f201,u+10f204,u+10f206-10f207,u+10f20b,u+10f218,u+10f21e,u+10f221-10f22c,u+10f235-10f236,u+10f239,u+10f240-10f245,u+10f247-10f249,u+10f24e,u+10f252-10f258}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-16.ttf) format("truetype");unicode-range:u+f259-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f318,u+f4e6,u+f8e5,u+10f259-10f25c,u+10f26c,u+10f271-10f274,u+10f276-10f277,u+10f279-10f27a,u+10f28b,u+10f28d,u+10f290-10f291,u+10f29a,u+10f29d,u+10f2a1-10f2a4,u+10f2a8,u+10f2b6,u+10f2b9,u+10f2bd,u+10f2c1-10f2c2,u+10f2c7-10f2cb,u+10f2ce,u+10f2d0-10f2d1,u+10f2d3,u+10f2dc,u+10f2e2-10f2eb,u+10f2ed-10f2ee,u+10f2f0-10f2f6,u+10f2f8-10f2fb,u+10f2fd-10f2fe,u+10f300-10f301,u+10f304-10f315,u+10f317-10f318,u+10f4e6,u+10f8e5}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-17.ttf) format("truetype");unicode-range:u+f319,u+f31c-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0,u+10f319,u+10f31c-10f31e,u+10f320-10f327,u+10f329-10f32e,u+10f330-10f331,u+10f333-10f334,u+10f336-10f33e,u+10f340-10f34e,u+10f350-10f35b,u+10f35d,u+10f360-10f362,u+10f364-10f367,u+10f376-10f377,u+10f386-10f387,u+10f389-10f38a,u+10f39b-10f39c,u+10f3a0,u+10f3a5,u+10f3b3,u+10f3be-10f3bf,u+10f3c1-10f3c2,u+10f3c9,u+10f3cd,u+10f3cf,u+10f3d1,u+10f3dd-10f3de,u+10f3e0,u+10f3ed,u+10f3f0}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-18.ttf) format("truetype");unicode-range:u+f3f1-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458,u+f45a-f47c,u+f47e-f480,u+f482-f485,u+f4a1,u+10f3f1-10f3f2,u+10f3fa,u+10f3fc,u+10f3ff-10f401,u+10f406,u+10f40f-10f410,u+10f422,u+10f424,u+10f432-10f434,u+10f436-10f44a,u+10f44c,u+10f44e-10f451,u+10f453-10f456,u+10f458,u+10f45a-10f47c,u+10f47e-10f480,u+10f482-10f485,u+10f4a1}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-19.ttf) format("truetype");unicode-range:u+f486-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4a0,u+f4a2-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f505,u+10f486-10f489,u+10f48b-10f48e,u+10f491-10f492,u+10f495-10f497,u+10f499-10f4a0,u+10f4a2-10f4b6,u+10f4b8-10f4c9,u+10f4cb,u+10f4cd-10f4d0,u+10f4d2-10f4d4,u+10f4d6,u+10f4d9-10f4e1,u+10f4e3,u+10f4fa-10f502,u+10f504-10f505}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-20.ttf) format("truetype");unicode-range:u+f506-f507,u+f509,u+f515-f51a,u+f51c,u+f51f-f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f588,u+10f506-10f507,u+10f509,u+10f515-10f51a,u+10f51c,u+10f51f-10f520,u+10f523-10f52a,u+10f52d-10f52f,u+10f532-10f534,u+10f537-10f53d,u+10f53f-10f540,u+10f542,u+10f546-10f547,u+10f54a-10f54d,u+10f54f-10f554,u+10f556-10f558,u+10f55a-10f55c,u+10f55e-10f563,u+10f565-10f56b,u+10f56d,u+10f571-10f576,u+10f579-10f588}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-21.ttf) format("truetype");unicode-range:u+f589-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f602,u+10f589-10f58f,u+10f591,u+10f593,u+10f596-10f59d,u+10f59f-10f5a0,u+10f5a4-10f5a7,u+10f5a9,u+10f5ac-10f5af,u+10f5b1,u+10f5b3-10f5b4,u+10f5b6,u+10f5b8-10f5b9,u+10f5bb-10f5bd,u+10f5c0-10f5c5,u+10f5c7-10f5c9,u+10f5cb,u+10f5cd-10f5ce,u+10f5d0-10f5da,u+10f5dd-10f5ee,u+10f5f0,u+10f5f3-10f5f5,u+10f5f8-10f5f9,u+10f5fc,u+10f5fe-10f602}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-22.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f603-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f668,u+10f0e4,u+10f3fd,u+10f603-10f60e,u+10f610-10f611,u+10f613-10f620,u+10f622-10f63a,u+10f63c-10f63e,u+10f640-10f641,u+10f643-10f648,u+10f64b-10f64e,u+10f650-10f652,u+10f655,u+10f657-10f65a,u+10f65c-10f663,u+10f665-10f668}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-23.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-23.ttf) format("truetype");unicode-range:u+f669-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d1,u+10f669-10f672,u+10f674,u+10f676-10f67c,u+10f67e,u+10f680-10f694,u+10f696-10f69b,u+10f69e,u+10f6a0-10f6a7,u+10f6a9-10f6ae,u+10f6b0-10f6be,u+10f6c1-10f6c8,u+10f6cb,u+10f6cd-10f6d1}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-24.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-24.ttf) format("truetype");unicode-range:u+f6d2-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f73e,u+10f6d2-10f6d4,u+10f6d6-10f6db,u+10f6dd-10f6e2,u+10f6e4-10f6e8,u+10f6ea-10f6f4,u+10f6f6-10f6f9,u+10f6fb-10f6fe,u+10f701-10f703,u+10f705-10f70a,u+10f70c-10f70d,u+10f70f-10f714,u+10f716-10f71a,u+10f71c-10f72d,u+10f732,u+10f735-10f73e}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-25.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-25.ttf) format("truetype");unicode-range:u+f73f-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+10f73f-10f746,u+10f748-10f754,u+10f756,u+10f758-10f75b,u+10f75e-10f761,u+10f763-10f772,u+10f774-10f777,u+10f779,u+10f77d-10f780,u+10f782-10f783,u+10f786-10f787,u+10f78a-10f78c,u+10f78e-10f78f,u+10f792-10f796,u+10f79a-10f7ae}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-26.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-26.ttf) format("truetype");unicode-range:u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f822,u+10f7b4-10f7b5,u+10f7b7-10f7ba,u+10f7be-10f7c5,u+10f7c7-10f7d2,u+10f7d4,u+10f7d7-10f7de,u+10f7e2,u+10f7e4-10f7ed,u+10f7ef-10f7fe,u+10f800,u+10f802-10f803,u+10f805-10f809,u+10f80d-10f812,u+10f815-10f822}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-27.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-27.ttf) format("truetype");unicode-range:u+f823-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f892,u+f895-f896,u+f898-f89a,u+10f823-10f82e,u+10f831-10f833,u+10f83e,u+10f843-10f844,u+10f847-10f84f,u+10f851-10f854,u+10f856-10f857,u+10f85a-10f85b,u+10f85d-10f865,u+10f867-10f86c,u+10f86e-10f870,u+10f872-10f874,u+10f876-10f892,u+10f895-10f896,u+10f898-10f89a}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-28.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-28.ttf) format("truetype");unicode-range:u+f89b-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff,u+10f89b-10f89d,u+10f8a0-10f8a5,u+10f8a7-10f8a8,u+10f8aa-10f8b0,u+10f8b3-10f8ba,u+10f8bd-10f8c6,u+10f8c8-10f8c9,u+10f8cb-10f8d1,u+10f8d3-10f8d5,u+10f8d8,u+10f8da-10f8de,u+10f8e2-10f8e4,u+10f8e6,u+10f8e9-10f8ed,u+10f8f0-10f8fc,u+10f8fe-10f8ff}</style><style media="all" id="fa-main">/*!
 * Font Awesome Pro 6.5.2 by @fontawesome - https://fontawesome.com
 * License - https://fontawesome.com/license (Commercial License)
 * Copyright 2024 Fonticons, Inc.
 */.fa{font-family:var(--fa-style-family,"Font Awesome 6 Pro");font-weight:var(--fa-style,900)}.fa,.fa-brands,.fa-classic,.fa-duotone,.fa-light,.fa-regular,.fa-sharp,.fa-sharp-solid,.fa-solid,.fa-thin,.fab,.fad,.fal,.far,.fas,.fasl,.fasr,.fass,.fast,.fat{-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:var(--fa-display,inline-block);font-style:normal;font-variant:normal;line-height:1;text-rendering:auto}.fa-classic,.fa-light,.fa-regular,.fa-solid,.fa-thin,.fal,.far,.fas,.fat{font-family:"Font Awesome 6 Pro"}.fa-brands,.fab{font-family:"Font Awesome 6 Brands"}.fa-classic.fa-duotone,.fa-duotone,.fad{font-family:"Font Awesome 6 Duotone"}.fa-sharp,.fasl,.fasr,.fass,.fast{font-family:"Font Awesome 6 Sharp"}.fa-sharp,.fass{font-weight:900}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-2xs{font-size:.625em;line-height:.1em;vertical-align:.225em}.fa-xs{font-size:.75em;line-height:.08333em;vertical-align:.125em}.fa-sm{font-size:.875em;line-height:.07143em;vertical-align:.05357em}.fa-lg{font-size:1.25em;line-height:.05em;vertical-align:-.075em}.fa-xl{font-size:1.5em;line-height:.04167em;vertical-align:-.125em}.fa-2xl{font-size:2em;line-height:.03125em;vertical-align:-.1875em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:var(--fa-li-margin,2.5em);padding-left:0}.fa-ul>li{position:relative}.fa-li{left:calc(var(--fa-li-width, 2em)*-1);position:absolute;text-align:center;width:var(--fa-li-width,2em);line-height:inherit}.fa-border{border-radius:var(--fa-border-radius,.1em);border:var(--fa-border-width,.08em) var(--fa-border-style,solid) var(--fa-border-color,#eee);padding:var(--fa-border-padding,.2em .25em .15em)}.fa-pull-left{float:left;margin-right:var(--fa-pull-margin,.3em)}.fa-pull-right{float:right;margin-left:var(--fa-pull-margin,.3em)}.fa-beat{-webkit-animation-name:fa-beat;animation-name:fa-beat;-webkit-animation-delay:var(--fa-animation-delay,0s);animation-delay:var(--fa-animation-delay,0s);-webkit-animation-direction:var(--fa-animation-direction,normal);animation-direction:var(--fa-animation-direction,normal);-webkit-animation-duration:var(--fa-animation-duration,1s);animation-duration:var(--fa-animation-duration,1s);-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,ease-in-out);animation-timing-function:var(--fa-animation-timing,ease-in-out)}.fa-bounce{-webkit-animation-name:fa-bounce;animation-name:fa-bounce;-webkit-animation-delay:var(--fa-animation-delay,0s);animation-delay:var(--fa-animation-delay,0s);-webkit-animation-direction:var(--fa-animation-direction,normal);animation-direction:var(--fa-animation-direction,normal);-webkit-animation-duration:var(--fa-animation-duration,1s);animation-duration:var(--fa-animation-duration,1s);-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,cubic-bezier(.28,.84,.42,1));animation-timing-function:var(--fa-animation-timing,cubic-bezier(.28,.84,.42,1))}.fa-fade{-webkit-animation-name:fa-fade;animation-name:fa-fade;-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,cubic-bezier(.4,0,.6,1));animation-timing-function:var(--fa-animation-timing,cubic-bezier(.4,0,.6,1))}.fa-beat-fade,.fa-fade{-webkit-animation-delay:var(--fa-animation-delay,0s);animation-delay:var(--fa-animation-delay,0s);-webkit-animation-direction:var(--fa-animation-direction,normal);animation-direction:var(--fa-animation-direction,normal);-webkit-animation-duration:var(--fa-animation-duration,1s);animation-duration:var(--fa-animation-duration,1s)}.fa-beat-fade{-webkit-animation-name:fa-beat-fade;animation-name:fa-beat-fade;-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,cubic-bezier(.4,0,.6,1));animation-timing-function:var(--fa-animation-timing,cubic-bezier(.4,0,.6,1))}.fa-flip{-webkit-animation-name:fa-flip;animation-name:fa-flip;-webkit-animation-delay:var(--fa-animation-delay,0s);animation-delay:var(--fa-animation-delay,0s);-webkit-animation-direction:var(--fa-animation-direction,normal);animation-direction:var(--fa-animation-direction,normal);-webkit-animation-duration:var(--fa-animation-duration,1s);animation-duration:var(--fa-animation-duration,1s);-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,ease-in-out);animation-timing-function:var(--fa-animation-timing,ease-in-out)}.fa-shake{-webkit-animation-name:fa-shake;animation-name:fa-shake;-webkit-animation-duration:var(--fa-animation-duration,1s);animation-duration:var(--fa-animation-duration,1s);-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,linear);animation-timing-function:var(--fa-animation-timing,linear)}.fa-shake,.fa-spin{-webkit-animation-delay:var(--fa-animation-delay,0s);animation-delay:var(--fa-animation-delay,0s);-webkit-animation-direction:var(--fa-animation-direction,normal);animation-direction:var(--fa-animation-direction,normal)}.fa-spin{-webkit-animation-name:fa-spin;animation-name:fa-spin;-webkit-animation-duration:var(--fa-animation-duration,2s);animation-duration:var(--fa-animation-duration,2s);-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,linear);animation-timing-function:var(--fa-animation-timing,linear)}.fa-spin-reverse{--fa-animation-direction:reverse}.fa-pulse,.fa-spin-pulse{-webkit-animation-name:fa-spin;animation-name:fa-spin;-webkit-animation-direction:var(--fa-animation-direction,normal);animation-direction:var(--fa-animation-direction,normal);-webkit-animation-duration:var(--fa-animation-duration,1s);animation-duration:var(--fa-animation-duration,1s);-webkit-animation-iteration-count:var(--fa-animation-iteration-count,infinite);animation-iteration-count:var(--fa-animation-iteration-count,infinite);-webkit-animation-timing-function:var(--fa-animation-timing,steps(8));animation-timing-function:var(--fa-animation-timing,steps(8))}@media (prefers-reduced-motion:reduce){.fa-beat,.fa-beat-fade,.fa-bounce,.fa-fade,.fa-flip,.fa-pulse,.fa-shake,.fa-spin,.fa-spin-pulse{-webkit-animation-delay:-1ms;animation-delay:-1ms;-webkit-animation-duration:1ms;animation-duration:1ms;-webkit-animation-iteration-count:1;animation-iteration-count:1;-webkit-transition-delay:0s;transition-delay:0s;-webkit-transition-duration:0s;transition-duration:0s}}@-webkit-keyframes fa-beat{0%,90%{-webkit-transform:scale(1);transform:scale(1)}45%{-webkit-transform:scale(var(--fa-beat-scale,1.25));transform:scale(var(--fa-beat-scale,1.25))}}@keyframes fa-beat{0%,90%{-webkit-transform:scale(1);transform:scale(1)}45%{-webkit-transform:scale(var(--fa-beat-scale,1.25));transform:scale(var(--fa-beat-scale,1.25))}}@-webkit-keyframes fa-bounce{0%{-webkit-transform:scale(1) translateY(0);transform:scale(1) translateY(0)}10%{-webkit-transform:scale(var(--fa-bounce-start-scale-x,1.1),var(--fa-bounce-start-scale-y,.9)) translateY(0);transform:scale(var(--fa-bounce-start-scale-x,1.1),var(--fa-bounce-start-scale-y,.9)) translateY(0)}30%{-webkit-transform:scale(var(--fa-bounce-jump-scale-x,.9),var(--fa-bounce-jump-scale-y,1.1)) translateY(var(--fa-bounce-height,-.5em));transform:scale(var(--fa-bounce-jump-scale-x,.9),var(--fa-bounce-jump-scale-y,1.1)) translateY(var(--fa-bounce-height,-.5em))}50%{-webkit-transform:scale(var(--fa-bounce-land-scale-x,1.05),var(--fa-bounce-land-scale-y,.95)) translateY(0);transform:scale(var(--fa-bounce-land-scale-x,1.05),var(--fa-bounce-land-scale-y,.95)) translateY(0)}57%{-webkit-transform:scale(1) translateY(var(--fa-bounce-rebound,-.125em));transform:scale(1) translateY(var(--fa-bounce-rebound,-.125em))}64%{-webkit-transform:scale(1) translateY(0);transform:scale(1) translateY(0)}to{-webkit-transform:scale(1) translateY(0);transform:scale(1) translateY(0)}}@keyframes fa-bounce{0%{-webkit-transform:scale(1) translateY(0);transform:scale(1) translateY(0)}10%{-webkit-transform:scale(var(--fa-bounce-start-scale-x,1.1),var(--fa-bounce-start-scale-y,.9)) translateY(0);transform:scale(var(--fa-bounce-start-scale-x,1.1),var(--fa-bounce-start-scale-y,.9)) translateY(0)}30%{-webkit-transform:scale(var(--fa-bounce-jump-scale-x,.9),var(--fa-bounce-jump-scale-y,1.1)) translateY(var(--fa-bounce-height,-.5em));transform:scale(var(--fa-bounce-jump-scale-x,.9),var(--fa-bounce-jump-scale-y,1.1)) translateY(var(--fa-bounce-height,-.5em))}50%{-webkit-transform:scale(var(--fa-bounce-land-scale-x,1.05),var(--fa-bounce-land-scale-y,.95)) translateY(0);transform:scale(var(--fa-bounce-land-scale-x,1.05),var(--fa-bounce-land-scale-y,.95)) translateY(0)}57%{-webkit-transform:scale(1) translateY(var(--fa-bounce-rebound,-.125em));transform:scale(1) translateY(var(--fa-bounce-rebound,-.125em))}64%{-webkit-transform:scale(1) translateY(0);transform:scale(1) translateY(0)}to{-webkit-transform:scale(1) translateY(0);transform:scale(1) translateY(0)}}@-webkit-keyframes fa-fade{50%{opacity:var(--fa-fade-opacity,.4)}}@keyframes fa-fade{50%{opacity:var(--fa-fade-opacity,.4)}}@-webkit-keyframes fa-beat-fade{0%,to{opacity:var(--fa-beat-fade-opacity,.4);-webkit-transform:scale(1);transform:scale(1)}50%{opacity:1;-webkit-transform:scale(var(--fa-beat-fade-scale,1.125));transform:scale(var(--fa-beat-fade-scale,1.125))}}@keyframes fa-beat-fade{0%,to{opacity:var(--fa-beat-fade-opacity,.4);-webkit-transform:scale(1);transform:scale(1)}50%{opacity:1;-webkit-transform:scale(var(--fa-beat-fade-scale,1.125));transform:scale(var(--fa-beat-fade-scale,1.125))}}@-webkit-keyframes fa-flip{50%{-webkit-transform:rotate3d(var(--fa-flip-x,0),var(--fa-flip-y,1),var(--fa-flip-z,0),var(--fa-flip-angle,-180deg));transform:rotate3d(var(--fa-flip-x,0),var(--fa-flip-y,1),var(--fa-flip-z,0),var(--fa-flip-angle,-180deg))}}@keyframes fa-flip{50%{-webkit-transform:rotate3d(var(--fa-flip-x,0),var(--fa-flip-y,1),var(--fa-flip-z,0),var(--fa-flip-angle,-180deg));transform:rotate3d(var(--fa-flip-x,0),var(--fa-flip-y,1),var(--fa-flip-z,0),var(--fa-flip-angle,-180deg))}}@-webkit-keyframes fa-shake{0%{-webkit-transform:rotate(-15deg);transform:rotate(-15deg)}4%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}8%,24%{-webkit-transform:rotate(-18deg);transform:rotate(-18deg)}12%,28%{-webkit-transform:rotate(18deg);transform:rotate(18deg)}16%{-webkit-transform:rotate(-22deg);transform:rotate(-22deg)}20%{-webkit-transform:rotate(22deg);transform:rotate(22deg)}32%{-webkit-transform:rotate(-12deg);transform:rotate(-12deg)}36%{-webkit-transform:rotate(12deg);transform:rotate(12deg)}40%,to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}@keyframes fa-shake{0%{-webkit-transform:rotate(-15deg);transform:rotate(-15deg)}4%{-webkit-transform:rotate(15deg);transform:rotate(15deg)}8%,24%{-webkit-transform:rotate(-18deg);transform:rotate(-18deg)}12%,28%{-webkit-transform:rotate(18deg);transform:rotate(18deg)}16%{-webkit-transform:rotate(-22deg);transform:rotate(-22deg)}20%{-webkit-transform:rotate(22deg);transform:rotate(22deg)}32%{-webkit-transform:rotate(-12deg);transform:rotate(-12deg)}36%{-webkit-transform:rotate(12deg);transform:rotate(12deg)}40%,to{-webkit-transform:rotate(0deg);transform:rotate(0deg)}}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(1turn);transform:rotate(1turn)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scaleX(-1);transform:scaleX(-1)}.fa-flip-vertical{-webkit-transform:scaleY(-1);transform:scaleY(-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1);transform:scale(-1)}.fa-rotate-by{-webkit-transform:rotate(var(--fa-rotate-angle,0));transform:rotate(var(--fa-rotate-angle,0))}.fa-stack{display:inline-block;height:2em;line-height:2em;position:relative;vertical-align:middle;width:2.5em}.fa-stack-1x,.fa-stack-2x{left:0;position:absolute;text-align:center;width:100%;z-index:var(--fa-stack-z-index,auto)}.fa-stack-1x{line-height:inherit}.fa-stack-2x{font-size:2em}.fa-inverse{color:var(--fa-inverse,#fff)}.fa-0:before{content:"\30"}.fa-1:before{content:"\31"}.fa-2:before{content:"\32"}.fa-3:before{content:"\33"}.fa-4:before{content:"\34"}.fa-5:before{content:"\35"}.fa-6:before{content:"\36"}.fa-7:before{content:"\37"}.fa-8:before{content:"\38"}.fa-9:before{content:"\39"}.fa-fill-drip:before{content:"\f576"}.fa-arrows-to-circle:before{content:"\e4bd"}.fa-chevron-circle-right:before,.fa-circle-chevron-right:before{content:"\f138"}.fa-wagon-covered:before{content:"\f8ee"}.fa-line-height:before{content:"\f871"}.fa-bagel:before{content:"\e3d7"}.fa-transporter-7:before{content:"\e2a8"}.fa-at:before{content:"\40"}.fa-rectangles-mixed:before{content:"\e323"}.fa-phone-arrow-up-right:before,.fa-phone-arrow-up:before,.fa-phone-outgoing:before{content:"\e224"}.fa-trash-alt:before,.fa-trash-can:before{content:"\f2ed"}.fa-circle-l:before{content:"\e114"}.fa-head-side-goggles:before,.fa-head-vr:before{content:"\f6ea"}.fa-text-height:before{content:"\f034"}.fa-user-times:before,.fa-user-xmark:before{content:"\f235"}.fa-face-hand-yawn:before{content:"\e379"}.fa-gauge-simple-min:before,.fa-tachometer-slowest:before{content:"\f62d"}.fa-stethoscope:before{content:"\f0f1"}.fa-coffin:before{content:"\f6c6"}.fa-comment-alt:before,.fa-message:before{content:"\f27a"}.fa-bowl-salad:before,.fa-salad:before{content:"\f81e"}.fa-info:before{content:"\f129"}.fa-robot-astromech:before{content:"\e2d2"}.fa-ring-diamond:before{content:"\e5ab"}.fa-fondue-pot:before{content:"\e40d"}.fa-theta:before{content:"\f69e"}.fa-face-hand-peeking:before{content:"\e481"}.fa-square-user:before{content:"\e283"}.fa-compress-alt:before,.fa-down-left-and-up-right-to-center:before{content:"\f422"}.fa-explosion:before{content:"\e4e9"}.fa-file-alt:before,.fa-file-lines:before,.fa-file-text:before{content:"\f15c"}.fa-wave-square:before{content:"\f83e"}.fa-ring:before{content:"\f70b"}.fa-building-un:before{content:"\e4d9"}.fa-dice-three:before{content:"\f527"}.fa-tire-pressure-warning:before{content:"\f633"}.fa-wifi-2:before,.fa-wifi-fair:before{content:"\f6ab"}.fa-calendar-alt:before,.fa-calendar-days:before{content:"\f073"}.fa-mp3-player:before{content:"\f8ce"}.fa-anchor-circle-check:before{content:"\e4aa"}.fa-tally-4:before{content:"\e297"}.fa-rectangle-history:before{content:"\e4a2"}.fa-building-circle-arrow-right:before{content:"\e4d1"}.fa-volleyball-ball:before,.fa-volleyball:before{content:"\f45f"}.fa-sun-haze:before{content:"\f765"}.fa-text-size:before{content:"\f894"}.fa-ufo:before{content:"\e047"}.fa-fork:before,.fa-utensil-fork:before{content:"\f2e3"}.fa-arrows-up-to-line:before{content:"\e4c2"}.fa-mobile-signal:before{content:"\e1ef"}.fa-barcode-scan:before{content:"\f465"}.fa-sort-desc:before,.fa-sort-down:before{content:"\f0dd"}.fa-folder-arrow-down:before,.fa-folder-download:before{content:"\e053"}.fa-circle-minus:before,.fa-minus-circle:before{content:"\f056"}.fa-face-icicles:before{content:"\e37c"}.fa-shovel:before{content:"\f713"}.fa-door-open:before{content:"\f52b"}.fa-films:before{content:"\e17a"}.fa-right-from-bracket:before,.fa-sign-out-alt:before{content:"\f2f5"}.fa-face-glasses:before{content:"\e377"}.fa-nfc:before{content:"\e1f7"}.fa-atom:before{content:"\f5d2"}.fa-soap:before{content:"\e06e"}.fa-heart-music-camera-bolt:before,.fa-icons:before{content:"\f86d"}.fa-microphone-alt-slash:before,.fa-microphone-lines-slash:before{content:"\f539"}.fa-closed-captioning-slash:before{content:"\e135"}.fa-calculator-alt:before,.fa-calculator-simple:before{content:"\f64c"}.fa-bridge-circle-check:before{content:"\e4c9"}.fa-sliders-up:before,.fa-sliders-v:before{content:"\f3f1"}.fa-location-minus:before,.fa-map-marker-minus:before{content:"\f609"}.fa-pump-medical:before{content:"\e06a"}.fa-fingerprint:before{content:"\f577"}.fa-ski-boot:before{content:"\e3cc"}.fa-rectangle-sd:before,.fa-standard-definition:before{content:"\e28a"}.fa-h1:before{content:"\f313"}.fa-hand-point-right:before{content:"\f0a4"}.fa-magnifying-glass-location:before,.fa-search-location:before{content:"\f689"}.fa-message-bot:before{content:"\e3b8"}.fa-forward-step:before,.fa-step-forward:before{content:"\f051"}.fa-face-smile-beam:before,.fa-smile-beam:before{content:"\f5b8"}.fa-light-ceiling:before{content:"\e016"}.fa-comment-alt-exclamation:before,.fa-message-exclamation:before{content:"\f4a5"}.fa-bowl-scoop:before,.fa-bowl-shaved-ice:before{content:"\e3de"}.fa-square-x:before{content:"\e286"}.fa-building-memo:before{content:"\e61e"}.fa-utility-pole-double:before{content:"\e2c4"}.fa-flag-checkered:before{content:"\f11e"}.fa-chevron-double-up:before,.fa-chevrons-up:before{content:"\f325"}.fa-football-ball:before,.fa-football:before{content:"\f44e"}.fa-user-vneck:before{content:"\e461"}.fa-school-circle-exclamation:before{content:"\e56c"}.fa-crop:before{content:"\f125"}.fa-angle-double-down:before,.fa-angles-down:before{content:"\f103"}.fa-users-rectangle:before{content:"\e594"}.fa-people-roof:before{content:"\e537"}.fa-arrow-square-right:before,.fa-square-arrow-right:before{content:"\f33b"}.fa-location-plus:before,.fa-map-marker-plus:before{content:"\f60a"}.fa-lightbulb-exclamation-on:before{content:"\e1ca"}.fa-people-line:before{content:"\e534"}.fa-beer-mug-empty:before,.fa-beer:before{content:"\f0fc"}.fa-crate-empty:before{content:"\e151"}.fa-diagram-predecessor:before{content:"\e477"}.fa-transporter:before{content:"\e042"}.fa-calendar-circle-user:before{content:"\e471"}.fa-arrow-up-long:before,.fa-long-arrow-up:before{content:"\f176"}.fa-person-carry-box:before,.fa-person-carry:before{content:"\f4cf"}.fa-burn:before,.fa-fire-flame-simple:before{content:"\f46a"}.fa-male:before,.fa-person:before{content:"\f183"}.fa-laptop:before{content:"\f109"}.fa-file-csv:before{content:"\f6dd"}.fa-menorah:before{content:"\f676"}.fa-union:before{content:"\f6a2"}.fa-chevron-double-left:before,.fa-chevrons-left:before{content:"\f323"}.fa-circle-heart:before,.fa-heart-circle:before{content:"\f4c7"}.fa-truck-plane:before{content:"\e58f"}.fa-record-vinyl:before{content:"\f8d9"}.fa-bring-forward:before{content:"\f856"}.fa-square-p:before{content:"\e279"}.fa-face-grin-stars:before,.fa-grin-stars:before{content:"\f587"}.fa-sigma:before{content:"\f68b"}.fa-camera-movie:before{content:"\f8a9"}.fa-bong:before{content:"\f55c"}.fa-clarinet:before{content:"\f8ad"}.fa-truck-flatbed:before{content:"\e2b6"}.fa-pastafarianism:before,.fa-spaghetti-monster-flying:before{content:"\f67b"}.fa-arrow-down-up-across-line:before{content:"\e4af"}.fa-arrows-rotate-reverse:before{content:"\e630"}.fa-leaf-heart:before{content:"\f4cb"}.fa-house-building:before{content:"\e1b1"}.fa-cheese-swiss:before{content:"\f7f0"}.fa-spoon:before,.fa-utensil-spoon:before{content:"\f2e5"}.fa-jar-wheat:before{content:"\e517"}.fa-envelopes-bulk:before,.fa-mail-bulk:before{content:"\f674"}.fa-file-circle-exclamation:before{content:"\e4eb"}.fa-bow-arrow:before{content:"\f6b9"}.fa-cart-xmark:before{content:"\e0dd"}.fa-hexagon-xmark:before,.fa-times-hexagon:before,.fa-xmark-hexagon:before{content:"\f2ee"}.fa-circle-h:before,.fa-hospital-symbol:before{content:"\f47e"}.fa-merge:before{content:"\e526"}.fa-pager:before{content:"\f815"}.fa-cart-minus:before{content:"\e0db"}.fa-address-book:before,.fa-contact-book:before{content:"\f2b9"}.fa-pan-frying:before{content:"\e42c"}.fa-grid-3:before,.fa-grid:before{content:"\e195"}.fa-football-helmet:before{content:"\f44f"}.fa-hand-love:before{content:"\e1a5"}.fa-trees:before{content:"\f724"}.fa-strikethrough:before{content:"\f0cc"}.fa-page:before{content:"\e428"}.fa-k:before{content:"\4b"}.fa-diagram-previous:before{content:"\e478"}.fa-gauge-min:before,.fa-tachometer-alt-slowest:before{content:"\f628"}.fa-folder-grid:before{content:"\e188"}.fa-eggplant:before{content:"\e16c"}.fa-excavator:before{content:"\e656"}.fa-ram:before{content:"\f70a"}.fa-landmark-flag:before{content:"\e51c"}.fa-lips:before{content:"\f600"}.fa-pencil-alt:before,.fa-pencil:before{content:"\f303"}.fa-backward:before{content:"\f04a"}.fa-caret-right:before{content:"\f0da"}.fa-comments:before{content:"\f086"}.fa-file-clipboard:before,.fa-paste:before{content:"\f0ea"}.fa-desktop-arrow-down:before{content:"\e155"}.fa-code-pull-request:before{content:"\e13c"}.fa-pumpkin:before{content:"\f707"}.fa-clipboard-list:before{content:"\f46d"}.fa-pen-field:before{content:"\e211"}.fa-blueberries:before{content:"\e2e8"}.fa-truck-loading:before,.fa-truck-ramp-box:before{content:"\f4de"}.fa-note:before{content:"\e1ff"}.fa-arrow-down-to-square:before{content:"\e096"}.fa-user-check:before{content:"\f4fc"}.fa-cloud-xmark:before{content:"\e35f"}.fa-vial-virus:before{content:"\e597"}.fa-book-alt:before,.fa-book-blank:before{content:"\f5d9"}.fa-golf-flag-hole:before{content:"\e3ac"}.fa-comment-alt-arrow-down:before,.fa-message-arrow-down:before{content:"\e1db"}.fa-face-unamused:before{content:"\e39f"}.fa-sheet-plastic:before{content:"\e571"}.fa-circle-9:before{content:"\e0f6"}.fa-blog:before{content:"\f781"}.fa-user-ninja:before{content:"\f504"}.fa-pencil-slash:before{content:"\e215"}.fa-bowling-pins:before{content:"\f437"}.fa-person-arrow-up-from-line:before{content:"\e539"}.fa-down-right:before{content:"\e16b"}.fa-scroll-torah:before,.fa-torah:before{content:"\f6a0"}.fa-webhook:before{content:"\e5d5"}.fa-blinds-open:before{content:"\f8fc"}.fa-fence:before{content:"\e303"}.fa-arrow-alt-up:before,.fa-up:before{content:"\f357"}.fa-broom-ball:before,.fa-quidditch-broom-ball:before,.fa-quidditch:before{content:"\f458"}.fa-drumstick:before{content:"\f6d6"}.fa-square-v:before{content:"\e284"}.fa-face-awesome:before,.fa-gave-dandy:before{content:"\e409"}.fa-dial-off:before{content:"\e162"}.fa-toggle-off:before{content:"\f204"}.fa-face-smile-horns:before{content:"\e391"}.fa-archive:before,.fa-box-archive:before{content:"\f187"}.fa-grapes:before{content:"\e306"}.fa-person-drowning:before{content:"\e545"}.fa-dial-max:before{content:"\e15e"}.fa-circle-m:before{content:"\e115"}.fa-calendar-image:before{content:"\e0d4"}.fa-caret-circle-down:before,.fa-circle-caret-down:before{content:"\f32d"}.fa-arrow-down-9-1:before,.fa-sort-numeric-desc:before,.fa-sort-numeric-down-alt:before{content:"\f886"}.fa-face-grin-tongue-squint:before,.fa-grin-tongue-squint:before{content:"\f58a"}.fa-shish-kebab:before{content:"\f821"}.fa-spray-can:before{content:"\f5bd"}.fa-alarm-snooze:before{content:"\f845"}.fa-scarecrow:before{content:"\f70d"}.fa-truck-monster:before{content:"\f63b"}.fa-gift-card:before{content:"\f663"}.fa-w:before{content:"\57"}.fa-code-pull-request-draft:before{content:"\e3fa"}.fa-square-b:before{content:"\e264"}.fa-elephant:before{content:"\f6da"}.fa-earth-africa:before,.fa-globe-africa:before{content:"\f57c"}.fa-rainbow:before{content:"\f75b"}.fa-circle-notch:before{content:"\f1ce"}.fa-tablet-alt:before,.fa-tablet-screen-button:before{content:"\f3fa"}.fa-paw:before{content:"\f1b0"}.fa-message-question:before{content:"\e1e3"}.fa-cloud:before{content:"\f0c2"}.fa-trowel-bricks:before{content:"\e58a"}.fa-square-3:before{content:"\e258"}.fa-face-flushed:before,.fa-flushed:before{content:"\f579"}.fa-hospital-user:before{content:"\f80d"}.fa-microwave:before{content:"\e01b"}.fa-chf-sign:before{content:"\e602"}.fa-tent-arrow-left-right:before{content:"\e57f"}.fa-cart-circle-arrow-up:before{content:"\e3f0"}.fa-trash-clock:before{content:"\e2b0"}.fa-reflect-both:before{content:"\e66f"}.fa-gavel:before,.fa-legal:before{content:"\f0e3"}.fa-sprinkler-ceiling:before{content:"\e44c"}.fa-browsers:before{content:"\e0cb"}.fa-trillium:before{content:"\e588"}.fa-music-slash:before{content:"\f8d1"}.fa-truck-ramp:before{content:"\f4e0"}.fa-binoculars:before{content:"\f1e5"}.fa-microphone-slash:before{content:"\f131"}.fa-box-tissue:before{content:"\e05b"}.fa-circle-c:before{content:"\e101"}.fa-star-christmas:before{content:"\f7d4"}.fa-chart-bullet:before{content:"\e0e1"}.fa-motorcycle:before{content:"\f21c"}.fa-tree-christmas:before{content:"\f7db"}.fa-tire-flat:before{content:"\f632"}.fa-sunglasses:before{content:"\f892"}.fa-badge:before{content:"\f335"}.fa-comment-alt-edit:before,.fa-message-edit:before,.fa-message-pen:before{content:"\f4a4"}.fa-bell-concierge:before,.fa-concierge-bell:before{content:"\f562"}.fa-pen-ruler:before,.fa-pencil-ruler:before{content:"\f5ae"}.fa-file-mp3:before{content:"\e648"}.fa-arrow-progress:before{content:"\e5df"}.fa-chess-rook-alt:before,.fa-chess-rook-piece:before{content:"\f448"}.fa-square-root:before{content:"\f697"}.fa-album-collection-circle-plus:before{content:"\e48e"}.fa-people-arrows-left-right:before,.fa-people-arrows:before{content:"\e068"}.fa-sign-post:before{content:"\e624"}.fa-face-angry-horns:before{content:"\e368"}.fa-mars-and-venus-burst:before{content:"\e523"}.fa-tombstone:before{content:"\f720"}.fa-caret-square-right:before,.fa-square-caret-right:before{content:"\f152"}.fa-cut:before,.fa-scissors:before{content:"\f0c4"}.fa-list-music:before{content:"\f8c9"}.fa-sun-plant-wilt:before{content:"\e57a"}.fa-toilets-portable:before{content:"\e584"}.fa-hockey-puck:before{content:"\f453"}.fa-mustache:before{content:"\e5bc"}.fa-hyphen:before{content:"\2d"}.fa-table:before{content:"\f0ce"}.fa-user-chef:before{content:"\e3d2"}.fa-comment-alt-image:before,.fa-message-image:before{content:"\e1e0"}.fa-users-medical:before{content:"\f830"}.fa-sensor-alert:before,.fa-sensor-triangle-exclamation:before{content:"\e029"}.fa-magnifying-glass-arrow-right:before{content:"\e521"}.fa-digital-tachograph:before,.fa-tachograph-digital:before{content:"\f566"}.fa-face-mask:before{content:"\e37f"}.fa-pickleball:before{content:"\e435"}.fa-star-sharp-half:before{content:"\e28c"}.fa-users-slash:before{content:"\e073"}.fa-clover:before{content:"\e139"}.fa-meat:before{content:"\f814"}.fa-mail-reply:before,.fa-reply:before{content:"\f3e5"}.fa-star-and-crescent:before{content:"\f699"}.fa-empty-set:before{content:"\f656"}.fa-house-fire:before{content:"\e50c"}.fa-minus-square:before,.fa-square-minus:before{content:"\f146"}.fa-helicopter:before{content:"\f533"}.fa-bird:before{content:"\e469"}.fa-compass:before{content:"\f14e"}.fa-caret-square-down:before,.fa-square-caret-down:before{content:"\f150"}.fa-heart-half-alt:before,.fa-heart-half-stroke:before{content:"\e1ac"}.fa-file-circle-question:before{content:"\e4ef"}.fa-truck-utensils:before{content:"\e628"}.fa-laptop-code:before{content:"\f5fc"}.fa-joystick:before{content:"\f8c5"}.fa-grill-fire:before{content:"\e5a4"}.fa-rectangle-vertical-history:before{content:"\e237"}.fa-swatchbook:before{content:"\f5c3"}.fa-prescription-bottle:before{content:"\f485"}.fa-bars:before,.fa-navicon:before{content:"\f0c9"}.fa-keyboard-left:before{content:"\e1c3"}.fa-people-group:before{content:"\e533"}.fa-hourglass-3:before,.fa-hourglass-end:before{content:"\f253"}.fa-heart-broken:before,.fa-heart-crack:before{content:"\f7a9"}.fa-face-beam-hand-over-mouth:before{content:"\e47c"}.fa-droplet-percent:before,.fa-humidity:before{content:"\f750"}.fa-external-link-square-alt:before,.fa-square-up-right:before{content:"\f360"}.fa-face-kiss-beam:before,.fa-kiss-beam:before{content:"\f597"}.fa-corn:before{content:"\f6c7"}.fa-roller-coaster:before{content:"\e324"}.fa-photo-film-music:before{content:"\e228"}.fa-radar:before{content:"\e024"}.fa-sickle:before{content:"\f822"}.fa-film:before{content:"\f008"}.fa-coconut:before{content:"\e2f6"}.fa-ruler-horizontal:before{content:"\f547"}.fa-shield-cross:before{content:"\f712"}.fa-cassette-tape:before{content:"\f8ab"}.fa-square-terminal:before{content:"\e32a"}.fa-people-robbery:before{content:"\e536"}.fa-lightbulb:before{content:"\f0eb"}.fa-caret-left:before{content:"\f0d9"}.fa-comment-middle:before{content:"\e149"}.fa-trash-can-list:before{content:"\e2ab"}.fa-block:before{content:"\e46a"}.fa-circle-exclamation:before,.fa-exclamation-circle:before{content:"\f06a"}.fa-school-circle-xmark:before{content:"\e56d"}.fa-arrow-right-from-bracket:before,.fa-sign-out:before{content:"\f08b"}.fa-face-frown-slight:before{content:"\e376"}.fa-chevron-circle-down:before,.fa-circle-chevron-down:before{content:"\f13a"}.fa-sidebar-flip:before{content:"\e24f"}.fa-unlock-alt:before,.fa-unlock-keyhole:before{content:"\f13e"}.fa-temperature-list:before{content:"\e299"}.fa-cloud-showers-heavy:before{content:"\f740"}.fa-headphones-alt:before,.fa-headphones-simple:before{content:"\f58f"}.fa-sitemap:before{content:"\f0e8"}.fa-pipe-section:before{content:"\e438"}.fa-space-station-moon-alt:before,.fa-space-station-moon-construction:before{content:"\e034"}.fa-circle-dollar-to-slot:before,.fa-donate:before{content:"\f4b9"}.fa-memory:before{content:"\f538"}.fa-face-sleeping:before{content:"\e38d"}.fa-road-spikes:before{content:"\e568"}.fa-fire-burner:before{content:"\e4f1"}.fa-squirrel:before{content:"\f71a"}.fa-arrow-to-top:before,.fa-arrow-up-to-line:before{content:"\f341"}.fa-flag:before{content:"\f024"}.fa-face-cowboy-hat:before{content:"\e36e"}.fa-hanukiah:before{content:"\f6e6"}.fa-chart-scatter-3d:before{content:"\e0e8"}.fa-display-chart-up:before{content:"\e5e3"}.fa-square-code:before{content:"\e267"}.fa-feather:before{content:"\f52d"}.fa-volume-down:before,.fa-volume-low:before{content:"\f027"}.fa-times-to-slot:before,.fa-vote-nay:before,.fa-xmark-to-slot:before{content:"\f771"}.fa-box-alt:before,.fa-box-taped:before{content:"\f49a"}.fa-comment-slash:before{content:"\f4b3"}.fa-swords:before{content:"\f71d"}.fa-cloud-sun-rain:before{content:"\f743"}.fa-album:before{content:"\f89f"}.fa-circle-n:before{content:"\e118"}.fa-compress:before{content:"\f066"}.fa-wheat-alt:before,.fa-wheat-awn:before{content:"\e2cd"}.fa-ankh:before{content:"\f644"}.fa-hands-holding-child:before{content:"\e4fa"}.fa-asterisk:before{content:"\2a"}.fa-key-skeleton-left-right:before{content:"\e3b4"}.fa-comment-lines:before{content:"\f4b0"}.fa-luchador-mask:before,.fa-luchador:before,.fa-mask-luchador:before{content:"\f455"}.fa-check-square:before,.fa-square-check:before{content:"\f14a"}.fa-shredder:before{content:"\f68a"}.fa-book-open-alt:before,.fa-book-open-cover:before{content:"\e0c0"}.fa-sandwich:before{content:"\f81f"}.fa-peseta-sign:before{content:"\e221"}.fa-parking-slash:before,.fa-square-parking-slash:before{content:"\f617"}.fa-train-tunnel:before{content:"\e454"}.fa-header:before,.fa-heading:before{content:"\f1dc"}.fa-ghost:before{content:"\f6e2"}.fa-face-anguished:before{content:"\e369"}.fa-hockey-sticks:before{content:"\f454"}.fa-abacus:before{content:"\f640"}.fa-film-alt:before,.fa-film-simple:before{content:"\f3a0"}.fa-list-squares:before,.fa-list:before{content:"\f03a"}.fa-tree-palm:before{content:"\f82b"}.fa-phone-square-alt:before,.fa-square-phone-flip:before{content:"\f87b"}.fa-cart-plus:before{content:"\f217"}.fa-gamepad:before{content:"\f11b"}.fa-border-center-v:before{content:"\f89d"}.fa-circle-dot:before,.fa-dot-circle:before{content:"\f192"}.fa-clipboard-medical:before{content:"\e133"}.fa-dizzy:before,.fa-face-dizzy:before{content:"\f567"}.fa-egg:before{content:"\f7fb"}.fa-arrow-alt-to-top:before,.fa-up-to-line:before{content:"\f34d"}.fa-house-medical-circle-xmark:before{content:"\e513"}.fa-watch-fitness:before{content:"\f63e"}.fa-clock-nine-thirty:before{content:"\e34d"}.fa-campground:before{content:"\f6bb"}.fa-folder-plus:before{content:"\f65e"}.fa-jug:before{content:"\f8c6"}.fa-futbol-ball:before,.fa-futbol:before,.fa-soccer-ball:before{content:"\f1e3"}.fa-snow-blowing:before{content:"\f761"}.fa-paint-brush:before,.fa-paintbrush:before{content:"\f1fc"}.fa-lock:before{content:"\f023"}.fa-arrow-down-from-line:before,.fa-arrow-from-top:before{content:"\f345"}.fa-gas-pump:before{content:"\f52f"}.fa-signal-alt-slash:before,.fa-signal-bars-slash:before{content:"\f694"}.fa-monkey:before{content:"\f6fb"}.fa-pro:before,.fa-rectangle-pro:before{content:"\e235"}.fa-house-night:before{content:"\e010"}.fa-hot-tub-person:before,.fa-hot-tub:before{content:"\f593"}.fa-globe-pointer:before{content:"\e60e"}.fa-blanket:before{content:"\f498"}.fa-map-location:before,.fa-map-marked:before{content:"\f59f"}.fa-house-flood-water:before{content:"\e50e"}.fa-comments-question-check:before{content:"\e14f"}.fa-tree:before{content:"\f1bb"}.fa-arrows-cross:before{content:"\e0a2"}.fa-backpack:before{content:"\f5d4"}.fa-square-small:before{content:"\e27e"}.fa-folder-arrow-up:before,.fa-folder-upload:before{content:"\e054"}.fa-bridge-lock:before{content:"\e4cc"}.fa-crosshairs-simple:before{content:"\e59f"}.fa-sack-dollar:before{content:"\f81d"}.fa-edit:before,.fa-pen-to-square:before{content:"\f044"}.fa-sliders-h-square:before,.fa-square-sliders:before{content:"\f3f0"}.fa-car-side:before{content:"\f5e4"}.fa-comment-middle-top-alt:before,.fa-message-middle-top:before{content:"\e1e2"}.fa-lightbulb-on:before{content:"\f672"}.fa-knife:before,.fa-utensil-knife:before{content:"\f2e4"}.fa-share-alt:before,.fa-share-nodes:before{content:"\f1e0"}.fa-display-chart-up-circle-dollar:before{content:"\e5e6"}.fa-wave-sine:before{content:"\f899"}.fa-heart-circle-minus:before{content:"\e4ff"}.fa-circle-w:before{content:"\e12c"}.fa-calendar-circle:before,.fa-circle-calendar:before{content:"\e102"}.fa-hourglass-2:before,.fa-hourglass-half:before{content:"\f252"}.fa-microscope:before{content:"\f610"}.fa-sunset:before{content:"\f767"}.fa-sink:before{content:"\e06d"}.fa-calendar-exclamation:before{content:"\f334"}.fa-truck-container-empty:before{content:"\e2b5"}.fa-hand-heart:before{content:"\f4bc"}.fa-bag-shopping:before,.fa-shopping-bag:before{content:"\f290"}.fa-arrow-down-z-a:before,.fa-sort-alpha-desc:before,.fa-sort-alpha-down-alt:before{content:"\f881"}.fa-mitten:before{content:"\f7b5"}.fa-reply-clock:before,.fa-reply-time:before{content:"\e239"}.fa-person-rays:before{content:"\e54d"}.fa-arrow-alt-right:before,.fa-right:before{content:"\f356"}.fa-circle-f:before{content:"\e10e"}.fa-users:before{content:"\f0c0"}.fa-face-pleading:before{content:"\e386"}.fa-eye-slash:before{content:"\f070"}.fa-flask-vial:before{content:"\e4f3"}.fa-police-box:before{content:"\e021"}.fa-cucumber:before{content:"\e401"}.fa-head-side-brain:before{content:"\f808"}.fa-hand-paper:before,.fa-hand:before{content:"\f256"}.fa-biking-mountain:before,.fa-person-biking-mountain:before{content:"\f84b"}.fa-utensils-slash:before{content:"\e464"}.fa-print-magnifying-glass:before,.fa-print-search:before{content:"\f81a"}.fa-turn-right:before{content:"\e639"}.fa-folder-bookmark:before{content:"\e186"}.fa-arrow-turn-left-down:before{content:"\e633"}.fa-om:before{content:"\f679"}.fa-pi:before{content:"\f67e"}.fa-flask-potion:before,.fa-flask-round-potion:before{content:"\f6e1"}.fa-face-shush:before{content:"\e38c"}.fa-worm:before{content:"\e599"}.fa-house-circle-xmark:before{content:"\e50b"}.fa-plug:before{content:"\f1e6"}.fa-calendar-circle-exclamation:before{content:"\e46e"}.fa-square-i:before{content:"\e272"}.fa-chevron-up:before{content:"\f077"}.fa-face-saluting:before{content:"\e484"}.fa-gauge-simple-low:before,.fa-tachometer-slow:before{content:"\f62c"}.fa-face-persevering:before{content:"\e385"}.fa-camera-circle:before,.fa-circle-camera:before{content:"\e103"}.fa-hand-spock:before{content:"\f259"}.fa-spider-web:before{content:"\f719"}.fa-circle-microphone:before,.fa-microphone-circle:before{content:"\e116"}.fa-book-arrow-up:before{content:"\e0ba"}.fa-popsicle:before{content:"\e43e"}.fa-command:before{content:"\e142"}.fa-blinds:before{content:"\f8fb"}.fa-stopwatch:before{content:"\f2f2"}.fa-saxophone:before{content:"\f8dc"}.fa-square-2:before{content:"\e257"}.fa-field-hockey-stick-ball:before,.fa-field-hockey:before{content:"\f44c"}.fa-arrow-up-square-triangle:before,.fa-sort-shapes-up-alt:before{content:"\f88b"}.fa-face-scream:before{content:"\e38b"}.fa-square-m:before{content:"\e276"}.fa-camera-web:before,.fa-webcam:before{content:"\f832"}.fa-comment-arrow-down:before{content:"\e143"}.fa-lightbulb-cfl:before{content:"\e5a6"}.fa-window-frame-open:before{content:"\e050"}.fa-face-kiss:before,.fa-kiss:before{content:"\f596"}.fa-bridge-circle-xmark:before{content:"\e4cb"}.fa-period:before{content:"\2e"}.fa-face-grin-tongue:before,.fa-grin-tongue:before{content:"\f589"}.fa-up-to-dotted-line:before{content:"\e457"}.fa-thought-bubble:before{content:"\e32e"}.fa-skeleton-ribs:before{content:"\e5cb"}.fa-raygun:before{content:"\e025"}.fa-flute:before{content:"\f8b9"}.fa-acorn:before{content:"\f6ae"}.fa-video-arrow-up-right:before{content:"\e2c9"}.fa-grate-droplet:before{content:"\e194"}.fa-seal-exclamation:before{content:"\e242"}.fa-chess-bishop:before{content:"\f43a"}.fa-message-sms:before{content:"\e1e5"}.fa-coffee-beans:before{content:"\e13f"}.fa-hat-witch:before{content:"\f6e7"}.fa-face-grin-wink:before,.fa-grin-wink:before{content:"\f58c"}.fa-clock-three-thirty:before{content:"\e357"}.fa-deaf:before,.fa-deafness:before,.fa-ear-deaf:before,.fa-hard-of-hearing:before{content:"\f2a4"}.fa-alarm-clock:before{content:"\f34e"}.fa-eclipse:before{content:"\f749"}.fa-face-relieved:before{content:"\e389"}.fa-road-circle-check:before{content:"\e564"}.fa-dice-five:before{content:"\f523"}.fa-minus-octagon:before,.fa-octagon-minus:before{content:"\f308"}.fa-rss-square:before,.fa-square-rss:before{content:"\f143"}.fa-face-zany:before{content:"\e3a4"}.fa-tricycle:before{content:"\e5c3"}.fa-land-mine-on:before{content:"\e51b"}.fa-square-arrow-up-left:before{content:"\e263"}.fa-i-cursor:before{content:"\f246"}.fa-chart-mixed-up-circle-dollar:before{content:"\e5d9"}.fa-salt-shaker:before{content:"\e446"}.fa-stamp:before{content:"\f5bf"}.fa-file-plus:before{content:"\f319"}.fa-draw-square:before{content:"\f5ef"}.fa-toilet-paper-reverse-slash:before,.fa-toilet-paper-under-slash:before{content:"\e2a1"}.fa-stairs:before{content:"\e289"}.fa-drone-alt:before,.fa-drone-front:before{content:"\f860"}.fa-glass-empty:before{content:"\e191"}.fa-dial-high:before{content:"\e15c"}.fa-user-construction:before,.fa-user-hard-hat:before,.fa-user-helmet-safety:before{content:"\f82c"}.fa-i:before{content:"\49"}.fa-hryvnia-sign:before,.fa-hryvnia:before{content:"\f6f2"}.fa-arrow-down-left-and-arrow-up-right-to-center:before{content:"\e092"}.fa-pills:before{content:"\f484"}.fa-face-grin-wide:before,.fa-grin-alt:before{content:"\f581"}.fa-tooth:before{content:"\f5c9"}.fa-basketball-hoop:before{content:"\f435"}.fa-objects-align-bottom:before{content:"\e3bb"}.fa-v:before{content:"\56"}.fa-sparkles:before{content:"\f890"}.fa-squid:before{content:"\e450"}.fa-leafy-green:before{content:"\e41d"}.fa-circle-arrow-up-right:before{content:"\e0fc"}.fa-calendars:before{content:"\e0d7"}.fa-bangladeshi-taka-sign:before{content:"\e2e6"}.fa-bicycle:before{content:"\f206"}.fa-hammer-war:before{content:"\f6e4"}.fa-circle-d:before{content:"\e104"}.fa-spider-black-widow:before{content:"\f718"}.fa-rod-asclepius:before,.fa-rod-snake:before,.fa-staff-aesculapius:before,.fa-staff-snake:before{content:"\e579"}.fa-pear:before{content:"\e20c"}.fa-head-side-cough-slash:before{content:"\e062"}.fa-file-mov:before{content:"\e647"}.fa-triangle:before{content:"\f2ec"}.fa-apartment:before{content:"\e468"}.fa-ambulance:before,.fa-truck-medical:before{content:"\f0f9"}.fa-pepper:before{content:"\e432"}.fa-piano:before{content:"\f8d4"}.fa-gun-squirt:before{content:"\e19d"}.fa-wheat-awn-circle-exclamation:before{content:"\e598"}.fa-snowman:before{content:"\f7d0"}.fa-user-alien:before{content:"\e04a"}.fa-shield-check:before{content:"\f2f7"}.fa-mortar-pestle:before{content:"\f5a7"}.fa-road-barrier:before{content:"\e562"}.fa-chart-candlestick:before{content:"\e0e2"}.fa-briefcase-blank:before{content:"\e0c8"}.fa-school:before{content:"\f549"}.fa-igloo:before{content:"\f7ae"}.fa-bracket-round:before,.fa-parenthesis:before{content:"\28"}.fa-joint:before{content:"\f595"}.fa-horse-saddle:before{content:"\f8c3"}.fa-mug-marshmallows:before{content:"\f7b7"}.fa-filters:before{content:"\e17e"}.fa-bell-on:before{content:"\f8fa"}.fa-angle-right:before{content:"\f105"}.fa-dial-med:before{content:"\e15f"}.fa-horse:before{content:"\f6f0"}.fa-q:before{content:"\51"}.fa-monitor-heart-rate:before,.fa-monitor-waveform:before{content:"\f611"}.fa-link-simple:before{content:"\e1cd"}.fa-whistle:before{content:"\f460"}.fa-g:before{content:"\47"}.fa-fragile:before,.fa-wine-glass-crack:before{content:"\f4bb"}.fa-slot-machine:before{content:"\e3ce"}.fa-notes-medical:before{content:"\f481"}.fa-car-wash:before{content:"\f5e6"}.fa-escalator:before{content:"\e171"}.fa-comment-image:before{content:"\e148"}.fa-temperature-2:before,.fa-temperature-half:before,.fa-thermometer-2:before,.fa-thermometer-half:before{content:"\f2c9"}.fa-dong-sign:before{content:"\e169"}.fa-donut:before,.fa-doughnut:before{content:"\e406"}.fa-capsules:before{content:"\f46b"}.fa-poo-bolt:before,.fa-poo-storm:before{content:"\f75a"}.fa-tally-1:before{content:"\e294"}.fa-file-vector:before{content:"\e64c"}.fa-face-frown-open:before,.fa-frown-open:before{content:"\f57a"}.fa-square-dashed:before{content:"\e269"}.fa-bag-shopping-plus:before{content:"\e651"}.fa-square-j:before{content:"\e273"}.fa-hand-point-up:before{content:"\f0a6"}.fa-money-bill:before{content:"\f0d6"}.fa-arrow-up-big-small:before,.fa-sort-size-up:before{content:"\f88e"}.fa-barcode-read:before{content:"\f464"}.fa-baguette:before{content:"\e3d8"}.fa-bowl-soft-serve:before{content:"\e46b"}.fa-face-holding-back-tears:before{content:"\e482"}.fa-arrow-alt-square-up:before,.fa-square-up:before{content:"\f353"}.fa-subway-tunnel:before,.fa-train-subway-tunnel:before{content:"\e2a3"}.fa-exclamation-square:before,.fa-square-exclamation:before{content:"\f321"}.fa-semicolon:before{content:"\3b"}.fa-bookmark:before{content:"\f02e"}.fa-fan-table:before{content:"\e004"}.fa-align-justify:before{content:"\f039"}.fa-battery-1:before,.fa-battery-low:before{content:"\e0b1"}.fa-credit-card-front:before{content:"\f38a"}.fa-brain-arrow-curved-right:before,.fa-mind-share:before{content:"\f677"}.fa-umbrella-beach:before{content:"\f5ca"}.fa-helmet-un:before{content:"\e503"}.fa-location-smile:before,.fa-map-marker-smile:before{content:"\f60d"}.fa-arrow-left-to-line:before,.fa-arrow-to-left:before{content:"\f33e"}.fa-bullseye:before{content:"\f140"}.fa-nigiri:before,.fa-sushi:before{content:"\e48a"}.fa-comment-alt-captions:before,.fa-message-captions:before{content:"\e1de"}.fa-trash-list:before{content:"\e2b1"}.fa-bacon:before{content:"\f7e5"}.fa-option:before{content:"\e318"}.fa-raccoon:before{content:"\e613"}.fa-hand-point-down:before{content:"\f0a7"}.fa-arrow-up-from-bracket:before{content:"\e09a"}.fa-head-side-gear:before{content:"\e611"}.fa-trash-plus:before{content:"\e2b2"}.fa-file-cad:before{content:"\e672"}.fa-objects-align-top:before{content:"\e3c0"}.fa-folder-blank:before,.fa-folder:before{content:"\f07b"}.fa-face-anxious-sweat:before{content:"\e36a"}.fa-credit-card-blank:before{content:"\f389"}.fa-file-medical-alt:before,.fa-file-waveform:before{content:"\f478"}.fa-microchip-ai:before{content:"\e1ec"}.fa-mug:before{content:"\f874"}.fa-plane-up-slash:before{content:"\e22e"}.fa-radiation:before{content:"\f7b9"}.fa-pen-circle:before{content:"\e20e"}.fa-bag-seedling:before{content:"\e5f2"}.fa-chart-simple:before{content:"\e473"}.fa-crutches:before{content:"\f7f8"}.fa-circle-parking:before,.fa-parking-circle:before{content:"\f615"}.fa-mars-stroke:before{content:"\f229"}.fa-leaf-oak:before{content:"\f6f7"}.fa-square-bolt:before{content:"\e265"}.fa-vial:before{content:"\f492"}.fa-dashboard:before,.fa-gauge-med:before,.fa-gauge:before,.fa-tachometer-alt-average:before{content:"\f624"}.fa-magic-wand-sparkles:before,.fa-wand-magic-sparkles:before{content:"\e2ca"}.fa-lambda:before{content:"\f66e"}.fa-e:before{content:"\45"}.fa-pizza:before{content:"\f817"}.fa-bowl-chopsticks-noodles:before{content:"\e2ea"}.fa-h3:before{content:"\f315"}.fa-pen-alt:before,.fa-pen-clip:before{content:"\f305"}.fa-bridge-circle-exclamation:before{content:"\e4ca"}.fa-badge-percent:before{content:"\f646"}.fa-rotate-reverse:before{content:"\e631"}.fa-user:before{content:"\f007"}.fa-sensor:before{content:"\e028"}.fa-comma:before{content:"\2c"}.fa-school-circle-check:before{content:"\e56b"}.fa-toilet-paper-reverse:before,.fa-toilet-paper-under:before{content:"\e2a0"}.fa-light-emergency:before{content:"\e41f"}.fa-arrow-down-to-arc:before{content:"\e4ae"}.fa-dumpster:before{content:"\f793"}.fa-shuttle-van:before,.fa-van-shuttle:before{content:"\f5b6"}.fa-building-user:before{content:"\e4da"}.fa-light-switch:before{content:"\e017"}.fa-caret-square-left:before,.fa-square-caret-left:before{content:"\f191"}.fa-highlighter:before{content:"\f591"}.fa-heart-rate:before,.fa-wave-pulse:before{content:"\f5f8"}.fa-key:before{content:"\f084"}.fa-arrow-left-to-bracket:before{content:"\e669"}.fa-hat-santa:before{content:"\f7a7"}.fa-tamale:before{content:"\e451"}.fa-box-check:before{content:"\f467"}.fa-bullhorn:before{content:"\f0a1"}.fa-steak:before{content:"\f824"}.fa-location-crosshairs-slash:before,.fa-location-slash:before{content:"\f603"}.fa-person-dolly:before{content:"\f4d0"}.fa-globe:before{content:"\f0ac"}.fa-synagogue:before{content:"\f69b"}.fa-file-chart-column:before,.fa-file-chart-line:before{content:"\f659"}.fa-person-half-dress:before{content:"\e548"}.fa-folder-image:before{content:"\e18a"}.fa-calendar-edit:before,.fa-calendar-pen:before{content:"\f333"}.fa-road-bridge:before{content:"\e563"}.fa-face-smile-tear:before{content:"\e393"}.fa-comment-alt-plus:before,.fa-message-plus:before{content:"\f4a8"}.fa-location-arrow:before{content:"\f124"}.fa-c:before{content:"\43"}.fa-tablet-button:before{content:"\f10a"}.fa-person-dress-fairy:before{content:"\e607"}.fa-rectangle-history-circle-user:before{content:"\e4a4"}.fa-building-lock:before{content:"\e4d6"}.fa-chart-line-up:before{content:"\e0e5"}.fa-mailbox:before{content:"\f813"}.fa-sign-posts:before{content:"\e625"}.fa-truck-bolt:before{content:"\e3d0"}.fa-pizza-slice:before{content:"\f818"}.fa-money-bill-wave:before{content:"\f53a"}.fa-area-chart:before,.fa-chart-area:before{content:"\f1fe"}.fa-house-flag:before{content:"\e50d"}.fa-circle-three-quarters-stroke:before{content:"\e5d4"}.fa-person-circle-minus:before{content:"\e540"}.fa-scalpel:before{content:"\f61d"}.fa-ban:before,.fa-cancel:before{content:"\f05e"}.fa-bell-exclamation:before{content:"\f848"}.fa-bookmark-circle:before,.fa-circle-bookmark:before{content:"\e100"}.fa-egg-fried:before{content:"\f7fc"}.fa-face-weary:before{content:"\e3a1"}.fa-uniform-martial-arts:before{content:"\e3d1"}.fa-camera-rotate:before{content:"\e0d8"}.fa-sun-dust:before{content:"\f764"}.fa-comment-text:before{content:"\e14d"}.fa-air-freshener:before,.fa-spray-can-sparkles:before{content:"\f5d0"}.fa-signal-alt-4:before,.fa-signal-alt:before,.fa-signal-bars-strong:before,.fa-signal-bars:before{content:"\f690"}.fa-diamond-exclamation:before{content:"\e405"}.fa-star:before{content:"\f005"}.fa-dial-min:before{content:"\e161"}.fa-repeat:before{content:"\f363"}.fa-cross:before{content:"\f654"}.fa-file-caret-down:before,.fa-page-caret-down:before{content:"\e429"}.fa-box:before{content:"\f466"}.fa-venus-mars:before{content:"\f228"}.fa-clock-seven-thirty:before{content:"\e351"}.fa-arrow-pointer:before,.fa-mouse-pointer:before{content:"\f245"}.fa-clock-four-thirty:before{content:"\e34b"}.fa-signal-alt-3:before,.fa-signal-bars-good:before{content:"\f693"}.fa-cactus:before{content:"\f8a7"}.fa-lightbulb-gear:before{content:"\e5fd"}.fa-expand-arrows-alt:before,.fa-maximize:before{content:"\f31e"}.fa-charging-station:before{content:"\f5e7"}.fa-shapes:before,.fa-triangle-circle-square:before{content:"\f61f"}.fa-plane-tail:before{content:"\e22c"}.fa-gauge-simple-max:before,.fa-tachometer-fastest:before{content:"\f62b"}.fa-circle-u:before{content:"\e127"}.fa-shield-slash:before{content:"\e24b"}.fa-phone-square-down:before,.fa-square-phone-hangup:before{content:"\e27a"}.fa-arrow-up-left:before{content:"\e09d"}.fa-transporter-1:before{content:"\e043"}.fa-peanuts:before{content:"\e431"}.fa-random:before,.fa-shuffle:before{content:"\f074"}.fa-person-running:before,.fa-running:before{content:"\f70c"}.fa-mobile-retro:before{content:"\e527"}.fa-grip-lines-vertical:before{content:"\f7a5"}.fa-bin-bottles-recycle:before{content:"\e5f6"}.fa-arrow-up-from-square:before{content:"\e09c"}.fa-file-dashed-line:before,.fa-page-break:before{content:"\f877"}.fa-bracket-curly-right:before{content:"\7d"}.fa-spider:before{content:"\f717"}.fa-clock-three:before{content:"\e356"}.fa-hands-bound:before{content:"\e4f9"}.fa-scalpel-line-dashed:before,.fa-scalpel-path:before{content:"\f61e"}.fa-file-invoice-dollar:before{content:"\f571"}.fa-pipe-smoking:before{content:"\e3c4"}.fa-face-astonished:before{content:"\e36b"}.fa-window:before{content:"\f40e"}.fa-plane-circle-exclamation:before{content:"\e556"}.fa-ear:before{content:"\f5f0"}.fa-file-lock:before{content:"\e3a6"}.fa-diagram-venn:before{content:"\e15a"}.fa-arrow-down-from-bracket:before{content:"\e667"}.fa-x-ray:before{content:"\f497"}.fa-goal-net:before{content:"\e3ab"}.fa-coffin-cross:before{content:"\e051"}.fa-spell-check:before{content:"\f891"}.fa-location-xmark:before,.fa-map-marker-times:before,.fa-map-marker-xmark:before{content:"\f60e"}.fa-circle-quarter-stroke:before{content:"\e5d3"}.fa-lasso:before{content:"\f8c8"}.fa-slash:before{content:"\f715"}.fa-person-to-portal:before,.fa-portal-enter:before{content:"\e022"}.fa-calendar-star:before{content:"\f736"}.fa-computer-mouse:before,.fa-mouse:before{content:"\f8cc"}.fa-arrow-right-to-bracket:before,.fa-sign-in:before{content:"\f090"}.fa-pegasus:before{content:"\f703"}.fa-files-medical:before{content:"\f7fd"}.fa-cannon:before{content:"\e642"}.fa-nfc-lock:before{content:"\e1f8"}.fa-person-ski-lift:before,.fa-ski-lift:before{content:"\f7c8"}.fa-square-6:before{content:"\e25b"}.fa-shop-slash:before,.fa-store-alt-slash:before{content:"\e070"}.fa-wind-turbine:before{content:"\f89b"}.fa-sliders-simple:before{content:"\e253"}.fa-grid-round:before{content:"\e5da"}.fa-badge-sheriff:before{content:"\f8a2"}.fa-server:before{content:"\f233"}.fa-virus-covid-slash:before{content:"\e4a9"}.fa-intersection:before{content:"\f668"}.fa-shop-lock:before{content:"\e4a5"}.fa-family:before{content:"\e300"}.fa-hourglass-1:before,.fa-hourglass-start:before{content:"\f251"}.fa-user-hair-buns:before{content:"\e3d3"}.fa-blender-phone:before{content:"\f6b6"}.fa-hourglass-clock:before{content:"\e41b"}.fa-person-seat-reclined:before{content:"\e21f"}.fa-paper-plane-alt:before,.fa-paper-plane-top:before,.fa-send:before{content:"\e20a"}.fa-comment-alt-arrow-up:before,.fa-message-arrow-up:before{content:"\e1dc"}.fa-lightbulb-exclamation:before{content:"\f671"}.fa-layer-group-minus:before,.fa-layer-minus:before{content:"\f5fe"}.fa-chart-pie-simple-circle-currency:before{content:"\e604"}.fa-circle-e:before{content:"\e109"}.fa-building-wheat:before{content:"\e4db"}.fa-gauge-max:before,.fa-tachometer-alt-fastest:before{content:"\f626"}.fa-person-breastfeeding:before{content:"\e53a"}.fa-apostrophe:before{content:"\27"}.fa-file-png:before{content:"\e666"}.fa-fire-hydrant:before{content:"\e17f"}.fa-right-to-bracket:before,.fa-sign-in-alt:before{content:"\f2f6"}.fa-video-plus:before{content:"\f4e1"}.fa-arrow-alt-square-right:before,.fa-square-right:before{content:"\f352"}.fa-comment-smile:before{content:"\f4b4"}.fa-venus:before{content:"\f221"}.fa-passport:before{content:"\f5ab"}.fa-inbox-arrow-down:before,.fa-inbox-in:before{content:"\f310"}.fa-heart-pulse:before,.fa-heartbeat:before{content:"\f21e"}.fa-circle-8:before{content:"\e0f5"}.fa-clouds-moon:before{content:"\f745"}.fa-clock-ten-thirty:before{content:"\e355"}.fa-people-carry-box:before,.fa-people-carry:before{content:"\f4ce"}.fa-folder-user:before{content:"\e18e"}.fa-trash-can-xmark:before{content:"\e2ae"}.fa-temperature-high:before{content:"\f769"}.fa-microchip:before{content:"\f2db"}.fa-left-long-to-line:before{content:"\e41e"}.fa-crown:before{content:"\f521"}.fa-weight-hanging:before{content:"\f5cd"}.fa-xmarks-lines:before{content:"\e59a"}.fa-file-prescription:before{content:"\f572"}.fa-table-cells-lock:before{content:"\e679"}.fa-calendar-range:before{content:"\e0d6"}.fa-flower-daffodil:before{content:"\f800"}.fa-hand-back-point-up:before{content:"\e1a2"}.fa-weight-scale:before,.fa-weight:before{content:"\f496"}.fa-arrow-up-to-arc:before{content:"\e617"}.fa-star-exclamation:before{content:"\f2f3"}.fa-books:before{content:"\f5db"}.fa-user-friends:before,.fa-user-group:before{content:"\f500"}.fa-arrow-up-a-z:before,.fa-sort-alpha-up:before{content:"\f15e"}.fa-layer-group-plus:before,.fa-layer-plus:before{content:"\f5ff"}.fa-play-pause:before{content:"\e22f"}.fa-block-question:before{content:"\e3dd"}.fa-snooze:before,.fa-zzz:before{content:"\f880"}.fa-scanner-image:before{content:"\f8f3"}.fa-tv-retro:before{content:"\f401"}.fa-square-t:before{content:"\e280"}.fa-barn-silo:before,.fa-farm:before{content:"\f864"}.fa-chess-knight:before{content:"\f441"}.fa-bars-sort:before{content:"\e0ae"}.fa-palette-boxes:before,.fa-pallet-alt:before,.fa-pallet-boxes:before{content:"\f483"}.fa-face-laugh-squint:before,.fa-laugh-squint:before{content:"\f59b"}.fa-code-simple:before{content:"\e13d"}.fa-bolt-slash:before{content:"\e0b8"}.fa-panel-fire:before{content:"\e42f"}.fa-binary-circle-check:before{content:"\e33c"}.fa-comment-minus:before{content:"\f4b1"}.fa-burrito:before{content:"\f7ed"}.fa-violin:before{content:"\f8ed"}.fa-objects-column:before{content:"\e3c1"}.fa-chevron-square-down:before,.fa-square-chevron-down:before{content:"\f329"}.fa-comment-plus:before{content:"\f4b2"}.fa-triangle-instrument:before,.fa-triangle-music:before{content:"\f8e2"}.fa-wheelchair:before{content:"\f193"}.fa-user-pilot-tie:before{content:"\e2c1"}.fa-piano-keyboard:before{content:"\f8d5"}.fa-bed-empty:before{content:"\f8f9"}.fa-arrow-circle-up:before,.fa-circle-arrow-up:before{content:"\f0aa"}.fa-toggle-on:before{content:"\f205"}.fa-rectangle-portrait:before,.fa-rectangle-vertical:before{content:"\f2fb"}.fa-person-walking:before,.fa-walking:before{content:"\f554"}.fa-l:before{content:"\4c"}.fa-signal-stream:before{content:"\f8dd"}.fa-down-to-bracket:before{content:"\e4e7"}.fa-circle-z:before{content:"\e130"}.fa-stars:before{content:"\f762"}.fa-fire:before{content:"\f06d"}.fa-bed-pulse:before,.fa-procedures:before{content:"\f487"}.fa-house-day:before{content:"\e00e"}.fa-shuttle-space:before,.fa-space-shuttle:before{content:"\f197"}.fa-shirt-long-sleeve:before{content:"\e3c7"}.fa-chart-pie-alt:before,.fa-chart-pie-simple:before{content:"\f64e"}.fa-face-laugh:before,.fa-laugh:before{content:"\f599"}.fa-folder-open:before{content:"\f07c"}.fa-album-collection-circle-user:before{content:"\e48f"}.fa-candy:before{content:"\e3e7"}.fa-bowl-hot:before,.fa-soup:before{content:"\f823"}.fa-flatbread:before{content:"\e40b"}.fa-heart-circle-plus:before{content:"\e500"}.fa-code-fork:before{content:"\e13b"}.fa-city:before{content:"\f64f"}.fa-signal-alt-1:before,.fa-signal-bars-weak:before{content:"\f691"}.fa-microphone-alt:before,.fa-microphone-lines:before{content:"\f3c9"}.fa-clock-twelve:before{content:"\e358"}.fa-pepper-hot:before{content:"\f816"}.fa-citrus-slice:before{content:"\e2f5"}.fa-sheep:before{content:"\f711"}.fa-unlock:before{content:"\f09c"}.fa-colon-sign:before{content:"\e140"}.fa-headset:before{content:"\f590"}.fa-badger-honey:before{content:"\f6b4"}.fa-h4:before{content:"\f86a"}.fa-store-slash:before{content:"\e071"}.fa-road-circle-xmark:before{content:"\e566"}.fa-signal-slash:before{content:"\f695"}.fa-user-minus:before{content:"\f503"}.fa-mars-stroke-up:before,.fa-mars-stroke-v:before{content:"\f22a"}.fa-champagne-glasses:before,.fa-glass-cheers:before{content:"\f79f"}.fa-taco:before{content:"\f826"}.fa-hexagon-plus:before,.fa-plus-hexagon:before{content:"\f300"}.fa-clipboard:before{content:"\f328"}.fa-house-circle-exclamation:before{content:"\e50a"}.fa-file-arrow-up:before,.fa-file-upload:before{content:"\f574"}.fa-wifi-3:before,.fa-wifi-strong:before,.fa-wifi:before{content:"\f1eb"}.fa-comments-alt:before,.fa-messages:before{content:"\f4b6"}.fa-bath:before,.fa-bathtub:before{content:"\f2cd"}.fa-umbrella-alt:before,.fa-umbrella-simple:before{content:"\e2bc"}.fa-rectangle-history-circle-plus:before{content:"\e4a3"}.fa-underline:before{content:"\f0cd"}.fa-prescription-bottle-pill:before{content:"\e5c0"}.fa-user-edit:before,.fa-user-pen:before{content:"\f4ff"}.fa-binary-slash:before{content:"\e33e"}.fa-square-o:before{content:"\e278"}.fa-caduceus:before{content:"\e681"}.fa-signature:before{content:"\f5b7"}.fa-stroopwafel:before{content:"\f551"}.fa-bold:before{content:"\f032"}.fa-anchor-lock:before{content:"\e4ad"}.fa-building-ngo:before{content:"\e4d7"}.fa-transporter-3:before{content:"\e045"}.fa-engine-exclamation:before,.fa-engine-warning:before{content:"\f5f2"}.fa-circle-down-right:before{content:"\e108"}.fa-square-k:before{content:"\e274"}.fa-manat-sign:before{content:"\e1d5"}.fa-money-check-edit:before,.fa-money-check-pen:before{content:"\f872"}.fa-not-equal:before{content:"\f53e"}.fa-border-style:before,.fa-border-top-left:before{content:"\f853"}.fa-map-location-dot:before,.fa-map-marked-alt:before{content:"\f5a0"}.fa-tilde:before{content:"\7e"}.fa-jedi:before{content:"\f669"}.fa-poll:before,.fa-square-poll-vertical:before{content:"\f681"}.fa-arrow-down-square-triangle:before,.fa-sort-shapes-down-alt:before{content:"\f889"}.fa-mug-hot:before{content:"\f7b6"}.fa-dog-leashed:before{content:"\f6d4"}.fa-battery-car:before,.fa-car-battery:before{content:"\f5df"}.fa-face-downcast-sweat:before{content:"\e371"}.fa-mailbox-flag-up:before{content:"\e5bb"}.fa-memo-circle-info:before{content:"\e49a"}.fa-gift:before{content:"\f06b"}.fa-dice-two:before{content:"\f528"}.fa-volume-medium:before,.fa-volume:before{content:"\f6a8"}.fa-transporter-5:before{content:"\e2a6"}.fa-gauge-circle-bolt:before{content:"\e496"}.fa-coin-front:before{content:"\e3fc"}.fa-file-slash:before{content:"\e3a7"}.fa-message-arrow-up-right:before{content:"\e1dd"}.fa-treasure-chest:before{content:"\f723"}.fa-chess-queen:before{content:"\f445"}.fa-paint-brush-alt:before,.fa-paint-brush-fine:before,.fa-paintbrush-alt:before,.fa-paintbrush-fine:before{content:"\f5a9"}.fa-glasses:before{content:"\f530"}.fa-hood-cloak:before{content:"\f6ef"}.fa-square-quote:before{content:"\e329"}.fa-up-left:before{content:"\e2bd"}.fa-bring-front:before{content:"\f857"}.fa-chess-board:before{content:"\f43c"}.fa-burger-cheese:before,.fa-cheeseburger:before{content:"\f7f1"}.fa-building-circle-check:before{content:"\e4d2"}.fa-repeat-1:before{content:"\f365"}.fa-arrow-down-to-line:before,.fa-arrow-to-bottom:before{content:"\f33d"}.fa-grid-5:before{content:"\e199"}.fa-swap-arrows:before{content:"\e60a"}.fa-right-long-to-line:before{content:"\e444"}.fa-person-chalkboard:before{content:"\e53d"}.fa-mars-stroke-h:before,.fa-mars-stroke-right:before{content:"\f22b"}.fa-hand-back-fist:before,.fa-hand-rock:before{content:"\f255"}.fa-grid-round-5:before{content:"\e5de"}.fa-tally-5:before,.fa-tally:before{content:"\f69c"}.fa-caret-square-up:before,.fa-square-caret-up:before{content:"\f151"}.fa-cloud-showers-water:before{content:"\e4e4"}.fa-bar-chart:before,.fa-chart-bar:before{content:"\f080"}.fa-hands-bubbles:before,.fa-hands-wash:before{content:"\e05e"}.fa-less-than-equal:before{content:"\f537"}.fa-train:before{content:"\f238"}.fa-up-from-dotted-line:before{content:"\e456"}.fa-eye-low-vision:before,.fa-low-vision:before{content:"\f2a8"}.fa-traffic-light-go:before{content:"\f638"}.fa-face-exhaling:before{content:"\e480"}.fa-sensor-fire:before{content:"\e02a"}.fa-user-unlock:before{content:"\e058"}.fa-hexagon-divide:before{content:"\e1ad"}.fa-00:before{content:"\e467"}.fa-crow:before{content:"\f520"}.fa-betamax:before,.fa-cassette-betamax:before{content:"\f8a4"}.fa-sailboat:before{content:"\e445"}.fa-window-restore:before{content:"\f2d2"}.fa-nfc-magnifying-glass:before{content:"\e1f9"}.fa-file-binary:before{content:"\e175"}.fa-circle-v:before{content:"\e12a"}.fa-plus-square:before,.fa-square-plus:before{content:"\f0fe"}.fa-bowl-scoops:before{content:"\e3df"}.fa-mistletoe:before{content:"\f7b4"}.fa-custard:before{content:"\e403"}.fa-lacrosse-stick:before{content:"\e3b5"}.fa-hockey-mask:before{content:"\f6ee"}.fa-sunrise:before{content:"\f766"}.fa-subtitles:before{content:"\e60f"}.fa-panel-ews:before{content:"\e42e"}.fa-torii-gate:before{content:"\f6a1"}.fa-cloud-exclamation:before{content:"\e491"}.fa-comment-alt-lines:before,.fa-message-lines:before{content:"\f4a6"}.fa-frog:before{content:"\f52e"}.fa-bucket:before{content:"\e4cf"}.fa-floppy-disk-pen:before{content:"\e182"}.fa-image:before{content:"\f03e"}.fa-window-frame:before{content:"\e04f"}.fa-microphone:before{content:"\f130"}.fa-cow:before{content:"\f6c8"}.fa-file-zip:before{content:"\e5ee"}.fa-square-ring:before{content:"\e44f"}.fa-arrow-alt-from-top:before,.fa-down-from-line:before{content:"\f349"}.fa-caret-up:before{content:"\f0d8"}.fa-shield-times:before,.fa-shield-xmark:before{content:"\e24c"}.fa-screwdriver:before{content:"\f54a"}.fa-circle-sort-down:before,.fa-sort-circle-down:before{content:"\e031"}.fa-folder-closed:before{content:"\e185"}.fa-house-tsunami:before{content:"\e515"}.fa-square-nfi:before{content:"\e576"}.fa-forklift:before{content:"\f47a"}.fa-arrow-up-from-ground-water:before{content:"\e4b5"}.fa-bracket-square-right:before{content:"\5d"}.fa-glass-martini-alt:before,.fa-martini-glass:before{content:"\f57b"}.fa-rotate-back:before,.fa-rotate-backward:before,.fa-rotate-left:before,.fa-undo-alt:before{content:"\f2ea"}.fa-columns:before,.fa-table-columns:before{content:"\f0db"}.fa-square-a:before{content:"\e25f"}.fa-tick:before{content:"\e32f"}.fa-lemon:before{content:"\f094"}.fa-head-side-mask:before{content:"\e063"}.fa-handshake:before{content:"\f2b5"}.fa-gem:before{content:"\f3a5"}.fa-dolly-box:before,.fa-dolly:before{content:"\f472"}.fa-smoking:before{content:"\f48d"}.fa-compress-arrows-alt:before,.fa-minimize:before{content:"\f78c"}.fa-refrigerator:before{content:"\e026"}.fa-monument:before{content:"\f5a6"}.fa-octagon-xmark:before,.fa-times-octagon:before,.fa-xmark-octagon:before{content:"\f2f0"}.fa-align-slash:before{content:"\f846"}.fa-snowplow:before{content:"\f7d2"}.fa-angle-double-right:before,.fa-angles-right:before{content:"\f101"}.fa-truck-couch:before,.fa-truck-ramp-couch:before{content:"\f4dd"}.fa-cannabis:before{content:"\f55f"}.fa-circle-play:before,.fa-play-circle:before{content:"\f144"}.fa-arrow-up-right-and-arrow-down-left-from-center:before{content:"\e0a0"}.fa-location-arrow-up:before{content:"\e63a"}.fa-tablets:before{content:"\f490"}.fa-360-degrees:before{content:"\e2dc"}.fa-ethernet:before{content:"\f796"}.fa-eur:before,.fa-euro-sign:before,.fa-euro:before{content:"\f153"}.fa-chair:before{content:"\f6c0"}.fa-check-circle:before,.fa-circle-check:before{content:"\f058"}.fa-square-dashed-circle-plus:before{content:"\e5c2"}.fa-hand-holding-circle-dollar:before{content:"\e621"}.fa-money-simple-from-bracket:before{content:"\e313"}.fa-bat:before{content:"\f6b5"}.fa-circle-stop:before,.fa-stop-circle:before{content:"\f28d"}.fa-head-side-headphones:before{content:"\f8c2"}.fa-phone-rotary:before{content:"\f8d3"}.fa-arrow-up-to-bracket:before{content:"\e66a"}.fa-compass-drafting:before,.fa-drafting-compass:before{content:"\f568"}.fa-plate-wheat:before{content:"\e55a"}.fa-calendar-circle-minus:before{content:"\e46f"}.fa-chopsticks:before{content:"\e3f7"}.fa-car-mechanic:before,.fa-car-wrench:before{content:"\f5e3"}.fa-icicles:before{content:"\f7ad"}.fa-person-shelter:before{content:"\e54f"}.fa-neuter:before{content:"\f22c"}.fa-id-badge:before{content:"\f2c1"}.fa-kazoo:before{content:"\f8c7"}.fa-marker:before{content:"\f5a1"}.fa-bin-bottles:before{content:"\e5f5"}.fa-face-laugh-beam:before,.fa-laugh-beam:before{content:"\f59a"}.fa-square-arrow-down-left:before{content:"\e261"}.fa-battery-bolt:before{content:"\f376"}.fa-tree-large:before{content:"\f7dd"}.fa-helicopter-symbol:before{content:"\e502"}.fa-aperture:before{content:"\e2df"}.fa-universal-access:before{content:"\f29a"}.fa-gear-complex:before{content:"\e5e9"}.fa-file-magnifying-glass:before,.fa-file-search:before{content:"\f865"}.fa-up-right:before{content:"\e2be"}.fa-chevron-circle-up:before,.fa-circle-chevron-up:before{content:"\f139"}.fa-user-police:before{content:"\e333"}.fa-lari-sign:before{content:"\e1c8"}.fa-volcano:before{content:"\f770"}.fa-teddy-bear:before{content:"\e3cf"}.fa-stocking:before{content:"\f7d5"}.fa-person-walking-dashed-line-arrow-right:before{content:"\e553"}.fa-image-slash:before{content:"\e1b7"}.fa-mask-snorkel:before{content:"\e3b7"}.fa-smoke:before{content:"\f760"}.fa-gbp:before,.fa-pound-sign:before,.fa-sterling-sign:before{content:"\f154"}.fa-battery-exclamation:before{content:"\e0b0"}.fa-viruses:before{content:"\e076"}.fa-square-person-confined:before{content:"\e577"}.fa-user-tie:before{content:"\f508"}.fa-up-to-bracket:before{content:"\e66e"}.fa-arrow-down-long:before,.fa-long-arrow-down:before{content:"\f175"}.fa-tent-arrow-down-to-line:before{content:"\e57e"}.fa-certificate:before{content:"\f0a3"}.fa-crystal-ball:before{content:"\e362"}.fa-mail-reply-all:before,.fa-reply-all:before{content:"\f122"}.fa-suitcase:before{content:"\f0f2"}.fa-person-skating:before,.fa-skating:before{content:"\f7c5"}.fa-star-shooting:before{content:"\e036"}.fa-binary-lock:before{content:"\e33d"}.fa-filter-circle-dollar:before,.fa-funnel-dollar:before{content:"\f662"}.fa-camera-retro:before{content:"\f083"}.fa-arrow-circle-down:before,.fa-circle-arrow-down:before{content:"\f0ab"}.fa-comment-edit:before,.fa-comment-pen:before{content:"\f4ae"}.fa-arrow-right-to-file:before,.fa-file-import:before{content:"\f56f"}.fa-banjo:before{content:"\f8a3"}.fa-external-link-square:before,.fa-square-arrow-up-right:before{content:"\f14c"}.fa-light-emergency-on:before{content:"\e420"}.fa-kerning:before{content:"\f86f"}.fa-box-open:before{content:"\f49e"}.fa-square-f:before{content:"\e270"}.fa-scroll:before{content:"\f70e"}.fa-spa:before{content:"\f5bb"}.fa-arrow-from-right:before,.fa-arrow-left-from-line:before{content:"\f344"}.fa-strawberry:before{content:"\e32b"}.fa-location-pin-lock:before{content:"\e51f"}.fa-pause:before{content:"\f04c"}.fa-clock-eight-thirty:before{content:"\e346"}.fa-plane-alt:before,.fa-plane-engines:before{content:"\f3de"}.fa-hill-avalanche:before{content:"\e507"}.fa-temperature-0:before,.fa-temperature-empty:before,.fa-thermometer-0:before,.fa-thermometer-empty:before{content:"\f2cb"}.fa-bomb:before{content:"\f1e2"}.fa-gauge-low:before,.fa-tachometer-alt-slow:before{content:"\f627"}.fa-registered:before{content:"\f25d"}.fa-trash-can-plus:before{content:"\e2ac"}.fa-address-card:before,.fa-contact-card:before,.fa-vcard:before{content:"\f2bb"}.fa-balance-scale-right:before,.fa-scale-unbalanced-flip:before{content:"\f516"}.fa-globe-snow:before{content:"\f7a3"}.fa-subscript:before{content:"\f12c"}.fa-diamond-turn-right:before,.fa-directions:before{content:"\f5eb"}.fa-integral:before{content:"\f667"}.fa-burst:before{content:"\e4dc"}.fa-house-laptop:before,.fa-laptop-house:before{content:"\e066"}.fa-face-tired:before,.fa-tired:before{content:"\f5c8"}.fa-money-bills:before{content:"\e1f3"}.fa-blinds-raised:before{content:"\f8fd"}.fa-smog:before{content:"\f75f"}.fa-ufo-beam:before{content:"\e048"}.fa-caret-circle-up:before,.fa-circle-caret-up:before{content:"\f331"}.fa-user-vneck-hair-long:before{content:"\e463"}.fa-square-a-lock:before{content:"\e44d"}.fa-crutch:before{content:"\f7f7"}.fa-gas-pump-slash:before{content:"\f5f4"}.fa-cloud-arrow-up:before,.fa-cloud-upload-alt:before,.fa-cloud-upload:before{content:"\f0ee"}.fa-palette:before{content:"\f53f"}.fa-transporter-4:before{content:"\e2a5"}.fa-chart-mixed-up-circle-currency:before{content:"\e5d8"}.fa-objects-align-right:before{content:"\e3bf"}.fa-arrows-turn-right:before{content:"\e4c0"}.fa-vest:before{content:"\e085"}.fa-pig:before{content:"\f706"}.fa-inbox-full:before{content:"\e1ba"}.fa-circle-envelope:before,.fa-envelope-circle:before{content:"\e10c"}.fa-construction:before,.fa-triangle-person-digging:before{content:"\f85d"}.fa-ferry:before{content:"\e4ea"}.fa-bullseye-arrow:before{content:"\f648"}.fa-arrows-down-to-people:before{content:"\e4b9"}.fa-seedling:before,.fa-sprout:before{content:"\f4d8"}.fa-clock-seven:before{content:"\e350"}.fa-arrows-alt-h:before,.fa-left-right:before{content:"\f337"}.fa-boxes-packing:before{content:"\e4c7"}.fa-arrow-circle-left:before,.fa-circle-arrow-left:before{content:"\f0a8"}.fa-flashlight:before{content:"\f8b8"}.fa-file-jpg:before{content:"\e646"}.fa-group-arrows-rotate:before{content:"\e4f6"}.fa-bowl-food:before{content:"\e4c6"}.fa-square-9:before{content:"\e25e"}.fa-candy-cane:before{content:"\f786"}.fa-arrow-down-wide-short:before,.fa-sort-amount-asc:before,.fa-sort-amount-down:before{content:"\f160"}.fa-dollar-square:before,.fa-square-dollar:before,.fa-usd-square:before{content:"\f2e9"}.fa-phone-arrow-right:before{content:"\e5be"}.fa-hand-holding-seedling:before{content:"\f4bf"}.fa-comment-alt-check:before,.fa-message-check:before{content:"\f4a2"}.fa-cloud-bolt:before,.fa-thunderstorm:before{content:"\f76c"}.fa-chart-line-up-down:before{content:"\e5d7"}.fa-remove-format:before,.fa-text-slash:before{content:"\f87d"}.fa-watch:before{content:"\f2e1"}.fa-circle-down-left:before{content:"\e107"}.fa-text:before{content:"\f893"}.fa-projector:before{content:"\f8d6"}.fa-face-smile-wink:before,.fa-smile-wink:before{content:"\f4da"}.fa-tombstone-alt:before,.fa-tombstone-blank:before{content:"\f721"}.fa-chess-king-alt:before,.fa-chess-king-piece:before{content:"\f440"}.fa-circle-6:before{content:"\e0f3"}.fa-waves-sine:before{content:"\e65d"}.fa-arrow-alt-left:before,.fa-left:before{content:"\f355"}.fa-file-word:before{content:"\f1c2"}.fa-file-powerpoint:before{content:"\f1c4"}.fa-arrow-alt-square-down:before,.fa-square-down:before{content:"\f350"}.fa-objects-align-center-vertical:before{content:"\e3bd"}.fa-arrows-h:before,.fa-arrows-left-right:before{content:"\f07e"}.fa-house-lock:before{content:"\e510"}.fa-cloud-arrow-down:before,.fa-cloud-download-alt:before,.fa-cloud-download:before{content:"\f0ed"}.fa-wreath:before{content:"\f7e2"}.fa-children:before{content:"\e4e1"}.fa-meter-droplet:before{content:"\e1ea"}.fa-blackboard:before,.fa-chalkboard:before{content:"\f51b"}.fa-user-alt-slash:before,.fa-user-large-slash:before{content:"\f4fa"}.fa-signal-4:before,.fa-signal-strong:before{content:"\f68f"}.fa-lollipop:before,.fa-lollypop:before{content:"\e424"}.fa-list-tree:before{content:"\e1d2"}.fa-envelope-open:before{content:"\f2b6"}.fa-draw-circle:before{content:"\f5ed"}.fa-cat-space:before{content:"\e001"}.fa-handshake-alt-slash:before,.fa-handshake-simple-slash:before{content:"\e05f"}.fa-rabbit-fast:before,.fa-rabbit-running:before{content:"\f709"}.fa-memo-pad:before{content:"\e1da"}.fa-mattress-pillow:before{content:"\e525"}.fa-alarm-plus:before{content:"\f844"}.fa-alicorn:before{content:"\f6b0"}.fa-comment-question:before{content:"\e14b"}.fa-gingerbread-man:before{content:"\f79d"}.fa-guarani-sign:before{content:"\e19a"}.fa-burger-fries:before{content:"\e0cd"}.fa-mug-tea:before{content:"\f875"}.fa-border-top:before{content:"\f855"}.fa-arrows-rotate:before,.fa-refresh:before,.fa-sync:before{content:"\f021"}.fa-book-circle:before,.fa-circle-book-open:before{content:"\e0ff"}.fa-arrows-to-dotted-line:before{content:"\e0a6"}.fa-fire-extinguisher:before{content:"\f134"}.fa-magnifying-glass-arrows-rotate:before{content:"\e65e"}.fa-garage-open:before{content:"\e00b"}.fa-shelves-empty:before{content:"\e246"}.fa-cruzeiro-sign:before{content:"\e152"}.fa-watch-apple:before{content:"\e2cb"}.fa-watch-calculator:before{content:"\f8f0"}.fa-list-dropdown:before{content:"\e1cf"}.fa-cabinet-filing:before{content:"\f64b"}.fa-burger-soda:before{content:"\f858"}.fa-arrow-square-up:before,.fa-square-arrow-up:before{content:"\f33c"}.fa-greater-than-equal:before{content:"\f532"}.fa-pallet-box:before{content:"\e208"}.fa-face-confounded:before{content:"\e36c"}.fa-shield-alt:before,.fa-shield-halved:before{content:"\f3ed"}.fa-truck-plow:before{content:"\f7de"}.fa-atlas:before,.fa-book-atlas:before{content:"\f558"}.fa-virus:before{content:"\e074"}.fa-grid-round-2:before{content:"\e5db"}.fa-comment-middle-top:before{content:"\e14a"}.fa-wave:before{content:"\e65b"}.fa-envelope-circle-check:before{content:"\e4e8"}.fa-layer-group:before{content:"\f5fd"}.fa-restroom-simple:before{content:"\e23a"}.fa-arrows-to-dot:before{content:"\e4be"}.fa-border-outer:before{content:"\f851"}.fa-hashtag-lock:before{content:"\e415"}.fa-clock-two-thirty:before{content:"\e35b"}.fa-archway:before{content:"\f557"}.fa-heart-circle-check:before{content:"\e4fd"}.fa-house-chimney-crack:before,.fa-house-damage:before{content:"\f6f1"}.fa-file-archive:before,.fa-file-zipper:before{content:"\f1c6"}.fa-ticket-perforated:before{content:"\e63e"}.fa-heart-half:before{content:"\e1ab"}.fa-comment-check:before{content:"\f4ac"}.fa-square:before{content:"\f0c8"}.fa-memo:before{content:"\e1d8"}.fa-glass-martini:before,.fa-martini-glass-empty:before{content:"\f000"}.fa-couch:before{content:"\f4b8"}.fa-cedi-sign:before{content:"\e0df"}.fa-italic:before{content:"\f033"}.fa-glass-citrus:before{content:"\f869"}.fa-calendar-lines-pen:before{content:"\e472"}.fa-table-cells-column-lock:before{content:"\e678"}.fa-church:before{content:"\f51d"}.fa-person-snowmobiling:before,.fa-snowmobile:before{content:"\f7d1"}.fa-face-hushed:before{content:"\e37b"}.fa-comments-dollar:before{content:"\f653"}.fa-tickets-simple:before{content:"\e659"}.fa-pickaxe:before{content:"\e5bf"}.fa-link-simple-slash:before{content:"\e1ce"}.fa-democrat:before{content:"\f747"}.fa-face-confused:before{content:"\e36d"}.fa-pinball:before{content:"\e229"}.fa-z:before{content:"\5a"}.fa-person-skiing:before,.fa-skiing:before{content:"\f7c9"}.fa-deer:before{content:"\f78e"}.fa-input-pipe:before{content:"\e1be"}.fa-road-lock:before{content:"\e567"}.fa-a:before{content:"\41"}.fa-bookmark-slash:before{content:"\e0c2"}.fa-temperature-arrow-down:before,.fa-temperature-down:before{content:"\e03f"}.fa-mace:before{content:"\f6f8"}.fa-feather-alt:before,.fa-feather-pointed:before{content:"\f56b"}.fa-sausage:before{content:"\f820"}.fa-trash-can-clock:before{content:"\e2aa"}.fa-p:before{content:"\50"}.fa-broom-wide:before{content:"\e5d1"}.fa-snowflake:before{content:"\f2dc"}.fa-stomach:before{content:"\f623"}.fa-newspaper:before{content:"\f1ea"}.fa-ad:before,.fa-rectangle-ad:before{content:"\f641"}.fa-guitar-electric:before{content:"\f8be"}.fa-arrow-turn-down-right:before{content:"\e3d6"}.fa-moon-cloud:before{content:"\f754"}.fa-bread-slice-butter:before{content:"\e3e1"}.fa-arrow-circle-right:before,.fa-circle-arrow-right:before{content:"\f0a9"}.fa-user-group-crown:before,.fa-users-crown:before{content:"\f6a5"}.fa-circle-i:before{content:"\e111"}.fa-toilet-paper-check:before{content:"\e5b2"}.fa-filter-circle-xmark:before{content:"\e17b"}.fa-locust:before{content:"\e520"}.fa-sort:before,.fa-unsorted:before{content:"\f0dc"}.fa-list-1-2:before,.fa-list-numeric:before,.fa-list-ol:before{content:"\f0cb"}.fa-chart-waterfall:before{content:"\e0eb"}.fa-sparkle:before{content:"\e5d6"}.fa-face-party:before{content:"\e383"}.fa-kidneys:before{content:"\f5fb"}.fa-wifi-exclamation:before{content:"\e2cf"}.fa-chart-network:before{content:"\f78a"}.fa-person-dress-burst:before{content:"\e544"}.fa-dice-d4:before{content:"\f6d0"}.fa-money-check-alt:before,.fa-money-check-dollar:before{content:"\f53d"}.fa-vector-square:before{content:"\f5cb"}.fa-bread-slice:before{content:"\f7ec"}.fa-language:before{content:"\f1ab"}.fa-wheat-awn-slash:before{content:"\e338"}.fa-face-kiss-wink-heart:before,.fa-kiss-wink-heart:before{content:"\f598"}.fa-dagger:before{content:"\f6cb"}.fa-podium:before{content:"\f680"}.fa-memo-circle-check:before{content:"\e1d9"}.fa-route-highway:before{content:"\f61a"}.fa-arrow-alt-to-bottom:before,.fa-down-to-line:before{content:"\f34a"}.fa-filter:before{content:"\f0b0"}.fa-square-g:before{content:"\e271"}.fa-circle-phone:before,.fa-phone-circle:before{content:"\e11b"}.fa-clipboard-prescription:before{content:"\f5e8"}.fa-user-nurse-hair:before{content:"\e45d"}.fa-question:before{content:"\3f"}.fa-file-signature:before{content:"\f573"}.fa-toggle-large-on:before{content:"\e5b1"}.fa-arrows-alt:before,.fa-up-down-left-right:before{content:"\f0b2"}.fa-dryer-alt:before,.fa-dryer-heat:before{content:"\f862"}.fa-house-chimney-user:before{content:"\e065"}.fa-hand-holding-heart:before{content:"\f4be"}.fa-arrow-up-small-big:before,.fa-sort-size-up-alt:before{content:"\f88f"}.fa-train-track:before{content:"\e453"}.fa-puzzle-piece:before{content:"\f12e"}.fa-money-check:before{content:"\f53c"}.fa-star-half-alt:before,.fa-star-half-stroke:before{content:"\f5c0"}.fa-file-exclamation:before{content:"\f31a"}.fa-code:before{content:"\f121"}.fa-glass-whiskey:before,.fa-whiskey-glass:before{content:"\f7a0"}.fa-moon-stars:before{content:"\f755"}.fa-building-circle-exclamation:before{content:"\e4d3"}.fa-clothes-hanger:before{content:"\e136"}.fa-mobile-iphone:before,.fa-mobile-notch:before{content:"\e1ee"}.fa-magnifying-glass-chart:before{content:"\e522"}.fa-arrow-up-right-from-square:before,.fa-external-link:before{content:"\f08e"}.fa-cubes-stacked:before{content:"\e4e6"}.fa-images-user:before{content:"\e1b9"}.fa-krw:before,.fa-won-sign:before,.fa-won:before{content:"\f159"}.fa-image-polaroid-user:before{content:"\e1b6"}.fa-virus-covid:before{content:"\e4a8"}.fa-square-ellipsis:before{content:"\e26e"}.fa-pie:before{content:"\f705"}.fa-chess-knight-alt:before,.fa-chess-knight-piece:before{content:"\f442"}.fa-austral-sign:before{content:"\e0a9"}.fa-cloud-plus:before{content:"\e35e"}.fa-f:before{content:"\46"}.fa-leaf:before{content:"\f06c"}.fa-bed-bunk:before{content:"\f8f8"}.fa-road:before{content:"\f018"}.fa-cab:before,.fa-taxi:before{content:"\f1ba"}.fa-person-circle-plus:before{content:"\e541"}.fa-chart-pie:before,.fa-pie-chart:before{content:"\f200"}.fa-bolt-lightning:before{content:"\e0b7"}.fa-clock-eight:before{content:"\e345"}.fa-sack-xmark:before{content:"\e56a"}.fa-file-xls:before{content:"\e64d"}.fa-file-excel:before{content:"\f1c3"}.fa-file-contract:before{content:"\f56c"}.fa-fish-fins:before{content:"\e4f2"}.fa-circle-q:before{content:"\e11e"}.fa-building-flag:before{content:"\e4d5"}.fa-face-grin-beam:before,.fa-grin-beam:before{content:"\f582"}.fa-object-ungroup:before{content:"\f248"}.fa-face-disguise:before{content:"\e370"}.fa-circle-arrow-down-right:before{content:"\e0fa"}.fa-alien-8bit:before,.fa-alien-monster:before{content:"\f8f6"}.fa-hand-point-ribbon:before{content:"\e1a6"}.fa-poop:before{content:"\f619"}.fa-object-exclude:before{content:"\e49c"}.fa-telescope:before{content:"\e03e"}.fa-location-pin:before,.fa-map-marker:before{content:"\f041"}.fa-square-list:before{content:"\e489"}.fa-kaaba:before{content:"\f66b"}.fa-toilet-paper:before{content:"\f71e"}.fa-hard-hat:before,.fa-hat-hard:before,.fa-helmet-safety:before{content:"\f807"}.fa-comment-code:before{content:"\e147"}.fa-sim-cards:before{content:"\e251"}.fa-starship:before{content:"\e039"}.fa-eject:before{content:"\f052"}.fa-arrow-alt-circle-right:before,.fa-circle-right:before{content:"\f35a"}.fa-plane-circle-check:before{content:"\e555"}.fa-seal:before{content:"\e241"}.fa-user-cowboy:before{content:"\f8ea"}.fa-hexagon-vertical-nft:before{content:"\e505"}.fa-face-rolling-eyes:before,.fa-meh-rolling-eyes:before{content:"\f5a5"}.fa-bread-loaf:before{content:"\f7eb"}.fa-rings-wedding:before{content:"\f81b"}.fa-object-group:before{content:"\f247"}.fa-french-fries:before{content:"\f803"}.fa-chart-line:before,.fa-line-chart:before{content:"\f201"}.fa-calendar-arrow-down:before,.fa-calendar-download:before{content:"\e0d0"}.fa-send-back:before{content:"\f87e"}.fa-mask-ventilator:before{content:"\e524"}.fa-tickets:before{content:"\e658"}.fa-signature-lock:before{content:"\e3ca"}.fa-arrow-right:before{content:"\f061"}.fa-map-signs:before,.fa-signs-post:before{content:"\f277"}.fa-octagon-plus:before,.fa-plus-octagon:before{content:"\f301"}.fa-cash-register:before{content:"\f788"}.fa-person-circle-question:before{content:"\e542"}.fa-melon-slice:before{content:"\e311"}.fa-space-station-moon:before{content:"\e033"}.fa-comment-alt-smile:before,.fa-message-smile:before{content:"\f4aa"}.fa-cup-straw:before{content:"\e363"}.fa-arrow-alt-from-right:before,.fa-left-from-line:before{content:"\f348"}.fa-h:before{content:"\48"}.fa-basket-shopping-simple:before,.fa-shopping-basket-alt:before{content:"\e0af"}.fa-hands-heart:before,.fa-hands-holding-heart:before{content:"\f4c3"}.fa-clock-nine:before{content:"\e34c"}.fa-hammer-brush:before{content:"\e620"}.fa-tarp:before{content:"\e57b"}.fa-face-sleepy:before{content:"\e38e"}.fa-hand-horns:before{content:"\e1a9"}.fa-screwdriver-wrench:before,.fa-tools:before{content:"\f7d9"}.fa-arrows-to-eye:before{content:"\e4bf"}.fa-circle-three-quarters:before{content:"\e125"}.fa-trophy-alt:before,.fa-trophy-star:before{content:"\f2eb"}.fa-plug-circle-bolt:before{content:"\e55b"}.fa-face-thermometer:before{content:"\e39a"}.fa-grid-round-4:before{content:"\e5dd"}.fa-sign-posts-wrench:before{content:"\e626"}.fa-shirt-running:before{content:"\e3c8"}.fa-book-circle-arrow-up:before{content:"\e0bd"}.fa-face-nauseated:before{content:"\e381"}.fa-heart:before{content:"\f004"}.fa-file-chart-pie:before{content:"\f65a"}.fa-mars-and-venus:before{content:"\f224"}.fa-home-user:before,.fa-house-user:before{content:"\e1b0"}.fa-circle-arrow-down-left:before{content:"\e0f9"}.fa-dumpster-fire:before{content:"\f794"}.fa-hexagon-minus:before,.fa-minus-hexagon:before{content:"\f307"}.fa-arrow-alt-to-left:before,.fa-left-to-line:before{content:"\f34b"}.fa-house-crack:before{content:"\e3b1"}.fa-paw-alt:before,.fa-paw-simple:before{content:"\f701"}.fa-arrow-left-long-to-line:before{content:"\e3d4"}.fa-brackets-round:before,.fa-parentheses:before{content:"\e0c5"}.fa-cocktail:before,.fa-martini-glass-citrus:before{content:"\f561"}.fa-user-shakespeare:before{content:"\e2c2"}.fa-arrow-right-to-arc:before{content:"\e4b2"}.fa-face-surprise:before,.fa-surprise:before{content:"\f5c2"}.fa-bottle-water:before{content:"\e4c5"}.fa-circle-pause:before,.fa-pause-circle:before{content:"\f28b"}.fa-gauge-circle-plus:before{content:"\e498"}.fa-folders:before{content:"\f660"}.fa-angel:before{content:"\f779"}.fa-value-absolute:before{content:"\f6a6"}.fa-rabbit:before{content:"\f708"}.fa-toilet-paper-slash:before{content:"\e072"}.fa-circle-euro:before{content:"\e5ce"}.fa-apple-alt:before,.fa-apple-whole:before{content:"\f5d1"}.fa-kitchen-set:before{content:"\e51a"}.fa-diamond-half:before{content:"\e5b7"}.fa-lock-alt:before,.fa-lock-keyhole:before{content:"\f30d"}.fa-r:before{content:"\52"}.fa-temperature-1:before,.fa-temperature-quarter:before,.fa-thermometer-1:before,.fa-thermometer-quarter:before{content:"\f2ca"}.fa-info-square:before,.fa-square-info:before{content:"\f30f"}.fa-wifi-slash:before{content:"\f6ac"}.fa-toilet-paper-xmark:before{content:"\e5b3"}.fa-hands-holding-dollar:before,.fa-hands-usd:before{content:"\f4c5"}.fa-cube:before{content:"\f1b2"}.fa-arrow-down-triangle-square:before,.fa-sort-shapes-down:before{content:"\f888"}.fa-bitcoin-sign:before{content:"\e0b4"}.fa-shutters:before{content:"\e449"}.fa-shield-dog:before{content:"\e573"}.fa-solar-panel:before{content:"\f5ba"}.fa-lock-open:before{content:"\f3c1"}.fa-table-tree:before{content:"\e293"}.fa-house-chimney-heart:before{content:"\e1b2"}.fa-tally-3:before{content:"\e296"}.fa-elevator:before{content:"\e16d"}.fa-money-bill-transfer:before{content:"\e528"}.fa-money-bill-trend-up:before{content:"\e529"}.fa-house-flood-water-circle-arrow-right:before{content:"\e50f"}.fa-poll-h:before,.fa-square-poll-horizontal:before{content:"\f682"}.fa-circle:before{content:"\f111"}.fa-left-to-bracket:before{content:"\e66d"}.fa-cart-circle-exclamation:before{content:"\e3f2"}.fa-sword:before{content:"\f71c"}.fa-backward-fast:before,.fa-fast-backward:before{content:"\f049"}.fa-recycle:before{content:"\f1b8"}.fa-user-astronaut:before{content:"\f4fb"}.fa-interrobang:before{content:"\e5ba"}.fa-plane-slash:before{content:"\e069"}.fa-circle-dashed:before{content:"\e105"}.fa-trademark:before{content:"\f25c"}.fa-basketball-ball:before,.fa-basketball:before{content:"\f434"}.fa-fork-knife:before,.fa-utensils-alt:before{content:"\f2e6"}.fa-satellite-dish:before{content:"\f7c0"}.fa-badge-check:before{content:"\f336"}.fa-arrow-alt-circle-up:before,.fa-circle-up:before{content:"\f35b"}.fa-slider:before{content:"\e252"}.fa-mobile-alt:before,.fa-mobile-screen-button:before{content:"\f3cd"}.fa-clock-one-thirty:before{content:"\e34f"}.fa-inbox-arrow-up:before,.fa-inbox-out:before{content:"\f311"}.fa-cloud-slash:before{content:"\e137"}.fa-volume-high:before,.fa-volume-up:before{content:"\f028"}.fa-users-rays:before{content:"\e593"}.fa-wallet:before{content:"\f555"}.fa-octagon-check:before{content:"\e426"}.fa-flatbread-stuffed:before{content:"\e40c"}.fa-clipboard-check:before{content:"\f46c"}.fa-cart-circle-plus:before{content:"\e3f3"}.fa-shipping-timed:before,.fa-truck-clock:before{content:"\f48c"}.fa-pool-8-ball:before{content:"\e3c5"}.fa-file-audio:before{content:"\f1c7"}.fa-turn-down-left:before{content:"\e331"}.fa-lock-hashtag:before{content:"\e423"}.fa-chart-radar:before{content:"\e0e7"}.fa-staff:before{content:"\f71b"}.fa-burger:before,.fa-hamburger:before{content:"\f805"}.fa-utility-pole:before{content:"\e2c3"}.fa-transporter-6:before{content:"\e2a7"}.fa-arrow-turn-left:before{content:"\e632"}.fa-wrench:before{content:"\f0ad"}.fa-bugs:before{content:"\e4d0"}.fa-vector-polygon:before{content:"\e2c7"}.fa-diagram-nested:before{content:"\e157"}.fa-rupee-sign:before,.fa-rupee:before{content:"\f156"}.fa-file-image:before{content:"\f1c5"}.fa-circle-question:before,.fa-question-circle:before{content:"\f059"}.fa-tickets-perforated:before{content:"\e63f"}.fa-image-user:before{content:"\e1b8"}.fa-buoy:before{content:"\e5b5"}.fa-plane-departure:before{content:"\f5b0"}.fa-handshake-slash:before{content:"\e060"}.fa-book-bookmark:before{content:"\e0bb"}.fa-border-center-h:before{content:"\f89c"}.fa-can-food:before{content:"\e3e6"}.fa-typewriter:before{content:"\f8e7"}.fa-arrow-right-from-arc:before{content:"\e4b1"}.fa-circle-k:before{content:"\e113"}.fa-face-hand-over-mouth:before{content:"\e378"}.fa-popcorn:before{content:"\f819"}.fa-house-flood:before,.fa-house-water:before{content:"\f74f"}.fa-object-subtract:before{content:"\e49e"}.fa-code-branch:before{content:"\f126"}.fa-warehouse-alt:before,.fa-warehouse-full:before{content:"\f495"}.fa-hat-cowboy:before{content:"\f8c0"}.fa-bridge:before{content:"\e4c8"}.fa-phone-alt:before,.fa-phone-flip:before{content:"\f879"}.fa-arrow-down-from-dotted-line:before{content:"\e090"}.fa-file-doc:before{content:"\e5ed"}.fa-square-quarters:before{content:"\e44e"}.fa-truck-front:before{content:"\e2b7"}.fa-cat:before{content:"\f6be"}.fa-trash-xmark:before{content:"\e2b4"}.fa-caret-circle-left:before,.fa-circle-caret-left:before{content:"\f32e"}.fa-files:before{content:"\e178"}.fa-anchor-circle-exclamation:before{content:"\e4ab"}.fa-face-clouds:before{content:"\e47d"}.fa-user-crown:before{content:"\f6a4"}.fa-basket-shopping-plus:before{content:"\e653"}.fa-truck-field:before{content:"\e58d"}.fa-route:before{content:"\f4d7"}.fa-cart-circle-check:before{content:"\e3f1"}.fa-clipboard-question:before{content:"\e4e3"}.fa-panorama:before{content:"\e209"}.fa-comment-medical:before{content:"\f7f5"}.fa-teeth-open:before{content:"\f62f"}.fa-user-tie-hair-long:before{content:"\e460"}.fa-file-circle-minus:before{content:"\e4ed"}.fa-head-side-medical:before{content:"\f809"}.fa-arrow-turn-right:before{content:"\e635"}.fa-tags:before{content:"\f02c"}.fa-wine-glass:before{content:"\f4e3"}.fa-fast-forward:before,.fa-forward-fast:before{content:"\f050"}.fa-face-meh-blank:before,.fa-meh-blank:before{content:"\f5a4"}.fa-user-robot:before{content:"\e04b"}.fa-parking:before,.fa-square-parking:before{content:"\f540"}.fa-card-diamond:before{content:"\e3ea"}.fa-face-zipper:before{content:"\e3a5"}.fa-face-raised-eyebrow:before{content:"\e388"}.fa-house-signal:before{content:"\e012"}.fa-chevron-square-up:before,.fa-square-chevron-up:before{content:"\f32c"}.fa-bars-progress:before,.fa-tasks-alt:before{content:"\f828"}.fa-faucet-drip:before{content:"\e006"}.fa-arrows-to-line:before{content:"\e0a7"}.fa-dolphin:before{content:"\e168"}.fa-arrow-up-right:before{content:"\e09f"}.fa-circle-r:before{content:"\e120"}.fa-cart-flatbed:before,.fa-dolly-flatbed:before{content:"\f474"}.fa-ban-smoking:before,.fa-smoking-ban:before{content:"\f54d"}.fa-circle-sort-up:before,.fa-sort-circle-up:before{content:"\e032"}.fa-terminal:before{content:"\f120"}.fa-mobile-button:before{content:"\f10b"}.fa-house-medical-flag:before{content:"\e514"}.fa-basket-shopping:before,.fa-shopping-basket:before{content:"\f291"}.fa-tape:before{content:"\f4db"}.fa-chestnut:before{content:"\e3f6"}.fa-bus-alt:before,.fa-bus-simple:before{content:"\f55e"}.fa-eye:before{content:"\f06e"}.fa-face-sad-cry:before,.fa-sad-cry:before{content:"\f5b3"}.fa-heat:before{content:"\e00c"}.fa-ticket-airline:before,.fa-ticket-perforated-plane:before,.fa-ticket-plane:before{content:"\e29a"}.fa-boot-heeled:before{content:"\e33f"}.fa-arrows-minimize:before,.fa-compress-arrows:before{content:"\e0a5"}.fa-audio-description:before{content:"\f29e"}.fa-person-military-to-person:before{content:"\e54c"}.fa-file-shield:before{content:"\e4f0"}.fa-hexagon:before{content:"\f312"}.fa-manhole:before{content:"\e1d6"}.fa-user-slash:before{content:"\f506"}.fa-pen:before{content:"\f304"}.fa-tower-observation:before{content:"\e586"}.fa-floppy-disks:before{content:"\e183"}.fa-toilet-paper-blank-under:before,.fa-toilet-paper-reverse-alt:before{content:"\e29f"}.fa-file-code:before{content:"\f1c9"}.fa-signal-5:before,.fa-signal-perfect:before,.fa-signal:before{content:"\f012"}.fa-pump:before{content:"\e442"}.fa-bus:before{content:"\f207"}.fa-heart-circle-xmark:before{content:"\e501"}.fa-arrow-up-left-from-circle:before{content:"\e09e"}.fa-home-lg:before,.fa-house-chimney:before{content:"\e3af"}.fa-window-maximize:before{content:"\f2d0"}.fa-dryer:before{content:"\f861"}.fa-face-frown:before,.fa-frown:before{content:"\f119"}.fa-chess-bishop-alt:before,.fa-chess-bishop-piece:before{content:"\f43b"}.fa-shirt-tank-top:before{content:"\e3c9"}.fa-diploma:before,.fa-scroll-ribbon:before{content:"\f5ea"}.fa-screencast:before{content:"\e23e"}.fa-walker:before{content:"\f831"}.fa-prescription:before{content:"\f5b1"}.fa-shop:before,.fa-store-alt:before{content:"\f54f"}.fa-floppy-disk:before,.fa-save:before{content:"\f0c7"}.fa-vihara:before{content:"\f6a7"}.fa-face-kiss-closed-eyes:before{content:"\e37d"}.fa-balance-scale-left:before,.fa-scale-unbalanced:before{content:"\f515"}.fa-file-user:before{content:"\f65c"}.fa-user-police-tie:before{content:"\e334"}.fa-face-tongue-money:before{content:"\e39d"}.fa-tennis-ball:before{content:"\f45e"}.fa-square-l:before{content:"\e275"}.fa-sort-asc:before,.fa-sort-up:before{content:"\f0de"}.fa-calendar-arrow-up:before,.fa-calendar-upload:before{content:"\e0d1"}.fa-comment-dots:before,.fa-commenting:before{content:"\f4ad"}.fa-plant-wilt:before{content:"\e5aa"}.fa-scarf:before{content:"\f7c1"}.fa-album-circle-plus:before{content:"\e48c"}.fa-user-nurse-hair-long:before{content:"\e45e"}.fa-diamond:before{content:"\f219"}.fa-arrow-alt-square-left:before,.fa-square-left:before{content:"\f351"}.fa-face-grin-squint:before,.fa-grin-squint:before{content:"\f585"}.fa-circle-ellipsis-vertical:before{content:"\e10b"}.fa-hand-holding-dollar:before,.fa-hand-holding-usd:before{content:"\f4c0"}.fa-grid-dividers:before{content:"\e3ad"}.fa-bacterium:before{content:"\e05a"}.fa-hand-pointer:before{content:"\f25a"}.fa-drum-steelpan:before{content:"\f56a"}.fa-hand-scissors:before{content:"\f257"}.fa-hands-praying:before,.fa-praying-hands:before{content:"\f684"}.fa-face-pensive:before{content:"\e384"}.fa-user-music:before{content:"\f8eb"}.fa-arrow-right-rotate:before,.fa-arrow-rotate-forward:before,.fa-arrow-rotate-right:before,.fa-redo:before{content:"\f01e"}.fa-comments-alt-dollar:before,.fa-messages-dollar:before{content:"\f652"}.fa-sensor-on:before{content:"\e02b"}.fa-balloon:before{content:"\e2e3"}.fa-biohazard:before{content:"\f780"}.fa-chess-queen-alt:before,.fa-chess-queen-piece:before{content:"\f446"}.fa-location-crosshairs:before,.fa-location:before{content:"\f601"}.fa-mars-double:before{content:"\f227"}.fa-left-from-bracket:before{content:"\e66c"}.fa-house-leave:before,.fa-house-person-depart:before,.fa-house-person-leave:before{content:"\e00f"}.fa-ruler-triangle:before{content:"\f61c"}.fa-card-club:before{content:"\e3e9"}.fa-child-dress:before{content:"\e59c"}.fa-users-between-lines:before{content:"\e591"}.fa-lungs-virus:before{content:"\e067"}.fa-spinner-third:before{content:"\f3f4"}.fa-face-grin-tears:before,.fa-grin-tears:before{content:"\f588"}.fa-phone:before{content:"\f095"}.fa-computer-mouse-scrollwheel:before,.fa-mouse-alt:before{content:"\f8cd"}.fa-calendar-times:before,.fa-calendar-xmark:before{content:"\f273"}.fa-child-reaching:before{content:"\e59d"}.fa-table-layout:before{content:"\e290"}.fa-narwhal:before{content:"\f6fe"}.fa-ramp-loading:before{content:"\f4d4"}.fa-calendar-circle-plus:before{content:"\e470"}.fa-toothbrush:before{content:"\f635"}.fa-border-inner:before{content:"\f84e"}.fa-paw-claws:before{content:"\f702"}.fa-kiwi-fruit:before{content:"\e30c"}.fa-traffic-light-slow:before{content:"\f639"}.fa-rectangle-code:before{content:"\e322"}.fa-head-side-virus:before{content:"\e064"}.fa-keyboard-brightness:before{content:"\e1c0"}.fa-books-medical:before{content:"\f7e8"}.fa-lightbulb-slash:before{content:"\f673"}.fa-home-blank:before,.fa-house-blank:before{content:"\e487"}.fa-square-5:before{content:"\e25a"}.fa-heart-square:before,.fa-square-heart:before{content:"\f4c8"}.fa-puzzle:before{content:"\e443"}.fa-user-cog:before,.fa-user-gear:before{content:"\f4fe"}.fa-pipe-circle-check:before{content:"\e436"}.fa-arrow-up-1-9:before,.fa-sort-numeric-up:before{content:"\f163"}.fa-octagon-exclamation:before{content:"\e204"}.fa-dial-low:before{content:"\e15d"}.fa-door-closed:before{content:"\f52a"}.fa-laptop-mobile:before,.fa-phone-laptop:before{content:"\f87a"}.fa-conveyor-belt-alt:before,.fa-conveyor-belt-boxes:before{content:"\f46f"}.fa-shield-virus:before{content:"\e06c"}.fa-starfighter-alt-advanced:before,.fa-starfighter-twin-ion-engine-advanced:before{content:"\e28e"}.fa-dice-six:before{content:"\f526"}.fa-starfighter-alt:before,.fa-starfighter-twin-ion-engine:before{content:"\e038"}.fa-rocket-launch:before{content:"\e027"}.fa-mosquito-net:before{content:"\e52c"}.fa-vent-damper:before{content:"\e465"}.fa-bridge-water:before{content:"\e4ce"}.fa-ban-bug:before,.fa-debug:before{content:"\f7f9"}.fa-person-booth:before{content:"\f756"}.fa-text-width:before{content:"\f035"}.fa-garage-car:before{content:"\e00a"}.fa-square-kanban:before{content:"\e488"}.fa-hat-wizard:before{content:"\f6e8"}.fa-chart-kanban:before{content:"\e64f"}.fa-pen-fancy:before{content:"\f5ac"}.fa-coffee-pot:before{content:"\e002"}.fa-mouse-field:before{content:"\e5a8"}.fa-digging:before,.fa-person-digging:before{content:"\f85e"}.fa-shower-alt:before,.fa-shower-down:before{content:"\e24d"}.fa-box-circle-check:before{content:"\e0c4"}.fa-brightness:before{content:"\e0c9"}.fa-car-side-bolt:before{content:"\e344"}.fa-file-xml:before{content:"\e654"}.fa-ornament:before{content:"\f7b8"}.fa-phone-arrow-down-left:before,.fa-phone-arrow-down:before,.fa-phone-incoming:before{content:"\e223"}.fa-cloud-word:before{content:"\e138"}.fa-hand-fingers-crossed:before{content:"\e1a3"}.fa-trash:before{content:"\f1f8"}.fa-gauge-simple-med:before,.fa-gauge-simple:before,.fa-tachometer-average:before{content:"\f629"}.fa-arrow-down-small-big:before,.fa-sort-size-down-alt:before{content:"\f88d"}.fa-book-medical:before{content:"\f7e6"}.fa-face-melting:before{content:"\e483"}.fa-poo:before{content:"\f2fe"}.fa-pen-alt-slash:before,.fa-pen-clip-slash:before{content:"\e20f"}.fa-quote-right-alt:before,.fa-quote-right:before{content:"\f10e"}.fa-scroll-old:before{content:"\f70f"}.fa-guitars:before{content:"\f8bf"}.fa-phone-xmark:before{content:"\e227"}.fa-hose:before{content:"\e419"}.fa-clock-six:before{content:"\e352"}.fa-shirt:before,.fa-t-shirt:before,.fa-tshirt:before{content:"\f553"}.fa-billboard:before{content:"\e5cd"}.fa-square-r:before{content:"\e27c"}.fa-cubes:before{content:"\f1b3"}.fa-envelope-open-dollar:before{content:"\f657"}.fa-divide:before{content:"\f529"}.fa-sun-cloud:before{content:"\f763"}.fa-lamp-floor:before{content:"\e015"}.fa-square-7:before{content:"\e25c"}.fa-tenge-sign:before,.fa-tenge:before{content:"\f7d7"}.fa-headphones:before{content:"\f025"}.fa-hands-holding:before{content:"\f4c2"}.fa-campfire:before{content:"\f6ba"}.fa-circle-ampersand:before{content:"\e0f8"}.fa-snowflakes:before{content:"\f7cf"}.fa-hands-clapping:before{content:"\e1a8"}.fa-republican:before{content:"\f75e"}.fa-leaf-maple:before{content:"\f6f6"}.fa-arrow-left:before{content:"\f060"}.fa-person-circle-xmark:before{content:"\e543"}.fa-ruler:before{content:"\f545"}.fa-arrow-left-from-bracket:before{content:"\e668"}.fa-cup-straw-swoosh:before{content:"\e364"}.fa-temperature-hot:before,.fa-temperature-sun:before{content:"\f76a"}.fa-align-left:before{content:"\f036"}.fa-dice-d6:before{content:"\f6d1"}.fa-restroom:before{content:"\f7bd"}.fa-high-definition:before,.fa-rectangle-hd:before{content:"\e1ae"}.fa-j:before{content:"\4a"}.fa-galaxy:before{content:"\e008"}.fa-users-viewfinder:before{content:"\e595"}.fa-file-video:before{content:"\f1c8"}.fa-cherries:before{content:"\e0ec"}.fa-external-link-alt:before,.fa-up-right-from-square:before{content:"\f35d"}.fa-circle-sort:before,.fa-sort-circle:before{content:"\e030"}.fa-table-cells:before,.fa-th:before{content:"\f00a"}.fa-bag-shopping-minus:before{content:"\e650"}.fa-file-pdf:before{content:"\f1c1"}.fa-siren:before{content:"\e02d"}.fa-arrow-up-to-dotted-line:before{content:"\e0a1"}.fa-image-landscape:before,.fa-landscape:before{content:"\e1b5"}.fa-tank-water:before{content:"\e452"}.fa-curling-stone:before,.fa-curling:before{content:"\f44a"}.fa-gamepad-alt:before,.fa-gamepad-modern:before{content:"\e5a2"}.fa-messages-question:before{content:"\e1e7"}.fa-bible:before,.fa-book-bible:before{content:"\f647"}.fa-o:before{content:"\4f"}.fa-medkit:before,.fa-suitcase-medical:before{content:"\f0fa"}.fa-briefcase-arrow-right:before{content:"\e2f2"}.fa-expand-wide:before{content:"\f320"}.fa-clock-eleven-thirty:before{content:"\e348"}.fa-rv:before{content:"\f7be"}.fa-user-secret:before{content:"\f21b"}.fa-otter:before{content:"\f700"}.fa-dreidel:before{content:"\f792"}.fa-female:before,.fa-person-dress:before{content:"\f182"}.fa-comment-dollar:before{content:"\f651"}.fa-briefcase-clock:before,.fa-business-time:before{content:"\f64a"}.fa-flower-tulip:before{content:"\f801"}.fa-people-pants-simple:before{content:"\e21a"}.fa-cloud-drizzle:before{content:"\f738"}.fa-table-cells-large:before,.fa-th-large:before{content:"\f009"}.fa-book-tanakh:before,.fa-tanakh:before{content:"\f827"}.fa-solar-system:before{content:"\e02f"}.fa-seal-question:before{content:"\e243"}.fa-phone-volume:before,.fa-volume-control-phone:before{content:"\f2a0"}.fa-disc-drive:before{content:"\f8b5"}.fa-hat-cowboy-side:before{content:"\f8c1"}.fa-rows:before,.fa-table-rows:before{content:"\e292"}.fa-location-exclamation:before,.fa-map-marker-exclamation:before{content:"\f608"}.fa-face-fearful:before{content:"\e375"}.fa-clipboard-user:before{content:"\f7f3"}.fa-bus-school:before{content:"\f5dd"}.fa-film-slash:before{content:"\e179"}.fa-square-arrow-down-right:before{content:"\e262"}.fa-book-sparkles:before,.fa-book-spells:before{content:"\f6b8"}.fa-washer:before,.fa-washing-machine:before{content:"\f898"}.fa-child:before{content:"\f1ae"}.fa-lira-sign:before{content:"\f195"}.fa-user-visor:before{content:"\e04c"}.fa-file-plus-minus:before{content:"\e177"}.fa-chess-clock-alt:before,.fa-chess-clock-flip:before{content:"\f43e"}.fa-satellite:before{content:"\f7bf"}.fa-truck-fire:before{content:"\e65a"}.fa-plane-lock:before{content:"\e558"}.fa-steering-wheel:before{content:"\f622"}.fa-tag:before{content:"\f02b"}.fa-stretcher:before{content:"\f825"}.fa-book-law:before,.fa-book-section:before{content:"\e0c1"}.fa-inboxes:before{content:"\e1bb"}.fa-coffee-bean:before{content:"\e13e"}.fa-circle-yen:before{content:"\e5d0"}.fa-brackets-curly:before{content:"\f7ea"}.fa-ellipsis-stroke-vertical:before,.fa-ellipsis-v-alt:before{content:"\f39c"}.fa-comment:before{content:"\f075"}.fa-square-1:before{content:"\e256"}.fa-birthday-cake:before,.fa-cake-candles:before,.fa-cake:before{content:"\f1fd"}.fa-head-side:before{content:"\f6e9"}.fa-truck-ladder:before{content:"\e657"}.fa-envelope:before{content:"\f0e0"}.fa-dolly-empty:before{content:"\f473"}.fa-face-tissue:before{content:"\e39c"}.fa-angle-double-up:before,.fa-angles-up:before{content:"\f102"}.fa-bin-recycle:before{content:"\e5f7"}.fa-paperclip:before{content:"\f0c6"}.fa-chart-line-down:before{content:"\f64d"}.fa-arrow-right-to-city:before{content:"\e4b3"}.fa-lock-a:before{content:"\e422"}.fa-ribbon:before{content:"\f4d6"}.fa-lungs:before{content:"\f604"}.fa-person-pinball:before{content:"\e21d"}.fa-arrow-up-9-1:before,.fa-sort-numeric-up-alt:before{content:"\f887"}.fa-apple-core:before{content:"\e08f"}.fa-circle-y:before{content:"\e12f"}.fa-h6:before{content:"\e413"}.fa-litecoin-sign:before{content:"\e1d3"}.fa-bottle-baby:before{content:"\e673"}.fa-circle-small:before{content:"\e122"}.fa-border-none:before{content:"\f850"}.fa-arrow-turn-down-left:before{content:"\e2e1"}.fa-circle-wifi-circle-wifi:before,.fa-circle-wifi-group:before{content:"\e67e"}.fa-circle-nodes:before{content:"\e4e2"}.fa-parachute-box:before{content:"\f4cd"}.fa-reflect-horizontal:before{content:"\e664"}.fa-comment-alt-medical:before,.fa-message-medical:before{content:"\f7f4"}.fa-rugby-ball:before{content:"\e3c6"}.fa-comment-music:before{content:"\f8b0"}.fa-indent:before{content:"\f03c"}.fa-tree-alt:before,.fa-tree-deciduous:before{content:"\f400"}.fa-puzzle-piece-alt:before,.fa-puzzle-piece-simple:before{content:"\e231"}.fa-truck-field-un:before{content:"\e58e"}.fa-nfc-trash:before{content:"\e1fd"}.fa-hourglass-empty:before,.fa-hourglass:before{content:"\f254"}.fa-mountain:before{content:"\f6fc"}.fa-file-times:before,.fa-file-xmark:before{content:"\f317"}.fa-home-heart:before,.fa-house-heart:before{content:"\f4c9"}.fa-house-chimney-blank:before{content:"\e3b0"}.fa-meter-bolt:before{content:"\e1e9"}.fa-user-doctor:before,.fa-user-md:before{content:"\f0f0"}.fa-slash-back:before{content:"\5c"}.fa-circle-info:before,.fa-info-circle:before{content:"\f05a"}.fa-fishing-rod:before{content:"\e3a8"}.fa-hammer-crash:before{content:"\e414"}.fa-message-heart:before{content:"\e5c9"}.fa-cloud-meatball:before{content:"\f73b"}.fa-camera-polaroid:before{content:"\f8aa"}.fa-camera-alt:before,.fa-camera:before{content:"\f030"}.fa-square-virus:before{content:"\e578"}.fa-cart-arrow-up:before{content:"\e3ee"}.fa-meteor:before{content:"\f753"}.fa-car-on:before{content:"\e4dd"}.fa-sleigh:before{content:"\f7cc"}.fa-arrow-down-1-9:before,.fa-sort-numeric-asc:before,.fa-sort-numeric-down:before{content:"\f162"}.fa-buoy-mooring:before{content:"\e5b6"}.fa-square-4:before{content:"\e259"}.fa-hand-holding-droplet:before,.fa-hand-holding-water:before{content:"\f4c1"}.fa-file-eps:before{content:"\e644"}.fa-tricycle-adult:before{content:"\e5c4"}.fa-waveform:before{content:"\f8f1"}.fa-water:before{content:"\f773"}.fa-star-sharp-half-alt:before,.fa-star-sharp-half-stroke:before{content:"\e28d"}.fa-nfc-signal:before{content:"\e1fb"}.fa-plane-prop:before{content:"\e22b"}.fa-calendar-check:before{content:"\f274"}.fa-clock-desk:before{content:"\e134"}.fa-calendar-clock:before,.fa-calendar-time:before{content:"\e0d2"}.fa-braille:before{content:"\f2a1"}.fa-prescription-bottle-alt:before,.fa-prescription-bottle-medical:before{content:"\f486"}.fa-plate-utensils:before{content:"\e43b"}.fa-family-pants:before{content:"\e302"}.fa-hose-reel:before{content:"\e41a"}.fa-house-window:before{content:"\e3b3"}.fa-landmark:before{content:"\f66f"}.fa-truck:before{content:"\f0d1"}.fa-music-magnifying-glass:before{content:"\e662"}.fa-crosshairs:before{content:"\f05b"}.fa-cloud-rainbow:before{content:"\f73e"}.fa-person-cane:before{content:"\e53c"}.fa-alien:before{content:"\f8f5"}.fa-tent:before{content:"\e57d"}.fa-laptop-binary:before{content:"\e5e7"}.fa-vest-patches:before{content:"\e086"}.fa-people-dress-simple:before{content:"\e218"}.fa-check-double:before{content:"\f560"}.fa-arrow-down-a-z:before,.fa-sort-alpha-asc:before,.fa-sort-alpha-down:before{content:"\f15d"}.fa-bowling-ball-pin:before{content:"\e0c3"}.fa-bell-school-slash:before{content:"\f5d6"}.fa-plus-large:before{content:"\e59e"}.fa-money-bill-wheat:before{content:"\e52a"}.fa-camera-viewfinder:before,.fa-screenshot:before{content:"\e0da"}.fa-comment-alt-music:before,.fa-message-music:before{content:"\f8af"}.fa-car-building:before{content:"\f859"}.fa-border-bottom-right:before,.fa-border-style-alt:before{content:"\f854"}.fa-octagon:before{content:"\f306"}.fa-comment-arrow-up-right:before{content:"\e145"}.fa-octagon-divide:before{content:"\e203"}.fa-cookie:before{content:"\f563"}.fa-arrow-left-rotate:before,.fa-arrow-rotate-back:before,.fa-arrow-rotate-backward:before,.fa-arrow-rotate-left:before,.fa-undo:before{content:"\f0e2"}.fa-tv-music:before{content:"\f8e6"}.fa-hard-drive:before,.fa-hdd:before{content:"\f0a0"}.fa-reel:before{content:"\e238"}.fa-face-grin-squint-tears:before,.fa-grin-squint-tears:before{content:"\f586"}.fa-dumbbell:before{content:"\f44b"}.fa-list-alt:before,.fa-rectangle-list:before{content:"\f022"}.fa-tarp-droplet:before{content:"\e57c"}.fa-alarm-exclamation:before{content:"\f843"}.fa-house-medical-circle-check:before{content:"\e511"}.fa-traffic-cone:before{content:"\f636"}.fa-grate:before{content:"\e193"}.fa-arrow-down-right:before{content:"\e093"}.fa-person-skiing-nordic:before,.fa-skiing-nordic:before{content:"\f7ca"}.fa-calendar-plus:before{content:"\f271"}.fa-person-from-portal:before,.fa-portal-exit:before{content:"\e023"}.fa-plane-arrival:before{content:"\f5af"}.fa-cowbell-circle-plus:before,.fa-cowbell-more:before{content:"\f8b4"}.fa-arrow-alt-circle-left:before,.fa-circle-left:before{content:"\f359"}.fa-distribute-spacing-vertical:before{content:"\e366"}.fa-signal-alt-2:before,.fa-signal-bars-fair:before{content:"\f692"}.fa-sportsball:before{content:"\e44b"}.fa-game-console-handheld-crank:before{content:"\e5b9"}.fa-subway:before,.fa-train-subway:before{content:"\f239"}.fa-chart-gantt:before{content:"\e0e4"}.fa-face-smile-upside-down:before{content:"\e395"}.fa-ball-pile:before{content:"\f77e"}.fa-badge-dollar:before{content:"\f645"}.fa-money-bills-alt:before,.fa-money-bills-simple:before{content:"\e1f4"}.fa-list-timeline:before{content:"\e1d1"}.fa-indian-rupee-sign:before,.fa-indian-rupee:before,.fa-inr:before{content:"\e1bc"}.fa-crop-alt:before,.fa-crop-simple:before{content:"\f565"}.fa-money-bill-1:before,.fa-money-bill-alt:before{content:"\f3d1"}.fa-left-long:before,.fa-long-arrow-alt-left:before{content:"\f30a"}.fa-keyboard-down:before{content:"\e1c2"}.fa-circle-up-right:before{content:"\e129"}.fa-cloud-bolt-moon:before,.fa-thunderstorm-moon:before{content:"\f76d"}.fa-turn-left-up:before{content:"\e638"}.fa-dna:before{content:"\f471"}.fa-virus-slash:before{content:"\e075"}.fa-bracket-round-right:before{content:"\29"}.fa-circle-sterling:before{content:"\e5cf"}.fa-circle-5:before{content:"\e0f2"}.fa-minus:before,.fa-subtract:before{content:"\f068"}.fa-fire-flame:before,.fa-flame:before{content:"\f6df"}.fa-arrow-alt-to-right:before,.fa-right-to-line:before{content:"\f34c"}.fa-gif:before{content:"\e190"}.fa-chess:before{content:"\f439"}.fa-trash-slash:before{content:"\e2b3"}.fa-arrow-left-long:before,.fa-long-arrow-left:before{content:"\f177"}.fa-plug-circle-check:before{content:"\e55c"}.fa-font-case:before{content:"\f866"}.fa-street-view:before{content:"\f21d"}.fa-arrow-down-left:before{content:"\e091"}.fa-franc-sign:before{content:"\e18f"}.fa-flask-poison:before,.fa-flask-round-poison:before{content:"\f6e0"}.fa-volume-off:before{content:"\f026"}.fa-book-circle-arrow-right:before{content:"\e0bc"}.fa-chart-user:before,.fa-user-chart:before{content:"\f6a3"}.fa-american-sign-language-interpreting:before,.fa-asl-interpreting:before,.fa-hands-american-sign-language-interpreting:before,.fa-hands-asl-interpreting:before{content:"\f2a3"}.fa-presentation-screen:before,.fa-presentation:before{content:"\f685"}.fa-circle-bolt:before{content:"\e0fe"}.fa-face-smile-halo:before{content:"\e38f"}.fa-cart-circle-arrow-down:before{content:"\e3ef"}.fa-house-person-arrive:before,.fa-house-person-return:before,.fa-house-return:before{content:"\e011"}.fa-comment-alt-times:before,.fa-message-times:before,.fa-message-xmark:before{content:"\f4ab"}.fa-file-award:before,.fa-file-certificate:before{content:"\f5f3"}.fa-user-doctor-hair-long:before{content:"\e459"}.fa-camera-home:before,.fa-camera-security:before{content:"\f8fe"}.fa-cog:before,.fa-gear:before{content:"\f013"}.fa-droplet-slash:before,.fa-tint-slash:before{content:"\f5c7"}.fa-book-heart:before{content:"\f499"}.fa-mosque:before{content:"\f678"}.fa-duck:before{content:"\f6d8"}.fa-mosquito:before{content:"\e52b"}.fa-star-of-david:before{content:"\f69a"}.fa-flag-alt:before,.fa-flag-swallowtail:before{content:"\f74c"}.fa-person-military-rifle:before{content:"\e54b"}.fa-car-garage:before{content:"\f5e2"}.fa-cart-shopping:before,.fa-shopping-cart:before{content:"\f07a"}.fa-book-font:before{content:"\e0bf"}.fa-shield-plus:before{content:"\e24a"}.fa-vials:before{content:"\f493"}.fa-eye-dropper-full:before{content:"\e172"}.fa-distribute-spacing-horizontal:before{content:"\e365"}.fa-tablet-rugged:before{content:"\f48f"}.fa-temperature-frigid:before,.fa-temperature-snow:before{content:"\f768"}.fa-moped:before{content:"\e3b9"}.fa-face-smile-plus:before,.fa-smile-plus:before{content:"\f5b9"}.fa-radio-alt:before,.fa-radio-tuner:before{content:"\f8d8"}.fa-face-swear:before{content:"\e399"}.fa-water-arrow-down:before,.fa-water-lower:before{content:"\f774"}.fa-scanner-touchscreen:before{content:"\f48a"}.fa-circle-7:before{content:"\e0f4"}.fa-plug-circle-plus:before{content:"\e55f"}.fa-person-ski-jumping:before,.fa-ski-jump:before{content:"\f7c7"}.fa-place-of-worship:before{content:"\f67f"}.fa-water-arrow-up:before,.fa-water-rise:before{content:"\f775"}.fa-waveform-lines:before,.fa-waveform-path:before{content:"\f8f2"}.fa-split:before{content:"\e254"}.fa-film-canister:before,.fa-film-cannister:before{content:"\f8b7"}.fa-folder-times:before,.fa-folder-xmark:before{content:"\f65f"}.fa-toilet-paper-alt:before,.fa-toilet-paper-blank:before{content:"\f71f"}.fa-tablet-android-alt:before,.fa-tablet-screen:before{content:"\f3fc"}.fa-hexagon-vertical-nft-slanted:before{content:"\e506"}.fa-folder-music:before{content:"\e18d"}.fa-desktop-medical:before,.fa-display-medical:before{content:"\e166"}.fa-share-all:before{content:"\f367"}.fa-peapod:before{content:"\e31c"}.fa-chess-clock:before{content:"\f43d"}.fa-axe:before{content:"\f6b2"}.fa-square-d:before{content:"\e268"}.fa-grip-vertical:before{content:"\f58e"}.fa-mobile-signal-out:before{content:"\e1f0"}.fa-arrow-turn-up:before,.fa-level-up:before{content:"\f148"}.fa-u:before{content:"\55"}.fa-arrow-up-from-dotted-line:before{content:"\e09b"}.fa-square-root-alt:before,.fa-square-root-variable:before{content:"\f698"}.fa-light-switch-on:before{content:"\e019"}.fa-arrow-down-arrow-up:before,.fa-sort-alt:before{content:"\f883"}.fa-raindrops:before{content:"\f75c"}.fa-dash:before,.fa-minus-large:before{content:"\e404"}.fa-clock-four:before,.fa-clock:before{content:"\f017"}.fa-input-numeric:before{content:"\e1bd"}.fa-truck-tow:before{content:"\e2b8"}.fa-backward-step:before,.fa-step-backward:before{content:"\f048"}.fa-pallet:before{content:"\f482"}.fa-car-bolt:before{content:"\e341"}.fa-arrows-maximize:before,.fa-expand-arrows:before{content:"\f31d"}.fa-faucet:before{content:"\e005"}.fa-cloud-sleet:before{content:"\f741"}.fa-lamp-street:before{content:"\e1c5"}.fa-list-radio:before{content:"\e1d0"}.fa-pen-nib-slash:before{content:"\e4a1"}.fa-baseball-bat-ball:before{content:"\f432"}.fa-square-up-left:before{content:"\e282"}.fa-overline:before{content:"\f876"}.fa-s:before{content:"\53"}.fa-timeline:before{content:"\e29c"}.fa-keyboard:before{content:"\f11c"}.fa-arrows-from-dotted-line:before{content:"\e0a3"}.fa-usb-drive:before{content:"\f8e9"}.fa-ballot:before{content:"\f732"}.fa-caret-down:before{content:"\f0d7"}.fa-location-dot-slash:before,.fa-map-marker-alt-slash:before{content:"\f605"}.fa-cards:before{content:"\e3ed"}.fa-clinic-medical:before,.fa-house-chimney-medical:before{content:"\f7f2"}.fa-boxing-glove:before,.fa-glove-boxing:before{content:"\f438"}.fa-temperature-3:before,.fa-temperature-three-quarters:before,.fa-thermometer-3:before,.fa-thermometer-three-quarters:before{content:"\f2c8"}.fa-bell-school:before{content:"\f5d5"}.fa-mobile-android-alt:before,.fa-mobile-screen:before{content:"\f3cf"}.fa-plane-up:before{content:"\e22d"}.fa-folder-heart:before{content:"\e189"}.fa-circle-location-arrow:before,.fa-location-circle:before{content:"\f602"}.fa-face-head-bandage:before{content:"\e37a"}.fa-maki-roll:before,.fa-makizushi:before,.fa-sushi-roll:before{content:"\e48b"}.fa-car-bump:before{content:"\f5e0"}.fa-piggy-bank:before{content:"\f4d3"}.fa-racquet:before{content:"\f45a"}.fa-car-mirrors:before{content:"\e343"}.fa-industry-alt:before,.fa-industry-windows:before{content:"\f3b3"}.fa-bolt-auto:before{content:"\e0b6"}.fa-battery-3:before,.fa-battery-half:before{content:"\f242"}.fa-flux-capacitor:before{content:"\f8ba"}.fa-mountain-city:before{content:"\e52e"}.fa-coins:before{content:"\f51e"}.fa-honey-pot:before{content:"\e418"}.fa-olive:before{content:"\e316"}.fa-khanda:before{content:"\f66d"}.fa-filter-list:before{content:"\e17c"}.fa-outlet:before{content:"\e01c"}.fa-sliders-h:before,.fa-sliders:before{content:"\f1de"}.fa-cauldron:before{content:"\f6bf"}.fa-people:before{content:"\e216"}.fa-folder-tree:before{content:"\f802"}.fa-network-wired:before{content:"\f6ff"}.fa-croissant:before{content:"\f7f6"}.fa-map-pin:before{content:"\f276"}.fa-hamsa:before{content:"\f665"}.fa-cent-sign:before{content:"\e3f5"}.fa-swords-laser:before{content:"\e03d"}.fa-flask:before{content:"\f0c3"}.fa-person-pregnant:before{content:"\e31e"}.fa-square-u:before{content:"\e281"}.fa-wand-sparkles:before{content:"\f72b"}.fa-router:before{content:"\f8da"}.fa-ellipsis-v:before,.fa-ellipsis-vertical:before{content:"\f142"}.fa-sword-laser-alt:before{content:"\e03c"}.fa-ticket:before{content:"\f145"}.fa-power-off:before{content:"\f011"}.fa-coin:before{content:"\f85c"}.fa-laptop-slash:before{content:"\e1c7"}.fa-long-arrow-alt-right:before,.fa-right-long:before{content:"\f30b"}.fa-circle-b:before{content:"\e0fd"}.fa-person-dress-simple:before{content:"\e21c"}.fa-pipe-collar:before{content:"\e437"}.fa-lights-holiday:before{content:"\f7b2"}.fa-citrus:before{content:"\e2f4"}.fa-flag-usa:before{content:"\f74d"}.fa-laptop-file:before{content:"\e51d"}.fa-teletype:before,.fa-tty:before{content:"\f1e4"}.fa-chart-tree-map:before{content:"\e0ea"}.fa-diagram-next:before{content:"\e476"}.fa-person-rifle:before{content:"\e54e"}.fa-clock-five-thirty:before{content:"\e34a"}.fa-pipe-valve:before{content:"\e439"}.fa-arrow-up-from-arc:before{content:"\e4b4"}.fa-face-spiral-eyes:before{content:"\e485"}.fa-compress-wide:before{content:"\f326"}.fa-circle-phone-hangup:before,.fa-phone-circle-down:before{content:"\e11d"}.fa-gear-complex-code:before{content:"\e5eb"}.fa-house-medical-circle-exclamation:before{content:"\e512"}.fa-badminton:before{content:"\e33a"}.fa-closed-captioning:before{content:"\f20a"}.fa-hiking:before,.fa-person-hiking:before{content:"\f6ec"}.fa-arrow-alt-from-left:before,.fa-right-from-line:before{content:"\f347"}.fa-venus-double:before{content:"\f226"}.fa-images:before{content:"\f302"}.fa-calculator:before{content:"\f1ec"}.fa-shuttlecock:before{content:"\f45b"}.fa-user-hair:before{content:"\e45a"}.fa-eye-evil:before{content:"\f6db"}.fa-people-pulling:before{content:"\e535"}.fa-n:before{content:"\4e"}.fa-swap:before{content:"\e609"}.fa-garage:before{content:"\e009"}.fa-cable-car:before,.fa-tram:before{content:"\f7da"}.fa-shovel-snow:before{content:"\f7c3"}.fa-cloud-rain:before{content:"\f73d"}.fa-face-lying:before{content:"\e37e"}.fa-sprinkler:before{content:"\e035"}.fa-building-circle-xmark:before{content:"\e4d4"}.fa-person-sledding:before,.fa-sledding:before{content:"\f7cb"}.fa-game-console-handheld:before{content:"\f8bb"}.fa-ship:before{content:"\f21a"}.fa-clock-six-thirty:before{content:"\e353"}.fa-battery-slash:before{content:"\f377"}.fa-tugrik-sign:before{content:"\e2ba"}.fa-arrows-down-to-line:before{content:"\e4b8"}.fa-download:before{content:"\f019"}.fa-angles-up-down:before{content:"\e60d"}.fa-inventory:before,.fa-shelves:before{content:"\f480"}.fa-cloud-snow:before{content:"\f742"}.fa-face-grin:before,.fa-grin:before{content:"\f580"}.fa-backspace:before,.fa-delete-left:before{content:"\f55a"}.fa-oven:before{content:"\e01d"}.fa-cloud-binary:before{content:"\e601"}.fa-eye-dropper-empty:before,.fa-eye-dropper:before,.fa-eyedropper:before{content:"\f1fb"}.fa-comment-captions:before{content:"\e146"}.fa-comments-question:before{content:"\e14e"}.fa-scribble:before{content:"\e23f"}.fa-rotate-exclamation:before{content:"\e23c"}.fa-file-circle-check:before{content:"\e5a0"}.fa-glass:before{content:"\f804"}.fa-loader:before{content:"\e1d4"}.fa-forward:before{content:"\f04e"}.fa-user-pilot:before{content:"\e2c0"}.fa-mobile-android:before,.fa-mobile-phone:before,.fa-mobile:before{content:"\f3ce"}.fa-code-pull-request-closed:before{content:"\e3f9"}.fa-face-meh:before,.fa-meh:before{content:"\f11a"}.fa-align-center:before{content:"\f037"}.fa-book-dead:before,.fa-book-skull:before{content:"\f6b7"}.fa-drivers-license:before,.fa-id-card:before{content:"\f2c2"}.fa-face-dotted:before{content:"\e47f"}.fa-face-worried:before{content:"\e3a3"}.fa-dedent:before,.fa-outdent:before{content:"\f03b"}.fa-court-sport:before{content:"\e643"}.fa-heart-circle-exclamation:before{content:"\e4fe"}.fa-home-alt:before,.fa-home-lg-alt:before,.fa-home:before,.fa-house:before{content:"\f015"}.fa-vector-circle:before{content:"\e2c6"}.fa-car-circle-bolt:before{content:"\e342"}.fa-calendar-week:before{content:"\f784"}.fa-flying-disc:before{content:"\e3a9"}.fa-laptop-medical:before{content:"\f812"}.fa-square-down-right:before{content:"\e26c"}.fa-b:before{content:"\42"}.fa-seat-airline:before{content:"\e244"}.fa-eclipse-alt:before,.fa-moon-over-sun:before{content:"\f74a"}.fa-pipe:before{content:"\7c"}.fa-file-medical:before{content:"\f477"}.fa-potato:before{content:"\e440"}.fa-dice-one:before{content:"\f525"}.fa-circle-a:before{content:"\e0f7"}.fa-helmet-battle:before{content:"\f6eb"}.fa-butter:before{content:"\e3e4"}.fa-blanket-fire:before{content:"\e3da"}.fa-kiwi-bird:before{content:"\f535"}.fa-castle:before{content:"\e0de"}.fa-golf-club:before{content:"\f451"}.fa-arrow-right-arrow-left:before,.fa-exchange:before{content:"\f0ec"}.fa-redo-alt:before,.fa-rotate-forward:before,.fa-rotate-right:before{content:"\f2f9"}.fa-cutlery:before,.fa-utensils:before{content:"\f2e7"}.fa-arrow-up-wide-short:before,.fa-sort-amount-up:before{content:"\f161"}.fa-chart-pie-simple-circle-dollar:before{content:"\e605"}.fa-balloons:before{content:"\e2e4"}.fa-mill-sign:before{content:"\e1ed"}.fa-bowl-rice:before{content:"\e2eb"}.fa-timeline-arrow:before{content:"\e29d"}.fa-skull:before{content:"\f54c"}.fa-game-board-alt:before,.fa-game-board-simple:before{content:"\f868"}.fa-circle-video:before,.fa-video-circle:before{content:"\e12b"}.fa-chart-scatter-bubble:before{content:"\e0e9"}.fa-house-turret:before{content:"\e1b4"}.fa-banana:before{content:"\e2e5"}.fa-hand-holding-skull:before{content:"\e1a4"}.fa-people-dress:before{content:"\e217"}.fa-couch-small:before,.fa-loveseat:before{content:"\f4cc"}.fa-broadcast-tower:before,.fa-tower-broadcast:before{content:"\f519"}.fa-truck-pickup:before{content:"\f63c"}.fa-block-quote:before{content:"\e0b5"}.fa-long-arrow-alt-up:before,.fa-up-long:before{content:"\f30c"}.fa-stop:before{content:"\f04d"}.fa-code-merge:before{content:"\f387"}.fa-money-check-dollar-pen:before,.fa-money-check-edit-alt:before{content:"\f873"}.fa-arrow-alt-from-bottom:before,.fa-up-from-line:before{content:"\f346"}.fa-upload:before{content:"\f093"}.fa-hurricane:before{content:"\f751"}.fa-grid-round-2-plus:before{content:"\e5dc"}.fa-people-pants:before{content:"\e219"}.fa-mound:before{content:"\e52d"}.fa-windsock:before{content:"\f777"}.fa-circle-half:before{content:"\e110"}.fa-brake-warning:before{content:"\e0c7"}.fa-toilet-portable:before{content:"\e583"}.fa-compact-disc:before{content:"\f51f"}.fa-file-arrow-down:before,.fa-file-download:before{content:"\f56d"}.fa-sax-hot:before,.fa-saxophone-fire:before{content:"\f8db"}.fa-camera-web-slash:before,.fa-webcam-slash:before{content:"\f833"}.fa-folder-medical:before{content:"\e18c"}.fa-folder-cog:before,.fa-folder-gear:before{content:"\e187"}.fa-hand-wave:before{content:"\e1a7"}.fa-arrow-up-arrow-down:before,.fa-sort-up-down:before{content:"\e099"}.fa-caravan:before{content:"\f8ff"}.fa-shield-cat:before{content:"\e572"}.fa-comment-alt-slash:before,.fa-message-slash:before{content:"\f4a9"}.fa-bolt:before,.fa-zap:before{content:"\f0e7"}.fa-trash-can-check:before{content:"\e2a9"}.fa-glass-water:before{content:"\e4f4"}.fa-oil-well:before{content:"\e532"}.fa-person-simple:before{content:"\e220"}.fa-arrow-turn-left-up:before{content:"\e634"}.fa-vault:before{content:"\e2c5"}.fa-mars:before{content:"\f222"}.fa-toilet:before{content:"\f7d8"}.fa-plane-circle-xmark:before{content:"\e557"}.fa-cny:before,.fa-jpy:before,.fa-rmb:before,.fa-yen-sign:before,.fa-yen:before{content:"\f157"}.fa-gear-code:before{content:"\e5e8"}.fa-notes:before{content:"\e202"}.fa-rouble:before,.fa-rub:before,.fa-ruble-sign:before,.fa-ruble:before{content:"\f158"}.fa-trash-arrow-turn-left:before,.fa-trash-undo:before{content:"\f895"}.fa-champagne-glass:before,.fa-glass-champagne:before{content:"\f79e"}.fa-objects-align-center-horizontal:before{content:"\e3bc"}.fa-sun:before{content:"\f185"}.fa-trash-alt-slash:before,.fa-trash-can-slash:before{content:"\e2ad"}.fa-screen-users:before,.fa-users-class:before{content:"\f63d"}.fa-guitar:before{content:"\f7a6"}.fa-arrow-square-left:before,.fa-square-arrow-left:before{content:"\f33a"}.fa-square-8:before{content:"\e25d"}.fa-face-smile-hearts:before{content:"\e390"}.fa-brackets-square:before,.fa-brackets:before{content:"\f7e9"}.fa-laptop-arrow-down:before{content:"\e1c6"}.fa-hockey-stick-puck:before{content:"\e3ae"}.fa-house-tree:before{content:"\e1b3"}.fa-signal-2:before,.fa-signal-fair:before{content:"\f68d"}.fa-face-laugh-wink:before,.fa-laugh-wink:before{content:"\f59c"}.fa-circle-dollar:before,.fa-dollar-circle:before,.fa-usd-circle:before{content:"\f2e8"}.fa-horse-head:before{content:"\f7ab"}.fa-arrows-repeat:before,.fa-repeat-alt:before{content:"\f364"}.fa-bore-hole:before{content:"\e4c3"}.fa-industry:before{content:"\f275"}.fa-image-polaroid:before{content:"\f8c4"}.fa-wave-triangle:before{content:"\f89a"}.fa-turn-left-down:before{content:"\e637"}.fa-person-running-fast:before{content:"\e5ff"}.fa-arrow-alt-circle-down:before,.fa-circle-down:before{content:"\f358"}.fa-grill:before{content:"\e5a3"}.fa-arrows-turn-to-dots:before{content:"\e4c1"}.fa-analytics:before,.fa-chart-mixed:before{content:"\f643"}.fa-florin-sign:before{content:"\e184"}.fa-arrow-down-short-wide:before,.fa-sort-amount-desc:before,.fa-sort-amount-down-alt:before{content:"\f884"}.fa-less-than:before{content:"\3c"}.fa-desktop-code:before,.fa-display-code:before{content:"\e165"}.fa-face-drooling:before{content:"\e372"}.fa-oil-temp:before,.fa-oil-temperature:before{content:"\f614"}.fa-question-square:before,.fa-square-question:before{content:"\f2fd"}.fa-air-conditioner:before{content:"\f8f4"}.fa-angle-down:before{content:"\f107"}.fa-mountains:before{content:"\f6fd"}.fa-omega:before{content:"\f67a"}.fa-car-tunnel:before{content:"\e4de"}.fa-person-dolly-empty:before{content:"\f4d1"}.fa-pan-food:before{content:"\e42b"}.fa-head-side-cough:before{content:"\e061"}.fa-grip-lines:before{content:"\f7a4"}.fa-thumbs-down:before{content:"\f165"}.fa-user-lock:before{content:"\f502"}.fa-arrow-right-long:before,.fa-long-arrow-right:before{content:"\f178"}.fa-tickets-airline:before,.fa-tickets-perforated-plane:before,.fa-tickets-plane:before{content:"\e29b"}.fa-tent-double-peak:before{content:"\e627"}.fa-anchor-circle-xmark:before{content:"\e4ac"}.fa-ellipsis-h:before,.fa-ellipsis:before{content:"\f141"}.fa-nfc-slash:before{content:"\e1fc"}.fa-chess-pawn:before{content:"\f443"}.fa-first-aid:before,.fa-kit-medical:before{content:"\f479"}.fa-grid-2-plus:before{content:"\e197"}.fa-bells:before{content:"\f77f"}.fa-person-through-window:before{content:"\e5a9"}.fa-toolbox:before{content:"\f552"}.fa-envelope-badge:before,.fa-envelope-dot:before{content:"\e16f"}.fa-magnifying-glass-waveform:before{content:"\e661"}.fa-hands-holding-circle:before{content:"\e4fb"}.fa-bug:before{content:"\f188"}.fa-bowl-chopsticks:before{content:"\e2e9"}.fa-credit-card-alt:before,.fa-credit-card:before{content:"\f09d"}.fa-circle-s:before{content:"\e121"}.fa-box-ballot:before{content:"\f735"}.fa-automobile:before,.fa-car:before{content:"\f1b9"}.fa-hand-holding-hand:before{content:"\e4f7"}.fa-user-tie-hair:before{content:"\e45f"}.fa-podium-star:before{content:"\f758"}.fa-business-front:before,.fa-party-back:before,.fa-trian-balbot:before,.fa-user-hair-mullet:before{content:"\e45c"}.fa-microphone-stand:before{content:"\f8cb"}.fa-book-open-reader:before,.fa-book-reader:before{content:"\f5da"}.fa-family-dress:before{content:"\e301"}.fa-circle-x:before{content:"\e12e"}.fa-cabin:before{content:"\e46d"}.fa-mountain-sun:before{content:"\e52f"}.fa-chart-simple-horizontal:before{content:"\e474"}.fa-arrows-left-right-to-line:before{content:"\e4ba"}.fa-hand-back-point-left:before{content:"\e19f"}.fa-comment-alt-dots:before,.fa-message-dots:before,.fa-messaging:before{content:"\f4a3"}.fa-file-heart:before{content:"\e176"}.fa-beer-foam:before,.fa-beer-mug:before{content:"\e0b3"}.fa-dice-d20:before{content:"\f6cf"}.fa-drone:before{content:"\f85f"}.fa-truck-droplet:before{content:"\e58c"}.fa-file-circle-xmark:before{content:"\e5a1"}.fa-temperature-arrow-up:before,.fa-temperature-up:before{content:"\e040"}.fa-medal:before{content:"\f5a2"}.fa-person-fairy:before{content:"\e608"}.fa-bed:before{content:"\f236"}.fa-book-copy:before{content:"\e0be"}.fa-h-square:before,.fa-square-h:before{content:"\f0fd"}.fa-square-c:before{content:"\e266"}.fa-clock-two:before{content:"\e35a"}.fa-square-ellipsis-vertical:before{content:"\e26f"}.fa-calendar-users:before{content:"\e5e2"}.fa-podcast:before{content:"\f2ce"}.fa-bee:before{content:"\e0b2"}.fa-temperature-4:before,.fa-temperature-full:before,.fa-thermometer-4:before,.fa-thermometer-full:before{content:"\f2c7"}.fa-bell:before{content:"\f0f3"}.fa-candy-bar:before,.fa-chocolate-bar:before{content:"\e3e8"}.fa-xmark-large:before{content:"\e59b"}.fa-pinata:before{content:"\e3c3"}.fa-file-ppt:before{content:"\e64a"}.fa-arrows-from-line:before{content:"\e0a4"}.fa-superscript:before{content:"\f12b"}.fa-bowl-spoon:before{content:"\e3e0"}.fa-hexagon-check:before{content:"\e416"}.fa-plug-circle-xmark:before{content:"\e560"}.fa-star-of-life:before{content:"\f621"}.fa-phone-slash:before{content:"\f3dd"}.fa-traffic-light-stop:before{content:"\f63a"}.fa-paint-roller:before{content:"\f5aa"}.fa-accent-grave:before{content:"\60"}.fa-hands-helping:before,.fa-handshake-angle:before{content:"\f4c4"}.fa-circle-0:before{content:"\e0ed"}.fa-dial-med-low:before{content:"\e160"}.fa-location-dot:before,.fa-map-marker-alt:before{content:"\f3c5"}.fa-crab:before{content:"\e3ff"}.fa-box-full:before,.fa-box-open-full:before{content:"\f49c"}.fa-file:before{content:"\f15b"}.fa-greater-than:before{content:"\3e"}.fa-quotes:before{content:"\e234"}.fa-pretzel:before{content:"\e441"}.fa-t-rex:before{content:"\e629"}.fa-person-swimming:before,.fa-swimmer:before{content:"\f5c4"}.fa-arrow-down:before{content:"\f063"}.fa-user-robot-xmarks:before{content:"\e4a7"}.fa-comment-alt-quote:before,.fa-message-quote:before{content:"\e1e4"}.fa-candy-corn:before{content:"\f6bd"}.fa-folder-magnifying-glass:before,.fa-folder-search:before{content:"\e18b"}.fa-notebook:before{content:"\e201"}.fa-circle-wifi:before{content:"\e67d"}.fa-droplet:before,.fa-tint:before{content:"\f043"}.fa-bullseye-pointer:before{content:"\f649"}.fa-eraser:before{content:"\f12d"}.fa-hexagon-image:before{content:"\e504"}.fa-earth-america:before,.fa-earth-americas:before,.fa-earth:before,.fa-globe-americas:before{content:"\f57d"}.fa-file-svg:before{content:"\e64b"}.fa-crate-apple:before{content:"\f6b1"}.fa-apple-crate:before{content:"\f6b1"}.fa-person-burst:before{content:"\e53b"}.fa-game-board:before{content:"\f867"}.fa-hat-chef:before{content:"\f86b"}.fa-hand-back-point-right:before{content:"\e1a1"}.fa-dove:before{content:"\f4ba"}.fa-snowflake-droplets:before{content:"\e5c1"}.fa-battery-0:before,.fa-battery-empty:before{content:"\f244"}.fa-grid-4:before{content:"\e198"}.fa-socks:before{content:"\f696"}.fa-face-sunglasses:before{content:"\e398"}.fa-inbox:before{content:"\f01c"}.fa-square-0:before{content:"\e255"}.fa-section:before{content:"\e447"}.fa-box-up:before,.fa-square-this-way-up:before{content:"\f49f"}.fa-gauge-high:before,.fa-tachometer-alt-fast:before,.fa-tachometer-alt:before{content:"\f625"}.fa-square-ampersand:before{content:"\e260"}.fa-envelope-open-text:before{content:"\f658"}.fa-lamp-desk:before{content:"\e014"}.fa-hospital-alt:before,.fa-hospital-wide:before,.fa-hospital:before{content:"\f0f8"}.fa-poll-people:before{content:"\f759"}.fa-glass-whiskey-rocks:before,.fa-whiskey-glass-ice:before{content:"\f7a1"}.fa-wine-bottle:before{content:"\f72f"}.fa-chess-rook:before{content:"\f447"}.fa-user-bounty-hunter:before{content:"\e2bf"}.fa-bars-staggered:before,.fa-reorder:before,.fa-stream:before{content:"\f550"}.fa-diagram-sankey:before{content:"\e158"}.fa-cloud-hail-mixed:before{content:"\f73a"}.fa-circle-up-left:before{content:"\e128"}.fa-dharmachakra:before{content:"\f655"}.fa-objects-align-left:before{content:"\e3be"}.fa-oil-can-drip:before{content:"\e205"}.fa-face-smiling-hands:before{content:"\e396"}.fa-broccoli:before{content:"\e3e2"}.fa-route-interstate:before{content:"\f61b"}.fa-ear-muffs:before{content:"\f795"}.fa-hotdog:before{content:"\f80f"}.fa-transporter-empty:before{content:"\e046"}.fa-blind:before,.fa-person-walking-with-cane:before{content:"\f29d"}.fa-angle-90:before{content:"\e08d"}.fa-rectangle-terminal:before{content:"\e236"}.fa-kite:before{content:"\f6f4"}.fa-drum:before{content:"\f569"}.fa-scrubber:before{content:"\f2f8"}.fa-ice-cream:before{content:"\f810"}.fa-heart-circle-bolt:before{content:"\e4fc"}.fa-fish-bones:before{content:"\e304"}.fa-deer-rudolph:before{content:"\f78f"}.fa-fax:before{content:"\f1ac"}.fa-paragraph:before{content:"\f1dd"}.fa-head-side-heart:before{content:"\e1aa"}.fa-square-e:before{content:"\e26d"}.fa-meter-fire:before{content:"\e1eb"}.fa-cloud-hail:before{content:"\f739"}.fa-check-to-slot:before,.fa-vote-yea:before{content:"\f772"}.fa-money-from-bracket:before{content:"\e312"}.fa-star-half:before{content:"\f089"}.fa-car-bus:before{content:"\f85a"}.fa-speaker:before{content:"\f8df"}.fa-timer:before{content:"\e29e"}.fa-boxes-alt:before,.fa-boxes-stacked:before,.fa-boxes:before{content:"\f468"}.fa-landmark-magnifying-glass:before{content:"\e622"}.fa-grill-hot:before{content:"\e5a5"}.fa-ballot-check:before{content:"\f733"}.fa-chain:before,.fa-link:before{content:"\f0c1"}.fa-assistive-listening-systems:before,.fa-ear-listen:before{content:"\f2a2"}.fa-file-minus:before{content:"\f318"}.fa-tree-city:before{content:"\e587"}.fa-play:before{content:"\f04b"}.fa-font:before{content:"\f031"}.fa-coffee-togo:before,.fa-cup-togo:before{content:"\f6c5"}.fa-square-down-left:before{content:"\e26b"}.fa-burger-lettuce:before{content:"\e3e3"}.fa-table-cells-row-lock:before{content:"\e67a"}.fa-rupiah-sign:before{content:"\e23d"}.fa-magnifying-glass:before,.fa-search:before{content:"\f002"}.fa-ping-pong-paddle-ball:before,.fa-table-tennis-paddle-ball:before,.fa-table-tennis:before{content:"\f45d"}.fa-diagnoses:before,.fa-person-dots-from-line:before{content:"\f470"}.fa-chevron-double-down:before,.fa-chevrons-down:before{content:"\f322"}.fa-trash-can-arrow-up:before,.fa-trash-restore-alt:before{content:"\f82a"}.fa-signal-3:before,.fa-signal-good:before{content:"\f68e"}.fa-location-question:before,.fa-map-marker-question:before{content:"\f60b"}.fa-floppy-disk-circle-xmark:before,.fa-floppy-disk-times:before,.fa-save-circle-xmark:before,.fa-save-times:before{content:"\e181"}.fa-naira-sign:before{content:"\e1f6"}.fa-peach:before{content:"\e20b"}.fa-taxi-bus:before{content:"\e298"}.fa-bracket-curly-left:before,.fa-bracket-curly:before{content:"\7b"}.fa-lobster:before{content:"\e421"}.fa-cart-flatbed-empty:before,.fa-dolly-flatbed-empty:before{content:"\f476"}.fa-colon:before{content:"\3a"}.fa-cart-arrow-down:before{content:"\f218"}.fa-wand:before{content:"\f72a"}.fa-walkie-talkie:before{content:"\f8ef"}.fa-file-edit:before,.fa-file-pen:before{content:"\f31c"}.fa-receipt:before{content:"\f543"}.fa-table-picnic:before{content:"\e32d"}.fa-pen-square:before,.fa-pencil-square:before,.fa-square-pen:before{content:"\f14b"}.fa-circle-microphone-lines:before,.fa-microphone-circle-alt:before{content:"\e117"}.fa-desktop-slash:before,.fa-display-slash:before{content:"\e2fa"}.fa-suitcase-rolling:before{content:"\f5c1"}.fa-person-circle-exclamation:before{content:"\e53f"}.fa-transporter-2:before{content:"\e044"}.fa-hand-receiving:before,.fa-hands-holding-diamond:before{content:"\f47c"}.fa-money-bill-simple-wave:before{content:"\e1f2"}.fa-chevron-down:before{content:"\f078"}.fa-battery-5:before,.fa-battery-full:before,.fa-battery:before{content:"\f240"}.fa-bell-plus:before{content:"\f849"}.fa-book-arrow-right:before{content:"\e0b9"}.fa-hospitals:before{content:"\f80e"}.fa-club:before{content:"\f327"}.fa-skull-crossbones:before{content:"\f714"}.fa-dewpoint:before,.fa-droplet-degree:before{content:"\f748"}.fa-code-compare:before{content:"\e13a"}.fa-list-dots:before,.fa-list-ul:before{content:"\f0ca"}.fa-hand-holding-magic:before{content:"\f6e5"}.fa-watermelon-slice:before{content:"\e337"}.fa-circle-ellipsis:before{content:"\e10a"}.fa-school-lock:before{content:"\e56f"}.fa-tower-cell:before{content:"\e585"}.fa-sd-cards:before{content:"\e240"}.fa-jug-bottle:before{content:"\e5fb"}.fa-down-long:before,.fa-long-arrow-alt-down:before{content:"\f309"}.fa-envelopes:before{content:"\e170"}.fa-phone-office:before{content:"\f67d"}.fa-ranking-star:before{content:"\e561"}.fa-chess-king:before{content:"\f43f"}.fa-nfc-pen:before{content:"\e1fa"}.fa-person-harassing:before{content:"\e549"}.fa-magnifying-glass-play:before{content:"\e660"}.fa-hat-winter:before{content:"\f7a8"}.fa-brazilian-real-sign:before{content:"\e46c"}.fa-landmark-alt:before,.fa-landmark-dome:before{content:"\f752"}.fa-bone-break:before{content:"\f5d8"}.fa-arrow-up:before{content:"\f062"}.fa-down-from-dotted-line:before{content:"\e407"}.fa-television:before,.fa-tv-alt:before,.fa-tv:before{content:"\f26c"}.fa-border-left:before{content:"\f84f"}.fa-circle-divide:before{content:"\e106"}.fa-shrimp:before{content:"\e448"}.fa-list-check:before,.fa-tasks:before{content:"\f0ae"}.fa-diagram-subtask:before{content:"\e479"}.fa-jug-detergent:before{content:"\e519"}.fa-circle-user:before,.fa-user-circle:before{content:"\f2bd"}.fa-square-y:before{content:"\e287"}.fa-user-doctor-hair:before{content:"\e458"}.fa-planet-ringed:before{content:"\e020"}.fa-mushroom:before{content:"\e425"}.fa-user-shield:before{content:"\f505"}.fa-megaphone:before{content:"\f675"}.fa-wreath-laurel:before{content:"\e5d2"}.fa-circle-exclamation-check:before{content:"\e10d"}.fa-wind:before{content:"\f72e"}.fa-box-dollar:before,.fa-box-usd:before{content:"\f4a0"}.fa-car-burst:before,.fa-car-crash:before{content:"\f5e1"}.fa-y:before{content:"\59"}.fa-user-headset:before{content:"\f82d"}.fa-arrows-retweet:before,.fa-retweet-alt:before{content:"\f361"}.fa-person-snowboarding:before,.fa-snowboarding:before{content:"\f7ce"}.fa-chevron-square-right:before,.fa-square-chevron-right:before{content:"\f32b"}.fa-lacrosse-stick-ball:before{content:"\e3b6"}.fa-shipping-fast:before,.fa-truck-fast:before{content:"\f48b"}.fa-user-magnifying-glass:before{content:"\e5c5"}.fa-star-sharp:before{content:"\e28b"}.fa-comment-heart:before{content:"\e5c8"}.fa-circle-1:before{content:"\e0ee"}.fa-circle-star:before,.fa-star-circle:before{content:"\e123"}.fa-fish:before{content:"\f578"}.fa-cloud-fog:before,.fa-fog:before{content:"\f74e"}.fa-waffle:before{content:"\e466"}.fa-music-alt:before,.fa-music-note:before{content:"\f8cf"}.fa-hexagon-exclamation:before{content:"\e417"}.fa-cart-shopping-fast:before{content:"\e0dc"}.fa-object-union:before{content:"\e49f"}.fa-user-graduate:before{content:"\f501"}.fa-starfighter:before{content:"\e037"}.fa-adjust:before,.fa-circle-half-stroke:before{content:"\f042"}.fa-arrow-right-long-to-line:before{content:"\e3d5"}.fa-arrow-square-down:before,.fa-square-arrow-down:before{content:"\f339"}.fa-diamond-half-stroke:before{content:"\e5b8"}.fa-clapperboard:before{content:"\e131"}.fa-chevron-square-left:before,.fa-square-chevron-left:before{content:"\f32a"}.fa-phone-intercom:before{content:"\e434"}.fa-chain-horizontal:before,.fa-link-horizontal:before{content:"\e1cb"}.fa-mango:before{content:"\e30f"}.fa-music-alt-slash:before,.fa-music-note-slash:before{content:"\f8d0"}.fa-circle-radiation:before,.fa-radiation-alt:before{content:"\f7ba"}.fa-face-tongue-sweat:before{content:"\e39e"}.fa-globe-stand:before{content:"\f5f6"}.fa-baseball-ball:before,.fa-baseball:before{content:"\f433"}.fa-circle-p:before{content:"\e11a"}.fa-award-simple:before{content:"\e0ab"}.fa-jet-fighter-up:before{content:"\e518"}.fa-diagram-project:before,.fa-project-diagram:before{content:"\f542"}.fa-pedestal:before{content:"\e20d"}.fa-chart-pyramid:before{content:"\e0e6"}.fa-sidebar:before{content:"\e24e"}.fa-frosty-head:before,.fa-snowman-head:before{content:"\f79b"}.fa-copy:before{content:"\f0c5"}.fa-burger-glass:before{content:"\e0ce"}.fa-volume-mute:before,.fa-volume-times:before,.fa-volume-xmark:before{content:"\f6a9"}.fa-hand-sparkles:before{content:"\e05d"}.fa-bars-filter:before{content:"\e0ad"}.fa-paintbrush-pencil:before{content:"\e206"}.fa-party-bell:before{content:"\e31a"}.fa-user-vneck-hair:before{content:"\e462"}.fa-jack-o-lantern:before{content:"\f30e"}.fa-grip-horizontal:before,.fa-grip:before{content:"\f58d"}.fa-share-from-square:before,.fa-share-square:before{content:"\f14d"}.fa-keynote:before{content:"\f66c"}.fa-child-combatant:before,.fa-child-rifle:before{content:"\e4e0"}.fa-gun:before{content:"\e19b"}.fa-phone-square:before,.fa-square-phone:before{content:"\f098"}.fa-hat-beach:before{content:"\e606"}.fa-add:before,.fa-plus:before{content:"\2b"}.fa-expand:before{content:"\f065"}.fa-computer:before{content:"\e4e5"}.fa-fort:before{content:"\e486"}.fa-cloud-check:before{content:"\e35c"}.fa-close:before,.fa-multiply:before,.fa-remove:before,.fa-times:before,.fa-xmark:before{content:"\f00d"}.fa-face-smirking:before{content:"\e397"}.fa-arrows-up-down-left-right:before,.fa-arrows:before{content:"\f047"}.fa-chalkboard-teacher:before,.fa-chalkboard-user:before{content:"\f51c"}.fa-rhombus:before{content:"\e23b"}.fa-claw-marks:before{content:"\f6c2"}.fa-peso-sign:before{content:"\e222"}.fa-face-smile-tongue:before{content:"\e394"}.fa-cart-circle-xmark:before{content:"\e3f4"}.fa-building-shield:before{content:"\e4d8"}.fa-circle-phone-flip:before,.fa-phone-circle-alt:before{content:"\e11c"}.fa-baby:before{content:"\f77c"}.fa-users-line:before{content:"\e592"}.fa-quote-left-alt:before,.fa-quote-left:before{content:"\f10d"}.fa-tractor:before{content:"\f722"}.fa-down-from-bracket:before{content:"\e66b"}.fa-key-skeleton:before{content:"\f6f3"}.fa-trash-arrow-up:before,.fa-trash-restore:before{content:"\f829"}.fa-arrow-down-up-lock:before{content:"\e4b0"}.fa-arrow-down-to-bracket:before{content:"\e094"}.fa-lines-leaning:before{content:"\e51e"}.fa-square-q:before{content:"\e27b"}.fa-ruler-combined:before{content:"\f546"}.fa-icons-alt:before,.fa-symbols:before{content:"\f86e"}.fa-copyright:before{content:"\f1f9"}.fa-flask-gear:before{content:"\e5f1"}.fa-highlighter-line:before{content:"\e1af"}.fa-bracket-left:before,.fa-bracket-square:before,.fa-bracket:before{content:"\5b"}.fa-island-tree-palm:before,.fa-island-tropical:before{content:"\f811"}.fa-arrow-from-left:before,.fa-arrow-right-from-line:before{content:"\f343"}.fa-h2:before{content:"\f314"}.fa-equals:before{content:"\3d"}.fa-cake-slice:before,.fa-shortcake:before{content:"\e3e5"}.fa-building-magnifying-glass:before{content:"\e61c"}.fa-peanut:before{content:"\e430"}.fa-wrench-simple:before{content:"\e2d1"}.fa-blender:before{content:"\f517"}.fa-teeth:before{content:"\f62e"}.fa-tally-2:before{content:"\e295"}.fa-ils:before,.fa-shekel-sign:before,.fa-shekel:before,.fa-sheqel-sign:before,.fa-sheqel:before{content:"\f20b"}.fa-cars:before{content:"\f85b"}.fa-axe-battle:before{content:"\f6b3"}.fa-user-hair-long:before{content:"\e45b"}.fa-map:before{content:"\f279"}.fa-arrow-left-from-arc:before{content:"\e615"}.fa-file-circle-info:before{content:"\e493"}.fa-face-disappointed:before{content:"\e36f"}.fa-lasso-sparkles:before{content:"\e1c9"}.fa-clock-eleven:before{content:"\e347"}.fa-rocket:before{content:"\f135"}.fa-siren-on:before{content:"\e02e"}.fa-clock-ten:before{content:"\e354"}.fa-candle-holder:before{content:"\f6bc"}.fa-video-arrow-down-left:before{content:"\e2c8"}.fa-photo-film:before,.fa-photo-video:before{content:"\f87c"}.fa-floppy-disk-circle-arrow-right:before,.fa-save-circle-arrow-right:before{content:"\e180"}.fa-folder-minus:before{content:"\f65d"}.fa-planet-moon:before{content:"\e01f"}.fa-face-eyes-xmarks:before{content:"\e374"}.fa-chart-scatter:before{content:"\f7ee"}.fa-circle-gf:before{content:"\e67f"}.fa-display-arrow-down:before{content:"\e164"}.fa-store:before{content:"\f54e"}.fa-arrow-trend-up:before{content:"\e098"}.fa-plug-circle-minus:before{content:"\e55e"}.fa-olive-branch:before{content:"\e317"}.fa-angle:before{content:"\e08c"}.fa-vacuum-robot:before{content:"\e04e"}.fa-sign-hanging:before,.fa-sign:before{content:"\f4d9"}.fa-square-divide:before{content:"\e26a"}.fa-folder-check:before{content:"\e64e"}.fa-signal-stream-slash:before{content:"\e250"}.fa-bezier-curve:before{content:"\f55b"}.fa-eye-dropper-half:before{content:"\e173"}.fa-store-lock:before{content:"\e4a6"}.fa-bell-slash:before{content:"\f1f6"}.fa-cloud-bolt-sun:before,.fa-thunderstorm-sun:before{content:"\f76e"}.fa-camera-slash:before{content:"\e0d9"}.fa-comment-quote:before{content:"\e14c"}.fa-tablet-android:before,.fa-tablet:before{content:"\f3fb"}.fa-school-flag:before{content:"\e56e"}.fa-message-code:before{content:"\e1df"}.fa-glass-half-empty:before,.fa-glass-half-full:before,.fa-glass-half:before{content:"\e192"}.fa-fill:before{content:"\f575"}.fa-comment-alt-minus:before,.fa-message-minus:before{content:"\f4a7"}.fa-angle-up:before{content:"\f106"}.fa-dinosaur:before{content:"\e5fe"}.fa-drumstick-bite:before{content:"\f6d7"}.fa-chain-horizontal-slash:before,.fa-link-horizontal-slash:before{content:"\e1cc"}.fa-holly-berry:before{content:"\f7aa"}.fa-nose:before{content:"\e5bd"}.fa-arrow-left-to-arc:before{content:"\e616"}.fa-chevron-left:before{content:"\f053"}.fa-bacteria:before{content:"\e059"}.fa-clouds:before{content:"\f744"}.fa-money-bill-simple:before{content:"\e1f1"}.fa-hand-lizard:before{content:"\f258"}.fa-table-pivot:before{content:"\e291"}.fa-filter-slash:before{content:"\e17d"}.fa-trash-can-arrow-turn-left:before,.fa-trash-can-undo:before,.fa-trash-undo-alt:before{content:"\f896"}.fa-notdef:before{content:"\e1fe"}.fa-disease:before{content:"\f7fa"}.fa-person-to-door:before{content:"\e433"}.fa-turntable:before{content:"\f8e4"}.fa-briefcase-medical:before{content:"\f469"}.fa-genderless:before{content:"\f22d"}.fa-chevron-right:before{content:"\f054"}.fa-signal-1:before,.fa-signal-weak:before{content:"\f68c"}.fa-clock-five:before{content:"\e349"}.fa-retweet:before{content:"\f079"}.fa-car-alt:before,.fa-car-rear:before{content:"\f5de"}.fa-pump-soap:before{content:"\e06b"}.fa-computer-classic:before{content:"\f8b1"}.fa-frame:before{content:"\e495"}.fa-video-slash:before{content:"\f4e2"}.fa-battery-2:before,.fa-battery-quarter:before{content:"\f243"}.fa-ellipsis-h-alt:before,.fa-ellipsis-stroke:before{content:"\f39b"}.fa-radio:before{content:"\f8d7"}.fa-baby-carriage:before,.fa-carriage-baby:before{content:"\f77d"}.fa-face-expressionless:before{content:"\e373"}.fa-down-to-dotted-line:before{content:"\e408"}.fa-cloud-music:before{content:"\f8ae"}.fa-traffic-light:before{content:"\f637"}.fa-cloud-minus:before{content:"\e35d"}.fa-thermometer:before{content:"\f491"}.fa-shield-minus:before{content:"\e249"}.fa-vr-cardboard:before{content:"\f729"}.fa-car-tilt:before{content:"\f5e5"}.fa-gauge-circle-minus:before{content:"\e497"}.fa-brightness-low:before{content:"\e0ca"}.fa-hand-middle-finger:before{content:"\f806"}.fa-percent:before,.fa-percentage:before{content:"\25"}.fa-truck-moving:before{content:"\f4df"}.fa-glass-water-droplet:before{content:"\e4f5"}.fa-conveyor-belt:before{content:"\f46e"}.fa-location-check:before,.fa-map-marker-check:before{content:"\f606"}.fa-coin-vertical:before{content:"\e3fd"}.fa-display:before{content:"\e163"}.fa-person-sign:before{content:"\f757"}.fa-face-smile:before,.fa-smile:before{content:"\f118"}.fa-phone-hangup:before{content:"\e225"}.fa-signature-slash:before{content:"\e3cb"}.fa-thumb-tack:before,.fa-thumbtack:before{content:"\f08d"}.fa-wheat-slash:before{content:"\e339"}.fa-trophy:before{content:"\f091"}.fa-clouds-sun:before{content:"\f746"}.fa-person-praying:before,.fa-pray:before{content:"\f683"}.fa-hammer:before{content:"\f6e3"}.fa-face-vomit:before{content:"\e3a0"}.fa-speakers:before{content:"\f8e0"}.fa-teletype-answer:before,.fa-tty-answer:before{content:"\e2b9"}.fa-mug-tea-saucer:before{content:"\e1f5"}.fa-diagram-lean-canvas:before{content:"\e156"}.fa-alt:before{content:"\e08a"}.fa-dial-med-high:before,.fa-dial:before{content:"\e15b"}.fa-hand-peace:before{content:"\f25b"}.fa-circle-trash:before,.fa-trash-circle:before{content:"\e126"}.fa-rotate:before,.fa-sync-alt:before{content:"\f2f1"}.fa-circle-quarters:before{content:"\e3f8"}.fa-spinner:before{content:"\f110"}.fa-tower-control:before{content:"\e2a2"}.fa-arrow-up-triangle-square:before,.fa-sort-shapes-up:before{content:"\f88a"}.fa-whale:before{content:"\f72c"}.fa-robot:before{content:"\f544"}.fa-peace:before{content:"\f67c"}.fa-party-horn:before{content:"\e31b"}.fa-cogs:before,.fa-gears:before{content:"\f085"}.fa-sun-alt:before,.fa-sun-bright:before{content:"\e28f"}.fa-warehouse:before{content:"\f494"}.fa-conveyor-belt-arm:before{content:"\e5f8"}.fa-lock-keyhole-open:before,.fa-lock-open-alt:before{content:"\f3c2"}.fa-box-fragile:before,.fa-square-fragile:before,.fa-square-wine-glass-crack:before{content:"\f49b"}.fa-arrow-up-right-dots:before{content:"\e4b7"}.fa-square-n:before{content:"\e277"}.fa-splotch:before{content:"\f5bc"}.fa-face-grin-hearts:before,.fa-grin-hearts:before{content:"\f584"}.fa-meter:before{content:"\e1e8"}.fa-mandolin:before{content:"\f6f9"}.fa-dice-four:before{content:"\f524"}.fa-sim-card:before{content:"\f7c4"}.fa-transgender-alt:before,.fa-transgender:before{content:"\f225"}.fa-mercury:before{content:"\f223"}.fa-up-from-bracket:before{content:"\e590"}.fa-knife-kitchen:before{content:"\f6f5"}.fa-border-right:before{content:"\f852"}.fa-arrow-turn-down:before,.fa-level-down:before{content:"\f149"}.fa-spade:before{content:"\f2f4"}.fa-card-spade:before{content:"\e3ec"}.fa-line-columns:before{content:"\f870"}.fa-ant:before{content:"\e680"}.fa-arrow-right-to-line:before,.fa-arrow-to-right:before{content:"\f340"}.fa-person-falling-burst:before{content:"\e547"}.fa-flag-pennant:before,.fa-pennant:before{content:"\f456"}.fa-conveyor-belt-empty:before{content:"\e150"}.fa-user-group-simple:before{content:"\e603"}.fa-award:before{content:"\f559"}.fa-ticket-alt:before,.fa-ticket-simple:before{content:"\f3ff"}.fa-building:before{content:"\f1ad"}.fa-angle-double-left:before,.fa-angles-left:before{content:"\f100"}.fa-camcorder:before,.fa-video-handheld:before{content:"\f8a8"}.fa-pancakes:before{content:"\e42d"}.fa-album-circle-user:before{content:"\e48d"}.fa-subtitles-slash:before{content:"\e610"}.fa-qrcode:before{content:"\f029"}.fa-dice-d10:before{content:"\f6cd"}.fa-fireplace:before{content:"\f79a"}.fa-browser:before{content:"\f37e"}.fa-pen-paintbrush:before,.fa-pencil-paintbrush:before{content:"\f618"}.fa-fish-cooked:before{content:"\f7fe"}.fa-chair-office:before{content:"\f6c1"}.fa-magnifying-glass-music:before{content:"\e65f"}.fa-nesting-dolls:before{content:"\e3ba"}.fa-clock-rotate-left:before,.fa-history:before{content:"\f1da"}.fa-trumpet:before{content:"\f8e3"}.fa-face-grin-beam-sweat:before,.fa-grin-beam-sweat:before{content:"\f583"}.fa-fire-smoke:before{content:"\f74b"}.fa-phone-missed:before{content:"\e226"}.fa-arrow-right-from-file:before,.fa-file-export:before{content:"\f56e"}.fa-shield-blank:before,.fa-shield:before{content:"\f132"}.fa-arrow-up-short-wide:before,.fa-sort-amount-up-alt:before{content:"\f885"}.fa-arrows-repeat-1:before,.fa-repeat-1-alt:before{content:"\f366"}.fa-gun-slash:before{content:"\e19c"}.fa-avocado:before{content:"\e0aa"}.fa-binary:before{content:"\e33b"}.fa-glasses-alt:before,.fa-glasses-round:before{content:"\f5f5"}.fa-phone-plus:before{content:"\f4d2"}.fa-ditto:before{content:"\22"}.fa-person-seat:before{content:"\e21e"}.fa-house-medical:before{content:"\e3b2"}.fa-golf-ball-tee:before,.fa-golf-ball:before{content:"\f450"}.fa-chevron-circle-left:before,.fa-circle-chevron-left:before{content:"\f137"}.fa-house-chimney-window:before{content:"\e00d"}.fa-scythe:before{content:"\f710"}.fa-pen-nib:before{content:"\f5ad"}.fa-ban-parking:before,.fa-parking-circle-slash:before{content:"\f616"}.fa-tent-arrow-turn-left:before{content:"\e580"}.fa-face-diagonal-mouth:before{content:"\e47e"}.fa-diagram-cells:before{content:"\e475"}.fa-cricket-bat-ball:before,.fa-cricket:before{content:"\f449"}.fa-tents:before{content:"\e582"}.fa-magic:before,.fa-wand-magic:before{content:"\f0d0"}.fa-dog:before{content:"\f6d3"}.fa-pen-line:before{content:"\e212"}.fa-atom-alt:before,.fa-atom-simple:before{content:"\f5d3"}.fa-ampersand:before{content:"\26"}.fa-carrot:before{content:"\f787"}.fa-arrow-from-bottom:before,.fa-arrow-up-from-line:before{content:"\f342"}.fa-moon:before{content:"\f186"}.fa-pen-slash:before{content:"\e213"}.fa-wine-glass-alt:before,.fa-wine-glass-empty:before{content:"\f5ce"}.fa-square-star:before{content:"\e27f"}.fa-cheese:before{content:"\f7ef"}.fa-send-backward:before{content:"\f87f"}.fa-yin-yang:before{content:"\f6ad"}.fa-music:before{content:"\f001"}.fa-compass-slash:before{content:"\f5e9"}.fa-clock-one:before{content:"\e34e"}.fa-file-music:before{content:"\f8b6"}.fa-code-commit:before{content:"\f386"}.fa-temperature-low:before{content:"\f76b"}.fa-biking:before,.fa-person-biking:before{content:"\f84a"}.fa-display-chart-up-circle-currency:before{content:"\e5e5"}.fa-skeleton:before{content:"\f620"}.fa-circle-g:before{content:"\e10f"}.fa-circle-arrow-up-left:before{content:"\e0fb"}.fa-coin-blank:before{content:"\e3fb"}.fa-broom:before{content:"\f51a"}.fa-vacuum:before{content:"\e04d"}.fa-shield-heart:before{content:"\e574"}.fa-card-heart:before{content:"\e3eb"}.fa-lightbulb-cfl-on:before{content:"\e5a7"}.fa-melon:before{content:"\e310"}.fa-gopuram:before{content:"\f664"}.fa-earth-oceania:before,.fa-globe-oceania:before{content:"\e47b"}.fa-container-storage:before{content:"\f4b7"}.fa-face-pouting:before{content:"\e387"}.fa-square-xmark:before,.fa-times-square:before,.fa-xmark-square:before{content:"\f2d3"}.fa-exploding-head:before,.fa-face-explode:before{content:"\e2fe"}.fa-hashtag:before{content:"\23"}.fa-expand-alt:before,.fa-up-right-and-down-left-from-center:before{content:"\f424"}.fa-oil-can:before{content:"\f613"}.fa-t:before{content:"\54"}.fa-transformer-bolt:before{content:"\e2a4"}.fa-hippo:before{content:"\f6ed"}.fa-chart-column:before{content:"\e0e3"}.fa-cassette-vhs:before,.fa-vhs:before{content:"\f8ec"}.fa-infinity:before{content:"\f534"}.fa-vial-circle-check:before{content:"\e596"}.fa-chimney:before{content:"\f78b"}.fa-object-intersect:before{content:"\e49d"}.fa-person-arrow-down-to-line:before{content:"\e538"}.fa-voicemail:before{content:"\f897"}.fa-block-brick:before,.fa-wall-brick:before{content:"\e3db"}.fa-fan:before{content:"\f863"}.fa-bags-shopping:before{content:"\f847"}.fa-paragraph-left:before,.fa-paragraph-rtl:before{content:"\f878"}.fa-person-walking-luggage:before{content:"\e554"}.fa-caravan-alt:before,.fa-caravan-simple:before{content:"\e000"}.fa-turtle:before{content:"\f726"}.fa-pencil-mechanical:before{content:"\e5ca"}.fa-arrows-alt-v:before,.fa-up-down:before{content:"\f338"}.fa-cloud-moon-rain:before{content:"\f73c"}.fa-booth-curtain:before{content:"\f734"}.fa-calendar:before{content:"\f133"}.fa-box-heart:before{content:"\f49d"}.fa-trailer:before{content:"\e041"}.fa-user-doctor-message:before,.fa-user-md-chat:before{content:"\f82e"}.fa-bahai:before,.fa-haykal:before{content:"\f666"}.fa-lighthouse:before{content:"\e612"}.fa-amp-guitar:before{content:"\f8a1"}.fa-sd-card:before{content:"\f7c2"}.fa-volume-slash:before{content:"\f2e2"}.fa-border-bottom:before{content:"\f84d"}.fa-wifi-1:before,.fa-wifi-weak:before{content:"\f6aa"}.fa-dragon:before{content:"\f6d5"}.fa-shoe-prints:before{content:"\f54b"}.fa-circle-plus:before,.fa-plus-circle:before{content:"\f055"}.fa-face-grin-tongue-wink:before,.fa-grin-tongue-wink:before{content:"\f58b"}.fa-hand-holding:before{content:"\f4bd"}.fa-plug-circle-exclamation:before{content:"\e55d"}.fa-chain-broken:before,.fa-chain-slash:before,.fa-link-slash:before,.fa-unlink:before{content:"\f127"}.fa-clone:before{content:"\f24d"}.fa-person-walking-arrow-loop-left:before{content:"\e551"}.fa-arrow-up-z-a:before,.fa-sort-alpha-up-alt:before{content:"\f882"}.fa-fire-alt:before,.fa-fire-flame-curved:before{content:"\f7e4"}.fa-tornado:before{content:"\f76f"}.fa-file-circle-plus:before{content:"\e494"}.fa-delete-right:before{content:"\e154"}.fa-book-quran:before,.fa-quran:before{content:"\f687"}.fa-circle-quarter:before{content:"\e11f"}.fa-anchor:before{content:"\f13d"}.fa-border-all:before{content:"\f84c"}.fa-function:before{content:"\f661"}.fa-angry:before,.fa-face-angry:before{content:"\f556"}.fa-people-simple:before{content:"\e21b"}.fa-cookie-bite:before{content:"\f564"}.fa-arrow-trend-down:before{content:"\e097"}.fa-feed:before,.fa-rss:before{content:"\f09e"}.fa-face-monocle:before{content:"\e380"}.fa-draw-polygon:before{content:"\f5ee"}.fa-balance-scale:before,.fa-scale-balanced:before{content:"\f24e"}.fa-calendar-lines:before,.fa-calendar-note:before{content:"\e0d5"}.fa-arrow-down-big-small:before,.fa-sort-size-down:before{content:"\f88c"}.fa-gauge-simple-high:before,.fa-tachometer-fast:before,.fa-tachometer:before{content:"\f62a"}.fa-do-not-enter:before{content:"\f5ec"}.fa-shower:before{content:"\f2cc"}.fa-dice-d8:before{content:"\f6d2"}.fa-desktop-alt:before,.fa-desktop:before{content:"\f390"}.fa-m:before{content:"\4d"}.fa-spinner-scale:before{content:"\e62a"}.fa-grip-dots-vertical:before{content:"\e411"}.fa-face-viewfinder:before{content:"\e2ff"}.fa-creemee:before,.fa-soft-serve:before{content:"\e400"}.fa-h5:before{content:"\e412"}.fa-hand-back-point-down:before{content:"\e19e"}.fa-table-list:before,.fa-th-list:before{content:"\f00b"}.fa-basket-shopping-minus:before{content:"\e652"}.fa-comment-sms:before,.fa-sms:before{content:"\f7cd"}.fa-rectangle-landscape:before,.fa-rectangle:before{content:"\f2fa"}.fa-clipboard-list-check:before{content:"\f737"}.fa-turkey:before{content:"\f725"}.fa-book:before{content:"\f02d"}.fa-user-plus:before{content:"\f234"}.fa-ice-skate:before{content:"\f7ac"}.fa-check:before{content:"\f00c"}.fa-battery-4:before,.fa-battery-three-quarters:before{content:"\f241"}.fa-tomato:before{content:"\e330"}.fa-sword-laser:before{content:"\e03b"}.fa-house-circle-check:before{content:"\e509"}.fa-buildings:before{content:"\e0cc"}.fa-angle-left:before{content:"\f104"}.fa-cart-flatbed-boxes:before,.fa-dolly-flatbed-alt:before{content:"\f475"}.fa-diagram-successor:before{content:"\e47a"}.fa-truck-arrow-right:before{content:"\e58b"}.fa-square-w:before{content:"\e285"}.fa-arrows-split-up-and-left:before{content:"\e4bc"}.fa-lamp:before{content:"\f4ca"}.fa-airplay:before{content:"\e089"}.fa-fist-raised:before,.fa-hand-fist:before{content:"\f6de"}.fa-shield-quartered:before{content:"\e575"}.fa-slash-forward:before{content:"\2f"}.fa-location-pen:before,.fa-map-marker-edit:before{content:"\f607"}.fa-cloud-moon:before{content:"\f6c3"}.fa-pot-food:before{content:"\e43f"}.fa-briefcase:before{content:"\f0b1"}.fa-person-falling:before{content:"\e546"}.fa-image-portrait:before,.fa-portrait:before{content:"\f3e0"}.fa-user-tag:before{content:"\f507"}.fa-rug:before{content:"\e569"}.fa-print-slash:before{content:"\f686"}.fa-earth-europe:before,.fa-globe-europe:before{content:"\f7a2"}.fa-cart-flatbed-suitcase:before,.fa-luggage-cart:before{content:"\f59d"}.fa-hand-back-point-ribbon:before{content:"\e1a0"}.fa-rectangle-times:before,.fa-rectangle-xmark:before,.fa-times-rectangle:before,.fa-window-close:before{content:"\f410"}.fa-tire-rugged:before{content:"\f634"}.fa-lightbulb-dollar:before{content:"\f670"}.fa-cowbell:before{content:"\f8b3"}.fa-baht-sign:before{content:"\e0ac"}.fa-corner:before{content:"\e3fe"}.fa-chevron-double-right:before,.fa-chevrons-right:before{content:"\f324"}.fa-book-open:before{content:"\f518"}.fa-book-journal-whills:before,.fa-journal-whills:before{content:"\f66a"}.fa-inhaler:before{content:"\f5f9"}.fa-handcuffs:before{content:"\e4f8"}.fa-snake:before{content:"\f716"}.fa-exclamation-triangle:before,.fa-triangle-exclamation:before,.fa-warning:before{content:"\f071"}.fa-note-medical:before{content:"\e200"}.fa-database:before{content:"\f1c0"}.fa-down-left:before{content:"\e16a"}.fa-mail-forward:before,.fa-share:before{content:"\f064"}.fa-face-thinking:before{content:"\e39b"}.fa-turn-down-right:before{content:"\e455"}.fa-bottle-droplet:before{content:"\e4c4"}.fa-mask-face:before{content:"\e1d7"}.fa-hill-rockslide:before{content:"\e508"}.fa-scanner-keyboard:before{content:"\f489"}.fa-circle-o:before{content:"\e119"}.fa-grid-horizontal:before{content:"\e307"}.fa-comment-alt-dollar:before,.fa-message-dollar:before{content:"\f650"}.fa-exchange-alt:before,.fa-right-left:before{content:"\f362"}.fa-columns-3:before{content:"\e361"}.fa-paper-plane:before{content:"\f1d8"}.fa-road-circle-exclamation:before{content:"\e565"}.fa-dungeon:before{content:"\f6d9"}.fa-hand-holding-box:before{content:"\f47b"}.fa-input-text:before{content:"\e1bf"}.fa-window-alt:before,.fa-window-flip:before{content:"\f40f"}.fa-align-right:before{content:"\f038"}.fa-scanner-gun:before,.fa-scanner:before{content:"\f488"}.fa-tire:before{content:"\f631"}.fa-engine:before{content:"\e16e"}.fa-money-bill-1-wave:before,.fa-money-bill-wave-alt:before{content:"\f53b"}.fa-life-ring:before{content:"\f1cd"}.fa-hands:before,.fa-sign-language:before,.fa-signing:before{content:"\f2a7"}.fa-caret-circle-right:before,.fa-circle-caret-right:before{content:"\f330"}.fa-turn-left:before{content:"\e636"}.fa-wheat:before{content:"\f72d"}.fa-file-spreadsheet:before{content:"\f65b"}.fa-audio-description-slash:before{content:"\e0a8"}.fa-bell-ring:before{content:"\e62c"}.fa-calendar-day:before{content:"\f783"}.fa-ladder-water:before,.fa-swimming-pool:before,.fa-water-ladder:before{content:"\f5c5"}.fa-arrows-up-down:before,.fa-arrows-v:before{content:"\f07d"}.fa-chess-pawn-alt:before,.fa-chess-pawn-piece:before{content:"\f444"}.fa-face-grimace:before,.fa-grimace:before{content:"\f57f"}.fa-wheelchair-alt:before,.fa-wheelchair-move:before{content:"\e2ce"}.fa-level-down-alt:before,.fa-turn-down:before{content:"\f3be"}.fa-square-s:before{content:"\e27d"}.fa-barcode-alt:before,.fa-rectangle-barcode:before{content:"\f463"}.fa-person-walking-arrow-right:before{content:"\e552"}.fa-envelope-square:before,.fa-square-envelope:before{content:"\f199"}.fa-dice:before{content:"\f522"}.fa-unicorn:before{content:"\f727"}.fa-bowling-ball:before{content:"\f436"}.fa-pompebled:before{content:"\e43d"}.fa-brain:before{content:"\f5dc"}.fa-watch-smart:before{content:"\e2cc"}.fa-book-user:before{content:"\f7e7"}.fa-sensor-cloud:before,.fa-sensor-smoke:before{content:"\e02c"}.fa-clapperboard-play:before{content:"\e132"}.fa-band-aid:before,.fa-bandage:before{content:"\f462"}.fa-calendar-minus:before{content:"\f272"}.fa-circle-xmark:before,.fa-times-circle:before,.fa-xmark-circle:before{content:"\f057"}.fa-circle-4:before{content:"\e0f1"}.fa-gifts:before{content:"\f79c"}.fa-album-collection:before{content:"\f8a0"}.fa-hotel:before{content:"\f594"}.fa-earth-asia:before,.fa-globe-asia:before{content:"\f57e"}.fa-id-card-alt:before,.fa-id-card-clip:before{content:"\f47f"}.fa-magnifying-glass-plus:before,.fa-search-plus:before{content:"\f00e"}.fa-thumbs-up:before{content:"\f164"}.fa-cloud-showers:before{content:"\f73f"}.fa-user-clock:before{content:"\f4fd"}.fa-onion:before{content:"\e427"}.fa-clock-twelve-thirty:before{content:"\e359"}.fa-arrow-down-to-dotted-line:before{content:"\e095"}.fa-allergies:before,.fa-hand-dots:before{content:"\f461"}.fa-file-invoice:before{content:"\f570"}.fa-window-minimize:before{content:"\f2d1"}.fa-rectangle-wide:before{content:"\f2fc"}.fa-comment-arrow-up:before{content:"\e144"}.fa-garlic:before{content:"\e40e"}.fa-coffee:before,.fa-mug-saucer:before{content:"\f0f4"}.fa-brush:before{content:"\f55d"}.fa-tree-decorated:before{content:"\f7dc"}.fa-mask:before{content:"\f6fa"}.fa-calendar-heart:before{content:"\e0d3"}.fa-magnifying-glass-minus:before,.fa-search-minus:before{content:"\f010"}.fa-flower:before{content:"\f7ff"}.fa-arrow-down-from-arc:before{content:"\e614"}.fa-right-left-large:before{content:"\e5e1"}.fa-ruler-vertical:before{content:"\f548"}.fa-circles-overlap:before{content:"\e600"}.fa-user-alt:before,.fa-user-large:before{content:"\f406"}.fa-starship-freighter:before{content:"\e03a"}.fa-train-tram:before{content:"\e5b4"}.fa-bridge-suspension:before{content:"\e4cd"}.fa-trash-check:before{content:"\e2af"}.fa-user-nurse:before{content:"\f82f"}.fa-boombox:before{content:"\f8a5"}.fa-syringe:before{content:"\f48e"}.fa-cloud-sun:before{content:"\f6c4"}.fa-shield-exclamation:before{content:"\e247"}.fa-stopwatch-20:before{content:"\e06f"}.fa-square-full:before{content:"\f45c"}.fa-grip-dots:before{content:"\e410"}.fa-comment-exclamation:before{content:"\f4af"}.fa-pen-swirl:before{content:"\e214"}.fa-falafel:before{content:"\e40a"}.fa-circle-2:before{content:"\e0ef"}.fa-magnet:before{content:"\f076"}.fa-jar:before{content:"\e516"}.fa-gramophone:before{content:"\f8bd"}.fa-dice-d12:before{content:"\f6ce"}.fa-note-sticky:before,.fa-sticky-note:before{content:"\f249"}.fa-arrow-alt-down:before,.fa-down:before{content:"\f354"}.fa-100:before,.fa-hundred-points:before{content:"\e41c"}.fa-paperclip-vertical:before{content:"\e3c2"}.fa-wind-circle-exclamation:before,.fa-wind-warning:before{content:"\f776"}.fa-location-pin-slash:before,.fa-map-marker-slash:before{content:"\f60c"}.fa-face-sad-sweat:before{content:"\e38a"}.fa-bug-slash:before{content:"\e490"}.fa-cupcake:before{content:"\e402"}.fa-light-switch-off:before{content:"\e018"}.fa-toggle-large-off:before{content:"\e5b0"}.fa-pen-fancy-slash:before{content:"\e210"}.fa-truck-container:before{content:"\f4dc"}.fa-boot:before{content:"\f782"}.fa-arrow-up-from-water-pump:before{content:"\e4b6"}.fa-file-check:before{content:"\f316"}.fa-bone:before{content:"\f5d7"}.fa-cards-blank:before{content:"\e4df"}.fa-circle-3:before{content:"\e0f0"}.fa-bench-tree:before{content:"\e2e7"}.fa-keyboard-brightness-low:before{content:"\e1c1"}.fa-ski-boot-ski:before{content:"\e3cd"}.fa-brain-circuit:before{content:"\e0c6"}.fa-user-injured:before{content:"\f728"}.fa-block-brick-fire:before,.fa-firewall:before{content:"\e3dc"}.fa-face-sad-tear:before,.fa-sad-tear:before{content:"\f5b4"}.fa-plane:before{content:"\f072"}.fa-tent-arrows-down:before{content:"\e581"}.fa-exclamation:before{content:"\21"}.fa-arrows-spin:before{content:"\e4bb"}.fa-face-smile-relaxed:before{content:"\e392"}.fa-comment-times:before,.fa-comment-xmark:before{content:"\f4b5"}.fa-print:before{content:"\f02f"}.fa-try:before,.fa-turkish-lira-sign:before,.fa-turkish-lira:before{content:"\e2bb"}.fa-face-nose-steam:before{content:"\e382"}.fa-circle-waveform-lines:before,.fa-waveform-circle:before{content:"\e12d"}.fa-dollar-sign:before,.fa-dollar:before,.fa-usd:before{content:"\24"}.fa-ferris-wheel:before{content:"\e174"}.fa-computer-speaker:before{content:"\f8b2"}.fa-skull-cow:before{content:"\f8de"}.fa-x:before{content:"\58"}.fa-magnifying-glass-dollar:before,.fa-search-dollar:before{content:"\f688"}.fa-users-cog:before,.fa-users-gear:before{content:"\f509"}.fa-person-military-pointing:before{content:"\e54a"}.fa-bank:before,.fa-building-columns:before,.fa-institution:before,.fa-museum:before,.fa-university:before{content:"\f19c"}.fa-circle-t:before{content:"\e124"}.fa-sack:before{content:"\f81c"}.fa-grid-2:before{content:"\e196"}.fa-camera-cctv:before,.fa-cctv:before{content:"\f8ac"}.fa-umbrella:before{content:"\f0e9"}.fa-trowel:before{content:"\e589"}.fa-horizontal-rule:before{content:"\f86c"}.fa-bed-alt:before,.fa-bed-front:before{content:"\f8f7"}.fa-d:before{content:"\44"}.fa-stapler:before{content:"\e5af"}.fa-masks-theater:before,.fa-theater-masks:before{content:"\f630"}.fa-file-gif:before{content:"\e645"}.fa-kip-sign:before{content:"\e1c4"}.fa-face-woozy:before{content:"\e3a2"}.fa-cloud-question:before{content:"\e492"}.fa-pineapple:before{content:"\e31f"}.fa-hand-point-left:before{content:"\f0a5"}.fa-gallery-thumbnails:before{content:"\e3aa"}.fa-circle-j:before{content:"\e112"}.fa-eyes:before{content:"\e367"}.fa-handshake-alt:before,.fa-handshake-simple:before{content:"\f4c6"}.fa-file-caret-up:before,.fa-page-caret-up:before{content:"\e42a"}.fa-fighter-jet:before,.fa-jet-fighter:before{content:"\f0fb"}.fa-comet:before{content:"\e003"}.fa-share-alt-square:before,.fa-square-share-nodes:before{content:"\f1e1"}.fa-reflect-vertical:before{content:"\e665"}.fa-shield-keyhole:before{content:"\e248"}.fa-file-mp4:before{content:"\e649"}.fa-barcode:before{content:"\f02a"}.fa-bulldozer:before{content:"\e655"}.fa-plus-minus:before{content:"\e43c"}.fa-sliders-v-square:before,.fa-square-sliders-vertical:before{content:"\f3f2"}.fa-video-camera:before,.fa-video:before{content:"\f03d"}.fa-comment-middle-alt:before,.fa-message-middle:before{content:"\e1e1"}.fa-graduation-cap:before,.fa-mortar-board:before{content:"\f19d"}.fa-hand-holding-medical:before{content:"\e05c"}.fa-person-circle-check:before{content:"\e53e"}.fa-square-z:before{content:"\e288"}.fa-comment-alt-text:before,.fa-message-text:before{content:"\e1e6"}.fa-level-up-alt:before,.fa-turn-up:before{content:"\f3bf"}.fa-sr-only,.fa-sr-only-focusable:not(:focus),.sr-only,.sr-only-focusable:not(:focus){position:absolute;width:1px;height:1px;padding:0;margin:-1px;overflow:hidden;clip:rect(0,0,0,0);white-space:nowrap;border-width:0}:host,:root{--fa-style-family-brands:"Font Awesome 6 Brands";--fa-font-brands:normal 400 1em/1 "Font Awesome 6 Brands"}@font-face{font-family:"Font Awesome 6 Brands";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Brands";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.ttf) format("truetype");unicode-range:u+f1aa-f3e7,u+f425,u+f4e6}@font-face{font-family:"Font Awesome 6 Brands";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.ttf) format("truetype");unicode-range:u+e007-f15a,u+f167-f1a9,u+f1b4,u+f1bc,u+f1be,u+f1e8,u+f1ed,u+f1f0-f1f1,u+f20e-f210,u+f213-f214,u+f230,u+f232,u+f23a,u+f26b,u+f270,u+f288,u+f299-f29b,u+f2a6,u+f2b0,u+f2c5-f2c6,u+f2e0,u+f368,u+f379,u+f392-f393,u+f39f,u+f3a9,u+f3ab-f3ac,u+f3c0,u+f3c7,u+f3ca,u+f3e2,u+f3eb-f3ec,u+f3ef,u+f3f8,u+f3fe,u+f419,u+f41b,u+f4d5-f4e4,u+f4f8-f4f9,u+f514,u+f5b5,u+f6c9,u+f731,u+f77b,u+f7af,u+f7e1,u+f83b}.fa-brands,.fab{font-weight:400}.fa-monero:before{content:"\f3d0"}.fa-hooli:before{content:"\f427"}.fa-yelp:before{content:"\f1e9"}.fa-cc-visa:before{content:"\f1f0"}.fa-lastfm:before{content:"\f202"}.fa-shopware:before{content:"\f5b5"}.fa-creative-commons-nc:before{content:"\f4e8"}.fa-aws:before{content:"\f375"}.fa-redhat:before{content:"\f7bc"}.fa-yoast:before{content:"\f2b1"}.fa-cloudflare:before{content:"\e07d"}.fa-ups:before{content:"\f7e0"}.fa-pixiv:before{content:"\e640"}.fa-wpexplorer:before{content:"\f2de"}.fa-dyalog:before{content:"\f399"}.fa-bity:before{content:"\f37a"}.fa-stackpath:before{content:"\f842"}.fa-buysellads:before{content:"\f20d"}.fa-first-order:before{content:"\f2b0"}.fa-modx:before{content:"\f285"}.fa-guilded:before{content:"\e07e"}.fa-vnv:before{content:"\f40b"}.fa-js-square:before,.fa-square-js:before{content:"\f3b9"}.fa-microsoft:before{content:"\f3ca"}.fa-qq:before{content:"\f1d6"}.fa-orcid:before{content:"\f8d2"}.fa-java:before{content:"\f4e4"}.fa-invision:before{content:"\f7b0"}.fa-creative-commons-pd-alt:before{content:"\f4ed"}.fa-centercode:before{content:"\f380"}.fa-glide-g:before{content:"\f2a6"}.fa-drupal:before{content:"\f1a9"}.fa-jxl:before{content:"\e67b"}.fa-hire-a-helper:before{content:"\f3b0"}.fa-creative-commons-by:before{content:"\f4e7"}.fa-unity:before{content:"\e049"}.fa-whmcs:before{content:"\f40d"}.fa-rocketchat:before{content:"\f3e8"}.fa-vk:before{content:"\f189"}.fa-untappd:before{content:"\f405"}.fa-mailchimp:before{content:"\f59e"}.fa-css3-alt:before{content:"\f38b"}.fa-reddit-square:before,.fa-square-reddit:before{content:"\f1a2"}.fa-vimeo-v:before{content:"\f27d"}.fa-contao:before{content:"\f26d"}.fa-square-font-awesome:before{content:"\e5ad"}.fa-deskpro:before{content:"\f38f"}.fa-brave:before{content:"\e63c"}.fa-sistrix:before{content:"\f3ee"}.fa-instagram-square:before,.fa-square-instagram:before{content:"\e055"}.fa-battle-net:before{content:"\f835"}.fa-the-red-yeti:before{content:"\f69d"}.fa-hacker-news-square:before,.fa-square-hacker-news:before{content:"\f3af"}.fa-edge:before{content:"\f282"}.fa-threads:before{content:"\e618"}.fa-napster:before{content:"\f3d2"}.fa-snapchat-square:before,.fa-square-snapchat:before{content:"\f2ad"}.fa-google-plus-g:before{content:"\f0d5"}.fa-artstation:before{content:"\f77a"}.fa-markdown:before{content:"\f60f"}.fa-sourcetree:before{content:"\f7d3"}.fa-google-plus:before{content:"\f2b3"}.fa-diaspora:before{content:"\f791"}.fa-foursquare:before{content:"\f180"}.fa-stack-overflow:before{content:"\f16c"}.fa-github-alt:before{content:"\f113"}.fa-phoenix-squadron:before{content:"\f511"}.fa-pagelines:before{content:"\f18c"}.fa-algolia:before{content:"\f36c"}.fa-red-river:before{content:"\f3e3"}.fa-creative-commons-sa:before{content:"\f4ef"}.fa-safari:before{content:"\f267"}.fa-google:before{content:"\f1a0"}.fa-font-awesome-alt:before,.fa-square-font-awesome-stroke:before{content:"\f35c"}.fa-atlassian:before{content:"\f77b"}.fa-linkedin-in:before{content:"\f0e1"}.fa-digital-ocean:before{content:"\f391"}.fa-nimblr:before{content:"\f5a8"}.fa-chromecast:before{content:"\f838"}.fa-evernote:before{content:"\f839"}.fa-hacker-news:before{content:"\f1d4"}.fa-creative-commons-sampling:before{content:"\f4f0"}.fa-adversal:before{content:"\f36a"}.fa-creative-commons:before{content:"\f25e"}.fa-watchman-monitoring:before{content:"\e087"}.fa-fonticons:before{content:"\f280"}.fa-weixin:before{content:"\f1d7"}.fa-shirtsinbulk:before{content:"\f214"}.fa-codepen:before{content:"\f1cb"}.fa-git-alt:before{content:"\f841"}.fa-lyft:before{content:"\f3c3"}.fa-rev:before{content:"\f5b2"}.fa-windows:before{content:"\f17a"}.fa-wizards-of-the-coast:before{content:"\f730"}.fa-square-viadeo:before,.fa-viadeo-square:before{content:"\f2aa"}.fa-meetup:before{content:"\f2e0"}.fa-centos:before{content:"\f789"}.fa-adn:before{content:"\f170"}.fa-cloudsmith:before{content:"\f384"}.fa-opensuse:before{content:"\e62b"}.fa-pied-piper-alt:before{content:"\f1a8"}.fa-dribbble-square:before,.fa-square-dribbble:before{content:"\f397"}.fa-codiepie:before{content:"\f284"}.fa-node:before{content:"\f419"}.fa-mix:before{content:"\f3cb"}.fa-steam:before{content:"\f1b6"}.fa-cc-apple-pay:before{content:"\f416"}.fa-scribd:before{content:"\f28a"}.fa-debian:before{content:"\e60b"}.fa-openid:before{content:"\f19b"}.fa-instalod:before{content:"\e081"}.fa-expeditedssl:before{content:"\f23e"}.fa-sellcast:before{content:"\f2da"}.fa-square-twitter:before,.fa-twitter-square:before{content:"\f081"}.fa-r-project:before{content:"\f4f7"}.fa-delicious:before{content:"\f1a5"}.fa-freebsd:before{content:"\f3a4"}.fa-vuejs:before{content:"\f41f"}.fa-accusoft:before{content:"\f369"}.fa-ioxhost:before{content:"\f208"}.fa-fonticons-fi:before{content:"\f3a2"}.fa-app-store:before{content:"\f36f"}.fa-cc-mastercard:before{content:"\f1f1"}.fa-itunes-note:before{content:"\f3b5"}.fa-golang:before{content:"\e40f"}.fa-kickstarter:before,.fa-square-kickstarter:before{content:"\f3bb"}.fa-grav:before{content:"\f2d6"}.fa-weibo:before{content:"\f18a"}.fa-uncharted:before{content:"\e084"}.fa-firstdraft:before{content:"\f3a1"}.fa-square-youtube:before,.fa-youtube-square:before{content:"\f431"}.fa-wikipedia-w:before{content:"\f266"}.fa-rendact:before,.fa-wpressr:before{content:"\f3e4"}.fa-angellist:before{content:"\f209"}.fa-galactic-republic:before{content:"\f50c"}.fa-nfc-directional:before{content:"\e530"}.fa-skype:before{content:"\f17e"}.fa-joget:before{content:"\f3b7"}.fa-fedora:before{content:"\f798"}.fa-stripe-s:before{content:"\f42a"}.fa-meta:before{content:"\e49b"}.fa-laravel:before{content:"\f3bd"}.fa-hotjar:before{content:"\f3b1"}.fa-bluetooth-b:before{content:"\f294"}.fa-square-letterboxd:before{content:"\e62e"}.fa-sticker-mule:before{content:"\f3f7"}.fa-creative-commons-zero:before{content:"\f4f3"}.fa-hips:before{content:"\f452"}.fa-behance:before{content:"\f1b4"}.fa-reddit:before{content:"\f1a1"}.fa-discord:before{content:"\f392"}.fa-chrome:before{content:"\f268"}.fa-app-store-ios:before{content:"\f370"}.fa-cc-discover:before{content:"\f1f2"}.fa-wpbeginner:before{content:"\f297"}.fa-confluence:before{content:"\f78d"}.fa-shoelace:before{content:"\e60c"}.fa-mdb:before{content:"\f8ca"}.fa-dochub:before{content:"\f394"}.fa-accessible-icon:before{content:"\f368"}.fa-ebay:before{content:"\f4f4"}.fa-amazon:before{content:"\f270"}.fa-unsplash:before{content:"\e07c"}.fa-yarn:before{content:"\f7e3"}.fa-square-steam:before,.fa-steam-square:before{content:"\f1b7"}.fa-500px:before{content:"\f26e"}.fa-square-vimeo:before,.fa-vimeo-square:before{content:"\f194"}.fa-asymmetrik:before{content:"\f372"}.fa-font-awesome-flag:before,.fa-font-awesome-logo-full:before,.fa-font-awesome:before{content:"\f2b4"}.fa-gratipay:before{content:"\f184"}.fa-apple:before{content:"\f179"}.fa-hive:before{content:"\e07f"}.fa-gitkraken:before{content:"\f3a6"}.fa-keybase:before{content:"\f4f5"}.fa-apple-pay:before{content:"\f415"}.fa-padlet:before{content:"\e4a0"}.fa-amazon-pay:before{content:"\f42c"}.fa-github-square:before,.fa-square-github:before{content:"\f092"}.fa-stumbleupon:before{content:"\f1a4"}.fa-fedex:before{content:"\f797"}.fa-phoenix-framework:before{content:"\f3dc"}.fa-shopify:before{content:"\e057"}.fa-neos:before{content:"\f612"}.fa-square-threads:before{content:"\e619"}.fa-hackerrank:before{content:"\f5f7"}.fa-researchgate:before{content:"\f4f8"}.fa-swift:before{content:"\f8e1"}.fa-angular:before{content:"\f420"}.fa-speakap:before{content:"\f3f3"}.fa-angrycreative:before{content:"\f36e"}.fa-y-combinator:before{content:"\f23b"}.fa-empire:before{content:"\f1d1"}.fa-envira:before{content:"\f299"}.fa-google-scholar:before{content:"\e63b"}.fa-gitlab-square:before,.fa-square-gitlab:before{content:"\e5ae"}.fa-studiovinari:before{content:"\f3f8"}.fa-pied-piper:before{content:"\f2ae"}.fa-wordpress:before{content:"\f19a"}.fa-product-hunt:before{content:"\f288"}.fa-firefox:before{content:"\f269"}.fa-linode:before{content:"\f2b8"}.fa-goodreads:before{content:"\f3a8"}.fa-odnoklassniki-square:before,.fa-square-odnoklassniki:before{content:"\f264"}.fa-jsfiddle:before{content:"\f1cc"}.fa-sith:before{content:"\f512"}.fa-themeisle:before{content:"\f2b2"}.fa-page4:before{content:"\f3d7"}.fa-hashnode:before{content:"\e499"}.fa-react:before{content:"\f41b"}.fa-cc-paypal:before{content:"\f1f4"}.fa-squarespace:before{content:"\f5be"}.fa-cc-stripe:before{content:"\f1f5"}.fa-creative-commons-share:before{content:"\f4f2"}.fa-bitcoin:before{content:"\f379"}.fa-keycdn:before{content:"\f3ba"}.fa-opera:before{content:"\f26a"}.fa-itch-io:before{content:"\f83a"}.fa-umbraco:before{content:"\f8e8"}.fa-galactic-senate:before{content:"\f50d"}.fa-ubuntu:before{content:"\f7df"}.fa-draft2digital:before{content:"\f396"}.fa-stripe:before{content:"\f429"}.fa-houzz:before{content:"\f27c"}.fa-gg:before{content:"\f260"}.fa-dhl:before{content:"\f790"}.fa-pinterest-square:before,.fa-square-pinterest:before{content:"\f0d3"}.fa-xing:before{content:"\f168"}.fa-blackberry:before{content:"\f37b"}.fa-creative-commons-pd:before{content:"\f4ec"}.fa-playstation:before{content:"\f3df"}.fa-quinscape:before{content:"\f459"}.fa-less:before{content:"\f41d"}.fa-blogger-b:before{content:"\f37d"}.fa-opencart:before{content:"\f23d"}.fa-vine:before{content:"\f1ca"}.fa-signal-messenger:before{content:"\e663"}.fa-paypal:before{content:"\f1ed"}.fa-gitlab:before{content:"\f296"}.fa-typo3:before{content:"\f42b"}.fa-reddit-alien:before{content:"\f281"}.fa-yahoo:before{content:"\f19e"}.fa-dailymotion:before{content:"\e052"}.fa-affiliatetheme:before{content:"\f36b"}.fa-pied-piper-pp:before{content:"\f1a7"}.fa-bootstrap:before{content:"\f836"}.fa-odnoklassniki:before{content:"\f263"}.fa-nfc-symbol:before{content:"\e531"}.fa-mintbit:before{content:"\e62f"}.fa-ethereum:before{content:"\f42e"}.fa-speaker-deck:before{content:"\f83c"}.fa-creative-commons-nc-eu:before{content:"\f4e9"}.fa-patreon:before{content:"\f3d9"}.fa-avianex:before{content:"\f374"}.fa-ello:before{content:"\f5f1"}.fa-gofore:before{content:"\f3a7"}.fa-bimobject:before{content:"\f378"}.fa-brave-reverse:before{content:"\e63d"}.fa-facebook-f:before{content:"\f39e"}.fa-google-plus-square:before,.fa-square-google-plus:before{content:"\f0d4"}.fa-web-awesome:before{content:"\e682"}.fa-mandalorian:before{content:"\f50f"}.fa-first-order-alt:before{content:"\f50a"}.fa-osi:before{content:"\f41a"}.fa-google-wallet:before{content:"\f1ee"}.fa-d-and-d-beyond:before{content:"\f6ca"}.fa-periscope:before{content:"\f3da"}.fa-fulcrum:before{content:"\f50b"}.fa-cloudscale:before{content:"\f383"}.fa-forumbee:before{content:"\f211"}.fa-mizuni:before{content:"\f3cc"}.fa-schlix:before{content:"\f3ea"}.fa-square-xing:before,.fa-xing-square:before{content:"\f169"}.fa-bandcamp:before{content:"\f2d5"}.fa-wpforms:before{content:"\f298"}.fa-cloudversify:before{content:"\f385"}.fa-usps:before{content:"\f7e1"}.fa-megaport:before{content:"\f5a3"}.fa-magento:before{content:"\f3c4"}.fa-spotify:before{content:"\f1bc"}.fa-optin-monster:before{content:"\f23c"}.fa-fly:before{content:"\f417"}.fa-aviato:before{content:"\f421"}.fa-itunes:before{content:"\f3b4"}.fa-cuttlefish:before{content:"\f38c"}.fa-blogger:before{content:"\f37c"}.fa-flickr:before{content:"\f16e"}.fa-viber:before{content:"\f409"}.fa-soundcloud:before{content:"\f1be"}.fa-digg:before{content:"\f1a6"}.fa-tencent-weibo:before{content:"\f1d5"}.fa-letterboxd:before{content:"\e62d"}.fa-symfony:before{content:"\f83d"}.fa-maxcdn:before{content:"\f136"}.fa-etsy:before{content:"\f2d7"}.fa-facebook-messenger:before{content:"\f39f"}.fa-audible:before{content:"\f373"}.fa-think-peaks:before{content:"\f731"}.fa-bilibili:before{content:"\e3d9"}.fa-erlang:before{content:"\f39d"}.fa-x-twitter:before{content:"\e61b"}.fa-cotton-bureau:before{content:"\f89e"}.fa-dashcube:before{content:"\f210"}.fa-42-group:before,.fa-innosoft:before{content:"\e080"}.fa-stack-exchange:before{content:"\f18d"}.fa-elementor:before{content:"\f430"}.fa-pied-piper-square:before,.fa-square-pied-piper:before{content:"\e01e"}.fa-creative-commons-nd:before{content:"\f4eb"}.fa-palfed:before{content:"\f3d8"}.fa-superpowers:before{content:"\f2dd"}.fa-resolving:before{content:"\f3e7"}.fa-xbox:before{content:"\f412"}.fa-square-web-awesome-stroke:before{content:"\e684"}.fa-searchengin:before{content:"\f3eb"}.fa-tiktok:before{content:"\e07b"}.fa-facebook-square:before,.fa-square-facebook:before{content:"\f082"}.fa-renren:before{content:"\f18b"}.fa-linux:before{content:"\f17c"}.fa-glide:before{content:"\f2a5"}.fa-linkedin:before{content:"\f08c"}.fa-hubspot:before{content:"\f3b2"}.fa-deploydog:before{content:"\f38e"}.fa-twitch:before{content:"\f1e8"}.fa-ravelry:before{content:"\f2d9"}.fa-mixer:before{content:"\e056"}.fa-lastfm-square:before,.fa-square-lastfm:before{content:"\f203"}.fa-vimeo:before{content:"\f40a"}.fa-mendeley:before{content:"\f7b3"}.fa-uniregistry:before{content:"\f404"}.fa-figma:before{content:"\f799"}.fa-creative-commons-remix:before{content:"\f4ee"}.fa-cc-amazon-pay:before{content:"\f42d"}.fa-dropbox:before{content:"\f16b"}.fa-instagram:before{content:"\f16d"}.fa-cmplid:before{content:"\e360"}.fa-upwork:before{content:"\e641"}.fa-facebook:before{content:"\f09a"}.fa-gripfire:before{content:"\f3ac"}.fa-jedi-order:before{content:"\f50e"}.fa-uikit:before{content:"\f403"}.fa-fort-awesome-alt:before{content:"\f3a3"}.fa-phabricator:before{content:"\f3db"}.fa-ussunnah:before{content:"\f407"}.fa-earlybirds:before{content:"\f39a"}.fa-trade-federation:before{content:"\f513"}.fa-autoprefixer:before{content:"\f41c"}.fa-whatsapp:before{content:"\f232"}.fa-square-upwork:before{content:"\e67c"}.fa-slideshare:before{content:"\f1e7"}.fa-google-play:before{content:"\f3ab"}.fa-viadeo:before{content:"\f2a9"}.fa-line:before{content:"\f3c0"}.fa-google-drive:before{content:"\f3aa"}.fa-servicestack:before{content:"\f3ec"}.fa-simplybuilt:before{content:"\f215"}.fa-bitbucket:before{content:"\f171"}.fa-imdb:before{content:"\f2d8"}.fa-deezer:before{content:"\e077"}.fa-raspberry-pi:before{content:"\f7bb"}.fa-jira:before{content:"\f7b1"}.fa-docker:before{content:"\f395"}.fa-screenpal:before{content:"\e570"}.fa-bluetooth:before{content:"\f293"}.fa-gitter:before{content:"\f426"}.fa-d-and-d:before{content:"\f38d"}.fa-microblog:before{content:"\e01a"}.fa-cc-diners-club:before{content:"\f24c"}.fa-gg-circle:before{content:"\f261"}.fa-pied-piper-hat:before{content:"\f4e5"}.fa-kickstarter-k:before{content:"\f3bc"}.fa-yandex:before{content:"\f413"}.fa-readme:before{content:"\f4d5"}.fa-html5:before{content:"\f13b"}.fa-sellsy:before{content:"\f213"}.fa-square-web-awesome:before{content:"\e683"}.fa-sass:before{content:"\f41e"}.fa-wirsindhandwerk:before,.fa-wsh:before{content:"\e2d0"}.fa-buromobelexperte:before{content:"\f37f"}.fa-salesforce:before{content:"\f83b"}.fa-octopus-deploy:before{content:"\e082"}.fa-medapps:before{content:"\f3c6"}.fa-ns8:before{content:"\f3d5"}.fa-pinterest-p:before{content:"\f231"}.fa-apper:before{content:"\f371"}.fa-fort-awesome:before{content:"\f286"}.fa-waze:before{content:"\f83f"}.fa-bluesky:before{content:"\e671"}.fa-cc-jcb:before{content:"\f24b"}.fa-snapchat-ghost:before,.fa-snapchat:before{content:"\f2ab"}.fa-fantasy-flight-games:before{content:"\f6dc"}.fa-rust:before{content:"\e07a"}.fa-wix:before{content:"\f5cf"}.fa-behance-square:before,.fa-square-behance:before{content:"\f1b5"}.fa-supple:before{content:"\f3f9"}.fa-webflow:before{content:"\e65c"}.fa-rebel:before{content:"\f1d0"}.fa-css3:before{content:"\f13c"}.fa-staylinked:before{content:"\f3f5"}.fa-kaggle:before{content:"\f5fa"}.fa-space-awesome:before{content:"\e5ac"}.fa-deviantart:before{content:"\f1bd"}.fa-cpanel:before{content:"\f388"}.fa-goodreads-g:before{content:"\f3a9"}.fa-git-square:before,.fa-square-git:before{content:"\f1d2"}.fa-square-tumblr:before,.fa-tumblr-square:before{content:"\f174"}.fa-trello:before{content:"\f181"}.fa-creative-commons-nc-jp:before{content:"\f4ea"}.fa-get-pocket:before{content:"\f265"}.fa-perbyte:before{content:"\e083"}.fa-grunt:before{content:"\f3ad"}.fa-weebly:before{content:"\f5cc"}.fa-connectdevelop:before{content:"\f20e"}.fa-leanpub:before{content:"\f212"}.fa-black-tie:before{content:"\f27e"}.fa-themeco:before{content:"\f5c6"}.fa-python:before{content:"\f3e2"}.fa-android:before{content:"\f17b"}.fa-bots:before{content:"\e340"}.fa-free-code-camp:before{content:"\f2c5"}.fa-hornbill:before{content:"\f592"}.fa-js:before{content:"\f3b8"}.fa-ideal:before{content:"\e013"}.fa-git:before{content:"\f1d3"}.fa-dev:before{content:"\f6cc"}.fa-sketch:before{content:"\f7c6"}.fa-yandex-international:before{content:"\f414"}.fa-cc-amex:before{content:"\f1f3"}.fa-uber:before{content:"\f402"}.fa-github:before{content:"\f09b"}.fa-php:before{content:"\f457"}.fa-alipay:before{content:"\f642"}.fa-youtube:before{content:"\f167"}.fa-skyatlas:before{content:"\f216"}.fa-firefox-browser:before{content:"\e007"}.fa-replyd:before{content:"\f3e6"}.fa-suse:before{content:"\f7d6"}.fa-jenkins:before{content:"\f3b6"}.fa-twitter:before{content:"\f099"}.fa-rockrms:before{content:"\f3e9"}.fa-pinterest:before{content:"\f0d2"}.fa-buffer:before{content:"\f837"}.fa-npm:before{content:"\f3d4"}.fa-yammer:before{content:"\f840"}.fa-btc:before{content:"\f15a"}.fa-dribbble:before{content:"\f17d"}.fa-stumbleupon-circle:before{content:"\f1a3"}.fa-internet-explorer:before{content:"\f26b"}.fa-stubber:before{content:"\e5c7"}.fa-telegram-plane:before,.fa-telegram:before{content:"\f2c6"}.fa-old-republic:before{content:"\f510"}.fa-odysee:before{content:"\e5c6"}.fa-square-whatsapp:before,.fa-whatsapp-square:before{content:"\f40c"}.fa-node-js:before{content:"\f3d3"}.fa-edge-legacy:before{content:"\e078"}.fa-slack-hash:before,.fa-slack:before{content:"\f198"}.fa-medrt:before{content:"\f3c8"}.fa-usb:before{content:"\f287"}.fa-tumblr:before{content:"\f173"}.fa-vaadin:before{content:"\f408"}.fa-quora:before{content:"\f2c4"}.fa-square-x-twitter:before{content:"\e61a"}.fa-reacteurope:before{content:"\f75d"}.fa-medium-m:before,.fa-medium:before{content:"\f23a"}.fa-amilia:before{content:"\f36d"}.fa-mixcloud:before{content:"\f289"}.fa-flipboard:before{content:"\f44d"}.fa-viacoin:before{content:"\f237"}.fa-critical-role:before{content:"\f6c9"}.fa-sitrox:before{content:"\e44a"}.fa-discourse:before{content:"\f393"}.fa-joomla:before{content:"\f1aa"}.fa-mastodon:before{content:"\f4f6"}.fa-airbnb:before{content:"\f834"}.fa-wolf-pack-battalion:before{content:"\f514"}.fa-buy-n-large:before{content:"\f8a6"}.fa-gulp:before{content:"\f3ae"}.fa-creative-commons-sampling-plus:before{content:"\f4f1"}.fa-strava:before{content:"\f428"}.fa-ember:before{content:"\f423"}.fa-canadian-maple-leaf:before{content:"\f785"}.fa-teamspeak:before{content:"\f4f9"}.fa-pushed:before{content:"\f3e1"}.fa-wordpress-simple:before{content:"\f411"}.fa-nutritionix:before{content:"\f3d6"}.fa-wodu:before{content:"\e088"}.fa-google-pay:before{content:"\e079"}.fa-intercom:before{content:"\f7af"}.fa-zhihu:before{content:"\f63f"}.fa-korvue:before{content:"\f42f"}.fa-pix:before{content:"\e43a"}.fa-steam-symbol:before{content:"\f3f6"}:host,:root{--fa-style-family-duotone:"Font Awesome 6 Duotone";--fa-font-duotone:normal 900 1em/1 "Font Awesome 6 Duotone"}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-28.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-28.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-27.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-27.ttf) format("truetype");unicode-range:u+f823-f89a,u+10f823-10f89a}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-26.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-26.ttf) format("truetype");unicode-range:u+f7b4-f822,u+10f7b4-10f822}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-25.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-25.ttf) format("truetype");unicode-range:u+f73f-f7ae,u+10f73f-10f7ae}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-24.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-24.ttf) format("truetype");unicode-range:u+f6d2-f73e,u+10f6d2-10f73e}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-23.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-23.ttf) format("truetype");unicode-range:u+f669-f6d1,u+10f669-10f6d1}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-22.ttf) format("truetype");unicode-range:u+f0e4-f668,u+10f0e4-10f668}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-21.ttf) format("truetype");unicode-range:u+f589-f602,u+10f589-10f602}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-20.ttf) format("truetype");unicode-range:u+f506-f588,u+10f506-10f588}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-19.ttf) format("truetype");unicode-range:u+f486-f505,u+10f486-10f505}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-18.ttf) format("truetype");unicode-range:u+e207,u+f3f1-f3fc,u+f3ff-f485,u+f4a1,u+10e207,u+10f3f1-10f3fc,u+10f3ff-10f485,u+10f4a1}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-17.ttf) format("truetype");unicode-range:u+f319-f3f0,u+10f319-10f3f0}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-16.ttf) format("truetype");unicode-range:u+f259-f318,u+f4e6,u+f8e5,u+10f259-10f318,u+10f4e6,u+10f8e5}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-15.ttf) format("truetype");unicode-range:u+f158-f258,u+10f158-10f258}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-14.ttf) format("truetype");unicode-range:u+f094-f0e3,u+f0e9-f157,u+f381-f382,u+10f094-10f0e3,u+10f0e9-10f157,u+10f381-10f382}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-13.ttf) format("truetype");unicode-range:u+e647-f090,u+10f000-10f090}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-12.ttf) format("truetype");unicode-range:u+e572-e646,u+f8bc,u+10f8bc}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-11.ttf) format("truetype");unicode-range:u+e4b8-e571}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-10.ttf) format("truetype");unicode-range:u+e3fe-e4b7}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-9.ttf) format("truetype");unicode-range:u+e34a-e3fd,u+f80b,u+10f80b}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-8.ttf) format("truetype");unicode-range:u+e265-e349}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-7.ttf) format("truetype");unicode-range:u+e1ab-e206,u+e208-e264}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-6.ttf) format("truetype");unicode-range:u+e0f3-e1aa}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-5.ttf) format("truetype");unicode-range:u+e048-e0f2,u+10e048-10e086}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-4.ttf) format("truetype");unicode-range:u+2c-e047,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+10e000-10e047,u+10f1fa,u+10f52c,u+10f531,u+10f536,u+10f69f}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-3.ttf) format("truetype");unicode-range:u+22-2a,u+102a,u+f069,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+10f069,u+10f5fd,u+10f621,u+10f63b,u+10f649-10f64a,u+10f64f,u+10f653-10f654,u+10f656,u+10f65b,u+10f664,u+10f673,u+10f675,u+10f67d,u+10f67f,u+10f695,u+10f69c,u+10f6a8,u+10f6bf-10f6c0,u+10f6d5,u+10f6e3,u+10f6e9,u+10f6f5,u+10f6fa,u+10f6ff-10f700,u+10f70b,u+10f70e,u+10f715,u+10f71b,u+10f72e-10f72f,u+10f733-10f734,u+10f747,u+10f755,u+10f757,u+10f75c,u+10f762,u+10f773,u+10f77c,u+10f781,u+10f784,u+10f788,u+10f7b2,u+10f7b6,u+10f7bd,u+10f7d5,u+10f7ee,u+10f7ff,u+10f801,u+10f804,u+10f813-10f814,u+10f82f-10f830,u+10f845-10f846,u+10f850,u+10f855,u+10f858-10f859,u+10f85c,u+10f866,u+10f86d,u+10f871,u+10f875,u+10f893-10f894,u+10f897,u+10f89f,u+10f8a9,u+10f8b1-10f8b2,u+10f8bb,u+10f8c7,u+10f8d6-10f8d7,u+10f8d9,u+10f8df-10f8e0,u+10f8e7,u+10f8ee-10f8ef,u+10f8fd}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-2.ttf) format("truetype");unicode-range:u+f040,u+f108,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+10f040,u+10f108,u+10f2a0,u+10f2a7,u+10f2b5,u+10f2bb,u+10f2cc-10f2cd,u+10f2d2,u+10f2db,u+10f2e1,u+10f2ec,u+10f2f7,u+10f2fc,u+10f302-10f303,u+10f316,u+10f31a,u+10f328,u+10f335,u+10f363,u+10f37e,u+10f390,u+10f3c5,u+10f3ce,u+10f3e5,u+10f3f4,u+10f3fb,u+10f40e,u+10f435,u+10f44b,u+10f481,u+10f48a,u+10f48f-10f490,u+10f493-10f494,u+10f498,u+10f4b7,u+10f4ca,u+10f4cc,u+10f4d1,u+10f4d7-10f4d8,u+10f4e2,u+10f503,u+10f508,u+10f51b,u+10f51d-10f51e,u+10f521-10f522,u+10f52b,u+10f530,u+10f535,u+10f53e,u+10f543-10f545,u+10f548-10f549,u+10f54e,u+10f555,u+10f559,u+10f55d,u+10f564,u+10f56c,u+10f56e-10f570,u+10f577-10f578,u+10f590,u+10f594-10f595,u+10f5a1-10f5a2,u+10f5aa-10f5ab,u+10f5b0,u+10f5b7,u+10f5ba,u+10f5bf,u+10f5ca,u+10f5db-10f5dc,u+10f5ef,u+10f5f2,u+10f5f6,u+10f5fb}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-1.ttf) format("truetype");unicode-range:u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8-f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f109,u+f10e-f111,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d8,u+f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f47d,u+10f0a6,u+10f0ac-10f0ad,u+10f0b0-10f0b1,u+10f0c0-10f0c2,u+10f0c5-10f0c6,u+10f0c8-10f0c9,u+10f0cc,u+10f0ce,u+10f0d1-10f0d7,u+10f0dc,u+10f0e0,u+10f0e7-10f0e8,u+10f0eb,u+10f0f3,u+10f0f8,u+10f106,u+10f109,u+10f10e-10f111,u+10f11c-10f11e,u+10f121,u+10f126,u+10f129,u+10f12c-10f133,u+10f135,u+10f13d,u+10f140,u+10f145,u+10f14e,u+10f15b,u+10f164,u+10f186,u+10f188,u+10f1ab,u+10f1ad-10f1ae,u+10f1b2,u+10f1b8,u+10f1bb-10f1c3,u+10f1ce-10f1d8,u+10f1dc,u+10f1e4-10f1ec,u+10f1f8-10f1f9,u+10f205,u+10f20a,u+10f217,u+10f219-10f21d,u+10f22d-10f234,u+10f238,u+10f246,u+10f24d,u+10f251,u+10f25d,u+10f275,u+10f29e,u+10f47d}@font-face{font-family:"Font Awesome 6 Duotone";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+1021-1025,u+102b,u+103f,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015-f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091-f093,u+f095,u+f09c-f09d,u+f0a3,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c,u+10e010,u+10e017,u+10e01b,u+10e01f-10e021,u+10e024,u+10e02f,u+10e03a,u+10e042,u+10e045-10e046,u+10e060,u+10e068,u+10e06e,u+10e074,u+10e076,u+10f001,u+10f004-10f008,u+10f00c,u+10f011-10f012,u+10f015-10f01c,u+10f023-10f025,u+10f02a,u+10f02c-10f031,u+10f03a,u+10f03d-10f03e,u+10f041,u+10f04a-10f04e,u+10f05b,u+10f060-10f065,u+10f067-10f068,u+10f06b-10f06e,u+10f072,u+10f075,u+10f077-10f078,u+10f07b,u+10f084,u+10f086,u+10f091-10f093,u+10f095,u+10f09c-10f09d,u+10f0a3,u+10f128,u+10f12a,u+10f155,u+10f292,u+10f295,u+10f332,u+10f541,u+10f80a,u+10f80c}.fa-duotone,.fad{position:relative;font-weight:900;letter-spacing:normal}.fa-duotone:before,.fad:before{position:absolute;color:var(--fa-primary-color,inherit);opacity:var(--fa-primary-opacity,1)}.fa-duotone:after,.fad:after{color:var(--fa-secondary-color,inherit)}.fa-duotone.fa-swap-opacity:before,.fa-duotone:after,.fa-swap-opacity .fa-duotone:before,.fa-swap-opacity .fad:before,.fad.fa-swap-opacity:before,.fad:after{opacity:var(--fa-secondary-opacity,.4)}.fa-duotone.fa-swap-opacity:after,.fa-swap-opacity .fa-duotone:after,.fa-swap-opacity .fad:after,.fad.fa-swap-opacity:after{opacity:var(--fa-primary-opacity,1)}.fa-duotone.fa-inverse,.fad.fa-inverse{color:var(--fa-inverse,#fff)}.fa-duotone.fa-stack-1x,.fa-duotone.fa-stack-2x,.fad.fa-stack-1x,.fad.fa-stack-2x{position:absolute}.fa-duotone.fa-0:after,.fad.fa-0:after{content:"\30\30"}.fa-duotone.fa-1:after,.fad.fa-1:after{content:"\31\31"}.fa-duotone.fa-2:after,.fad.fa-2:after{content:"\32\32"}.fa-duotone.fa-3:after,.fad.fa-3:after{content:"\33\33"}.fa-duotone.fa-4:after,.fad.fa-4:after{content:"\34\34"}.fa-duotone.fa-5:after,.fad.fa-5:after{content:"\35\35"}.fa-duotone.fa-6:after,.fad.fa-6:after{content:"\36\36"}.fa-duotone.fa-7:after,.fad.fa-7:after{content:"\37\37"}.fa-duotone.fa-8:after,.fad.fa-8:after{content:"\38\38"}.fa-duotone.fa-9:after,.fad.fa-9:after{content:"\39\39"}.fa-duotone.fa-fill-drip:after,.fad.fa-fill-drip:after{content:"\f576\f576"}.fa-duotone.fa-arrows-to-circle:after,.fad.fa-arrows-to-circle:after{content:"\e4bd\e4bd"}.fa-duotone.fa-chevron-circle-right:after,.fa-duotone.fa-circle-chevron-right:after,.fad.fa-chevron-circle-right:after,.fad.fa-circle-chevron-right:after{content:"\f138\f138"}.fa-duotone.fa-wagon-covered:after,.fad.fa-wagon-covered:after{content:"\f8ee\f8ee"}.fa-duotone.fa-line-height:after,.fad.fa-line-height:after{content:"\f871\f871"}.fa-duotone.fa-bagel:after,.fad.fa-bagel:after{content:"\e3d7\e3d7"}.fa-duotone.fa-transporter-7:after,.fad.fa-transporter-7:after{content:"\e2a8\e2a8"}.fa-duotone.fa-at:after,.fad.fa-at:after{content:"\40\40"}.fa-duotone.fa-rectangles-mixed:after,.fad.fa-rectangles-mixed:after{content:"\e323\e323"}.fa-duotone.fa-phone-arrow-up-right:after,.fa-duotone.fa-phone-arrow-up:after,.fa-duotone.fa-phone-outgoing:after,.fad.fa-phone-arrow-up-right:after,.fad.fa-phone-arrow-up:after,.fad.fa-phone-outgoing:after{content:"\e224\e224"}.fa-duotone.fa-trash-alt:after,.fa-duotone.fa-trash-can:after,.fad.fa-trash-alt:after,.fad.fa-trash-can:after{content:"\f2ed\f2ed"}.fa-duotone.fa-circle-l:after,.fad.fa-circle-l:after{content:"\e114\e114"}.fa-duotone.fa-head-side-goggles:after,.fa-duotone.fa-head-vr:after,.fad.fa-head-side-goggles:after,.fad.fa-head-vr:after{content:"\f6ea\f6ea"}.fa-duotone.fa-text-height:after,.fad.fa-text-height:after{content:"\f034\f034"}.fa-duotone.fa-user-times:after,.fa-duotone.fa-user-xmark:after,.fad.fa-user-times:after,.fad.fa-user-xmark:after{content:"\f235\f235"}.fa-duotone.fa-face-hand-yawn:after,.fad.fa-face-hand-yawn:after{content:"\e379\e379"}.fa-duotone.fa-gauge-simple-min:after,.fa-duotone.fa-tachometer-slowest:after,.fad.fa-gauge-simple-min:after,.fad.fa-tachometer-slowest:after{content:"\f62d\f62d"}.fa-duotone.fa-stethoscope:after,.fad.fa-stethoscope:after{content:"\f0f1\f0f1"}.fa-duotone.fa-coffin:after,.fad.fa-coffin:after{content:"\f6c6\f6c6"}.fa-duotone.fa-comment-alt:after,.fa-duotone.fa-message:after,.fad.fa-comment-alt:after,.fad.fa-message:after{content:"\f27a\f27a"}.fa-duotone.fa-bowl-salad:after,.fa-duotone.fa-salad:after,.fad.fa-bowl-salad:after,.fad.fa-salad:after{content:"\f81e\f81e"}.fa-duotone.fa-info:after,.fad.fa-info:after{content:"\f129\f129"}.fa-duotone.fa-robot-astromech:after,.fad.fa-robot-astromech:after{content:"\e2d2\e2d2"}.fa-duotone.fa-ring-diamond:after,.fad.fa-ring-diamond:after{content:"\e5ab\e5ab"}.fa-duotone.fa-fondue-pot:after,.fad.fa-fondue-pot:after{content:"\e40d\e40d"}.fa-duotone.fa-theta:after,.fad.fa-theta:after{content:"\f69e\f69e"}.fa-duotone.fa-face-hand-peeking:after,.fad.fa-face-hand-peeking:after{content:"\e481\e481"}.fa-duotone.fa-square-user:after,.fad.fa-square-user:after{content:"\e283\e283"}.fa-duotone.fa-compress-alt:after,.fa-duotone.fa-down-left-and-up-right-to-center:after,.fad.fa-compress-alt:after,.fad.fa-down-left-and-up-right-to-center:after{content:"\f422\f422"}.fa-duotone.fa-explosion:after,.fad.fa-explosion:after{content:"\e4e9\e4e9"}.fa-duotone.fa-file-alt:after,.fa-duotone.fa-file-lines:after,.fa-duotone.fa-file-text:after,.fad.fa-file-alt:after,.fad.fa-file-lines:after,.fad.fa-file-text:after{content:"\f15c\f15c"}.fa-duotone.fa-wave-square:after,.fad.fa-wave-square:after{content:"\f83e\f83e"}.fa-duotone.fa-ring:after,.fad.fa-ring:after{content:"\f70b\f70b"}.fa-duotone.fa-building-un:after,.fad.fa-building-un:after{content:"\e4d9\e4d9"}.fa-duotone.fa-dice-three:after,.fad.fa-dice-three:after{content:"\f527\f527"}.fa-duotone.fa-tire-pressure-warning:after,.fad.fa-tire-pressure-warning:after{content:"\f633\f633"}.fa-duotone.fa-wifi-2:after,.fa-duotone.fa-wifi-fair:after,.fad.fa-wifi-2:after,.fad.fa-wifi-fair:after{content:"\f6ab\f6ab"}.fa-duotone.fa-calendar-alt:after,.fa-duotone.fa-calendar-days:after,.fad.fa-calendar-alt:after,.fad.fa-calendar-days:after{content:"\f073\f073"}.fa-duotone.fa-mp3-player:after,.fad.fa-mp3-player:after{content:"\f8ce\f8ce"}.fa-duotone.fa-anchor-circle-check:after,.fad.fa-anchor-circle-check:after{content:"\e4aa\e4aa"}.fa-duotone.fa-tally-4:after,.fad.fa-tally-4:after{content:"\e297\e297"}.fa-duotone.fa-rectangle-history:after,.fad.fa-rectangle-history:after{content:"\e4a2\e4a2"}.fa-duotone.fa-building-circle-arrow-right:after,.fad.fa-building-circle-arrow-right:after{content:"\e4d1\e4d1"}.fa-duotone.fa-volleyball-ball:after,.fa-duotone.fa-volleyball:after,.fad.fa-volleyball-ball:after,.fad.fa-volleyball:after{content:"\f45f\f45f"}.fa-duotone.fa-sun-haze:after,.fad.fa-sun-haze:after{content:"\f765\f765"}.fa-duotone.fa-text-size:after,.fad.fa-text-size:after{content:"\f894\f894"}.fa-duotone.fa-ufo:after,.fad.fa-ufo:after{content:"\e047\e047"}.fa-duotone.fa-fork:after,.fa-duotone.fa-utensil-fork:after,.fad.fa-fork:after,.fad.fa-utensil-fork:after{content:"\f2e3\f2e3"}.fa-duotone.fa-arrows-up-to-line:after,.fad.fa-arrows-up-to-line:after{content:"\e4c2\e4c2"}.fa-duotone.fa-mobile-signal:after,.fad.fa-mobile-signal:after{content:"\e1ef\e1ef"}.fa-duotone.fa-barcode-scan:after,.fad.fa-barcode-scan:after{content:"\f465\f465"}.fa-duotone.fa-sort-desc:after,.fa-duotone.fa-sort-down:after,.fad.fa-sort-desc:after,.fad.fa-sort-down:after{content:"\f0dd\f0dd"}.fa-duotone.fa-folder-arrow-down:after,.fa-duotone.fa-folder-download:after,.fad.fa-folder-arrow-down:after,.fad.fa-folder-download:after{content:"\e053\e053"}.fa-duotone.fa-circle-minus:after,.fa-duotone.fa-minus-circle:after,.fad.fa-circle-minus:after,.fad.fa-minus-circle:after{content:"\f056\f056"}.fa-duotone.fa-face-icicles:after,.fad.fa-face-icicles:after{content:"\e37c\e37c"}.fa-duotone.fa-shovel:after,.fad.fa-shovel:after{content:"\f713\f713"}.fa-duotone.fa-door-open:after,.fad.fa-door-open:after{content:"\f52b\f52b"}.fa-duotone.fa-films:after,.fad.fa-films:after{content:"\e17a\e17a"}.fa-duotone.fa-right-from-bracket:after,.fa-duotone.fa-sign-out-alt:after,.fad.fa-right-from-bracket:after,.fad.fa-sign-out-alt:after{content:"\f2f5\f2f5"}.fa-duotone.fa-face-glasses:after,.fad.fa-face-glasses:after{content:"\e377\e377"}.fa-duotone.fa-nfc:after,.fad.fa-nfc:after{content:"\e1f7\e1f7"}.fa-duotone.fa-atom:after,.fad.fa-atom:after{content:"\f5d2\f5d2"}.fa-duotone.fa-soap:after,.fad.fa-soap:after{content:"\e06e\e06e"}.fa-duotone.fa-heart-music-camera-bolt:after,.fa-duotone.fa-icons:after,.fad.fa-heart-music-camera-bolt:after,.fad.fa-icons:after{content:"\f86d\f86d"}.fa-duotone.fa-microphone-alt-slash:after,.fa-duotone.fa-microphone-lines-slash:after,.fad.fa-microphone-alt-slash:after,.fad.fa-microphone-lines-slash:after{content:"\f539\f539"}.fa-duotone.fa-closed-captioning-slash:after,.fad.fa-closed-captioning-slash:after{content:"\e135\e135"}.fa-duotone.fa-calculator-alt:after,.fa-duotone.fa-calculator-simple:after,.fad.fa-calculator-alt:after,.fad.fa-calculator-simple:after{content:"\f64c\f64c"}.fa-duotone.fa-bridge-circle-check:after,.fad.fa-bridge-circle-check:after{content:"\e4c9\e4c9"}.fa-duotone.fa-sliders-up:after,.fa-duotone.fa-sliders-v:after,.fad.fa-sliders-up:after,.fad.fa-sliders-v:after{content:"\f3f1\f3f1"}.fa-duotone.fa-location-minus:after,.fa-duotone.fa-map-marker-minus:after,.fad.fa-location-minus:after,.fad.fa-map-marker-minus:after{content:"\f609\f609"}.fa-duotone.fa-pump-medical:after,.fad.fa-pump-medical:after{content:"\e06a\e06a"}.fa-duotone.fa-fingerprint:after,.fad.fa-fingerprint:after{content:"\f577\f577"}.fa-duotone.fa-ski-boot:after,.fad.fa-ski-boot:after{content:"\e3cc\e3cc"}.fa-duotone.fa-rectangle-sd:after,.fa-duotone.fa-standard-definition:after,.fad.fa-rectangle-sd:after,.fad.fa-standard-definition:after{content:"\e28a\e28a"}.fa-duotone.fa-h1:after,.fad.fa-h1:after{content:"\f313\f313"}.fa-duotone.fa-hand-point-right:after,.fad.fa-hand-point-right:after{content:"\f0a4\f0a4"}.fa-duotone.fa-magnifying-glass-location:after,.fa-duotone.fa-search-location:after,.fad.fa-magnifying-glass-location:after,.fad.fa-search-location:after{content:"\f689\f689"}.fa-duotone.fa-message-bot:after,.fad.fa-message-bot:after{content:"\e3b8\e3b8"}.fa-duotone.fa-forward-step:after,.fa-duotone.fa-step-forward:after,.fad.fa-forward-step:after,.fad.fa-step-forward:after{content:"\f051\f051"}.fa-duotone.fa-face-smile-beam:after,.fa-duotone.fa-smile-beam:after,.fad.fa-face-smile-beam:after,.fad.fa-smile-beam:after{content:"\f5b8\f5b8"}.fa-duotone.fa-light-ceiling:after,.fad.fa-light-ceiling:after{content:"\e016\e016"}.fa-duotone.fa-comment-alt-exclamation:after,.fa-duotone.fa-message-exclamation:after,.fad.fa-comment-alt-exclamation:after,.fad.fa-message-exclamation:after{content:"\f4a5\f4a5"}.fa-duotone.fa-bowl-scoop:after,.fa-duotone.fa-bowl-shaved-ice:after,.fad.fa-bowl-scoop:after,.fad.fa-bowl-shaved-ice:after{content:"\e3de\e3de"}.fa-duotone.fa-square-x:after,.fad.fa-square-x:after{content:"\e286\e286"}.fa-duotone.fa-building-memo:after,.fad.fa-building-memo:after{content:"\e61e\e61e"}.fa-duotone.fa-utility-pole-double:after,.fad.fa-utility-pole-double:after{content:"\e2c4\e2c4"}.fa-duotone.fa-flag-checkered:after,.fad.fa-flag-checkered:after{content:"\f11e\f11e"}.fa-duotone.fa-chevron-double-up:after,.fa-duotone.fa-chevrons-up:after,.fad.fa-chevron-double-up:after,.fad.fa-chevrons-up:after{content:"\f325\f325"}.fa-duotone.fa-football-ball:after,.fa-duotone.fa-football:after,.fad.fa-football-ball:after,.fad.fa-football:after{content:"\f44e\f44e"}.fa-duotone.fa-user-vneck:after,.fad.fa-user-vneck:after{content:"\e461\e461"}.fa-duotone.fa-school-circle-exclamation:after,.fad.fa-school-circle-exclamation:after{content:"\e56c\e56c"}.fa-duotone.fa-crop:after,.fad.fa-crop:after{content:"\f125\f125"}.fa-duotone.fa-angle-double-down:after,.fa-duotone.fa-angles-down:after,.fad.fa-angle-double-down:after,.fad.fa-angles-down:after{content:"\f103\f103"}.fa-duotone.fa-users-rectangle:after,.fad.fa-users-rectangle:after{content:"\e594\e594"}.fa-duotone.fa-people-roof:after,.fad.fa-people-roof:after{content:"\e537\e537"}.fa-duotone.fa-arrow-square-right:after,.fa-duotone.fa-square-arrow-right:after,.fad.fa-arrow-square-right:after,.fad.fa-square-arrow-right:after{content:"\f33b\f33b"}.fa-duotone.fa-location-plus:after,.fa-duotone.fa-map-marker-plus:after,.fad.fa-location-plus:after,.fad.fa-map-marker-plus:after{content:"\f60a\f60a"}.fa-duotone.fa-lightbulb-exclamation-on:after,.fad.fa-lightbulb-exclamation-on:after{content:"\e1ca\e1ca"}.fa-duotone.fa-people-line:after,.fad.fa-people-line:after{content:"\e534\e534"}.fa-duotone.fa-beer-mug-empty:after,.fa-duotone.fa-beer:after,.fad.fa-beer-mug-empty:after,.fad.fa-beer:after{content:"\f0fc\f0fc"}.fa-duotone.fa-crate-empty:after,.fad.fa-crate-empty:after{content:"\e151\e151"}.fa-duotone.fa-diagram-predecessor:after,.fad.fa-diagram-predecessor:after{content:"\e477\e477"}.fa-duotone.fa-transporter:after,.fad.fa-transporter:after{content:"\e042\e042"}.fa-duotone.fa-calendar-circle-user:after,.fad.fa-calendar-circle-user:after{content:"\e471\e471"}.fa-duotone.fa-arrow-up-long:after,.fa-duotone.fa-long-arrow-up:after,.fad.fa-arrow-up-long:after,.fad.fa-long-arrow-up:after{content:"\f176\f176"}.fa-duotone.fa-person-carry-box:after,.fa-duotone.fa-person-carry:after,.fad.fa-person-carry-box:after,.fad.fa-person-carry:after{content:"\f4cf\f4cf"}.fa-duotone.fa-burn:after,.fa-duotone.fa-fire-flame-simple:after,.fad.fa-burn:after,.fad.fa-fire-flame-simple:after{content:"\f46a\f46a"}.fa-duotone.fa-male:after,.fa-duotone.fa-person:after,.fad.fa-male:after,.fad.fa-person:after{content:"\f183\f183"}.fa-duotone.fa-laptop:after,.fad.fa-laptop:after{content:"\f109\f109"}.fa-duotone.fa-file-csv:after,.fad.fa-file-csv:after{content:"\f6dd\f6dd"}.fa-duotone.fa-menorah:after,.fad.fa-menorah:after{content:"\f676\f676"}.fa-duotone.fa-union:after,.fad.fa-union:after{content:"\f6a2\f6a2"}.fa-duotone.fa-chevron-double-left:after,.fa-duotone.fa-chevrons-left:after,.fad.fa-chevron-double-left:after,.fad.fa-chevrons-left:after{content:"\f323\f323"}.fa-duotone.fa-circle-heart:after,.fa-duotone.fa-heart-circle:after,.fad.fa-circle-heart:after,.fad.fa-heart-circle:after{content:"\f4c7\f4c7"}.fa-duotone.fa-truck-plane:after,.fad.fa-truck-plane:after{content:"\e58f\e58f"}.fa-duotone.fa-record-vinyl:after,.fad.fa-record-vinyl:after{content:"\f8d9\f8d9"}.fa-duotone.fa-bring-forward:after,.fad.fa-bring-forward:after{content:"\f856\f856"}.fa-duotone.fa-square-p:after,.fad.fa-square-p:after{content:"\e279\e279"}.fa-duotone.fa-face-grin-stars:after,.fa-duotone.fa-grin-stars:after,.fad.fa-face-grin-stars:after,.fad.fa-grin-stars:after{content:"\f587\f587"}.fa-duotone.fa-sigma:after,.fad.fa-sigma:after{content:"\f68b\f68b"}.fa-duotone.fa-camera-movie:after,.fad.fa-camera-movie:after{content:"\f8a9\f8a9"}.fa-duotone.fa-bong:after,.fad.fa-bong:after{content:"\f55c\f55c"}.fa-duotone.fa-clarinet:after,.fad.fa-clarinet:after{content:"\f8ad\f8ad"}.fa-duotone.fa-truck-flatbed:after,.fad.fa-truck-flatbed:after{content:"\e2b6\e2b6"}.fa-duotone.fa-pastafarianism:after,.fa-duotone.fa-spaghetti-monster-flying:after,.fad.fa-pastafarianism:after,.fad.fa-spaghetti-monster-flying:after{content:"\f67b\f67b"}.fa-duotone.fa-arrow-down-up-across-line:after,.fad.fa-arrow-down-up-across-line:after{content:"\e4af\e4af"}.fa-duotone.fa-arrows-rotate-reverse:after,.fad.fa-arrows-rotate-reverse:after{content:"\e630\e630"}.fa-duotone.fa-leaf-heart:after,.fad.fa-leaf-heart:after{content:"\f4cb\f4cb"}.fa-duotone.fa-house-building:after,.fad.fa-house-building:after{content:"\e1b1\e1b1"}.fa-duotone.fa-cheese-swiss:after,.fad.fa-cheese-swiss:after{content:"\f7f0\f7f0"}.fa-duotone.fa-spoon:after,.fa-duotone.fa-utensil-spoon:after,.fad.fa-spoon:after,.fad.fa-utensil-spoon:after{content:"\f2e5\f2e5"}.fa-duotone.fa-jar-wheat:after,.fad.fa-jar-wheat:after{content:"\e517\e517"}.fa-duotone.fa-envelopes-bulk:after,.fa-duotone.fa-mail-bulk:after,.fad.fa-envelopes-bulk:after,.fad.fa-mail-bulk:after{content:"\f674\f674"}.fa-duotone.fa-file-circle-exclamation:after,.fad.fa-file-circle-exclamation:after{content:"\e4eb\e4eb"}.fa-duotone.fa-bow-arrow:after,.fad.fa-bow-arrow:after{content:"\f6b9\f6b9"}.fa-duotone.fa-cart-xmark:after,.fad.fa-cart-xmark:after{content:"\e0dd\e0dd"}.fa-duotone.fa-hexagon-xmark:after,.fa-duotone.fa-times-hexagon:after,.fa-duotone.fa-xmark-hexagon:after,.fad.fa-hexagon-xmark:after,.fad.fa-times-hexagon:after,.fad.fa-xmark-hexagon:after{content:"\f2ee\f2ee"}.fa-duotone.fa-circle-h:after,.fa-duotone.fa-hospital-symbol:after,.fad.fa-circle-h:after,.fad.fa-hospital-symbol:after{content:"\f47e\f47e"}.fa-duotone.fa-merge:after,.fad.fa-merge:after{content:"\e526\e526"}.fa-duotone.fa-pager:after,.fad.fa-pager:after{content:"\f815\f815"}.fa-duotone.fa-cart-minus:after,.fad.fa-cart-minus:after{content:"\e0db\e0db"}.fa-duotone.fa-address-book:after,.fa-duotone.fa-contact-book:after,.fad.fa-address-book:after,.fad.fa-contact-book:after{content:"\f2b9\f2b9"}.fa-duotone.fa-pan-frying:after,.fad.fa-pan-frying:after{content:"\e42c\e42c"}.fa-duotone.fa-grid-3:after,.fa-duotone.fa-grid:after,.fad.fa-grid-3:after,.fad.fa-grid:after{content:"\e195\e195"}.fa-duotone.fa-football-helmet:after,.fad.fa-football-helmet:after{content:"\f44f\f44f"}.fa-duotone.fa-hand-love:after,.fad.fa-hand-love:after{content:"\e1a5\e1a5"}.fa-duotone.fa-trees:after,.fad.fa-trees:after{content:"\f724\f724"}.fa-duotone.fa-strikethrough:after,.fad.fa-strikethrough:after{content:"\f0cc\f0cc"}.fa-duotone.fa-page:after,.fad.fa-page:after{content:"\e428\e428"}.fa-duotone.fa-k:after,.fad.fa-k:after{content:"\4b\4b"}.fa-duotone.fa-diagram-previous:after,.fad.fa-diagram-previous:after{content:"\e478\e478"}.fa-duotone.fa-gauge-min:after,.fa-duotone.fa-tachometer-alt-slowest:after,.fad.fa-gauge-min:after,.fad.fa-tachometer-alt-slowest:after{content:"\f628\f628"}.fa-duotone.fa-folder-grid:after,.fad.fa-folder-grid:after{content:"\e188\e188"}.fa-duotone.fa-eggplant:after,.fad.fa-eggplant:after{content:"\e16c\e16c"}.fa-duotone.fa-excavator:after,.fad.fa-excavator:after{content:"\e656\e656"}.fa-duotone.fa-ram:after,.fad.fa-ram:after{content:"\f70a\f70a"}.fa-duotone.fa-landmark-flag:after,.fad.fa-landmark-flag:after{content:"\e51c\e51c"}.fa-duotone.fa-lips:after,.fad.fa-lips:after{content:"\f600\f600"}.fa-duotone.fa-pencil-alt:after,.fa-duotone.fa-pencil:after,.fad.fa-pencil-alt:after,.fad.fa-pencil:after{content:"\f303\f303"}.fa-duotone.fa-backward:after,.fad.fa-backward:after{content:"\f04a\f04a"}.fa-duotone.fa-caret-right:after,.fad.fa-caret-right:after{content:"\f0da\f0da"}.fa-duotone.fa-comments:after,.fad.fa-comments:after{content:"\f086\f086"}.fa-duotone.fa-file-clipboard:after,.fa-duotone.fa-paste:after,.fad.fa-file-clipboard:after,.fad.fa-paste:after{content:"\f0ea\f0ea"}.fa-duotone.fa-desktop-arrow-down:after,.fad.fa-desktop-arrow-down:after{content:"\e155\e155"}.fa-duotone.fa-code-pull-request:after,.fad.fa-code-pull-request:after{content:"\e13c\e13c"}.fa-duotone.fa-pumpkin:after,.fad.fa-pumpkin:after{content:"\f707\f707"}.fa-duotone.fa-clipboard-list:after,.fad.fa-clipboard-list:after{content:"\f46d\f46d"}.fa-duotone.fa-pen-field:after,.fad.fa-pen-field:after{content:"\e211\e211"}.fa-duotone.fa-blueberries:after,.fad.fa-blueberries:after{content:"\e2e8\e2e8"}.fa-duotone.fa-truck-loading:after,.fa-duotone.fa-truck-ramp-box:after,.fad.fa-truck-loading:after,.fad.fa-truck-ramp-box:after{content:"\f4de\f4de"}.fa-duotone.fa-note:after,.fad.fa-note:after{content:"\e1ff\e1ff"}.fa-duotone.fa-arrow-down-to-square:after,.fad.fa-arrow-down-to-square:after{content:"\e096\e096"}.fa-duotone.fa-user-check:after,.fad.fa-user-check:after{content:"\f4fc\f4fc"}.fa-duotone.fa-cloud-xmark:after,.fad.fa-cloud-xmark:after{content:"\e35f\e35f"}.fa-duotone.fa-vial-virus:after,.fad.fa-vial-virus:after{content:"\e597\e597"}.fa-duotone.fa-book-alt:after,.fa-duotone.fa-book-blank:after,.fad.fa-book-alt:after,.fad.fa-book-blank:after{content:"\f5d9\f5d9"}.fa-duotone.fa-golf-flag-hole:after,.fad.fa-golf-flag-hole:after{content:"\e3ac\e3ac"}.fa-duotone.fa-comment-alt-arrow-down:after,.fa-duotone.fa-message-arrow-down:after,.fad.fa-comment-alt-arrow-down:after,.fad.fa-message-arrow-down:after{content:"\e1db\e1db"}.fa-duotone.fa-face-unamused:after,.fad.fa-face-unamused:after{content:"\e39f\e39f"}.fa-duotone.fa-sheet-plastic:after,.fad.fa-sheet-plastic:after{content:"\e571\e571"}.fa-duotone.fa-circle-9:after,.fad.fa-circle-9:after{content:"\e0f6\e0f6"}.fa-duotone.fa-blog:after,.fad.fa-blog:after{content:"\f781\f781"}.fa-duotone.fa-user-ninja:after,.fad.fa-user-ninja:after{content:"\f504\f504"}.fa-duotone.fa-pencil-slash:after,.fad.fa-pencil-slash:after{content:"\e215\e215"}.fa-duotone.fa-bowling-pins:after,.fad.fa-bowling-pins:after{content:"\f437\f437"}.fa-duotone.fa-person-arrow-up-from-line:after,.fad.fa-person-arrow-up-from-line:after{content:"\e539\e539"}.fa-duotone.fa-down-right:after,.fad.fa-down-right:after{content:"\e16b\e16b"}.fa-duotone.fa-scroll-torah:after,.fa-duotone.fa-torah:after,.fad.fa-scroll-torah:after,.fad.fa-torah:after{content:"\f6a0\f6a0"}.fa-duotone.fa-webhook:after,.fad.fa-webhook:after{content:"\e5d5\e5d5"}.fa-duotone.fa-blinds-open:after,.fad.fa-blinds-open:after{content:"\f8fc\f8fc"}.fa-duotone.fa-fence:after,.fad.fa-fence:after{content:"\e303\e303"}.fa-duotone.fa-arrow-alt-up:after,.fa-duotone.fa-up:after,.fad.fa-arrow-alt-up:after,.fad.fa-up:after{content:"\f357\f357"}.fa-duotone.fa-broom-ball:after,.fa-duotone.fa-quidditch-broom-ball:after,.fa-duotone.fa-quidditch:after,.fad.fa-broom-ball:after,.fad.fa-quidditch-broom-ball:after,.fad.fa-quidditch:after{content:"\f458\f458"}.fa-duotone.fa-drumstick:after,.fad.fa-drumstick:after{content:"\f6d6\f6d6"}.fa-duotone.fa-square-v:after,.fad.fa-square-v:after{content:"\e284\e284"}.fa-duotone.fa-face-awesome:after,.fa-duotone.fa-gave-dandy:after,.fad.fa-face-awesome:after,.fad.fa-gave-dandy:after{content:"\e409\e409"}.fa-duotone.fa-dial-off:after,.fad.fa-dial-off:after{content:"\e162\e162"}.fa-duotone.fa-toggle-off:after,.fad.fa-toggle-off:after{content:"\f204\f204"}.fa-duotone.fa-face-smile-horns:after,.fad.fa-face-smile-horns:after{content:"\e391\e391"}.fa-duotone.fa-archive:after,.fa-duotone.fa-box-archive:after,.fad.fa-archive:after,.fad.fa-box-archive:after{content:"\f187\f187"}.fa-duotone.fa-grapes:after,.fad.fa-grapes:after{content:"\e306\e306"}.fa-duotone.fa-person-drowning:after,.fad.fa-person-drowning:after{content:"\e545\e545"}.fa-duotone.fa-dial-max:after,.fad.fa-dial-max:after{content:"\e15e\e15e"}.fa-duotone.fa-circle-m:after,.fad.fa-circle-m:after{content:"\e115\e115"}.fa-duotone.fa-calendar-image:after,.fad.fa-calendar-image:after{content:"\e0d4\e0d4"}.fa-duotone.fa-caret-circle-down:after,.fa-duotone.fa-circle-caret-down:after,.fad.fa-caret-circle-down:after,.fad.fa-circle-caret-down:after{content:"\f32d\f32d"}.fa-duotone.fa-arrow-down-9-1:after,.fa-duotone.fa-sort-numeric-desc:after,.fa-duotone.fa-sort-numeric-down-alt:after,.fad.fa-arrow-down-9-1:after,.fad.fa-sort-numeric-desc:after,.fad.fa-sort-numeric-down-alt:after{content:"\f886\f886"}.fa-duotone.fa-face-grin-tongue-squint:after,.fa-duotone.fa-grin-tongue-squint:after,.fad.fa-face-grin-tongue-squint:after,.fad.fa-grin-tongue-squint:after{content:"\f58a\f58a"}.fa-duotone.fa-shish-kebab:after,.fad.fa-shish-kebab:after{content:"\f821\f821"}.fa-duotone.fa-spray-can:after,.fad.fa-spray-can:after{content:"\f5bd\f5bd"}.fa-duotone.fa-alarm-snooze:after,.fad.fa-alarm-snooze:after{content:"\f845\f845"}.fa-duotone.fa-scarecrow:after,.fad.fa-scarecrow:after{content:"\f70d\f70d"}.fa-duotone.fa-truck-monster:after,.fad.fa-truck-monster:after{content:"\f63b\f63b"}.fa-duotone.fa-gift-card:after,.fad.fa-gift-card:after{content:"\f663\f663"}.fa-duotone.fa-w:after,.fad.fa-w:after{content:"\57\57"}.fa-duotone.fa-code-pull-request-draft:after,.fad.fa-code-pull-request-draft:after{content:"\e3fa\e3fa"}.fa-duotone.fa-square-b:after,.fad.fa-square-b:after{content:"\e264\e264"}.fa-duotone.fa-elephant:after,.fad.fa-elephant:after{content:"\f6da\f6da"}.fa-duotone.fa-earth-africa:after,.fa-duotone.fa-globe-africa:after,.fad.fa-earth-africa:after,.fad.fa-globe-africa:after{content:"\f57c\f57c"}.fa-duotone.fa-rainbow:after,.fad.fa-rainbow:after{content:"\f75b\f75b"}.fa-duotone.fa-circle-notch:after,.fad.fa-circle-notch:after{content:"\f1ce\f1ce"}.fa-duotone.fa-tablet-alt:after,.fa-duotone.fa-tablet-screen-button:after,.fad.fa-tablet-alt:after,.fad.fa-tablet-screen-button:after{content:"\f3fa\f3fa"}.fa-duotone.fa-paw:after,.fad.fa-paw:after{content:"\f1b0\f1b0"}.fa-duotone.fa-message-question:after,.fad.fa-message-question:after{content:"\e1e3\e1e3"}.fa-duotone.fa-cloud:after,.fad.fa-cloud:after{content:"\f0c2\f0c2"}.fa-duotone.fa-trowel-bricks:after,.fad.fa-trowel-bricks:after{content:"\e58a\e58a"}.fa-duotone.fa-square-3:after,.fad.fa-square-3:after{content:"\e258\e258"}.fa-duotone.fa-face-flushed:after,.fa-duotone.fa-flushed:after,.fad.fa-face-flushed:after,.fad.fa-flushed:after{content:"\f579\f579"}.fa-duotone.fa-hospital-user:after,.fad.fa-hospital-user:after{content:"\f80d\f80d"}.fa-duotone.fa-microwave:after,.fad.fa-microwave:after{content:"\e01b\e01b"}.fa-duotone.fa-chf-sign:after,.fad.fa-chf-sign:after{content:"\e602\e602"}.fa-duotone.fa-tent-arrow-left-right:after,.fad.fa-tent-arrow-left-right:after{content:"\e57f\e57f"}.fa-duotone.fa-cart-circle-arrow-up:after,.fad.fa-cart-circle-arrow-up:after{content:"\e3f0\e3f0"}.fa-duotone.fa-trash-clock:after,.fad.fa-trash-clock:after{content:"\e2b0\e2b0"}.fa-duotone.fa-reflect-both:after,.fad.fa-reflect-both:after{content:"\e66f\e66f"}.fa-duotone.fa-gavel:after,.fa-duotone.fa-legal:after,.fad.fa-gavel:after,.fad.fa-legal:after{content:"\f0e3\f0e3"}.fa-duotone.fa-sprinkler-ceiling:after,.fad.fa-sprinkler-ceiling:after{content:"\e44c\e44c"}.fa-duotone.fa-browsers:after,.fad.fa-browsers:after{content:"\e0cb\e0cb"}.fa-duotone.fa-trillium:after,.fad.fa-trillium:after{content:"\e588\e588"}.fa-duotone.fa-music-slash:after,.fad.fa-music-slash:after{content:"\f8d1\f8d1"}.fa-duotone.fa-truck-ramp:after,.fad.fa-truck-ramp:after{content:"\f4e0\f4e0"}.fa-duotone.fa-binoculars:after,.fad.fa-binoculars:after{content:"\f1e5\f1e5"}.fa-duotone.fa-microphone-slash:after,.fad.fa-microphone-slash:after{content:"\f131\f131"}.fa-duotone.fa-box-tissue:after,.fad.fa-box-tissue:after{content:"\e05b\e05b"}.fa-duotone.fa-circle-c:after,.fad.fa-circle-c:after{content:"\e101\e101"}.fa-duotone.fa-star-christmas:after,.fad.fa-star-christmas:after{content:"\f7d4\f7d4"}.fa-duotone.fa-chart-bullet:after,.fad.fa-chart-bullet:after{content:"\e0e1\e0e1"}.fa-duotone.fa-motorcycle:after,.fad.fa-motorcycle:after{content:"\f21c\f21c"}.fa-duotone.fa-tree-christmas:after,.fad.fa-tree-christmas:after{content:"\f7db\f7db"}.fa-duotone.fa-tire-flat:after,.fad.fa-tire-flat:after{content:"\f632\f632"}.fa-duotone.fa-sunglasses:after,.fad.fa-sunglasses:after{content:"\f892\f892"}.fa-duotone.fa-badge:after,.fad.fa-badge:after{content:"\f335\f335"}.fa-duotone.fa-comment-alt-edit:after,.fa-duotone.fa-message-edit:after,.fa-duotone.fa-message-pen:after,.fad.fa-comment-alt-edit:after,.fad.fa-message-edit:after,.fad.fa-message-pen:after{content:"\f4a4\f4a4"}.fa-duotone.fa-bell-concierge:after,.fa-duotone.fa-concierge-bell:after,.fad.fa-bell-concierge:after,.fad.fa-concierge-bell:after{content:"\f562\f562"}.fa-duotone.fa-pen-ruler:after,.fa-duotone.fa-pencil-ruler:after,.fad.fa-pen-ruler:after,.fad.fa-pencil-ruler:after{content:"\f5ae\f5ae"}.fa-duotone.fa-file-mp3:after,.fad.fa-file-mp3:after{content:"\e648\e648"}.fa-duotone.fa-arrow-progress:after,.fad.fa-arrow-progress:after{content:"\e5df\e5df"}.fa-duotone.fa-chess-rook-alt:after,.fa-duotone.fa-chess-rook-piece:after,.fad.fa-chess-rook-alt:after,.fad.fa-chess-rook-piece:after{content:"\f448\f448"}.fa-duotone.fa-square-root:after,.fad.fa-square-root:after{content:"\f697\f697"}.fa-duotone.fa-album-collection-circle-plus:after,.fad.fa-album-collection-circle-plus:after{content:"\e48e\e48e"}.fa-duotone.fa-people-arrows-left-right:after,.fa-duotone.fa-people-arrows:after,.fad.fa-people-arrows-left-right:after,.fad.fa-people-arrows:after{content:"\e068\e068"}.fa-duotone.fa-sign-post:after,.fad.fa-sign-post:after{content:"\e624\e624"}.fa-duotone.fa-face-angry-horns:after,.fad.fa-face-angry-horns:after{content:"\e368\e368"}.fa-duotone.fa-mars-and-venus-burst:after,.fad.fa-mars-and-venus-burst:after{content:"\e523\e523"}.fa-duotone.fa-tombstone:after,.fad.fa-tombstone:after{content:"\f720\f720"}.fa-duotone.fa-caret-square-right:after,.fa-duotone.fa-square-caret-right:after,.fad.fa-caret-square-right:after,.fad.fa-square-caret-right:after{content:"\f152\f152"}.fa-duotone.fa-cut:after,.fa-duotone.fa-scissors:after,.fad.fa-cut:after,.fad.fa-scissors:after{content:"\f0c4\f0c4"}.fa-duotone.fa-list-music:after,.fad.fa-list-music:after{content:"\f8c9\f8c9"}.fa-duotone.fa-sun-plant-wilt:after,.fad.fa-sun-plant-wilt:after{content:"\e57a\e57a"}.fa-duotone.fa-toilets-portable:after,.fad.fa-toilets-portable:after{content:"\e584\e584"}.fa-duotone.fa-hockey-puck:after,.fad.fa-hockey-puck:after{content:"\f453\f453"}.fa-duotone.fa-mustache:after,.fad.fa-mustache:after{content:"\e5bc\e5bc"}.fa-duotone.fa-hyphen:after,.fad.fa-hyphen:after{content:"\2d\2d"}.fa-duotone.fa-table:after,.fad.fa-table:after{content:"\f0ce\f0ce"}.fa-duotone.fa-user-chef:after,.fad.fa-user-chef:after{content:"\e3d2\e3d2"}.fa-duotone.fa-comment-alt-image:after,.fa-duotone.fa-message-image:after,.fad.fa-comment-alt-image:after,.fad.fa-message-image:after{content:"\e1e0\e1e0"}.fa-duotone.fa-users-medical:after,.fad.fa-users-medical:after{content:"\f830\f830"}.fa-duotone.fa-sensor-alert:after,.fa-duotone.fa-sensor-triangle-exclamation:after,.fad.fa-sensor-alert:after,.fad.fa-sensor-triangle-exclamation:after{content:"\e029\e029"}.fa-duotone.fa-magnifying-glass-arrow-right:after,.fad.fa-magnifying-glass-arrow-right:after{content:"\e521\e521"}.fa-duotone.fa-digital-tachograph:after,.fa-duotone.fa-tachograph-digital:after,.fad.fa-digital-tachograph:after,.fad.fa-tachograph-digital:after{content:"\f566\f566"}.fa-duotone.fa-face-mask:after,.fad.fa-face-mask:after{content:"\e37f\e37f"}.fa-duotone.fa-pickleball:after,.fad.fa-pickleball:after{content:"\e435\e435"}.fa-duotone.fa-star-sharp-half:after,.fad.fa-star-sharp-half:after{content:"\e28c\e28c"}.fa-duotone.fa-users-slash:after,.fad.fa-users-slash:after{content:"\e073\e073"}.fa-duotone.fa-clover:after,.fad.fa-clover:after{content:"\e139\e139"}.fa-duotone.fa-meat:after,.fad.fa-meat:after{content:"\f814\f814"}.fa-duotone.fa-mail-reply:after,.fa-duotone.fa-reply:after,.fad.fa-mail-reply:after,.fad.fa-reply:after{content:"\f3e5\f3e5"}.fa-duotone.fa-star-and-crescent:after,.fad.fa-star-and-crescent:after{content:"\f699\f699"}.fa-duotone.fa-empty-set:after,.fad.fa-empty-set:after{content:"\f656\f656"}.fa-duotone.fa-house-fire:after,.fad.fa-house-fire:after{content:"\e50c\e50c"}.fa-duotone.fa-minus-square:after,.fa-duotone.fa-square-minus:after,.fad.fa-minus-square:after,.fad.fa-square-minus:after{content:"\f146\f146"}.fa-duotone.fa-helicopter:after,.fad.fa-helicopter:after{content:"\f533\f533"}.fa-duotone.fa-bird:after,.fad.fa-bird:after{content:"\e469\e469"}.fa-duotone.fa-compass:after,.fad.fa-compass:after{content:"\f14e\f14e"}.fa-duotone.fa-caret-square-down:after,.fa-duotone.fa-square-caret-down:after,.fad.fa-caret-square-down:after,.fad.fa-square-caret-down:after{content:"\f150\f150"}.fa-duotone.fa-heart-half-alt:after,.fa-duotone.fa-heart-half-stroke:after,.fad.fa-heart-half-alt:after,.fad.fa-heart-half-stroke:after{content:"\e1ac\e1ac"}.fa-duotone.fa-file-circle-question:after,.fad.fa-file-circle-question:after{content:"\e4ef\e4ef"}.fa-duotone.fa-truck-utensils:after,.fad.fa-truck-utensils:after{content:"\e628\e628"}.fa-duotone.fa-laptop-code:after,.fad.fa-laptop-code:after{content:"\f5fc\f5fc"}.fa-duotone.fa-joystick:after,.fad.fa-joystick:after{content:"\f8c5\f8c5"}.fa-duotone.fa-grill-fire:after,.fad.fa-grill-fire:after{content:"\e5a4\e5a4"}.fa-duotone.fa-rectangle-vertical-history:after,.fad.fa-rectangle-vertical-history:after{content:"\e237\e237"}.fa-duotone.fa-swatchbook:after,.fad.fa-swatchbook:after{content:"\f5c3\f5c3"}.fa-duotone.fa-prescription-bottle:after,.fad.fa-prescription-bottle:after{content:"\f485\f485"}.fa-duotone.fa-bars:after,.fa-duotone.fa-navicon:after,.fad.fa-bars:after,.fad.fa-navicon:after{content:"\f0c9\f0c9"}.fa-duotone.fa-keyboard-left:after,.fad.fa-keyboard-left:after{content:"\e1c3\e1c3"}.fa-duotone.fa-people-group:after,.fad.fa-people-group:after{content:"\e533\e533"}.fa-duotone.fa-hourglass-3:after,.fa-duotone.fa-hourglass-end:after,.fad.fa-hourglass-3:after,.fad.fa-hourglass-end:after{content:"\f253\f253"}.fa-duotone.fa-heart-broken:after,.fa-duotone.fa-heart-crack:after,.fad.fa-heart-broken:after,.fad.fa-heart-crack:after{content:"\f7a9\f7a9"}.fa-duotone.fa-face-beam-hand-over-mouth:after,.fad.fa-face-beam-hand-over-mouth:after{content:"\e47c\e47c"}.fa-duotone.fa-droplet-percent:after,.fa-duotone.fa-humidity:after,.fad.fa-droplet-percent:after,.fad.fa-humidity:after{content:"\f750\f750"}.fa-duotone.fa-external-link-square-alt:after,.fa-duotone.fa-square-up-right:after,.fad.fa-external-link-square-alt:after,.fad.fa-square-up-right:after{content:"\f360\f360"}.fa-duotone.fa-face-kiss-beam:after,.fa-duotone.fa-kiss-beam:after,.fad.fa-face-kiss-beam:after,.fad.fa-kiss-beam:after{content:"\f597\f597"}.fa-duotone.fa-corn:after,.fad.fa-corn:after{content:"\f6c7\f6c7"}.fa-duotone.fa-roller-coaster:after,.fad.fa-roller-coaster:after{content:"\e324\e324"}.fa-duotone.fa-photo-film-music:after,.fad.fa-photo-film-music:after{content:"\e228\e228"}.fa-duotone.fa-radar:after,.fad.fa-radar:after{content:"\e024\e024"}.fa-duotone.fa-sickle:after,.fad.fa-sickle:after{content:"\f822\f822"}.fa-duotone.fa-film:after,.fad.fa-film:after{content:"\f008\f008"}.fa-duotone.fa-coconut:after,.fad.fa-coconut:after{content:"\e2f6\e2f6"}.fa-duotone.fa-ruler-horizontal:after,.fad.fa-ruler-horizontal:after{content:"\f547\f547"}.fa-duotone.fa-shield-cross:after,.fad.fa-shield-cross:after{content:"\f712\f712"}.fa-duotone.fa-cassette-tape:after,.fad.fa-cassette-tape:after{content:"\f8ab\f8ab"}.fa-duotone.fa-square-terminal:after,.fad.fa-square-terminal:after{content:"\e32a\e32a"}.fa-duotone.fa-people-robbery:after,.fad.fa-people-robbery:after{content:"\e536\e536"}.fa-duotone.fa-lightbulb:after,.fad.fa-lightbulb:after{content:"\f0eb\f0eb"}.fa-duotone.fa-caret-left:after,.fad.fa-caret-left:after{content:"\f0d9\f0d9"}.fa-duotone.fa-comment-middle:after,.fad.fa-comment-middle:after{content:"\e149\e149"}.fa-duotone.fa-trash-can-list:after,.fad.fa-trash-can-list:after{content:"\e2ab\e2ab"}.fa-duotone.fa-block:after,.fad.fa-block:after{content:"\e46a\e46a"}.fa-duotone.fa-circle-exclamation:after,.fa-duotone.fa-exclamation-circle:after,.fad.fa-circle-exclamation:after,.fad.fa-exclamation-circle:after{content:"\f06a\f06a"}.fa-duotone.fa-school-circle-xmark:after,.fad.fa-school-circle-xmark:after{content:"\e56d\e56d"}.fa-duotone.fa-arrow-right-from-bracket:after,.fa-duotone.fa-sign-out:after,.fad.fa-arrow-right-from-bracket:after,.fad.fa-sign-out:after{content:"\f08b\f08b"}.fa-duotone.fa-face-frown-slight:after,.fad.fa-face-frown-slight:after{content:"\e376\e376"}.fa-duotone.fa-chevron-circle-down:after,.fa-duotone.fa-circle-chevron-down:after,.fad.fa-chevron-circle-down:after,.fad.fa-circle-chevron-down:after{content:"\f13a\f13a"}.fa-duotone.fa-sidebar-flip:after,.fad.fa-sidebar-flip:after{content:"\e24f\e24f"}.fa-duotone.fa-unlock-alt:after,.fa-duotone.fa-unlock-keyhole:after,.fad.fa-unlock-alt:after,.fad.fa-unlock-keyhole:after{content:"\f13e\f13e"}.fa-duotone.fa-temperature-list:after,.fad.fa-temperature-list:after{content:"\e299\e299"}.fa-duotone.fa-cloud-showers-heavy:after,.fad.fa-cloud-showers-heavy:after{content:"\f740\f740"}.fa-duotone.fa-headphones-alt:after,.fa-duotone.fa-headphones-simple:after,.fad.fa-headphones-alt:after,.fad.fa-headphones-simple:after{content:"\f58f\f58f"}.fa-duotone.fa-sitemap:after,.fad.fa-sitemap:after{content:"\f0e8\f0e8"}.fa-duotone.fa-pipe-section:after,.fad.fa-pipe-section:after{content:"\e438\e438"}.fa-duotone.fa-space-station-moon-alt:after,.fa-duotone.fa-space-station-moon-construction:after,.fad.fa-space-station-moon-alt:after,.fad.fa-space-station-moon-construction:after{content:"\e034\e034"}.fa-duotone.fa-circle-dollar-to-slot:after,.fa-duotone.fa-donate:after,.fad.fa-circle-dollar-to-slot:after,.fad.fa-donate:after{content:"\f4b9\f4b9"}.fa-duotone.fa-memory:after,.fad.fa-memory:after{content:"\f538\f538"}.fa-duotone.fa-face-sleeping:after,.fad.fa-face-sleeping:after{content:"\e38d\e38d"}.fa-duotone.fa-road-spikes:after,.fad.fa-road-spikes:after{content:"\e568\e568"}.fa-duotone.fa-fire-burner:after,.fad.fa-fire-burner:after{content:"\e4f1\e4f1"}.fa-duotone.fa-squirrel:after,.fad.fa-squirrel:after{content:"\f71a\f71a"}.fa-duotone.fa-arrow-to-top:after,.fa-duotone.fa-arrow-up-to-line:after,.fad.fa-arrow-to-top:after,.fad.fa-arrow-up-to-line:after{content:"\f341\f341"}.fa-duotone.fa-flag:after,.fad.fa-flag:after{content:"\f024\f024"}.fa-duotone.fa-face-cowboy-hat:after,.fad.fa-face-cowboy-hat:after{content:"\e36e\e36e"}.fa-duotone.fa-hanukiah:after,.fad.fa-hanukiah:after{content:"\f6e6\f6e6"}.fa-duotone.fa-chart-scatter-3d:after,.fad.fa-chart-scatter-3d:after{content:"\e0e8\e0e8"}.fa-duotone.fa-display-chart-up:after,.fad.fa-display-chart-up:after{content:"\e5e3\e5e3"}.fa-duotone.fa-square-code:after,.fad.fa-square-code:after{content:"\e267\e267"}.fa-duotone.fa-feather:after,.fad.fa-feather:after{content:"\f52d\f52d"}.fa-duotone.fa-volume-down:after,.fa-duotone.fa-volume-low:after,.fad.fa-volume-down:after,.fad.fa-volume-low:after{content:"\f027\f027"}.fa-duotone.fa-times-to-slot:after,.fa-duotone.fa-vote-nay:after,.fa-duotone.fa-xmark-to-slot:after,.fad.fa-times-to-slot:after,.fad.fa-vote-nay:after,.fad.fa-xmark-to-slot:after{content:"\f771\f771"}.fa-duotone.fa-box-alt:after,.fa-duotone.fa-box-taped:after,.fad.fa-box-alt:after,.fad.fa-box-taped:after{content:"\f49a\f49a"}.fa-duotone.fa-comment-slash:after,.fad.fa-comment-slash:after{content:"\f4b3\f4b3"}.fa-duotone.fa-swords:after,.fad.fa-swords:after{content:"\f71d\f71d"}.fa-duotone.fa-cloud-sun-rain:after,.fad.fa-cloud-sun-rain:after{content:"\f743\f743"}.fa-duotone.fa-album:after,.fad.fa-album:after{content:"\f89f\f89f"}.fa-duotone.fa-circle-n:after,.fad.fa-circle-n:after{content:"\e118\e118"}.fa-duotone.fa-compress:after,.fad.fa-compress:after{content:"\f066\f066"}.fa-duotone.fa-wheat-alt:after,.fa-duotone.fa-wheat-awn:after,.fad.fa-wheat-alt:after,.fad.fa-wheat-awn:after{content:"\e2cd\e2cd"}.fa-duotone.fa-ankh:after,.fad.fa-ankh:after{content:"\f644\f644"}.fa-duotone.fa-hands-holding-child:after,.fad.fa-hands-holding-child:after{content:"\e4fa\e4fa"}.fa-duotone.fa-asterisk:after,.fad.fa-asterisk:after{content:"\2a\2a"}.fa-duotone.fa-key-skeleton-left-right:after,.fad.fa-key-skeleton-left-right:after{content:"\e3b4\e3b4"}.fa-duotone.fa-comment-lines:after,.fad.fa-comment-lines:after{content:"\f4b0\f4b0"}.fa-duotone.fa-luchador-mask:after,.fa-duotone.fa-luchador:after,.fa-duotone.fa-mask-luchador:after,.fad.fa-luchador-mask:after,.fad.fa-luchador:after,.fad.fa-mask-luchador:after{content:"\f455\f455"}.fa-duotone.fa-check-square:after,.fa-duotone.fa-square-check:after,.fad.fa-check-square:after,.fad.fa-square-check:after{content:"\f14a\f14a"}.fa-duotone.fa-shredder:after,.fad.fa-shredder:after{content:"\f68a\f68a"}.fa-duotone.fa-book-open-alt:after,.fa-duotone.fa-book-open-cover:after,.fad.fa-book-open-alt:after,.fad.fa-book-open-cover:after{content:"\e0c0\e0c0"}.fa-duotone.fa-sandwich:after,.fad.fa-sandwich:after{content:"\f81f\f81f"}.fa-duotone.fa-peseta-sign:after,.fad.fa-peseta-sign:after{content:"\e221\e221"}.fa-duotone.fa-parking-slash:after,.fa-duotone.fa-square-parking-slash:after,.fad.fa-parking-slash:after,.fad.fa-square-parking-slash:after{content:"\f617\f617"}.fa-duotone.fa-train-tunnel:after,.fad.fa-train-tunnel:after{content:"\e454\e454"}.fa-duotone.fa-header:after,.fa-duotone.fa-heading:after,.fad.fa-header:after,.fad.fa-heading:after{content:"\f1dc\f1dc"}.fa-duotone.fa-ghost:after,.fad.fa-ghost:after{content:"\f6e2\f6e2"}.fa-duotone.fa-face-anguished:after,.fad.fa-face-anguished:after{content:"\e369\e369"}.fa-duotone.fa-hockey-sticks:after,.fad.fa-hockey-sticks:after{content:"\f454\f454"}.fa-duotone.fa-abacus:after,.fad.fa-abacus:after{content:"\f640\f640"}.fa-duotone.fa-film-alt:after,.fa-duotone.fa-film-simple:after,.fad.fa-film-alt:after,.fad.fa-film-simple:after{content:"\f3a0\f3a0"}.fa-duotone.fa-list-squares:after,.fa-duotone.fa-list:after,.fad.fa-list-squares:after,.fad.fa-list:after{content:"\f03a\f03a"}.fa-duotone.fa-tree-palm:after,.fad.fa-tree-palm:after{content:"\f82b\f82b"}.fa-duotone.fa-phone-square-alt:after,.fa-duotone.fa-square-phone-flip:after,.fad.fa-phone-square-alt:after,.fad.fa-square-phone-flip:after{content:"\f87b\f87b"}.fa-duotone.fa-cart-plus:after,.fad.fa-cart-plus:after{content:"\f217\f217"}.fa-duotone.fa-gamepad:after,.fad.fa-gamepad:after{content:"\f11b\f11b"}.fa-duotone.fa-border-center-v:after,.fad.fa-border-center-v:after{content:"\f89d\f89d"}.fa-duotone.fa-circle-dot:after,.fa-duotone.fa-dot-circle:after,.fad.fa-circle-dot:after,.fad.fa-dot-circle:after{content:"\f192\f192"}.fa-duotone.fa-clipboard-medical:after,.fad.fa-clipboard-medical:after{content:"\e133\e133"}.fa-duotone.fa-dizzy:after,.fa-duotone.fa-face-dizzy:after,.fad.fa-dizzy:after,.fad.fa-face-dizzy:after{content:"\f567\f567"}.fa-duotone.fa-egg:after,.fad.fa-egg:after{content:"\f7fb\f7fb"}.fa-duotone.fa-arrow-alt-to-top:after,.fa-duotone.fa-up-to-line:after,.fad.fa-arrow-alt-to-top:after,.fad.fa-up-to-line:after{content:"\f34d\f34d"}.fa-duotone.fa-house-medical-circle-xmark:after,.fad.fa-house-medical-circle-xmark:after{content:"\e513\e513"}.fa-duotone.fa-watch-fitness:after,.fad.fa-watch-fitness:after{content:"\f63e\f63e"}.fa-duotone.fa-clock-nine-thirty:after,.fad.fa-clock-nine-thirty:after{content:"\e34d\e34d"}.fa-duotone.fa-campground:after,.fad.fa-campground:after{content:"\f6bb\f6bb"}.fa-duotone.fa-folder-plus:after,.fad.fa-folder-plus:after{content:"\f65e\f65e"}.fa-duotone.fa-jug:after,.fad.fa-jug:after{content:"\f8c6\f8c6"}.fa-duotone.fa-futbol-ball:after,.fa-duotone.fa-futbol:after,.fa-duotone.fa-soccer-ball:after,.fad.fa-futbol-ball:after,.fad.fa-futbol:after,.fad.fa-soccer-ball:after{content:"\f1e3\f1e3"}.fa-duotone.fa-snow-blowing:after,.fad.fa-snow-blowing:after{content:"\f761\f761"}.fa-duotone.fa-paint-brush:after,.fa-duotone.fa-paintbrush:after,.fad.fa-paint-brush:after,.fad.fa-paintbrush:after{content:"\f1fc\f1fc"}.fa-duotone.fa-lock:after,.fad.fa-lock:after{content:"\f023\f023"}.fa-duotone.fa-arrow-down-from-line:after,.fa-duotone.fa-arrow-from-top:after,.fad.fa-arrow-down-from-line:after,.fad.fa-arrow-from-top:after{content:"\f345\f345"}.fa-duotone.fa-gas-pump:after,.fad.fa-gas-pump:after{content:"\f52f\f52f"}.fa-duotone.fa-signal-alt-slash:after,.fa-duotone.fa-signal-bars-slash:after,.fad.fa-signal-alt-slash:after,.fad.fa-signal-bars-slash:after{content:"\f694\f694"}.fa-duotone.fa-monkey:after,.fad.fa-monkey:after{content:"\f6fb\f6fb"}.fa-duotone.fa-pro:after,.fa-duotone.fa-rectangle-pro:after,.fad.fa-pro:after,.fad.fa-rectangle-pro:after{content:"\e235\e235"}.fa-duotone.fa-house-night:after,.fad.fa-house-night:after{content:"\e010\e010"}.fa-duotone.fa-hot-tub-person:after,.fa-duotone.fa-hot-tub:after,.fad.fa-hot-tub-person:after,.fad.fa-hot-tub:after{content:"\f593\f593"}.fa-duotone.fa-globe-pointer:after,.fad.fa-globe-pointer:after{content:"\e60e\e60e"}.fa-duotone.fa-blanket:after,.fad.fa-blanket:after{content:"\f498\f498"}.fa-duotone.fa-map-location:after,.fa-duotone.fa-map-marked:after,.fad.fa-map-location:after,.fad.fa-map-marked:after{content:"\f59f\f59f"}.fa-duotone.fa-house-flood-water:after,.fad.fa-house-flood-water:after{content:"\e50e\e50e"}.fa-duotone.fa-comments-question-check:after,.fad.fa-comments-question-check:after{content:"\e14f\e14f"}.fa-duotone.fa-tree:after,.fad.fa-tree:after{content:"\f1bb\f1bb"}.fa-duotone.fa-arrows-cross:after,.fad.fa-arrows-cross:after{content:"\e0a2\e0a2"}.fa-duotone.fa-backpack:after,.fad.fa-backpack:after{content:"\f5d4\f5d4"}.fa-duotone.fa-square-small:after,.fad.fa-square-small:after{content:"\e27e\e27e"}.fa-duotone.fa-folder-arrow-up:after,.fa-duotone.fa-folder-upload:after,.fad.fa-folder-arrow-up:after,.fad.fa-folder-upload:after{content:"\e054\e054"}.fa-duotone.fa-bridge-lock:after,.fad.fa-bridge-lock:after{content:"\e4cc\e4cc"}.fa-duotone.fa-crosshairs-simple:after,.fad.fa-crosshairs-simple:after{content:"\e59f\e59f"}.fa-duotone.fa-sack-dollar:after,.fad.fa-sack-dollar:after{content:"\f81d\f81d"}.fa-duotone.fa-edit:after,.fa-duotone.fa-pen-to-square:after,.fad.fa-edit:after,.fad.fa-pen-to-square:after{content:"\f044\f044"}.fa-duotone.fa-sliders-h-square:after,.fa-duotone.fa-square-sliders:after,.fad.fa-sliders-h-square:after,.fad.fa-square-sliders:after{content:"\f3f0\f3f0"}.fa-duotone.fa-car-side:after,.fad.fa-car-side:after{content:"\f5e4\f5e4"}.fa-duotone.fa-comment-middle-top-alt:after,.fa-duotone.fa-message-middle-top:after,.fad.fa-comment-middle-top-alt:after,.fad.fa-message-middle-top:after{content:"\e1e2\e1e2"}.fa-duotone.fa-lightbulb-on:after,.fad.fa-lightbulb-on:after{content:"\f672\f672"}.fa-duotone.fa-knife:after,.fa-duotone.fa-utensil-knife:after,.fad.fa-knife:after,.fad.fa-utensil-knife:after{content:"\f2e4\f2e4"}.fa-duotone.fa-share-alt:after,.fa-duotone.fa-share-nodes:after,.fad.fa-share-alt:after,.fad.fa-share-nodes:after{content:"\f1e0\f1e0"}.fa-duotone.fa-display-chart-up-circle-dollar:after,.fad.fa-display-chart-up-circle-dollar:after{content:"\e5e6\e5e6"}.fa-duotone.fa-wave-sine:after,.fad.fa-wave-sine:after{content:"\f899\f899"}.fa-duotone.fa-heart-circle-minus:after,.fad.fa-heart-circle-minus:after{content:"\e4ff\e4ff"}.fa-duotone.fa-circle-w:after,.fad.fa-circle-w:after{content:"\e12c\e12c"}.fa-duotone.fa-calendar-circle:after,.fa-duotone.fa-circle-calendar:after,.fad.fa-calendar-circle:after,.fad.fa-circle-calendar:after{content:"\e102\e102"}.fa-duotone.fa-hourglass-2:after,.fa-duotone.fa-hourglass-half:after,.fad.fa-hourglass-2:after,.fad.fa-hourglass-half:after{content:"\f252\f252"}.fa-duotone.fa-microscope:after,.fad.fa-microscope:after{content:"\f610\f610"}.fa-duotone.fa-sunset:after,.fad.fa-sunset:after{content:"\f767\f767"}.fa-duotone.fa-sink:after,.fad.fa-sink:after{content:"\e06d\e06d"}.fa-duotone.fa-calendar-exclamation:after,.fad.fa-calendar-exclamation:after{content:"\f334\f334"}.fa-duotone.fa-truck-container-empty:after,.fad.fa-truck-container-empty:after{content:"\e2b5\e2b5"}.fa-duotone.fa-hand-heart:after,.fad.fa-hand-heart:after{content:"\f4bc\f4bc"}.fa-duotone.fa-bag-shopping:after,.fa-duotone.fa-shopping-bag:after,.fad.fa-bag-shopping:after,.fad.fa-shopping-bag:after{content:"\f290\f290"}.fa-duotone.fa-arrow-down-z-a:after,.fa-duotone.fa-sort-alpha-desc:after,.fa-duotone.fa-sort-alpha-down-alt:after,.fad.fa-arrow-down-z-a:after,.fad.fa-sort-alpha-desc:after,.fad.fa-sort-alpha-down-alt:after{content:"\f881\f881"}.fa-duotone.fa-mitten:after,.fad.fa-mitten:after{content:"\f7b5\f7b5"}.fa-duotone.fa-reply-clock:after,.fa-duotone.fa-reply-time:after,.fad.fa-reply-clock:after,.fad.fa-reply-time:after{content:"\e239\e239"}.fa-duotone.fa-person-rays:after,.fad.fa-person-rays:after{content:"\e54d\e54d"}.fa-duotone.fa-arrow-alt-right:after,.fa-duotone.fa-right:after,.fad.fa-arrow-alt-right:after,.fad.fa-right:after{content:"\f356\f356"}.fa-duotone.fa-circle-f:after,.fad.fa-circle-f:after{content:"\e10e\e10e"}.fa-duotone.fa-users:after,.fad.fa-users:after{content:"\f0c0\f0c0"}.fa-duotone.fa-face-pleading:after,.fad.fa-face-pleading:after{content:"\e386\e386"}.fa-duotone.fa-eye-slash:after,.fad.fa-eye-slash:after{content:"\f070\f070"}.fa-duotone.fa-flask-vial:after,.fad.fa-flask-vial:after{content:"\e4f3\e4f3"}.fa-duotone.fa-police-box:after,.fad.fa-police-box:after{content:"\e021\e021"}.fa-duotone.fa-cucumber:after,.fad.fa-cucumber:after{content:"\e401\e401"}.fa-duotone.fa-head-side-brain:after,.fad.fa-head-side-brain:after{content:"\f808\f808"}.fa-duotone.fa-hand-paper:after,.fa-duotone.fa-hand:after,.fad.fa-hand-paper:after,.fad.fa-hand:after{content:"\f256\f256"}.fa-duotone.fa-biking-mountain:after,.fa-duotone.fa-person-biking-mountain:after,.fad.fa-biking-mountain:after,.fad.fa-person-biking-mountain:after{content:"\f84b\f84b"}.fa-duotone.fa-utensils-slash:after,.fad.fa-utensils-slash:after{content:"\e464\e464"}.fa-duotone.fa-print-magnifying-glass:after,.fa-duotone.fa-print-search:after,.fad.fa-print-magnifying-glass:after,.fad.fa-print-search:after{content:"\f81a\f81a"}.fa-duotone.fa-turn-right:after,.fad.fa-turn-right:after{content:"\e639\e639"}.fa-duotone.fa-folder-bookmark:after,.fad.fa-folder-bookmark:after{content:"\e186\e186"}.fa-duotone.fa-arrow-turn-left-down:after,.fad.fa-arrow-turn-left-down:after{content:"\e633\e633"}.fa-duotone.fa-om:after,.fad.fa-om:after{content:"\f679\f679"}.fa-duotone.fa-pi:after,.fad.fa-pi:after{content:"\f67e\f67e"}.fa-duotone.fa-flask-potion:after,.fa-duotone.fa-flask-round-potion:after,.fad.fa-flask-potion:after,.fad.fa-flask-round-potion:after{content:"\f6e1\f6e1"}.fa-duotone.fa-face-shush:after,.fad.fa-face-shush:after{content:"\e38c\e38c"}.fa-duotone.fa-worm:after,.fad.fa-worm:after{content:"\e599\e599"}.fa-duotone.fa-house-circle-xmark:after,.fad.fa-house-circle-xmark:after{content:"\e50b\e50b"}.fa-duotone.fa-plug:after,.fad.fa-plug:after{content:"\f1e6\f1e6"}.fa-duotone.fa-calendar-circle-exclamation:after,.fad.fa-calendar-circle-exclamation:after{content:"\e46e\e46e"}.fa-duotone.fa-square-i:after,.fad.fa-square-i:after{content:"\e272\e272"}.fa-duotone.fa-chevron-up:after,.fad.fa-chevron-up:after{content:"\f077\f077"}.fa-duotone.fa-face-saluting:after,.fad.fa-face-saluting:after{content:"\e484\e484"}.fa-duotone.fa-gauge-simple-low:after,.fa-duotone.fa-tachometer-slow:after,.fad.fa-gauge-simple-low:after,.fad.fa-tachometer-slow:after{content:"\f62c\f62c"}.fa-duotone.fa-face-persevering:after,.fad.fa-face-persevering:after{content:"\e385\e385"}.fa-duotone.fa-camera-circle:after,.fa-duotone.fa-circle-camera:after,.fad.fa-camera-circle:after,.fad.fa-circle-camera:after{content:"\e103\e103"}.fa-duotone.fa-hand-spock:after,.fad.fa-hand-spock:after{content:"\f259\f259"}.fa-duotone.fa-spider-web:after,.fad.fa-spider-web:after{content:"\f719\f719"}.fa-duotone.fa-circle-microphone:after,.fa-duotone.fa-microphone-circle:after,.fad.fa-circle-microphone:after,.fad.fa-microphone-circle:after{content:"\e116\e116"}.fa-duotone.fa-book-arrow-up:after,.fad.fa-book-arrow-up:after{content:"\e0ba\e0ba"}.fa-duotone.fa-popsicle:after,.fad.fa-popsicle:after{content:"\e43e\e43e"}.fa-duotone.fa-command:after,.fad.fa-command:after{content:"\e142\e142"}.fa-duotone.fa-blinds:after,.fad.fa-blinds:after{content:"\f8fb\f8fb"}.fa-duotone.fa-stopwatch:after,.fad.fa-stopwatch:after{content:"\f2f2\f2f2"}.fa-duotone.fa-saxophone:after,.fad.fa-saxophone:after{content:"\f8dc\f8dc"}.fa-duotone.fa-square-2:after,.fad.fa-square-2:after{content:"\e257\e257"}.fa-duotone.fa-field-hockey-stick-ball:after,.fa-duotone.fa-field-hockey:after,.fad.fa-field-hockey-stick-ball:after,.fad.fa-field-hockey:after{content:"\f44c\f44c"}.fa-duotone.fa-arrow-up-square-triangle:after,.fa-duotone.fa-sort-shapes-up-alt:after,.fad.fa-arrow-up-square-triangle:after,.fad.fa-sort-shapes-up-alt:after{content:"\f88b\f88b"}.fa-duotone.fa-face-scream:after,.fad.fa-face-scream:after{content:"\e38b\e38b"}.fa-duotone.fa-square-m:after,.fad.fa-square-m:after{content:"\e276\e276"}.fa-duotone.fa-camera-web:after,.fa-duotone.fa-webcam:after,.fad.fa-camera-web:after,.fad.fa-webcam:after{content:"\f832\f832"}.fa-duotone.fa-comment-arrow-down:after,.fad.fa-comment-arrow-down:after{content:"\e143\e143"}.fa-duotone.fa-lightbulb-cfl:after,.fad.fa-lightbulb-cfl:after{content:"\e5a6\e5a6"}.fa-duotone.fa-window-frame-open:after,.fad.fa-window-frame-open:after{content:"\e050\e050"}.fa-duotone.fa-face-kiss:after,.fa-duotone.fa-kiss:after,.fad.fa-face-kiss:after,.fad.fa-kiss:after{content:"\f596\f596"}.fa-duotone.fa-bridge-circle-xmark:after,.fad.fa-bridge-circle-xmark:after{content:"\e4cb\e4cb"}.fa-duotone.fa-period:after,.fad.fa-period:after{content:"\2e\2e"}.fa-duotone.fa-face-grin-tongue:after,.fa-duotone.fa-grin-tongue:after,.fad.fa-face-grin-tongue:after,.fad.fa-grin-tongue:after{content:"\f589\f589"}.fa-duotone.fa-up-to-dotted-line:after,.fad.fa-up-to-dotted-line:after{content:"\e457\e457"}.fa-duotone.fa-thought-bubble:after,.fad.fa-thought-bubble:after{content:"\e32e\e32e"}.fa-duotone.fa-skeleton-ribs:after,.fad.fa-skeleton-ribs:after{content:"\e5cb\e5cb"}.fa-duotone.fa-raygun:after,.fad.fa-raygun:after{content:"\e025\e025"}.fa-duotone.fa-flute:after,.fad.fa-flute:after{content:"\f8b9\f8b9"}.fa-duotone.fa-acorn:after,.fad.fa-acorn:after{content:"\f6ae\f6ae"}.fa-duotone.fa-video-arrow-up-right:after,.fad.fa-video-arrow-up-right:after{content:"\e2c9\e2c9"}.fa-duotone.fa-grate-droplet:after,.fad.fa-grate-droplet:after{content:"\e194\e194"}.fa-duotone.fa-seal-exclamation:after,.fad.fa-seal-exclamation:after{content:"\e242\e242"}.fa-duotone.fa-chess-bishop:after,.fad.fa-chess-bishop:after{content:"\f43a\f43a"}.fa-duotone.fa-message-sms:after,.fad.fa-message-sms:after{content:"\e1e5\e1e5"}.fa-duotone.fa-coffee-beans:after,.fad.fa-coffee-beans:after{content:"\e13f\e13f"}.fa-duotone.fa-hat-witch:after,.fad.fa-hat-witch:after{content:"\f6e7\f6e7"}.fa-duotone.fa-face-grin-wink:after,.fa-duotone.fa-grin-wink:after,.fad.fa-face-grin-wink:after,.fad.fa-grin-wink:after{content:"\f58c\f58c"}.fa-duotone.fa-clock-three-thirty:after,.fad.fa-clock-three-thirty:after{content:"\e357\e357"}.fa-duotone.fa-deaf:after,.fa-duotone.fa-deafness:after,.fa-duotone.fa-ear-deaf:after,.fa-duotone.fa-hard-of-hearing:after,.fad.fa-deaf:after,.fad.fa-deafness:after,.fad.fa-ear-deaf:after,.fad.fa-hard-of-hearing:after{content:"\f2a4\f2a4"}.fa-duotone.fa-alarm-clock:after,.fad.fa-alarm-clock:after{content:"\f34e\f34e"}.fa-duotone.fa-eclipse:after,.fad.fa-eclipse:after{content:"\f749\f749"}.fa-duotone.fa-face-relieved:after,.fad.fa-face-relieved:after{content:"\e389\e389"}.fa-duotone.fa-road-circle-check:after,.fad.fa-road-circle-check:after{content:"\e564\e564"}.fa-duotone.fa-dice-five:after,.fad.fa-dice-five:after{content:"\f523\f523"}.fa-duotone.fa-minus-octagon:after,.fa-duotone.fa-octagon-minus:after,.fad.fa-minus-octagon:after,.fad.fa-octagon-minus:after{content:"\f308\f308"}.fa-duotone.fa-rss-square:after,.fa-duotone.fa-square-rss:after,.fad.fa-rss-square:after,.fad.fa-square-rss:after{content:"\f143\f143"}.fa-duotone.fa-face-zany:after,.fad.fa-face-zany:after{content:"\e3a4\e3a4"}.fa-duotone.fa-tricycle:after,.fad.fa-tricycle:after{content:"\e5c3\e5c3"}.fa-duotone.fa-land-mine-on:after,.fad.fa-land-mine-on:after{content:"\e51b\e51b"}.fa-duotone.fa-square-arrow-up-left:after,.fad.fa-square-arrow-up-left:after{content:"\e263\e263"}.fa-duotone.fa-i-cursor:after,.fad.fa-i-cursor:after{content:"\f246\f246"}.fa-duotone.fa-chart-mixed-up-circle-dollar:after,.fad.fa-chart-mixed-up-circle-dollar:after{content:"\e5d9\e5d9"}.fa-duotone.fa-salt-shaker:after,.fad.fa-salt-shaker:after{content:"\e446\e446"}.fa-duotone.fa-stamp:after,.fad.fa-stamp:after{content:"\f5bf\f5bf"}.fa-duotone.fa-file-plus:after,.fad.fa-file-plus:after{content:"\f319\f319"}.fa-duotone.fa-draw-square:after,.fad.fa-draw-square:after{content:"\f5ef\f5ef"}.fa-duotone.fa-toilet-paper-reverse-slash:after,.fa-duotone.fa-toilet-paper-under-slash:after,.fad.fa-toilet-paper-reverse-slash:after,.fad.fa-toilet-paper-under-slash:after{content:"\e2a1\e2a1"}.fa-duotone.fa-stairs:after,.fad.fa-stairs:after{content:"\e289\e289"}.fa-duotone.fa-drone-alt:after,.fa-duotone.fa-drone-front:after,.fad.fa-drone-alt:after,.fad.fa-drone-front:after{content:"\f860\f860"}.fa-duotone.fa-glass-empty:after,.fad.fa-glass-empty:after{content:"\e191\e191"}.fa-duotone.fa-dial-high:after,.fad.fa-dial-high:after{content:"\e15c\e15c"}.fa-duotone.fa-user-construction:after,.fa-duotone.fa-user-hard-hat:after,.fa-duotone.fa-user-helmet-safety:after,.fad.fa-user-construction:after,.fad.fa-user-hard-hat:after,.fad.fa-user-helmet-safety:after{content:"\f82c\f82c"}.fa-duotone.fa-i:after,.fad.fa-i:after{content:"\49\49"}.fa-duotone.fa-hryvnia-sign:after,.fa-duotone.fa-hryvnia:after,.fad.fa-hryvnia-sign:after,.fad.fa-hryvnia:after{content:"\f6f2\f6f2"}.fa-duotone.fa-arrow-down-left-and-arrow-up-right-to-center:after,.fad.fa-arrow-down-left-and-arrow-up-right-to-center:after{content:"\e092\e092"}.fa-duotone.fa-pills:after,.fad.fa-pills:after{content:"\f484\f484"}.fa-duotone.fa-face-grin-wide:after,.fa-duotone.fa-grin-alt:after,.fad.fa-face-grin-wide:after,.fad.fa-grin-alt:after{content:"\f581\f581"}.fa-duotone.fa-tooth:after,.fad.fa-tooth:after{content:"\f5c9\f5c9"}.fa-duotone.fa-basketball-hoop:after,.fad.fa-basketball-hoop:after{content:"\f435\f435"}.fa-duotone.fa-objects-align-bottom:after,.fad.fa-objects-align-bottom:after{content:"\e3bb\e3bb"}.fa-duotone.fa-v:after,.fad.fa-v:after{content:"\56\56"}.fa-duotone.fa-sparkles:after,.fad.fa-sparkles:after{content:"\f890\f890"}.fa-duotone.fa-squid:after,.fad.fa-squid:after{content:"\e450\e450"}.fa-duotone.fa-leafy-green:after,.fad.fa-leafy-green:after{content:"\e41d\e41d"}.fa-duotone.fa-circle-arrow-up-right:after,.fad.fa-circle-arrow-up-right:after{content:"\e0fc\e0fc"}.fa-duotone.fa-calendars:after,.fad.fa-calendars:after{content:"\e0d7\e0d7"}.fa-duotone.fa-bangladeshi-taka-sign:after,.fad.fa-bangladeshi-taka-sign:after{content:"\e2e6\e2e6"}.fa-duotone.fa-bicycle:after,.fad.fa-bicycle:after{content:"\f206\f206"}.fa-duotone.fa-hammer-war:after,.fad.fa-hammer-war:after{content:"\f6e4\f6e4"}.fa-duotone.fa-circle-d:after,.fad.fa-circle-d:after{content:"\e104\e104"}.fa-duotone.fa-spider-black-widow:after,.fad.fa-spider-black-widow:after{content:"\f718\f718"}.fa-duotone.fa-rod-asclepius:after,.fa-duotone.fa-rod-snake:after,.fa-duotone.fa-staff-aesculapius:after,.fa-duotone.fa-staff-snake:after,.fad.fa-rod-asclepius:after,.fad.fa-rod-snake:after,.fad.fa-staff-aesculapius:after,.fad.fa-staff-snake:after{content:"\e579\e579"}.fa-duotone.fa-pear:after,.fad.fa-pear:after{content:"\e20c\e20c"}.fa-duotone.fa-head-side-cough-slash:after,.fad.fa-head-side-cough-slash:after{content:"\e062\e062"}.fa-duotone.fa-file-mov:after,.fad.fa-file-mov:after{content:"\e647\e647"}.fa-duotone.fa-triangle:after,.fad.fa-triangle:after{content:"\f2ec\f2ec"}.fa-duotone.fa-apartment:after,.fad.fa-apartment:after{content:"\e468\e468"}.fa-duotone.fa-ambulance:after,.fa-duotone.fa-truck-medical:after,.fad.fa-ambulance:after,.fad.fa-truck-medical:after{content:"\f0f9\f0f9"}.fa-duotone.fa-pepper:after,.fad.fa-pepper:after{content:"\e432\e432"}.fa-duotone.fa-piano:after,.fad.fa-piano:after{content:"\f8d4\f8d4"}.fa-duotone.fa-gun-squirt:after,.fad.fa-gun-squirt:after{content:"\e19d\e19d"}.fa-duotone.fa-wheat-awn-circle-exclamation:after,.fad.fa-wheat-awn-circle-exclamation:after{content:"\e598\e598"}.fa-duotone.fa-snowman:after,.fad.fa-snowman:after{content:"\f7d0\f7d0"}.fa-duotone.fa-user-alien:after,.fad.fa-user-alien:after{content:"\e04a\e04a"}.fa-duotone.fa-shield-check:after,.fad.fa-shield-check:after{content:"\f2f7\f2f7"}.fa-duotone.fa-mortar-pestle:after,.fad.fa-mortar-pestle:after{content:"\f5a7\f5a7"}.fa-duotone.fa-road-barrier:after,.fad.fa-road-barrier:after{content:"\e562\e562"}.fa-duotone.fa-chart-candlestick:after,.fad.fa-chart-candlestick:after{content:"\e0e2\e0e2"}.fa-duotone.fa-briefcase-blank:after,.fad.fa-briefcase-blank:after{content:"\e0c8\e0c8"}.fa-duotone.fa-school:after,.fad.fa-school:after{content:"\f549\f549"}.fa-duotone.fa-igloo:after,.fad.fa-igloo:after{content:"\f7ae\f7ae"}.fa-duotone.fa-bracket-round:after,.fa-duotone.fa-parenthesis:after,.fad.fa-bracket-round:after,.fad.fa-parenthesis:after{content:"\28\28"}.fa-duotone.fa-joint:after,.fad.fa-joint:after{content:"\f595\f595"}.fa-duotone.fa-horse-saddle:after,.fad.fa-horse-saddle:after{content:"\f8c3\f8c3"}.fa-duotone.fa-mug-marshmallows:after,.fad.fa-mug-marshmallows:after{content:"\f7b7\f7b7"}.fa-duotone.fa-filters:after,.fad.fa-filters:after{content:"\e17e\e17e"}.fa-duotone.fa-bell-on:after,.fad.fa-bell-on:after{content:"\f8fa\f8fa"}.fa-duotone.fa-angle-right:after,.fad.fa-angle-right:after{content:"\f105\f105"}.fa-duotone.fa-dial-med:after,.fad.fa-dial-med:after{content:"\e15f\e15f"}.fa-duotone.fa-horse:after,.fad.fa-horse:after{content:"\f6f0\f6f0"}.fa-duotone.fa-q:after,.fad.fa-q:after{content:"\51\51"}.fa-duotone.fa-monitor-heart-rate:after,.fa-duotone.fa-monitor-waveform:after,.fad.fa-monitor-heart-rate:after,.fad.fa-monitor-waveform:after{content:"\f611\f611"}.fa-duotone.fa-link-simple:after,.fad.fa-link-simple:after{content:"\e1cd\e1cd"}.fa-duotone.fa-whistle:after,.fad.fa-whistle:after{content:"\f460\f460"}.fa-duotone.fa-g:after,.fad.fa-g:after{content:"\47\47"}.fa-duotone.fa-fragile:after,.fa-duotone.fa-wine-glass-crack:after,.fad.fa-fragile:after,.fad.fa-wine-glass-crack:after{content:"\f4bb\f4bb"}.fa-duotone.fa-slot-machine:after,.fad.fa-slot-machine:after{content:"\e3ce\e3ce"}.fa-duotone.fa-notes-medical:after,.fad.fa-notes-medical:after{content:"\f481\f481"}.fa-duotone.fa-car-wash:after,.fad.fa-car-wash:after{content:"\f5e6\f5e6"}.fa-duotone.fa-escalator:after,.fad.fa-escalator:after{content:"\e171\e171"}.fa-duotone.fa-comment-image:after,.fad.fa-comment-image:after{content:"\e148\e148"}.fa-duotone.fa-temperature-2:after,.fa-duotone.fa-temperature-half:after,.fa-duotone.fa-thermometer-2:after,.fa-duotone.fa-thermometer-half:after,.fad.fa-temperature-2:after,.fad.fa-temperature-half:after,.fad.fa-thermometer-2:after,.fad.fa-thermometer-half:after{content:"\f2c9\f2c9"}.fa-duotone.fa-dong-sign:after,.fad.fa-dong-sign:after{content:"\e169\e169"}.fa-duotone.fa-donut:after,.fa-duotone.fa-doughnut:after,.fad.fa-donut:after,.fad.fa-doughnut:after{content:"\e406\e406"}.fa-duotone.fa-capsules:after,.fad.fa-capsules:after{content:"\f46b\f46b"}.fa-duotone.fa-poo-bolt:after,.fa-duotone.fa-poo-storm:after,.fad.fa-poo-bolt:after,.fad.fa-poo-storm:after{content:"\f75a\f75a"}.fa-duotone.fa-tally-1:after,.fad.fa-tally-1:after{content:"\e294\e294"}.fa-duotone.fa-file-vector:after,.fad.fa-file-vector:after{content:"\e64c\e64c"}.fa-duotone.fa-face-frown-open:after,.fa-duotone.fa-frown-open:after,.fad.fa-face-frown-open:after,.fad.fa-frown-open:after{content:"\f57a\f57a"}.fa-duotone.fa-square-dashed:after,.fad.fa-square-dashed:after{content:"\e269\e269"}.fa-duotone.fa-bag-shopping-plus:after,.fad.fa-bag-shopping-plus:after{content:"\e651\e651"}.fa-duotone.fa-square-j:after,.fad.fa-square-j:after{content:"\e273\e273"}.fa-duotone.fa-hand-point-up:after,.fad.fa-hand-point-up:after{content:"\f0a6\f0a6"}.fa-duotone.fa-money-bill:after,.fad.fa-money-bill:after{content:"\f0d6\f0d6"}.fa-duotone.fa-arrow-up-big-small:after,.fa-duotone.fa-sort-size-up:after,.fad.fa-arrow-up-big-small:after,.fad.fa-sort-size-up:after{content:"\f88e\f88e"}.fa-duotone.fa-barcode-read:after,.fad.fa-barcode-read:after{content:"\f464\f464"}.fa-duotone.fa-baguette:after,.fad.fa-baguette:after{content:"\e3d8\e3d8"}.fa-duotone.fa-bowl-soft-serve:after,.fad.fa-bowl-soft-serve:after{content:"\e46b\e46b"}.fa-duotone.fa-face-holding-back-tears:after,.fad.fa-face-holding-back-tears:after{content:"\e482\e482"}.fa-duotone.fa-arrow-alt-square-up:after,.fa-duotone.fa-square-up:after,.fad.fa-arrow-alt-square-up:after,.fad.fa-square-up:after{content:"\f353\f353"}.fa-duotone.fa-subway-tunnel:after,.fa-duotone.fa-train-subway-tunnel:after,.fad.fa-subway-tunnel:after,.fad.fa-train-subway-tunnel:after{content:"\e2a3\e2a3"}.fa-duotone.fa-exclamation-square:after,.fa-duotone.fa-square-exclamation:after,.fad.fa-exclamation-square:after,.fad.fa-square-exclamation:after{content:"\f321\f321"}.fa-duotone.fa-semicolon:after,.fad.fa-semicolon:after{content:"\3b\3b"}.fa-duotone.fa-bookmark:after,.fad.fa-bookmark:after{content:"\f02e\f02e"}.fa-duotone.fa-fan-table:after,.fad.fa-fan-table:after{content:"\e004\e004"}.fa-duotone.fa-align-justify:after,.fad.fa-align-justify:after{content:"\f039\f039"}.fa-duotone.fa-battery-1:after,.fa-duotone.fa-battery-low:after,.fad.fa-battery-1:after,.fad.fa-battery-low:after{content:"\e0b1\e0b1"}.fa-duotone.fa-credit-card-front:after,.fad.fa-credit-card-front:after{content:"\f38a\f38a"}.fa-duotone.fa-brain-arrow-curved-right:after,.fa-duotone.fa-mind-share:after,.fad.fa-brain-arrow-curved-right:after,.fad.fa-mind-share:after{content:"\f677\f677"}.fa-duotone.fa-umbrella-beach:after,.fad.fa-umbrella-beach:after{content:"\f5ca\f5ca"}.fa-duotone.fa-helmet-un:after,.fad.fa-helmet-un:after{content:"\e503\e503"}.fa-duotone.fa-location-smile:after,.fa-duotone.fa-map-marker-smile:after,.fad.fa-location-smile:after,.fad.fa-map-marker-smile:after{content:"\f60d\f60d"}.fa-duotone.fa-arrow-left-to-line:after,.fa-duotone.fa-arrow-to-left:after,.fad.fa-arrow-left-to-line:after,.fad.fa-arrow-to-left:after{content:"\f33e\f33e"}.fa-duotone.fa-bullseye:after,.fad.fa-bullseye:after{content:"\f140\f140"}.fa-duotone.fa-nigiri:after,.fa-duotone.fa-sushi:after,.fad.fa-nigiri:after,.fad.fa-sushi:after{content:"\e48a\e48a"}.fa-duotone.fa-comment-alt-captions:after,.fa-duotone.fa-message-captions:after,.fad.fa-comment-alt-captions:after,.fad.fa-message-captions:after{content:"\e1de\e1de"}.fa-duotone.fa-trash-list:after,.fad.fa-trash-list:after{content:"\e2b1\e2b1"}.fa-duotone.fa-bacon:after,.fad.fa-bacon:after{content:"\f7e5\f7e5"}.fa-duotone.fa-option:after,.fad.fa-option:after{content:"\e318\e318"}.fa-duotone.fa-raccoon:after,.fad.fa-raccoon:after{content:"\e613\e613"}.fa-duotone.fa-hand-point-down:after,.fad.fa-hand-point-down:after{content:"\f0a7\f0a7"}.fa-duotone.fa-arrow-up-from-bracket:after,.fad.fa-arrow-up-from-bracket:after{content:"\e09a\e09a"}.fa-duotone.fa-head-side-gear:after,.fad.fa-head-side-gear:after{content:"\e611\e611"}.fa-duotone.fa-trash-plus:after,.fad.fa-trash-plus:after{content:"\e2b2\e2b2"}.fa-duotone.fa-file-cad:after,.fad.fa-file-cad:after{content:"\e672\e672"}.fa-duotone.fa-objects-align-top:after,.fad.fa-objects-align-top:after{content:"\e3c0\e3c0"}.fa-duotone.fa-folder-blank:after,.fa-duotone.fa-folder:after,.fad.fa-folder-blank:after,.fad.fa-folder:after{content:"\f07b\f07b"}.fa-duotone.fa-face-anxious-sweat:after,.fad.fa-face-anxious-sweat:after{content:"\e36a\e36a"}.fa-duotone.fa-credit-card-blank:after,.fad.fa-credit-card-blank:after{content:"\f389\f389"}.fa-duotone.fa-file-medical-alt:after,.fa-duotone.fa-file-waveform:after,.fad.fa-file-medical-alt:after,.fad.fa-file-waveform:after{content:"\f478\f478"}.fa-duotone.fa-microchip-ai:after,.fad.fa-microchip-ai:after{content:"\e1ec\e1ec"}.fa-duotone.fa-mug:after,.fad.fa-mug:after{content:"\f874\f874"}.fa-duotone.fa-plane-up-slash:after,.fad.fa-plane-up-slash:after{content:"\e22e\e22e"}.fa-duotone.fa-radiation:after,.fad.fa-radiation:after{content:"\f7b9\f7b9"}.fa-duotone.fa-pen-circle:after,.fad.fa-pen-circle:after{content:"\e20e\e20e"}.fa-duotone.fa-bag-seedling:after,.fad.fa-bag-seedling:after{content:"\e5f2\e5f2"}.fa-duotone.fa-chart-simple:after,.fad.fa-chart-simple:after{content:"\e473\e473"}.fa-duotone.fa-crutches:after,.fad.fa-crutches:after{content:"\f7f8\f7f8"}.fa-duotone.fa-circle-parking:after,.fa-duotone.fa-parking-circle:after,.fad.fa-circle-parking:after,.fad.fa-parking-circle:after{content:"\f615\f615"}.fa-duotone.fa-mars-stroke:after,.fad.fa-mars-stroke:after{content:"\f229\f229"}.fa-duotone.fa-leaf-oak:after,.fad.fa-leaf-oak:after{content:"\f6f7\f6f7"}.fa-duotone.fa-square-bolt:after,.fad.fa-square-bolt:after{content:"\e265\e265"}.fa-duotone.fa-vial:after,.fad.fa-vial:after{content:"\f492\f492"}.fa-duotone.fa-dashboard:after,.fa-duotone.fa-gauge-med:after,.fa-duotone.fa-gauge:after,.fa-duotone.fa-tachometer-alt-average:after,.fad.fa-dashboard:after,.fad.fa-gauge-med:after,.fad.fa-gauge:after,.fad.fa-tachometer-alt-average:after{content:"\f624\f624"}.fa-duotone.fa-magic-wand-sparkles:after,.fa-duotone.fa-wand-magic-sparkles:after,.fad.fa-magic-wand-sparkles:after,.fad.fa-wand-magic-sparkles:after{content:"\e2ca\e2ca"}.fa-duotone.fa-lambda:after,.fad.fa-lambda:after{content:"\f66e\f66e"}.fa-duotone.fa-e:after,.fad.fa-e:after{content:"\45\45"}.fa-duotone.fa-pizza:after,.fad.fa-pizza:after{content:"\f817\f817"}.fa-duotone.fa-bowl-chopsticks-noodles:after,.fad.fa-bowl-chopsticks-noodles:after{content:"\e2ea\e2ea"}.fa-duotone.fa-h3:after,.fad.fa-h3:after{content:"\f315\f315"}.fa-duotone.fa-pen-alt:after,.fa-duotone.fa-pen-clip:after,.fad.fa-pen-alt:after,.fad.fa-pen-clip:after{content:"\f305\f305"}.fa-duotone.fa-bridge-circle-exclamation:after,.fad.fa-bridge-circle-exclamation:after{content:"\e4ca\e4ca"}.fa-duotone.fa-badge-percent:after,.fad.fa-badge-percent:after{content:"\f646\f646"}.fa-duotone.fa-rotate-reverse:after,.fad.fa-rotate-reverse:after{content:"\e631\e631"}.fa-duotone.fa-user:after,.fad.fa-user:after{content:"\f007\f007"}.fa-duotone.fa-sensor:after,.fad.fa-sensor:after{content:"\e028\e028"}.fa-duotone.fa-comma:after,.fad.fa-comma:after{content:"\2c\2c"}.fa-duotone.fa-school-circle-check:after,.fad.fa-school-circle-check:after{content:"\e56b\e56b"}.fa-duotone.fa-toilet-paper-reverse:after,.fa-duotone.fa-toilet-paper-under:after,.fad.fa-toilet-paper-reverse:after,.fad.fa-toilet-paper-under:after{content:"\e2a0\e2a0"}.fa-duotone.fa-light-emergency:after,.fad.fa-light-emergency:after{content:"\e41f\e41f"}.fa-duotone.fa-arrow-down-to-arc:after,.fad.fa-arrow-down-to-arc:after{content:"\e4ae\e4ae"}.fa-duotone.fa-dumpster:after,.fad.fa-dumpster:after{content:"\f793\f793"}.fa-duotone.fa-shuttle-van:after,.fa-duotone.fa-van-shuttle:after,.fad.fa-shuttle-van:after,.fad.fa-van-shuttle:after{content:"\f5b6\f5b6"}.fa-duotone.fa-building-user:after,.fad.fa-building-user:after{content:"\e4da\e4da"}.fa-duotone.fa-light-switch:after,.fad.fa-light-switch:after{content:"\e017\e017"}.fa-duotone.fa-caret-square-left:after,.fa-duotone.fa-square-caret-left:after,.fad.fa-caret-square-left:after,.fad.fa-square-caret-left:after{content:"\f191\f191"}.fa-duotone.fa-highlighter:after,.fad.fa-highlighter:after{content:"\f591\f591"}.fa-duotone.fa-heart-rate:after,.fa-duotone.fa-wave-pulse:after,.fad.fa-heart-rate:after,.fad.fa-wave-pulse:after{content:"\f5f8\f5f8"}.fa-duotone.fa-key:after,.fad.fa-key:after{content:"\f084\f084"}.fa-duotone.fa-arrow-left-to-bracket:after,.fad.fa-arrow-left-to-bracket:after{content:"\e669\e669"}.fa-duotone.fa-hat-santa:after,.fad.fa-hat-santa:after{content:"\f7a7\f7a7"}.fa-duotone.fa-tamale:after,.fad.fa-tamale:after{content:"\e451\e451"}.fa-duotone.fa-box-check:after,.fad.fa-box-check:after{content:"\f467\f467"}.fa-duotone.fa-bullhorn:after,.fad.fa-bullhorn:after{content:"\f0a1\f0a1"}.fa-duotone.fa-steak:after,.fad.fa-steak:after{content:"\f824\f824"}.fa-duotone.fa-location-crosshairs-slash:after,.fa-duotone.fa-location-slash:after,.fad.fa-location-crosshairs-slash:after,.fad.fa-location-slash:after{content:"\f603\f603"}.fa-duotone.fa-person-dolly:after,.fad.fa-person-dolly:after{content:"\f4d0\f4d0"}.fa-duotone.fa-globe:after,.fad.fa-globe:after{content:"\f0ac\f0ac"}.fa-duotone.fa-synagogue:after,.fad.fa-synagogue:after{content:"\f69b\f69b"}.fa-duotone.fa-file-chart-column:after,.fa-duotone.fa-file-chart-line:after,.fad.fa-file-chart-column:after,.fad.fa-file-chart-line:after{content:"\f659\f659"}.fa-duotone.fa-person-half-dress:after,.fad.fa-person-half-dress:after{content:"\e548\e548"}.fa-duotone.fa-folder-image:after,.fad.fa-folder-image:after{content:"\e18a\e18a"}.fa-duotone.fa-calendar-edit:after,.fa-duotone.fa-calendar-pen:after,.fad.fa-calendar-edit:after,.fad.fa-calendar-pen:after{content:"\f333\f333"}.fa-duotone.fa-road-bridge:after,.fad.fa-road-bridge:after{content:"\e563\e563"}.fa-duotone.fa-face-smile-tear:after,.fad.fa-face-smile-tear:after{content:"\e393\e393"}.fa-duotone.fa-comment-alt-plus:after,.fa-duotone.fa-message-plus:after,.fad.fa-comment-alt-plus:after,.fad.fa-message-plus:after{content:"\f4a8\f4a8"}.fa-duotone.fa-location-arrow:after,.fad.fa-location-arrow:after{content:"\f124\f124"}.fa-duotone.fa-c:after,.fad.fa-c:after{content:"\43\43"}.fa-duotone.fa-tablet-button:after,.fad.fa-tablet-button:after{content:"\f10a\f10a"}.fa-duotone.fa-person-dress-fairy:after,.fad.fa-person-dress-fairy:after{content:"\e607\e607"}.fa-duotone.fa-rectangle-history-circle-user:after,.fad.fa-rectangle-history-circle-user:after{content:"\e4a4\e4a4"}.fa-duotone.fa-building-lock:after,.fad.fa-building-lock:after{content:"\e4d6\e4d6"}.fa-duotone.fa-chart-line-up:after,.fad.fa-chart-line-up:after{content:"\e0e5\e0e5"}.fa-duotone.fa-mailbox:after,.fad.fa-mailbox:after{content:"\f813\f813"}.fa-duotone.fa-sign-posts:after,.fad.fa-sign-posts:after{content:"\e625\e625"}.fa-duotone.fa-truck-bolt:after,.fad.fa-truck-bolt:after{content:"\e3d0\e3d0"}.fa-duotone.fa-pizza-slice:after,.fad.fa-pizza-slice:after{content:"\f818\f818"}.fa-duotone.fa-money-bill-wave:after,.fad.fa-money-bill-wave:after{content:"\f53a\f53a"}.fa-duotone.fa-area-chart:after,.fa-duotone.fa-chart-area:after,.fad.fa-area-chart:after,.fad.fa-chart-area:after{content:"\f1fe\f1fe"}.fa-duotone.fa-house-flag:after,.fad.fa-house-flag:after{content:"\e50d\e50d"}.fa-duotone.fa-circle-three-quarters-stroke:after,.fad.fa-circle-three-quarters-stroke:after{content:"\e5d4\e5d4"}.fa-duotone.fa-person-circle-minus:after,.fad.fa-person-circle-minus:after{content:"\e540\e540"}.fa-duotone.fa-scalpel:after,.fad.fa-scalpel:after{content:"\f61d\f61d"}.fa-duotone.fa-ban:after,.fa-duotone.fa-cancel:after,.fad.fa-ban:after,.fad.fa-cancel:after{content:"\f05e\f05e"}.fa-duotone.fa-bell-exclamation:after,.fad.fa-bell-exclamation:after{content:"\f848\f848"}.fa-duotone.fa-bookmark-circle:after,.fa-duotone.fa-circle-bookmark:after,.fad.fa-bookmark-circle:after,.fad.fa-circle-bookmark:after{content:"\e100\e100"}.fa-duotone.fa-egg-fried:after,.fad.fa-egg-fried:after{content:"\f7fc\f7fc"}.fa-duotone.fa-face-weary:after,.fad.fa-face-weary:after{content:"\e3a1\e3a1"}.fa-duotone.fa-uniform-martial-arts:after,.fad.fa-uniform-martial-arts:after{content:"\e3d1\e3d1"}.fa-duotone.fa-camera-rotate:after,.fad.fa-camera-rotate:after{content:"\e0d8\e0d8"}.fa-duotone.fa-sun-dust:after,.fad.fa-sun-dust:after{content:"\f764\f764"}.fa-duotone.fa-comment-text:after,.fad.fa-comment-text:after{content:"\e14d\e14d"}.fa-duotone.fa-air-freshener:after,.fa-duotone.fa-spray-can-sparkles:after,.fad.fa-air-freshener:after,.fad.fa-spray-can-sparkles:after{content:"\f5d0\f5d0"}.fa-duotone.fa-signal-alt-4:after,.fa-duotone.fa-signal-alt:after,.fa-duotone.fa-signal-bars-strong:after,.fa-duotone.fa-signal-bars:after,.fad.fa-signal-alt-4:after,.fad.fa-signal-alt:after,.fad.fa-signal-bars-strong:after,.fad.fa-signal-bars:after{content:"\f690\f690"}.fa-duotone.fa-diamond-exclamation:after,.fad.fa-diamond-exclamation:after{content:"\e405\e405"}.fa-duotone.fa-star:after,.fad.fa-star:after{content:"\f005\f005"}.fa-duotone.fa-dial-min:after,.fad.fa-dial-min:after{content:"\e161\e161"}.fa-duotone.fa-repeat:after,.fad.fa-repeat:after{content:"\f363\f363"}.fa-duotone.fa-cross:after,.fad.fa-cross:after{content:"\f654\f654"}.fa-duotone.fa-file-caret-down:after,.fa-duotone.fa-page-caret-down:after,.fad.fa-file-caret-down:after,.fad.fa-page-caret-down:after{content:"\e429\e429"}.fa-duotone.fa-box:after,.fad.fa-box:after{content:"\f466\f466"}.fa-duotone.fa-venus-mars:after,.fad.fa-venus-mars:after{content:"\f228\f228"}.fa-duotone.fa-clock-seven-thirty:after,.fad.fa-clock-seven-thirty:after{content:"\e351\e351"}.fa-duotone.fa-arrow-pointer:after,.fa-duotone.fa-mouse-pointer:after,.fad.fa-arrow-pointer:after,.fad.fa-mouse-pointer:after{content:"\f245\f245"}.fa-duotone.fa-clock-four-thirty:after,.fad.fa-clock-four-thirty:after{content:"\e34b\e34b"}.fa-duotone.fa-signal-alt-3:after,.fa-duotone.fa-signal-bars-good:after,.fad.fa-signal-alt-3:after,.fad.fa-signal-bars-good:after{content:"\f693\f693"}.fa-duotone.fa-cactus:after,.fad.fa-cactus:after{content:"\f8a7\f8a7"}.fa-duotone.fa-lightbulb-gear:after,.fad.fa-lightbulb-gear:after{content:"\e5fd\e5fd"}.fa-duotone.fa-expand-arrows-alt:after,.fa-duotone.fa-maximize:after,.fad.fa-expand-arrows-alt:after,.fad.fa-maximize:after{content:"\f31e\f31e"}.fa-duotone.fa-charging-station:after,.fad.fa-charging-station:after{content:"\f5e7\f5e7"}.fa-duotone.fa-shapes:after,.fa-duotone.fa-triangle-circle-square:after,.fad.fa-shapes:after,.fad.fa-triangle-circle-square:after{content:"\f61f\f61f"}.fa-duotone.fa-plane-tail:after,.fad.fa-plane-tail:after{content:"\e22c\e22c"}.fa-duotone.fa-gauge-simple-max:after,.fa-duotone.fa-tachometer-fastest:after,.fad.fa-gauge-simple-max:after,.fad.fa-tachometer-fastest:after{content:"\f62b\f62b"}.fa-duotone.fa-circle-u:after,.fad.fa-circle-u:after{content:"\e127\e127"}.fa-duotone.fa-shield-slash:after,.fad.fa-shield-slash:after{content:"\e24b\e24b"}.fa-duotone.fa-phone-square-down:after,.fa-duotone.fa-square-phone-hangup:after,.fad.fa-phone-square-down:after,.fad.fa-square-phone-hangup:after{content:"\e27a\e27a"}.fa-duotone.fa-arrow-up-left:after,.fad.fa-arrow-up-left:after{content:"\e09d\e09d"}.fa-duotone.fa-transporter-1:after,.fad.fa-transporter-1:after{content:"\e043\e043"}.fa-duotone.fa-peanuts:after,.fad.fa-peanuts:after{content:"\e431\e431"}.fa-duotone.fa-random:after,.fa-duotone.fa-shuffle:after,.fad.fa-random:after,.fad.fa-shuffle:after{content:"\f074\f074"}.fa-duotone.fa-person-running:after,.fa-duotone.fa-running:after,.fad.fa-person-running:after,.fad.fa-running:after{content:"\f70c\f70c"}.fa-duotone.fa-mobile-retro:after,.fad.fa-mobile-retro:after{content:"\e527\e527"}.fa-duotone.fa-grip-lines-vertical:after,.fad.fa-grip-lines-vertical:after{content:"\f7a5\f7a5"}.fa-duotone.fa-bin-bottles-recycle:after,.fad.fa-bin-bottles-recycle:after{content:"\e5f6\e5f6"}.fa-duotone.fa-arrow-up-from-square:after,.fad.fa-arrow-up-from-square:after{content:"\e09c\e09c"}.fa-duotone.fa-file-dashed-line:after,.fa-duotone.fa-page-break:after,.fad.fa-file-dashed-line:after,.fad.fa-page-break:after{content:"\f877\f877"}.fa-duotone.fa-bracket-curly-right:after,.fad.fa-bracket-curly-right:after{content:"\7d\7d"}.fa-duotone.fa-spider:after,.fad.fa-spider:after{content:"\f717\f717"}.fa-duotone.fa-clock-three:after,.fad.fa-clock-three:after{content:"\e356\e356"}.fa-duotone.fa-hands-bound:after,.fad.fa-hands-bound:after{content:"\e4f9\e4f9"}.fa-duotone.fa-scalpel-line-dashed:after,.fa-duotone.fa-scalpel-path:after,.fad.fa-scalpel-line-dashed:after,.fad.fa-scalpel-path:after{content:"\f61e\f61e"}.fa-duotone.fa-file-invoice-dollar:after,.fad.fa-file-invoice-dollar:after{content:"\f571\f571"}.fa-duotone.fa-pipe-smoking:after,.fad.fa-pipe-smoking:after{content:"\e3c4\e3c4"}.fa-duotone.fa-face-astonished:after,.fad.fa-face-astonished:after{content:"\e36b\e36b"}.fa-duotone.fa-window:after,.fad.fa-window:after{content:"\f40e\f40e"}.fa-duotone.fa-plane-circle-exclamation:after,.fad.fa-plane-circle-exclamation:after{content:"\e556\e556"}.fa-duotone.fa-ear:after,.fad.fa-ear:after{content:"\f5f0\f5f0"}.fa-duotone.fa-file-lock:after,.fad.fa-file-lock:after{content:"\e3a6\e3a6"}.fa-duotone.fa-diagram-venn:after,.fad.fa-diagram-venn:after{content:"\e15a\e15a"}.fa-duotone.fa-arrow-down-from-bracket:after,.fad.fa-arrow-down-from-bracket:after{content:"\e667\e667"}.fa-duotone.fa-x-ray:after,.fad.fa-x-ray:after{content:"\f497\f497"}.fa-duotone.fa-goal-net:after,.fad.fa-goal-net:after{content:"\e3ab\e3ab"}.fa-duotone.fa-coffin-cross:after,.fad.fa-coffin-cross:after{content:"\e051\e051"}.fa-duotone.fa-spell-check:after,.fad.fa-spell-check:after{content:"\f891\f891"}.fa-duotone.fa-location-xmark:after,.fa-duotone.fa-map-marker-times:after,.fa-duotone.fa-map-marker-xmark:after,.fad.fa-location-xmark:after,.fad.fa-map-marker-times:after,.fad.fa-map-marker-xmark:after{content:"\f60e\f60e"}.fa-duotone.fa-circle-quarter-stroke:after,.fad.fa-circle-quarter-stroke:after{content:"\e5d3\e5d3"}.fa-duotone.fa-lasso:after,.fad.fa-lasso:after{content:"\f8c8\f8c8"}.fa-duotone.fa-slash:after,.fad.fa-slash:after{content:"\f715\f715"}.fa-duotone.fa-person-to-portal:after,.fa-duotone.fa-portal-enter:after,.fad.fa-person-to-portal:after,.fad.fa-portal-enter:after{content:"\e022\e022"}.fa-duotone.fa-calendar-star:after,.fad.fa-calendar-star:after{content:"\f736\f736"}.fa-duotone.fa-computer-mouse:after,.fa-duotone.fa-mouse:after,.fad.fa-computer-mouse:after,.fad.fa-mouse:after{content:"\f8cc\f8cc"}.fa-duotone.fa-arrow-right-to-bracket:after,.fa-duotone.fa-sign-in:after,.fad.fa-arrow-right-to-bracket:after,.fad.fa-sign-in:after{content:"\f090\f090"}.fa-duotone.fa-pegasus:after,.fad.fa-pegasus:after{content:"\f703\f703"}.fa-duotone.fa-files-medical:after,.fad.fa-files-medical:after{content:"\f7fd\f7fd"}.fa-duotone.fa-cannon:after,.fad.fa-cannon:after{content:"\e642\e642"}.fa-duotone.fa-nfc-lock:after,.fad.fa-nfc-lock:after{content:"\e1f8\e1f8"}.fa-duotone.fa-person-ski-lift:after,.fa-duotone.fa-ski-lift:after,.fad.fa-person-ski-lift:after,.fad.fa-ski-lift:after{content:"\f7c8\f7c8"}.fa-duotone.fa-square-6:after,.fad.fa-square-6:after{content:"\e25b\e25b"}.fa-duotone.fa-shop-slash:after,.fa-duotone.fa-store-alt-slash:after,.fad.fa-shop-slash:after,.fad.fa-store-alt-slash:after{content:"\e070\e070"}.fa-duotone.fa-wind-turbine:after,.fad.fa-wind-turbine:after{content:"\f89b\f89b"}.fa-duotone.fa-sliders-simple:after,.fad.fa-sliders-simple:after{content:"\e253\e253"}.fa-duotone.fa-grid-round:after,.fad.fa-grid-round:after{content:"\e5da\e5da"}.fa-duotone.fa-badge-sheriff:after,.fad.fa-badge-sheriff:after{content:"\f8a2\f8a2"}.fa-duotone.fa-server:after,.fad.fa-server:after{content:"\f233\f233"}.fa-duotone.fa-virus-covid-slash:after,.fad.fa-virus-covid-slash:after{content:"\e4a9\e4a9"}.fa-duotone.fa-intersection:after,.fad.fa-intersection:after{content:"\f668\f668"}.fa-duotone.fa-shop-lock:after,.fad.fa-shop-lock:after{content:"\e4a5\e4a5"}.fa-duotone.fa-family:after,.fad.fa-family:after{content:"\e300\e300"}.fa-duotone.fa-hourglass-1:after,.fa-duotone.fa-hourglass-start:after,.fad.fa-hourglass-1:after,.fad.fa-hourglass-start:after{content:"\f251\f251"}.fa-duotone.fa-user-hair-buns:after,.fad.fa-user-hair-buns:after{content:"\e3d3\e3d3"}.fa-duotone.fa-blender-phone:after,.fad.fa-blender-phone:after{content:"\f6b6\f6b6"}.fa-duotone.fa-hourglass-clock:after,.fad.fa-hourglass-clock:after{content:"\e41b\e41b"}.fa-duotone.fa-person-seat-reclined:after,.fad.fa-person-seat-reclined:after{content:"\e21f\e21f"}.fa-duotone.fa-paper-plane-alt:after,.fa-duotone.fa-paper-plane-top:after,.fa-duotone.fa-send:after,.fad.fa-paper-plane-alt:after,.fad.fa-paper-plane-top:after,.fad.fa-send:after{content:"\e20a\e20a"}.fa-duotone.fa-comment-alt-arrow-up:after,.fa-duotone.fa-message-arrow-up:after,.fad.fa-comment-alt-arrow-up:after,.fad.fa-message-arrow-up:after{content:"\e1dc\e1dc"}.fa-duotone.fa-lightbulb-exclamation:after,.fad.fa-lightbulb-exclamation:after{content:"\f671\f671"}.fa-duotone.fa-layer-group-minus:after,.fa-duotone.fa-layer-minus:after,.fad.fa-layer-group-minus:after,.fad.fa-layer-minus:after{content:"\f5fe\f5fe"}.fa-duotone.fa-chart-pie-simple-circle-currency:after,.fad.fa-chart-pie-simple-circle-currency:after{content:"\e604\e604"}.fa-duotone.fa-circle-e:after,.fad.fa-circle-e:after{content:"\e109\e109"}.fa-duotone.fa-building-wheat:after,.fad.fa-building-wheat:after{content:"\e4db\e4db"}.fa-duotone.fa-gauge-max:after,.fa-duotone.fa-tachometer-alt-fastest:after,.fad.fa-gauge-max:after,.fad.fa-tachometer-alt-fastest:after{content:"\f626\f626"}.fa-duotone.fa-person-breastfeeding:after,.fad.fa-person-breastfeeding:after{content:"\e53a\e53a"}.fa-duotone.fa-apostrophe:after,.fad.fa-apostrophe:after{content:"\27\27"}.fa-duotone.fa-file-png:after,.fad.fa-file-png:after{content:"\e666\e666"}.fa-duotone.fa-fire-hydrant:after,.fad.fa-fire-hydrant:after{content:"\e17f\e17f"}.fa-duotone.fa-right-to-bracket:after,.fa-duotone.fa-sign-in-alt:after,.fad.fa-right-to-bracket:after,.fad.fa-sign-in-alt:after{content:"\f2f6\f2f6"}.fa-duotone.fa-video-plus:after,.fad.fa-video-plus:after{content:"\f4e1\f4e1"}.fa-duotone.fa-arrow-alt-square-right:after,.fa-duotone.fa-square-right:after,.fad.fa-arrow-alt-square-right:after,.fad.fa-square-right:after{content:"\f352\f352"}.fa-duotone.fa-comment-smile:after,.fad.fa-comment-smile:after{content:"\f4b4\f4b4"}.fa-duotone.fa-venus:after,.fad.fa-venus:after{content:"\f221\f221"}.fa-duotone.fa-passport:after,.fad.fa-passport:after{content:"\f5ab\f5ab"}.fa-duotone.fa-inbox-arrow-down:after,.fa-duotone.fa-inbox-in:after,.fad.fa-inbox-arrow-down:after,.fad.fa-inbox-in:after{content:"\f310\f310"}.fa-duotone.fa-heart-pulse:after,.fa-duotone.fa-heartbeat:after,.fad.fa-heart-pulse:after,.fad.fa-heartbeat:after{content:"\f21e\f21e"}.fa-duotone.fa-circle-8:after,.fad.fa-circle-8:after{content:"\e0f5\e0f5"}.fa-duotone.fa-clouds-moon:after,.fad.fa-clouds-moon:after{content:"\f745\f745"}.fa-duotone.fa-clock-ten-thirty:after,.fad.fa-clock-ten-thirty:after{content:"\e355\e355"}.fa-duotone.fa-people-carry-box:after,.fa-duotone.fa-people-carry:after,.fad.fa-people-carry-box:after,.fad.fa-people-carry:after{content:"\f4ce\f4ce"}.fa-duotone.fa-folder-user:after,.fad.fa-folder-user:after{content:"\e18e\e18e"}.fa-duotone.fa-trash-can-xmark:after,.fad.fa-trash-can-xmark:after{content:"\e2ae\e2ae"}.fa-duotone.fa-temperature-high:after,.fad.fa-temperature-high:after{content:"\f769\f769"}.fa-duotone.fa-microchip:after,.fad.fa-microchip:after{content:"\f2db\f2db"}.fa-duotone.fa-left-long-to-line:after,.fad.fa-left-long-to-line:after{content:"\e41e\e41e"}.fa-duotone.fa-crown:after,.fad.fa-crown:after{content:"\f521\f521"}.fa-duotone.fa-weight-hanging:after,.fad.fa-weight-hanging:after{content:"\f5cd\f5cd"}.fa-duotone.fa-xmarks-lines:after,.fad.fa-xmarks-lines:after{content:"\e59a\e59a"}.fa-duotone.fa-file-prescription:after,.fad.fa-file-prescription:after{content:"\f572\f572"}.fa-duotone.fa-table-cells-lock:after,.fad.fa-table-cells-lock:after{content:"\e679\e679"}.fa-duotone.fa-calendar-range:after,.fad.fa-calendar-range:after{content:"\e0d6\e0d6"}.fa-duotone.fa-flower-daffodil:after,.fad.fa-flower-daffodil:after{content:"\f800\f800"}.fa-duotone.fa-hand-back-point-up:after,.fad.fa-hand-back-point-up:after{content:"\e1a2\e1a2"}.fa-duotone.fa-weight-scale:after,.fa-duotone.fa-weight:after,.fad.fa-weight-scale:after,.fad.fa-weight:after{content:"\f496\f496"}.fa-duotone.fa-arrow-up-to-arc:after,.fad.fa-arrow-up-to-arc:after{content:"\e617\e617"}.fa-duotone.fa-star-exclamation:after,.fad.fa-star-exclamation:after{content:"\f2f3\f2f3"}.fa-duotone.fa-books:after,.fad.fa-books:after{content:"\f5db\f5db"}.fa-duotone.fa-user-friends:after,.fa-duotone.fa-user-group:after,.fad.fa-user-friends:after,.fad.fa-user-group:after{content:"\f500\f500"}.fa-duotone.fa-arrow-up-a-z:after,.fa-duotone.fa-sort-alpha-up:after,.fad.fa-arrow-up-a-z:after,.fad.fa-sort-alpha-up:after{content:"\f15e\f15e"}.fa-duotone.fa-layer-group-plus:after,.fa-duotone.fa-layer-plus:after,.fad.fa-layer-group-plus:after,.fad.fa-layer-plus:after{content:"\f5ff\f5ff"}.fa-duotone.fa-play-pause:after,.fad.fa-play-pause:after{content:"\e22f\e22f"}.fa-duotone.fa-block-question:after,.fad.fa-block-question:after{content:"\e3dd\e3dd"}.fa-duotone.fa-snooze:after,.fa-duotone.fa-zzz:after,.fad.fa-snooze:after,.fad.fa-zzz:after{content:"\f880\f880"}.fa-duotone.fa-scanner-image:after,.fad.fa-scanner-image:after{content:"\f8f3\f8f3"}.fa-duotone.fa-tv-retro:after,.fad.fa-tv-retro:after{content:"\f401\f401"}.fa-duotone.fa-square-t:after,.fad.fa-square-t:after{content:"\e280\e280"}.fa-duotone.fa-barn-silo:after,.fa-duotone.fa-farm:after,.fad.fa-barn-silo:after,.fad.fa-farm:after{content:"\f864\f864"}.fa-duotone.fa-chess-knight:after,.fad.fa-chess-knight:after{content:"\f441\f441"}.fa-duotone.fa-bars-sort:after,.fad.fa-bars-sort:after{content:"\e0ae\e0ae"}.fa-duotone.fa-palette-boxes:after,.fa-duotone.fa-pallet-alt:after,.fa-duotone.fa-pallet-boxes:after,.fad.fa-palette-boxes:after,.fad.fa-pallet-alt:after,.fad.fa-pallet-boxes:after{content:"\f483\f483"}.fa-duotone.fa-face-laugh-squint:after,.fa-duotone.fa-laugh-squint:after,.fad.fa-face-laugh-squint:after,.fad.fa-laugh-squint:after{content:"\f59b\f59b"}.fa-duotone.fa-code-simple:after,.fad.fa-code-simple:after{content:"\e13d\e13d"}.fa-duotone.fa-bolt-slash:after,.fad.fa-bolt-slash:after{content:"\e0b8\e0b8"}.fa-duotone.fa-panel-fire:after,.fad.fa-panel-fire:after{content:"\e42f\e42f"}.fa-duotone.fa-binary-circle-check:after,.fad.fa-binary-circle-check:after{content:"\e33c\e33c"}.fa-duotone.fa-comment-minus:after,.fad.fa-comment-minus:after{content:"\f4b1\f4b1"}.fa-duotone.fa-burrito:after,.fad.fa-burrito:after{content:"\f7ed\f7ed"}.fa-duotone.fa-violin:after,.fad.fa-violin:after{content:"\f8ed\f8ed"}.fa-duotone.fa-objects-column:after,.fad.fa-objects-column:after{content:"\e3c1\e3c1"}.fa-duotone.fa-chevron-square-down:after,.fa-duotone.fa-square-chevron-down:after,.fad.fa-chevron-square-down:after,.fad.fa-square-chevron-down:after{content:"\f329\f329"}.fa-duotone.fa-comment-plus:after,.fad.fa-comment-plus:after{content:"\f4b2\f4b2"}.fa-duotone.fa-triangle-instrument:after,.fa-duotone.fa-triangle-music:after,.fad.fa-triangle-instrument:after,.fad.fa-triangle-music:after{content:"\f8e2\f8e2"}.fa-duotone.fa-wheelchair:after,.fad.fa-wheelchair:after{content:"\f193\f193"}.fa-duotone.fa-user-pilot-tie:after,.fad.fa-user-pilot-tie:after{content:"\e2c1\e2c1"}.fa-duotone.fa-piano-keyboard:after,.fad.fa-piano-keyboard:after{content:"\f8d5\f8d5"}.fa-duotone.fa-bed-empty:after,.fad.fa-bed-empty:after{content:"\f8f9\f8f9"}.fa-duotone.fa-arrow-circle-up:after,.fa-duotone.fa-circle-arrow-up:after,.fad.fa-arrow-circle-up:after,.fad.fa-circle-arrow-up:after{content:"\f0aa\f0aa"}.fa-duotone.fa-toggle-on:after,.fad.fa-toggle-on:after{content:"\f205\f205"}.fa-duotone.fa-rectangle-portrait:after,.fa-duotone.fa-rectangle-vertical:after,.fad.fa-rectangle-portrait:after,.fad.fa-rectangle-vertical:after{content:"\f2fb\f2fb"}.fa-duotone.fa-person-walking:after,.fa-duotone.fa-walking:after,.fad.fa-person-walking:after,.fad.fa-walking:after{content:"\f554\f554"}.fa-duotone.fa-l:after,.fad.fa-l:after{content:"\4c\4c"}.fa-duotone.fa-signal-stream:after,.fad.fa-signal-stream:after{content:"\f8dd\f8dd"}.fa-duotone.fa-down-to-bracket:after,.fad.fa-down-to-bracket:after{content:"\e4e7\e4e7"}.fa-duotone.fa-circle-z:after,.fad.fa-circle-z:after{content:"\e130\e130"}.fa-duotone.fa-stars:after,.fad.fa-stars:after{content:"\f762\f762"}.fa-duotone.fa-fire:after,.fad.fa-fire:after{content:"\f06d\f06d"}.fa-duotone.fa-bed-pulse:after,.fa-duotone.fa-procedures:after,.fad.fa-bed-pulse:after,.fad.fa-procedures:after{content:"\f487\f487"}.fa-duotone.fa-house-day:after,.fad.fa-house-day:after{content:"\e00e\e00e"}.fa-duotone.fa-shuttle-space:after,.fa-duotone.fa-space-shuttle:after,.fad.fa-shuttle-space:after,.fad.fa-space-shuttle:after{content:"\f197\f197"}.fa-duotone.fa-shirt-long-sleeve:after,.fad.fa-shirt-long-sleeve:after{content:"\e3c7\e3c7"}.fa-duotone.fa-chart-pie-alt:after,.fa-duotone.fa-chart-pie-simple:after,.fad.fa-chart-pie-alt:after,.fad.fa-chart-pie-simple:after{content:"\f64e\f64e"}.fa-duotone.fa-face-laugh:after,.fa-duotone.fa-laugh:after,.fad.fa-face-laugh:after,.fad.fa-laugh:after{content:"\f599\f599"}.fa-duotone.fa-folder-open:after,.fad.fa-folder-open:after{content:"\f07c\f07c"}.fa-duotone.fa-album-collection-circle-user:after,.fad.fa-album-collection-circle-user:after{content:"\e48f\e48f"}.fa-duotone.fa-candy:after,.fad.fa-candy:after{content:"\e3e7\e3e7"}.fa-duotone.fa-bowl-hot:after,.fa-duotone.fa-soup:after,.fad.fa-bowl-hot:after,.fad.fa-soup:after{content:"\f823\f823"}.fa-duotone.fa-flatbread:after,.fad.fa-flatbread:after{content:"\e40b\e40b"}.fa-duotone.fa-heart-circle-plus:after,.fad.fa-heart-circle-plus:after{content:"\e500\e500"}.fa-duotone.fa-code-fork:after,.fad.fa-code-fork:after{content:"\e13b\e13b"}.fa-duotone.fa-city:after,.fad.fa-city:after{content:"\f64f\f64f"}.fa-duotone.fa-signal-alt-1:after,.fa-duotone.fa-signal-bars-weak:after,.fad.fa-signal-alt-1:after,.fad.fa-signal-bars-weak:after{content:"\f691\f691"}.fa-duotone.fa-microphone-alt:after,.fa-duotone.fa-microphone-lines:after,.fad.fa-microphone-alt:after,.fad.fa-microphone-lines:after{content:"\f3c9\f3c9"}.fa-duotone.fa-clock-twelve:after,.fad.fa-clock-twelve:after{content:"\e358\e358"}.fa-duotone.fa-pepper-hot:after,.fad.fa-pepper-hot:after{content:"\f816\f816"}.fa-duotone.fa-citrus-slice:after,.fad.fa-citrus-slice:after{content:"\e2f5\e2f5"}.fa-duotone.fa-sheep:after,.fad.fa-sheep:after{content:"\f711\f711"}.fa-duotone.fa-unlock:after,.fad.fa-unlock:after{content:"\f09c\f09c"}.fa-duotone.fa-colon-sign:after,.fad.fa-colon-sign:after{content:"\e140\e140"}.fa-duotone.fa-headset:after,.fad.fa-headset:after{content:"\f590\f590"}.fa-duotone.fa-badger-honey:after,.fad.fa-badger-honey:after{content:"\f6b4\f6b4"}.fa-duotone.fa-h4:after,.fad.fa-h4:after{content:"\f86a\f86a"}.fa-duotone.fa-store-slash:after,.fad.fa-store-slash:after{content:"\e071\e071"}.fa-duotone.fa-road-circle-xmark:after,.fad.fa-road-circle-xmark:after{content:"\e566\e566"}.fa-duotone.fa-signal-slash:after,.fad.fa-signal-slash:after{content:"\f695\f695"}.fa-duotone.fa-user-minus:after,.fad.fa-user-minus:after{content:"\f503\f503"}.fa-duotone.fa-mars-stroke-up:after,.fa-duotone.fa-mars-stroke-v:after,.fad.fa-mars-stroke-up:after,.fad.fa-mars-stroke-v:after{content:"\f22a\f22a"}.fa-duotone.fa-champagne-glasses:after,.fa-duotone.fa-glass-cheers:after,.fad.fa-champagne-glasses:after,.fad.fa-glass-cheers:after{content:"\f79f\f79f"}.fa-duotone.fa-taco:after,.fad.fa-taco:after{content:"\f826\f826"}.fa-duotone.fa-hexagon-plus:after,.fa-duotone.fa-plus-hexagon:after,.fad.fa-hexagon-plus:after,.fad.fa-plus-hexagon:after{content:"\f300\f300"}.fa-duotone.fa-clipboard:after,.fad.fa-clipboard:after{content:"\f328\f328"}.fa-duotone.fa-house-circle-exclamation:after,.fad.fa-house-circle-exclamation:after{content:"\e50a\e50a"}.fa-duotone.fa-file-arrow-up:after,.fa-duotone.fa-file-upload:after,.fad.fa-file-arrow-up:after,.fad.fa-file-upload:after{content:"\f574\f574"}.fa-duotone.fa-wifi-3:after,.fa-duotone.fa-wifi-strong:after,.fa-duotone.fa-wifi:after,.fad.fa-wifi-3:after,.fad.fa-wifi-strong:after,.fad.fa-wifi:after{content:"\f1eb\f1eb"}.fa-duotone.fa-comments-alt:after,.fa-duotone.fa-messages:after,.fad.fa-comments-alt:after,.fad.fa-messages:after{content:"\f4b6\f4b6"}.fa-duotone.fa-bath:after,.fa-duotone.fa-bathtub:after,.fad.fa-bath:after,.fad.fa-bathtub:after{content:"\f2cd\f2cd"}.fa-duotone.fa-umbrella-alt:after,.fa-duotone.fa-umbrella-simple:after,.fad.fa-umbrella-alt:after,.fad.fa-umbrella-simple:after{content:"\e2bc\e2bc"}.fa-duotone.fa-rectangle-history-circle-plus:after,.fad.fa-rectangle-history-circle-plus:after{content:"\e4a3\e4a3"}.fa-duotone.fa-underline:after,.fad.fa-underline:after{content:"\f0cd\f0cd"}.fa-duotone.fa-prescription-bottle-pill:after,.fad.fa-prescription-bottle-pill:after{content:"\e5c0\e5c0"}.fa-duotone.fa-user-edit:after,.fa-duotone.fa-user-pen:after,.fad.fa-user-edit:after,.fad.fa-user-pen:after{content:"\f4ff\f4ff"}.fa-duotone.fa-binary-slash:after,.fad.fa-binary-slash:after{content:"\e33e\e33e"}.fa-duotone.fa-square-o:after,.fad.fa-square-o:after{content:"\e278\e278"}.fa-duotone.fa-caduceus:after,.fad.fa-caduceus:after{content:"\e681\e681"}.fa-duotone.fa-signature:after,.fad.fa-signature:after{content:"\f5b7\f5b7"}.fa-duotone.fa-stroopwafel:after,.fad.fa-stroopwafel:after{content:"\f551\f551"}.fa-duotone.fa-bold:after,.fad.fa-bold:after{content:"\f032\f032"}.fa-duotone.fa-anchor-lock:after,.fad.fa-anchor-lock:after{content:"\e4ad\e4ad"}.fa-duotone.fa-building-ngo:after,.fad.fa-building-ngo:after{content:"\e4d7\e4d7"}.fa-duotone.fa-transporter-3:after,.fad.fa-transporter-3:after{content:"\e045\e045"}.fa-duotone.fa-engine-exclamation:after,.fa-duotone.fa-engine-warning:after,.fad.fa-engine-exclamation:after,.fad.fa-engine-warning:after{content:"\f5f2\f5f2"}.fa-duotone.fa-circle-down-right:after,.fad.fa-circle-down-right:after{content:"\e108\e108"}.fa-duotone.fa-square-k:after,.fad.fa-square-k:after{content:"\e274\e274"}.fa-duotone.fa-manat-sign:after,.fad.fa-manat-sign:after{content:"\e1d5\e1d5"}.fa-duotone.fa-money-check-edit:after,.fa-duotone.fa-money-check-pen:after,.fad.fa-money-check-edit:after,.fad.fa-money-check-pen:after{content:"\f872\f872"}.fa-duotone.fa-not-equal:after,.fad.fa-not-equal:after{content:"\f53e\f53e"}.fa-duotone.fa-border-style:after,.fa-duotone.fa-border-top-left:after,.fad.fa-border-style:after,.fad.fa-border-top-left:after{content:"\f853\f853"}.fa-duotone.fa-map-location-dot:after,.fa-duotone.fa-map-marked-alt:after,.fad.fa-map-location-dot:after,.fad.fa-map-marked-alt:after{content:"\f5a0\f5a0"}.fa-duotone.fa-tilde:after,.fad.fa-tilde:after{content:"\7e\7e"}.fa-duotone.fa-jedi:after,.fad.fa-jedi:after{content:"\f669\f669"}.fa-duotone.fa-poll:after,.fa-duotone.fa-square-poll-vertical:after,.fad.fa-poll:after,.fad.fa-square-poll-vertical:after{content:"\f681\f681"}.fa-duotone.fa-arrow-down-square-triangle:after,.fa-duotone.fa-sort-shapes-down-alt:after,.fad.fa-arrow-down-square-triangle:after,.fad.fa-sort-shapes-down-alt:after{content:"\f889\f889"}.fa-duotone.fa-mug-hot:after,.fad.fa-mug-hot:after{content:"\f7b6\f7b6"}.fa-duotone.fa-dog-leashed:after,.fad.fa-dog-leashed:after{content:"\f6d4\f6d4"}.fa-duotone.fa-battery-car:after,.fa-duotone.fa-car-battery:after,.fad.fa-battery-car:after,.fad.fa-car-battery:after{content:"\f5df\f5df"}.fa-duotone.fa-face-downcast-sweat:after,.fad.fa-face-downcast-sweat:after{content:"\e371\e371"}.fa-duotone.fa-mailbox-flag-up:after,.fad.fa-mailbox-flag-up:after{content:"\e5bb\e5bb"}.fa-duotone.fa-memo-circle-info:after,.fad.fa-memo-circle-info:after{content:"\e49a\e49a"}.fa-duotone.fa-gift:after,.fad.fa-gift:after{content:"\f06b\f06b"}.fa-duotone.fa-dice-two:after,.fad.fa-dice-two:after{content:"\f528\f528"}.fa-duotone.fa-volume-medium:after,.fa-duotone.fa-volume:after,.fad.fa-volume-medium:after,.fad.fa-volume:after{content:"\f6a8\f6a8"}.fa-duotone.fa-transporter-5:after,.fad.fa-transporter-5:after{content:"\e2a6\e2a6"}.fa-duotone.fa-gauge-circle-bolt:after,.fad.fa-gauge-circle-bolt:after{content:"\e496\e496"}.fa-duotone.fa-coin-front:after,.fad.fa-coin-front:after{content:"\e3fc\e3fc"}.fa-duotone.fa-file-slash:after,.fad.fa-file-slash:after{content:"\e3a7\e3a7"}.fa-duotone.fa-message-arrow-up-right:after,.fad.fa-message-arrow-up-right:after{content:"\e1dd\e1dd"}.fa-duotone.fa-treasure-chest:after,.fad.fa-treasure-chest:after{content:"\f723\f723"}.fa-duotone.fa-chess-queen:after,.fad.fa-chess-queen:after{content:"\f445\f445"}.fa-duotone.fa-paint-brush-alt:after,.fa-duotone.fa-paint-brush-fine:after,.fa-duotone.fa-paintbrush-alt:after,.fa-duotone.fa-paintbrush-fine:after,.fad.fa-paint-brush-alt:after,.fad.fa-paint-brush-fine:after,.fad.fa-paintbrush-alt:after,.fad.fa-paintbrush-fine:after{content:"\f5a9\f5a9"}.fa-duotone.fa-glasses:after,.fad.fa-glasses:after{content:"\f530\f530"}.fa-duotone.fa-hood-cloak:after,.fad.fa-hood-cloak:after{content:"\f6ef\f6ef"}.fa-duotone.fa-square-quote:after,.fad.fa-square-quote:after{content:"\e329\e329"}.fa-duotone.fa-up-left:after,.fad.fa-up-left:after{content:"\e2bd\e2bd"}.fa-duotone.fa-bring-front:after,.fad.fa-bring-front:after{content:"\f857\f857"}.fa-duotone.fa-chess-board:after,.fad.fa-chess-board:after{content:"\f43c\f43c"}.fa-duotone.fa-burger-cheese:after,.fa-duotone.fa-cheeseburger:after,.fad.fa-burger-cheese:after,.fad.fa-cheeseburger:after{content:"\f7f1\f7f1"}.fa-duotone.fa-building-circle-check:after,.fad.fa-building-circle-check:after{content:"\e4d2\e4d2"}.fa-duotone.fa-repeat-1:after,.fad.fa-repeat-1:after{content:"\f365\f365"}.fa-duotone.fa-arrow-down-to-line:after,.fa-duotone.fa-arrow-to-bottom:after,.fad.fa-arrow-down-to-line:after,.fad.fa-arrow-to-bottom:after{content:"\f33d\f33d"}.fa-duotone.fa-grid-5:after,.fad.fa-grid-5:after{content:"\e199\e199"}.fa-duotone.fa-swap-arrows:after,.fad.fa-swap-arrows:after{content:"\e60a\e60a"}.fa-duotone.fa-right-long-to-line:after,.fad.fa-right-long-to-line:after{content:"\e444\e444"}.fa-duotone.fa-person-chalkboard:after,.fad.fa-person-chalkboard:after{content:"\e53d\e53d"}.fa-duotone.fa-mars-stroke-h:after,.fa-duotone.fa-mars-stroke-right:after,.fad.fa-mars-stroke-h:after,.fad.fa-mars-stroke-right:after{content:"\f22b\f22b"}.fa-duotone.fa-hand-back-fist:after,.fa-duotone.fa-hand-rock:after,.fad.fa-hand-back-fist:after,.fad.fa-hand-rock:after{content:"\f255\f255"}.fa-duotone.fa-grid-round-5:after,.fad.fa-grid-round-5:after{content:"\e5de\e5de"}.fa-duotone.fa-tally-5:after,.fa-duotone.fa-tally:after,.fad.fa-tally-5:after,.fad.fa-tally:after{content:"\f69c\f69c"}.fa-duotone.fa-caret-square-up:after,.fa-duotone.fa-square-caret-up:after,.fad.fa-caret-square-up:after,.fad.fa-square-caret-up:after{content:"\f151\f151"}.fa-duotone.fa-cloud-showers-water:after,.fad.fa-cloud-showers-water:after{content:"\e4e4\e4e4"}.fa-duotone.fa-bar-chart:after,.fa-duotone.fa-chart-bar:after,.fad.fa-bar-chart:after,.fad.fa-chart-bar:after{content:"\f080\f080"}.fa-duotone.fa-hands-bubbles:after,.fa-duotone.fa-hands-wash:after,.fad.fa-hands-bubbles:after,.fad.fa-hands-wash:after{content:"\e05e\e05e"}.fa-duotone.fa-less-than-equal:after,.fad.fa-less-than-equal:after{content:"\f537\f537"}.fa-duotone.fa-train:after,.fad.fa-train:after{content:"\f238\f238"}.fa-duotone.fa-up-from-dotted-line:after,.fad.fa-up-from-dotted-line:after{content:"\e456\e456"}.fa-duotone.fa-eye-low-vision:after,.fa-duotone.fa-low-vision:after,.fad.fa-eye-low-vision:after,.fad.fa-low-vision:after{content:"\f2a8\f2a8"}.fa-duotone.fa-traffic-light-go:after,.fad.fa-traffic-light-go:after{content:"\f638\f638"}.fa-duotone.fa-face-exhaling:after,.fad.fa-face-exhaling:after{content:"\e480\e480"}.fa-duotone.fa-sensor-fire:after,.fad.fa-sensor-fire:after{content:"\e02a\e02a"}.fa-duotone.fa-user-unlock:after,.fad.fa-user-unlock:after{content:"\e058\e058"}.fa-duotone.fa-hexagon-divide:after,.fad.fa-hexagon-divide:after{content:"\e1ad\e1ad"}.fa-duotone.fa-00:after,.fad.fa-00:after{content:"\e467\e467"}.fa-duotone.fa-crow:after,.fad.fa-crow:after{content:"\f520\f520"}.fa-duotone.fa-betamax:after,.fa-duotone.fa-cassette-betamax:after,.fad.fa-betamax:after,.fad.fa-cassette-betamax:after{content:"\f8a4\f8a4"}.fa-duotone.fa-sailboat:after,.fad.fa-sailboat:after{content:"\e445\e445"}.fa-duotone.fa-window-restore:after,.fad.fa-window-restore:after{content:"\f2d2\f2d2"}.fa-duotone.fa-nfc-magnifying-glass:after,.fad.fa-nfc-magnifying-glass:after{content:"\e1f9\e1f9"}.fa-duotone.fa-file-binary:after,.fad.fa-file-binary:after{content:"\e175\e175"}.fa-duotone.fa-circle-v:after,.fad.fa-circle-v:after{content:"\e12a\e12a"}.fa-duotone.fa-plus-square:after,.fa-duotone.fa-square-plus:after,.fad.fa-plus-square:after,.fad.fa-square-plus:after{content:"\f0fe\f0fe"}.fa-duotone.fa-bowl-scoops:after,.fad.fa-bowl-scoops:after{content:"\e3df\e3df"}.fa-duotone.fa-mistletoe:after,.fad.fa-mistletoe:after{content:"\f7b4\f7b4"}.fa-duotone.fa-custard:after,.fad.fa-custard:after{content:"\e403\e403"}.fa-duotone.fa-lacrosse-stick:after,.fad.fa-lacrosse-stick:after{content:"\e3b5\e3b5"}.fa-duotone.fa-hockey-mask:after,.fad.fa-hockey-mask:after{content:"\f6ee\f6ee"}.fa-duotone.fa-sunrise:after,.fad.fa-sunrise:after{content:"\f766\f766"}.fa-duotone.fa-subtitles:after,.fad.fa-subtitles:after{content:"\e60f\e60f"}.fa-duotone.fa-panel-ews:after,.fad.fa-panel-ews:after{content:"\e42e\e42e"}.fa-duotone.fa-torii-gate:after,.fad.fa-torii-gate:after{content:"\f6a1\f6a1"}.fa-duotone.fa-cloud-exclamation:after,.fad.fa-cloud-exclamation:after{content:"\e491\e491"}.fa-duotone.fa-comment-alt-lines:after,.fa-duotone.fa-message-lines:after,.fad.fa-comment-alt-lines:after,.fad.fa-message-lines:after{content:"\f4a6\f4a6"}.fa-duotone.fa-frog:after,.fad.fa-frog:after{content:"\f52e\f52e"}.fa-duotone.fa-bucket:after,.fad.fa-bucket:after{content:"\e4cf\e4cf"}.fa-duotone.fa-floppy-disk-pen:after,.fad.fa-floppy-disk-pen:after{content:"\e182\e182"}.fa-duotone.fa-image:after,.fad.fa-image:after{content:"\f03e\f03e"}.fa-duotone.fa-window-frame:after,.fad.fa-window-frame:after{content:"\e04f\e04f"}.fa-duotone.fa-microphone:after,.fad.fa-microphone:after{content:"\f130\f130"}.fa-duotone.fa-cow:after,.fad.fa-cow:after{content:"\f6c8\f6c8"}.fa-duotone.fa-file-zip:after,.fad.fa-file-zip:after{content:"\e5ee\e5ee"}.fa-duotone.fa-square-ring:after,.fad.fa-square-ring:after{content:"\e44f\e44f"}.fa-duotone.fa-arrow-alt-from-top:after,.fa-duotone.fa-down-from-line:after,.fad.fa-arrow-alt-from-top:after,.fad.fa-down-from-line:after{content:"\f349\f349"}.fa-duotone.fa-caret-up:after,.fad.fa-caret-up:after{content:"\f0d8\f0d8"}.fa-duotone.fa-shield-times:after,.fa-duotone.fa-shield-xmark:after,.fad.fa-shield-times:after,.fad.fa-shield-xmark:after{content:"\e24c\e24c"}.fa-duotone.fa-screwdriver:after,.fad.fa-screwdriver:after{content:"\f54a\f54a"}.fa-duotone.fa-circle-sort-down:after,.fa-duotone.fa-sort-circle-down:after,.fad.fa-circle-sort-down:after,.fad.fa-sort-circle-down:after{content:"\e031\e031"}.fa-duotone.fa-folder-closed:after,.fad.fa-folder-closed:after{content:"\e185\e185"}.fa-duotone.fa-house-tsunami:after,.fad.fa-house-tsunami:after{content:"\e515\e515"}.fa-duotone.fa-square-nfi:after,.fad.fa-square-nfi:after{content:"\e576\e576"}.fa-duotone.fa-forklift:after,.fad.fa-forklift:after{content:"\f47a\f47a"}.fa-duotone.fa-arrow-up-from-ground-water:after,.fad.fa-arrow-up-from-ground-water:after{content:"\e4b5\e4b5"}.fa-duotone.fa-bracket-square-right:after,.fad.fa-bracket-square-right:after{content:"\5d\5d"}.fa-duotone.fa-glass-martini-alt:after,.fa-duotone.fa-martini-glass:after,.fad.fa-glass-martini-alt:after,.fad.fa-martini-glass:after{content:"\f57b\f57b"}.fa-duotone.fa-rotate-back:after,.fa-duotone.fa-rotate-backward:after,.fa-duotone.fa-rotate-left:after,.fa-duotone.fa-undo-alt:after,.fad.fa-rotate-back:after,.fad.fa-rotate-backward:after,.fad.fa-rotate-left:after,.fad.fa-undo-alt:after{content:"\f2ea\f2ea"}.fa-duotone.fa-columns:after,.fa-duotone.fa-table-columns:after,.fad.fa-columns:after,.fad.fa-table-columns:after{content:"\f0db\f0db"}.fa-duotone.fa-square-a:after,.fad.fa-square-a:after{content:"\e25f\e25f"}.fa-duotone.fa-tick:after,.fad.fa-tick:after{content:"\e32f\e32f"}.fa-duotone.fa-lemon:after,.fad.fa-lemon:after{content:"\f094\f094"}.fa-duotone.fa-head-side-mask:after,.fad.fa-head-side-mask:after{content:"\e063\e063"}.fa-duotone.fa-handshake:after,.fad.fa-handshake:after{content:"\f2b5\f2b5"}.fa-duotone.fa-gem:after,.fad.fa-gem:after{content:"\f3a5\f3a5"}.fa-duotone.fa-dolly-box:after,.fa-duotone.fa-dolly:after,.fad.fa-dolly-box:after,.fad.fa-dolly:after{content:"\f472\f472"}.fa-duotone.fa-smoking:after,.fad.fa-smoking:after{content:"\f48d\f48d"}.fa-duotone.fa-compress-arrows-alt:after,.fa-duotone.fa-minimize:after,.fad.fa-compress-arrows-alt:after,.fad.fa-minimize:after{content:"\f78c\f78c"}.fa-duotone.fa-refrigerator:after,.fad.fa-refrigerator:after{content:"\e026\e026"}.fa-duotone.fa-monument:after,.fad.fa-monument:after{content:"\f5a6\f5a6"}.fa-duotone.fa-octagon-xmark:after,.fa-duotone.fa-times-octagon:after,.fa-duotone.fa-xmark-octagon:after,.fad.fa-octagon-xmark:after,.fad.fa-times-octagon:after,.fad.fa-xmark-octagon:after{content:"\f2f0\f2f0"}.fa-duotone.fa-align-slash:after,.fad.fa-align-slash:after{content:"\f846\f846"}.fa-duotone.fa-snowplow:after,.fad.fa-snowplow:after{content:"\f7d2\f7d2"}.fa-duotone.fa-angle-double-right:after,.fa-duotone.fa-angles-right:after,.fad.fa-angle-double-right:after,.fad.fa-angles-right:after{content:"\f101\f101"}.fa-duotone.fa-truck-couch:after,.fa-duotone.fa-truck-ramp-couch:after,.fad.fa-truck-couch:after,.fad.fa-truck-ramp-couch:after{content:"\f4dd\f4dd"}.fa-duotone.fa-cannabis:after,.fad.fa-cannabis:after{content:"\f55f\f55f"}.fa-duotone.fa-circle-play:after,.fa-duotone.fa-play-circle:after,.fad.fa-circle-play:after,.fad.fa-play-circle:after{content:"\f144\f144"}.fa-duotone.fa-arrow-up-right-and-arrow-down-left-from-center:after,.fad.fa-arrow-up-right-and-arrow-down-left-from-center:after{content:"\e0a0\e0a0"}.fa-duotone.fa-location-arrow-up:after,.fad.fa-location-arrow-up:after{content:"\e63a\e63a"}.fa-duotone.fa-tablets:after,.fad.fa-tablets:after{content:"\f490\f490"}.fa-duotone.fa-360-degrees:after,.fad.fa-360-degrees:after{content:"\e2dc\e2dc"}.fa-duotone.fa-ethernet:after,.fad.fa-ethernet:after{content:"\f796\f796"}.fa-duotone.fa-eur:after,.fa-duotone.fa-euro-sign:after,.fa-duotone.fa-euro:after,.fad.fa-eur:after,.fad.fa-euro-sign:after,.fad.fa-euro:after{content:"\f153\f153"}.fa-duotone.fa-chair:after,.fad.fa-chair:after{content:"\f6c0\f6c0"}.fa-duotone.fa-check-circle:after,.fa-duotone.fa-circle-check:after,.fad.fa-check-circle:after,.fad.fa-circle-check:after{content:"\f058\f058"}.fa-duotone.fa-square-dashed-circle-plus:after,.fad.fa-square-dashed-circle-plus:after{content:"\e5c2\e5c2"}.fa-duotone.fa-hand-holding-circle-dollar:after,.fad.fa-hand-holding-circle-dollar:after{content:"\e621\e621"}.fa-duotone.fa-money-simple-from-bracket:after,.fad.fa-money-simple-from-bracket:after{content:"\e313\e313"}.fa-duotone.fa-bat:after,.fad.fa-bat:after{content:"\f6b5\f6b5"}.fa-duotone.fa-circle-stop:after,.fa-duotone.fa-stop-circle:after,.fad.fa-circle-stop:after,.fad.fa-stop-circle:after{content:"\f28d\f28d"}.fa-duotone.fa-head-side-headphones:after,.fad.fa-head-side-headphones:after{content:"\f8c2\f8c2"}.fa-duotone.fa-phone-rotary:after,.fad.fa-phone-rotary:after{content:"\f8d3\f8d3"}.fa-duotone.fa-arrow-up-to-bracket:after,.fad.fa-arrow-up-to-bracket:after{content:"\e66a\e66a"}.fa-duotone.fa-compass-drafting:after,.fa-duotone.fa-drafting-compass:after,.fad.fa-compass-drafting:after,.fad.fa-drafting-compass:after{content:"\f568\f568"}.fa-duotone.fa-plate-wheat:after,.fad.fa-plate-wheat:after{content:"\e55a\e55a"}.fa-duotone.fa-calendar-circle-minus:after,.fad.fa-calendar-circle-minus:after{content:"\e46f\e46f"}.fa-duotone.fa-chopsticks:after,.fad.fa-chopsticks:after{content:"\e3f7\e3f7"}.fa-duotone.fa-car-mechanic:after,.fa-duotone.fa-car-wrench:after,.fad.fa-car-mechanic:after,.fad.fa-car-wrench:after{content:"\f5e3\f5e3"}.fa-duotone.fa-icicles:after,.fad.fa-icicles:after{content:"\f7ad\f7ad"}.fa-duotone.fa-person-shelter:after,.fad.fa-person-shelter:after{content:"\e54f\e54f"}.fa-duotone.fa-neuter:after,.fad.fa-neuter:after{content:"\f22c\f22c"}.fa-duotone.fa-id-badge:after,.fad.fa-id-badge:after{content:"\f2c1\f2c1"}.fa-duotone.fa-kazoo:after,.fad.fa-kazoo:after{content:"\f8c7\f8c7"}.fa-duotone.fa-marker:after,.fad.fa-marker:after{content:"\f5a1\f5a1"}.fa-duotone.fa-bin-bottles:after,.fad.fa-bin-bottles:after{content:"\e5f5\e5f5"}.fa-duotone.fa-face-laugh-beam:after,.fa-duotone.fa-laugh-beam:after,.fad.fa-face-laugh-beam:after,.fad.fa-laugh-beam:after{content:"\f59a\f59a"}.fa-duotone.fa-square-arrow-down-left:after,.fad.fa-square-arrow-down-left:after{content:"\e261\e261"}.fa-duotone.fa-battery-bolt:after,.fad.fa-battery-bolt:after{content:"\f376\f376"}.fa-duotone.fa-tree-large:after,.fad.fa-tree-large:after{content:"\f7dd\f7dd"}.fa-duotone.fa-helicopter-symbol:after,.fad.fa-helicopter-symbol:after{content:"\e502\e502"}.fa-duotone.fa-aperture:after,.fad.fa-aperture:after{content:"\e2df\e2df"}.fa-duotone.fa-universal-access:after,.fad.fa-universal-access:after{content:"\f29a\f29a"}.fa-duotone.fa-gear-complex:after,.fad.fa-gear-complex:after{content:"\e5e9\e5e9"}.fa-duotone.fa-file-magnifying-glass:after,.fa-duotone.fa-file-search:after,.fad.fa-file-magnifying-glass:after,.fad.fa-file-search:after{content:"\f865\f865"}.fa-duotone.fa-up-right:after,.fad.fa-up-right:after{content:"\e2be\e2be"}.fa-duotone.fa-chevron-circle-up:after,.fa-duotone.fa-circle-chevron-up:after,.fad.fa-chevron-circle-up:after,.fad.fa-circle-chevron-up:after{content:"\f139\f139"}.fa-duotone.fa-user-police:after,.fad.fa-user-police:after{content:"\e333\e333"}.fa-duotone.fa-lari-sign:after,.fad.fa-lari-sign:after{content:"\e1c8\e1c8"}.fa-duotone.fa-volcano:after,.fad.fa-volcano:after{content:"\f770\f770"}.fa-duotone.fa-teddy-bear:after,.fad.fa-teddy-bear:after{content:"\e3cf\e3cf"}.fa-duotone.fa-stocking:after,.fad.fa-stocking:after{content:"\f7d5\f7d5"}.fa-duotone.fa-person-walking-dashed-line-arrow-right:after,.fad.fa-person-walking-dashed-line-arrow-right:after{content:"\e553\e553"}.fa-duotone.fa-image-slash:after,.fad.fa-image-slash:after{content:"\e1b7\e1b7"}.fa-duotone.fa-mask-snorkel:after,.fad.fa-mask-snorkel:after{content:"\e3b7\e3b7"}.fa-duotone.fa-smoke:after,.fad.fa-smoke:after{content:"\f760\f760"}.fa-duotone.fa-gbp:after,.fa-duotone.fa-pound-sign:after,.fa-duotone.fa-sterling-sign:after,.fad.fa-gbp:after,.fad.fa-pound-sign:after,.fad.fa-sterling-sign:after{content:"\f154\f154"}.fa-duotone.fa-battery-exclamation:after,.fad.fa-battery-exclamation:after{content:"\e0b0\e0b0"}.fa-duotone.fa-viruses:after,.fad.fa-viruses:after{content:"\e076\e076"}.fa-duotone.fa-square-person-confined:after,.fad.fa-square-person-confined:after{content:"\e577\e577"}.fa-duotone.fa-user-tie:after,.fad.fa-user-tie:after{content:"\f508\f508"}.fa-duotone.fa-up-to-bracket:after,.fad.fa-up-to-bracket:after{content:"\e66e\e66e"}.fa-duotone.fa-arrow-down-long:after,.fa-duotone.fa-long-arrow-down:after,.fad.fa-arrow-down-long:after,.fad.fa-long-arrow-down:after{content:"\f175\f175"}.fa-duotone.fa-tent-arrow-down-to-line:after,.fad.fa-tent-arrow-down-to-line:after{content:"\e57e\e57e"}.fa-duotone.fa-certificate:after,.fad.fa-certificate:after{content:"\f0a3\f0a3"}.fa-duotone.fa-crystal-ball:after,.fad.fa-crystal-ball:after{content:"\e362\e362"}.fa-duotone.fa-mail-reply-all:after,.fa-duotone.fa-reply-all:after,.fad.fa-mail-reply-all:after,.fad.fa-reply-all:after{content:"\f122\f122"}.fa-duotone.fa-suitcase:after,.fad.fa-suitcase:after{content:"\f0f2\f0f2"}.fa-duotone.fa-person-skating:after,.fa-duotone.fa-skating:after,.fad.fa-person-skating:after,.fad.fa-skating:after{content:"\f7c5\f7c5"}.fa-duotone.fa-star-shooting:after,.fad.fa-star-shooting:after{content:"\e036\e036"}.fa-duotone.fa-binary-lock:after,.fad.fa-binary-lock:after{content:"\e33d\e33d"}.fa-duotone.fa-filter-circle-dollar:after,.fa-duotone.fa-funnel-dollar:after,.fad.fa-filter-circle-dollar:after,.fad.fa-funnel-dollar:after{content:"\f662\f662"}.fa-duotone.fa-camera-retro:after,.fad.fa-camera-retro:after{content:"\f083\f083"}.fa-duotone.fa-arrow-circle-down:after,.fa-duotone.fa-circle-arrow-down:after,.fad.fa-arrow-circle-down:after,.fad.fa-circle-arrow-down:after{content:"\f0ab\f0ab"}.fa-duotone.fa-comment-edit:after,.fa-duotone.fa-comment-pen:after,.fad.fa-comment-edit:after,.fad.fa-comment-pen:after{content:"\f4ae\f4ae"}.fa-duotone.fa-arrow-right-to-file:after,.fa-duotone.fa-file-import:after,.fad.fa-arrow-right-to-file:after,.fad.fa-file-import:after{content:"\f56f\f56f"}.fa-duotone.fa-banjo:after,.fad.fa-banjo:after{content:"\f8a3\f8a3"}.fa-duotone.fa-external-link-square:after,.fa-duotone.fa-square-arrow-up-right:after,.fad.fa-external-link-square:after,.fad.fa-square-arrow-up-right:after{content:"\f14c\f14c"}.fa-duotone.fa-light-emergency-on:after,.fad.fa-light-emergency-on:after{content:"\e420\e420"}.fa-duotone.fa-kerning:after,.fad.fa-kerning:after{content:"\f86f\f86f"}.fa-duotone.fa-box-open:after,.fad.fa-box-open:after{content:"\f49e\f49e"}.fa-duotone.fa-square-f:after,.fad.fa-square-f:after{content:"\e270\e270"}.fa-duotone.fa-scroll:after,.fad.fa-scroll:after{content:"\f70e\f70e"}.fa-duotone.fa-spa:after,.fad.fa-spa:after{content:"\f5bb\f5bb"}.fa-duotone.fa-arrow-from-right:after,.fa-duotone.fa-arrow-left-from-line:after,.fad.fa-arrow-from-right:after,.fad.fa-arrow-left-from-line:after{content:"\f344\f344"}.fa-duotone.fa-strawberry:after,.fad.fa-strawberry:after{content:"\e32b\e32b"}.fa-duotone.fa-location-pin-lock:after,.fad.fa-location-pin-lock:after{content:"\e51f\e51f"}.fa-duotone.fa-pause:after,.fad.fa-pause:after{content:"\f04c\f04c"}.fa-duotone.fa-clock-eight-thirty:after,.fad.fa-clock-eight-thirty:after{content:"\e346\e346"}.fa-duotone.fa-plane-alt:after,.fa-duotone.fa-plane-engines:after,.fad.fa-plane-alt:after,.fad.fa-plane-engines:after{content:"\f3de\f3de"}.fa-duotone.fa-hill-avalanche:after,.fad.fa-hill-avalanche:after{content:"\e507\e507"}.fa-duotone.fa-temperature-0:after,.fa-duotone.fa-temperature-empty:after,.fa-duotone.fa-thermometer-0:after,.fa-duotone.fa-thermometer-empty:after,.fad.fa-temperature-0:after,.fad.fa-temperature-empty:after,.fad.fa-thermometer-0:after,.fad.fa-thermometer-empty:after{content:"\f2cb\f2cb"}.fa-duotone.fa-bomb:after,.fad.fa-bomb:after{content:"\f1e2\f1e2"}.fa-duotone.fa-gauge-low:after,.fa-duotone.fa-tachometer-alt-slow:after,.fad.fa-gauge-low:after,.fad.fa-tachometer-alt-slow:after{content:"\f627\f627"}.fa-duotone.fa-registered:after,.fad.fa-registered:after{content:"\f25d\f25d"}.fa-duotone.fa-trash-can-plus:after,.fad.fa-trash-can-plus:after{content:"\e2ac\e2ac"}.fa-duotone.fa-address-card:after,.fa-duotone.fa-contact-card:after,.fa-duotone.fa-vcard:after,.fad.fa-address-card:after,.fad.fa-contact-card:after,.fad.fa-vcard:after{content:"\f2bb\f2bb"}.fa-duotone.fa-balance-scale-right:after,.fa-duotone.fa-scale-unbalanced-flip:after,.fad.fa-balance-scale-right:after,.fad.fa-scale-unbalanced-flip:after{content:"\f516\f516"}.fa-duotone.fa-globe-snow:after,.fad.fa-globe-snow:after{content:"\f7a3\f7a3"}.fa-duotone.fa-subscript:after,.fad.fa-subscript:after{content:"\f12c\f12c"}.fa-duotone.fa-diamond-turn-right:after,.fa-duotone.fa-directions:after,.fad.fa-diamond-turn-right:after,.fad.fa-directions:after{content:"\f5eb\f5eb"}.fa-duotone.fa-integral:after,.fad.fa-integral:after{content:"\f667\f667"}.fa-duotone.fa-burst:after,.fad.fa-burst:after{content:"\e4dc\e4dc"}.fa-duotone.fa-house-laptop:after,.fa-duotone.fa-laptop-house:after,.fad.fa-house-laptop:after,.fad.fa-laptop-house:after{content:"\e066\e066"}.fa-duotone.fa-face-tired:after,.fa-duotone.fa-tired:after,.fad.fa-face-tired:after,.fad.fa-tired:after{content:"\f5c8\f5c8"}.fa-duotone.fa-money-bills:after,.fad.fa-money-bills:after{content:"\e1f3\e1f3"}.fa-duotone.fa-blinds-raised:after,.fad.fa-blinds-raised:after{content:"\f8fd\f8fd"}.fa-duotone.fa-smog:after,.fad.fa-smog:after{content:"\f75f\f75f"}.fa-duotone.fa-ufo-beam:after,.fad.fa-ufo-beam:after{content:"\e048\e048"}.fa-duotone.fa-caret-circle-up:after,.fa-duotone.fa-circle-caret-up:after,.fad.fa-caret-circle-up:after,.fad.fa-circle-caret-up:after{content:"\f331\f331"}.fa-duotone.fa-user-vneck-hair-long:after,.fad.fa-user-vneck-hair-long:after{content:"\e463\e463"}.fa-duotone.fa-square-a-lock:after,.fad.fa-square-a-lock:after{content:"\e44d\e44d"}.fa-duotone.fa-crutch:after,.fad.fa-crutch:after{content:"\f7f7\f7f7"}.fa-duotone.fa-gas-pump-slash:after,.fad.fa-gas-pump-slash:after{content:"\f5f4\f5f4"}.fa-duotone.fa-cloud-arrow-up:after,.fa-duotone.fa-cloud-upload-alt:after,.fa-duotone.fa-cloud-upload:after,.fad.fa-cloud-arrow-up:after,.fad.fa-cloud-upload-alt:after,.fad.fa-cloud-upload:after{content:"\f0ee\f0ee"}.fa-duotone.fa-palette:after,.fad.fa-palette:after{content:"\f53f\f53f"}.fa-duotone.fa-transporter-4:after,.fad.fa-transporter-4:after{content:"\e2a5\e2a5"}.fa-duotone.fa-chart-mixed-up-circle-currency:after,.fad.fa-chart-mixed-up-circle-currency:after{content:"\e5d8\e5d8"}.fa-duotone.fa-objects-align-right:after,.fad.fa-objects-align-right:after{content:"\e3bf\e3bf"}.fa-duotone.fa-arrows-turn-right:after,.fad.fa-arrows-turn-right:after{content:"\e4c0\e4c0"}.fa-duotone.fa-vest:after,.fad.fa-vest:after{content:"\e085\e085"}.fa-duotone.fa-pig:after,.fad.fa-pig:after{content:"\f706\f706"}.fa-duotone.fa-inbox-full:after,.fad.fa-inbox-full:after{content:"\e1ba\e1ba"}.fa-duotone.fa-circle-envelope:after,.fa-duotone.fa-envelope-circle:after,.fad.fa-circle-envelope:after,.fad.fa-envelope-circle:after{content:"\e10c\e10c"}.fa-duotone.fa-construction:after,.fa-duotone.fa-triangle-person-digging:after,.fad.fa-construction:after,.fad.fa-triangle-person-digging:after{content:"\f85d\f85d"}.fa-duotone.fa-ferry:after,.fad.fa-ferry:after{content:"\e4ea\e4ea"}.fa-duotone.fa-bullseye-arrow:after,.fad.fa-bullseye-arrow:after{content:"\f648\f648"}.fa-duotone.fa-arrows-down-to-people:after,.fad.fa-arrows-down-to-people:after{content:"\e4b9\e4b9"}.fa-duotone.fa-seedling:after,.fa-duotone.fa-sprout:after,.fad.fa-seedling:after,.fad.fa-sprout:after{content:"\f4d8\f4d8"}.fa-duotone.fa-clock-seven:after,.fad.fa-clock-seven:after{content:"\e350\e350"}.fa-duotone.fa-arrows-alt-h:after,.fa-duotone.fa-left-right:after,.fad.fa-arrows-alt-h:after,.fad.fa-left-right:after{content:"\f337\f337"}.fa-duotone.fa-boxes-packing:after,.fad.fa-boxes-packing:after{content:"\e4c7\e4c7"}.fa-duotone.fa-arrow-circle-left:after,.fa-duotone.fa-circle-arrow-left:after,.fad.fa-arrow-circle-left:after,.fad.fa-circle-arrow-left:after{content:"\f0a8\f0a8"}.fa-duotone.fa-flashlight:after,.fad.fa-flashlight:after{content:"\f8b8\f8b8"}.fa-duotone.fa-file-jpg:after,.fad.fa-file-jpg:after{content:"\e646\e646"}.fa-duotone.fa-group-arrows-rotate:after,.fad.fa-group-arrows-rotate:after{content:"\e4f6\e4f6"}.fa-duotone.fa-bowl-food:after,.fad.fa-bowl-food:after{content:"\e4c6\e4c6"}.fa-duotone.fa-square-9:after,.fad.fa-square-9:after{content:"\e25e\e25e"}.fa-duotone.fa-candy-cane:after,.fad.fa-candy-cane:after{content:"\f786\f786"}.fa-duotone.fa-arrow-down-wide-short:after,.fa-duotone.fa-sort-amount-asc:after,.fa-duotone.fa-sort-amount-down:after,.fad.fa-arrow-down-wide-short:after,.fad.fa-sort-amount-asc:after,.fad.fa-sort-amount-down:after{content:"\f160\f160"}.fa-duotone.fa-dollar-square:after,.fa-duotone.fa-square-dollar:after,.fa-duotone.fa-usd-square:after,.fad.fa-dollar-square:after,.fad.fa-square-dollar:after,.fad.fa-usd-square:after{content:"\f2e9\f2e9"}.fa-duotone.fa-phone-arrow-right:after,.fad.fa-phone-arrow-right:after{content:"\e5be\e5be"}.fa-duotone.fa-hand-holding-seedling:after,.fad.fa-hand-holding-seedling:after{content:"\f4bf\f4bf"}.fa-duotone.fa-comment-alt-check:after,.fa-duotone.fa-message-check:after,.fad.fa-comment-alt-check:after,.fad.fa-message-check:after{content:"\f4a2\f4a2"}.fa-duotone.fa-cloud-bolt:after,.fa-duotone.fa-thunderstorm:after,.fad.fa-cloud-bolt:after,.fad.fa-thunderstorm:after{content:"\f76c\f76c"}.fa-duotone.fa-chart-line-up-down:after,.fad.fa-chart-line-up-down:after{content:"\e5d7\e5d7"}.fa-duotone.fa-remove-format:after,.fa-duotone.fa-text-slash:after,.fad.fa-remove-format:after,.fad.fa-text-slash:after{content:"\f87d\f87d"}.fa-duotone.fa-watch:after,.fad.fa-watch:after{content:"\f2e1\f2e1"}.fa-duotone.fa-circle-down-left:after,.fad.fa-circle-down-left:after{content:"\e107\e107"}.fa-duotone.fa-text:after,.fad.fa-text:after{content:"\f893\f893"}.fa-duotone.fa-projector:after,.fad.fa-projector:after{content:"\f8d6\f8d6"}.fa-duotone.fa-face-smile-wink:after,.fa-duotone.fa-smile-wink:after,.fad.fa-face-smile-wink:after,.fad.fa-smile-wink:after{content:"\f4da\f4da"}.fa-duotone.fa-tombstone-alt:after,.fa-duotone.fa-tombstone-blank:after,.fad.fa-tombstone-alt:after,.fad.fa-tombstone-blank:after{content:"\f721\f721"}.fa-duotone.fa-chess-king-alt:after,.fa-duotone.fa-chess-king-piece:after,.fad.fa-chess-king-alt:after,.fad.fa-chess-king-piece:after{content:"\f440\f440"}.fa-duotone.fa-circle-6:after,.fad.fa-circle-6:after{content:"\e0f3\e0f3"}.fa-duotone.fa-waves-sine:after,.fad.fa-waves-sine:after{content:"\e65d\e65d"}.fa-duotone.fa-arrow-alt-left:after,.fa-duotone.fa-left:after,.fad.fa-arrow-alt-left:after,.fad.fa-left:after{content:"\f355\f355"}.fa-duotone.fa-file-word:after,.fad.fa-file-word:after{content:"\f1c2\f1c2"}.fa-duotone.fa-file-powerpoint:after,.fad.fa-file-powerpoint:after{content:"\f1c4\f1c4"}.fa-duotone.fa-arrow-alt-square-down:after,.fa-duotone.fa-square-down:after,.fad.fa-arrow-alt-square-down:after,.fad.fa-square-down:after{content:"\f350\f350"}.fa-duotone.fa-objects-align-center-vertical:after,.fad.fa-objects-align-center-vertical:after{content:"\e3bd\e3bd"}.fa-duotone.fa-arrows-h:after,.fa-duotone.fa-arrows-left-right:after,.fad.fa-arrows-h:after,.fad.fa-arrows-left-right:after{content:"\f07e\f07e"}.fa-duotone.fa-house-lock:after,.fad.fa-house-lock:after{content:"\e510\e510"}.fa-duotone.fa-cloud-arrow-down:after,.fa-duotone.fa-cloud-download-alt:after,.fa-duotone.fa-cloud-download:after,.fad.fa-cloud-arrow-down:after,.fad.fa-cloud-download-alt:after,.fad.fa-cloud-download:after{content:"\f0ed\f0ed"}.fa-duotone.fa-wreath:after,.fad.fa-wreath:after{content:"\f7e2\f7e2"}.fa-duotone.fa-children:after,.fad.fa-children:after{content:"\e4e1\e4e1"}.fa-duotone.fa-meter-droplet:after,.fad.fa-meter-droplet:after{content:"\e1ea\e1ea"}.fa-duotone.fa-blackboard:after,.fa-duotone.fa-chalkboard:after,.fad.fa-blackboard:after,.fad.fa-chalkboard:after{content:"\f51b\f51b"}.fa-duotone.fa-user-alt-slash:after,.fa-duotone.fa-user-large-slash:after,.fad.fa-user-alt-slash:after,.fad.fa-user-large-slash:after{content:"\f4fa\f4fa"}.fa-duotone.fa-signal-4:after,.fa-duotone.fa-signal-strong:after,.fad.fa-signal-4:after,.fad.fa-signal-strong:after{content:"\f68f\f68f"}.fa-duotone.fa-lollipop:after,.fa-duotone.fa-lollypop:after,.fad.fa-lollipop:after,.fad.fa-lollypop:after{content:"\e424\e424"}.fa-duotone.fa-list-tree:after,.fad.fa-list-tree:after{content:"\e1d2\e1d2"}.fa-duotone.fa-envelope-open:after,.fad.fa-envelope-open:after{content:"\f2b6\f2b6"}.fa-duotone.fa-draw-circle:after,.fad.fa-draw-circle:after{content:"\f5ed\f5ed"}.fa-duotone.fa-cat-space:after,.fad.fa-cat-space:after{content:"\e001\e001"}.fa-duotone.fa-handshake-alt-slash:after,.fa-duotone.fa-handshake-simple-slash:after,.fad.fa-handshake-alt-slash:after,.fad.fa-handshake-simple-slash:after{content:"\e05f\e05f"}.fa-duotone.fa-rabbit-fast:after,.fa-duotone.fa-rabbit-running:after,.fad.fa-rabbit-fast:after,.fad.fa-rabbit-running:after{content:"\f709\f709"}.fa-duotone.fa-memo-pad:after,.fad.fa-memo-pad:after{content:"\e1da\e1da"}.fa-duotone.fa-mattress-pillow:after,.fad.fa-mattress-pillow:after{content:"\e525\e525"}.fa-duotone.fa-alarm-plus:after,.fad.fa-alarm-plus:after{content:"\f844\f844"}.fa-duotone.fa-alicorn:after,.fad.fa-alicorn:after{content:"\f6b0\f6b0"}.fa-duotone.fa-comment-question:after,.fad.fa-comment-question:after{content:"\e14b\e14b"}.fa-duotone.fa-gingerbread-man:after,.fad.fa-gingerbread-man:after{content:"\f79d\f79d"}.fa-duotone.fa-guarani-sign:after,.fad.fa-guarani-sign:after{content:"\e19a\e19a"}.fa-duotone.fa-burger-fries:after,.fad.fa-burger-fries:after{content:"\e0cd\e0cd"}.fa-duotone.fa-mug-tea:after,.fad.fa-mug-tea:after{content:"\f875\f875"}.fa-duotone.fa-border-top:after,.fad.fa-border-top:after{content:"\f855\f855"}.fa-duotone.fa-arrows-rotate:after,.fa-duotone.fa-refresh:after,.fa-duotone.fa-sync:after,.fad.fa-arrows-rotate:after,.fad.fa-refresh:after,.fad.fa-sync:after{content:"\f021\f021"}.fa-duotone.fa-book-circle:after,.fa-duotone.fa-circle-book-open:after,.fad.fa-book-circle:after,.fad.fa-circle-book-open:after{content:"\e0ff\e0ff"}.fa-duotone.fa-arrows-to-dotted-line:after,.fad.fa-arrows-to-dotted-line:after{content:"\e0a6\e0a6"}.fa-duotone.fa-fire-extinguisher:after,.fad.fa-fire-extinguisher:after{content:"\f134\f134"}.fa-duotone.fa-magnifying-glass-arrows-rotate:after,.fad.fa-magnifying-glass-arrows-rotate:after{content:"\e65e\e65e"}.fa-duotone.fa-garage-open:after,.fad.fa-garage-open:after{content:"\e00b\e00b"}.fa-duotone.fa-shelves-empty:after,.fad.fa-shelves-empty:after{content:"\e246\e246"}.fa-duotone.fa-cruzeiro-sign:after,.fad.fa-cruzeiro-sign:after{content:"\e152\e152"}.fa-duotone.fa-watch-apple:after,.fad.fa-watch-apple:after{content:"\e2cb\e2cb"}.fa-duotone.fa-watch-calculator:after,.fad.fa-watch-calculator:after{content:"\f8f0\f8f0"}.fa-duotone.fa-list-dropdown:after,.fad.fa-list-dropdown:after{content:"\e1cf\e1cf"}.fa-duotone.fa-cabinet-filing:after,.fad.fa-cabinet-filing:after{content:"\f64b\f64b"}.fa-duotone.fa-burger-soda:after,.fad.fa-burger-soda:after{content:"\f858\f858"}.fa-duotone.fa-arrow-square-up:after,.fa-duotone.fa-square-arrow-up:after,.fad.fa-arrow-square-up:after,.fad.fa-square-arrow-up:after{content:"\f33c\f33c"}.fa-duotone.fa-greater-than-equal:after,.fad.fa-greater-than-equal:after{content:"\f532\f532"}.fa-duotone.fa-pallet-box:after,.fad.fa-pallet-box:after{content:"\e208\e208"}.fa-duotone.fa-face-confounded:after,.fad.fa-face-confounded:after{content:"\e36c\e36c"}.fa-duotone.fa-shield-alt:after,.fa-duotone.fa-shield-halved:after,.fad.fa-shield-alt:after,.fad.fa-shield-halved:after{content:"\f3ed\f3ed"}.fa-duotone.fa-truck-plow:after,.fad.fa-truck-plow:after{content:"\f7de\f7de"}.fa-duotone.fa-atlas:after,.fa-duotone.fa-book-atlas:after,.fad.fa-atlas:after,.fad.fa-book-atlas:after{content:"\f558\f558"}.fa-duotone.fa-virus:after,.fad.fa-virus:after{content:"\e074\e074"}.fa-duotone.fa-grid-round-2:after,.fad.fa-grid-round-2:after{content:"\e5db\e5db"}.fa-duotone.fa-comment-middle-top:after,.fad.fa-comment-middle-top:after{content:"\e14a\e14a"}.fa-duotone.fa-wave:after,.fad.fa-wave:after{content:"\e65b\e65b"}.fa-duotone.fa-envelope-circle-check:after,.fad.fa-envelope-circle-check:after{content:"\e4e8\e4e8"}.fa-duotone.fa-layer-group:after,.fad.fa-layer-group:after{content:"\f5fd\f5fd"}.fa-duotone.fa-restroom-simple:after,.fad.fa-restroom-simple:after{content:"\e23a\e23a"}.fa-duotone.fa-arrows-to-dot:after,.fad.fa-arrows-to-dot:after{content:"\e4be\e4be"}.fa-duotone.fa-border-outer:after,.fad.fa-border-outer:after{content:"\f851\f851"}.fa-duotone.fa-hashtag-lock:after,.fad.fa-hashtag-lock:after{content:"\e415\e415"}.fa-duotone.fa-clock-two-thirty:after,.fad.fa-clock-two-thirty:after{content:"\e35b\e35b"}.fa-duotone.fa-archway:after,.fad.fa-archway:after{content:"\f557\f557"}.fa-duotone.fa-heart-circle-check:after,.fad.fa-heart-circle-check:after{content:"\e4fd\e4fd"}.fa-duotone.fa-house-chimney-crack:after,.fa-duotone.fa-house-damage:after,.fad.fa-house-chimney-crack:after,.fad.fa-house-damage:after{content:"\f6f1\f6f1"}.fa-duotone.fa-file-archive:after,.fa-duotone.fa-file-zipper:after,.fad.fa-file-archive:after,.fad.fa-file-zipper:after{content:"\f1c6\f1c6"}.fa-duotone.fa-ticket-perforated:after,.fad.fa-ticket-perforated:after{content:"\e63e\e63e"}.fa-duotone.fa-heart-half:after,.fad.fa-heart-half:after{content:"\e1ab\e1ab"}.fa-duotone.fa-comment-check:after,.fad.fa-comment-check:after{content:"\f4ac\f4ac"}.fa-duotone.fa-square:after,.fad.fa-square:after{content:"\f0c8\f0c8"}.fa-duotone.fa-memo:after,.fad.fa-memo:after{content:"\e1d8\e1d8"}.fa-duotone.fa-glass-martini:after,.fa-duotone.fa-martini-glass-empty:after,.fad.fa-glass-martini:after,.fad.fa-martini-glass-empty:after{content:"\f000\f000"}.fa-duotone.fa-couch:after,.fad.fa-couch:after{content:"\f4b8\f4b8"}.fa-duotone.fa-cedi-sign:after,.fad.fa-cedi-sign:after{content:"\e0df\e0df"}.fa-duotone.fa-italic:after,.fad.fa-italic:after{content:"\f033\f033"}.fa-duotone.fa-glass-citrus:after,.fad.fa-glass-citrus:after{content:"\f869\f869"}.fa-duotone.fa-calendar-lines-pen:after,.fad.fa-calendar-lines-pen:after{content:"\e472\e472"}.fa-duotone.fa-table-cells-column-lock:after,.fad.fa-table-cells-column-lock:after{content:"\e678\e678"}.fa-duotone.fa-church:after,.fad.fa-church:after{content:"\f51d\f51d"}.fa-duotone.fa-person-snowmobiling:after,.fa-duotone.fa-snowmobile:after,.fad.fa-person-snowmobiling:after,.fad.fa-snowmobile:after{content:"\f7d1\f7d1"}.fa-duotone.fa-face-hushed:after,.fad.fa-face-hushed:after{content:"\e37b\e37b"}.fa-duotone.fa-comments-dollar:after,.fad.fa-comments-dollar:after{content:"\f653\f653"}.fa-duotone.fa-tickets-simple:after,.fad.fa-tickets-simple:after{content:"\e659\e659"}.fa-duotone.fa-pickaxe:after,.fad.fa-pickaxe:after{content:"\e5bf\e5bf"}.fa-duotone.fa-link-simple-slash:after,.fad.fa-link-simple-slash:after{content:"\e1ce\e1ce"}.fa-duotone.fa-democrat:after,.fad.fa-democrat:after{content:"\f747\f747"}.fa-duotone.fa-face-confused:after,.fad.fa-face-confused:after{content:"\e36d\e36d"}.fa-duotone.fa-pinball:after,.fad.fa-pinball:after{content:"\e229\e229"}.fa-duotone.fa-z:after,.fad.fa-z:after{content:"\5a\5a"}.fa-duotone.fa-person-skiing:after,.fa-duotone.fa-skiing:after,.fad.fa-person-skiing:after,.fad.fa-skiing:after{content:"\f7c9\f7c9"}.fa-duotone.fa-deer:after,.fad.fa-deer:after{content:"\f78e\f78e"}.fa-duotone.fa-input-pipe:after,.fad.fa-input-pipe:after{content:"\e1be\e1be"}.fa-duotone.fa-road-lock:after,.fad.fa-road-lock:after{content:"\e567\e567"}.fa-duotone.fa-a:after,.fad.fa-a:after{content:"\41\41"}.fa-duotone.fa-bookmark-slash:after,.fad.fa-bookmark-slash:after{content:"\e0c2\e0c2"}.fa-duotone.fa-temperature-arrow-down:after,.fa-duotone.fa-temperature-down:after,.fad.fa-temperature-arrow-down:after,.fad.fa-temperature-down:after{content:"\e03f\e03f"}.fa-duotone.fa-mace:after,.fad.fa-mace:after{content:"\f6f8\f6f8"}.fa-duotone.fa-feather-alt:after,.fa-duotone.fa-feather-pointed:after,.fad.fa-feather-alt:after,.fad.fa-feather-pointed:after{content:"\f56b\f56b"}.fa-duotone.fa-sausage:after,.fad.fa-sausage:after{content:"\f820\f820"}.fa-duotone.fa-trash-can-clock:after,.fad.fa-trash-can-clock:after{content:"\e2aa\e2aa"}.fa-duotone.fa-p:after,.fad.fa-p:after{content:"\50\50"}.fa-duotone.fa-broom-wide:after,.fad.fa-broom-wide:after{content:"\e5d1\e5d1"}.fa-duotone.fa-snowflake:after,.fad.fa-snowflake:after{content:"\f2dc\f2dc"}.fa-duotone.fa-stomach:after,.fad.fa-stomach:after{content:"\f623\f623"}.fa-duotone.fa-newspaper:after,.fad.fa-newspaper:after{content:"\f1ea\f1ea"}.fa-duotone.fa-ad:after,.fa-duotone.fa-rectangle-ad:after,.fad.fa-ad:after,.fad.fa-rectangle-ad:after{content:"\f641\f641"}.fa-duotone.fa-guitar-electric:after,.fad.fa-guitar-electric:after{content:"\f8be\f8be"}.fa-duotone.fa-arrow-turn-down-right:after,.fad.fa-arrow-turn-down-right:after{content:"\e3d6\e3d6"}.fa-duotone.fa-moon-cloud:after,.fad.fa-moon-cloud:after{content:"\f754\f754"}.fa-duotone.fa-bread-slice-butter:after,.fad.fa-bread-slice-butter:after{content:"\e3e1\e3e1"}.fa-duotone.fa-arrow-circle-right:after,.fa-duotone.fa-circle-arrow-right:after,.fad.fa-arrow-circle-right:after,.fad.fa-circle-arrow-right:after{content:"\f0a9\f0a9"}.fa-duotone.fa-user-group-crown:after,.fa-duotone.fa-users-crown:after,.fad.fa-user-group-crown:after,.fad.fa-users-crown:after{content:"\f6a5\f6a5"}.fa-duotone.fa-circle-i:after,.fad.fa-circle-i:after{content:"\e111\e111"}.fa-duotone.fa-toilet-paper-check:after,.fad.fa-toilet-paper-check:after{content:"\e5b2\e5b2"}.fa-duotone.fa-filter-circle-xmark:after,.fad.fa-filter-circle-xmark:after{content:"\e17b\e17b"}.fa-duotone.fa-locust:after,.fad.fa-locust:after{content:"\e520\e520"}.fa-duotone.fa-sort:after,.fa-duotone.fa-unsorted:after,.fad.fa-sort:after,.fad.fa-unsorted:after{content:"\f0dc\f0dc"}.fa-duotone.fa-list-1-2:after,.fa-duotone.fa-list-numeric:after,.fa-duotone.fa-list-ol:after,.fad.fa-list-1-2:after,.fad.fa-list-numeric:after,.fad.fa-list-ol:after{content:"\f0cb\f0cb"}.fa-duotone.fa-chart-waterfall:after,.fad.fa-chart-waterfall:after{content:"\e0eb\e0eb"}.fa-duotone.fa-sparkle:after,.fad.fa-sparkle:after{content:"\e5d6\e5d6"}.fa-duotone.fa-face-party:after,.fad.fa-face-party:after{content:"\e383\e383"}.fa-duotone.fa-kidneys:after,.fad.fa-kidneys:after{content:"\f5fb\f5fb"}.fa-duotone.fa-wifi-exclamation:after,.fad.fa-wifi-exclamation:after{content:"\e2cf\e2cf"}.fa-duotone.fa-chart-network:after,.fad.fa-chart-network:after{content:"\f78a\f78a"}.fa-duotone.fa-person-dress-burst:after,.fad.fa-person-dress-burst:after{content:"\e544\e544"}.fa-duotone.fa-dice-d4:after,.fad.fa-dice-d4:after{content:"\f6d0\f6d0"}.fa-duotone.fa-money-check-alt:after,.fa-duotone.fa-money-check-dollar:after,.fad.fa-money-check-alt:after,.fad.fa-money-check-dollar:after{content:"\f53d\f53d"}.fa-duotone.fa-vector-square:after,.fad.fa-vector-square:after{content:"\f5cb\f5cb"}.fa-duotone.fa-bread-slice:after,.fad.fa-bread-slice:after{content:"\f7ec\f7ec"}.fa-duotone.fa-language:after,.fad.fa-language:after{content:"\f1ab\f1ab"}.fa-duotone.fa-wheat-awn-slash:after,.fad.fa-wheat-awn-slash:after{content:"\e338\e338"}.fa-duotone.fa-face-kiss-wink-heart:after,.fa-duotone.fa-kiss-wink-heart:after,.fad.fa-face-kiss-wink-heart:after,.fad.fa-kiss-wink-heart:after{content:"\f598\f598"}.fa-duotone.fa-dagger:after,.fad.fa-dagger:after{content:"\f6cb\f6cb"}.fa-duotone.fa-podium:after,.fad.fa-podium:after{content:"\f680\f680"}.fa-duotone.fa-memo-circle-check:after,.fad.fa-memo-circle-check:after{content:"\e1d9\e1d9"}.fa-duotone.fa-route-highway:after,.fad.fa-route-highway:after{content:"\f61a\f61a"}.fa-duotone.fa-arrow-alt-to-bottom:after,.fa-duotone.fa-down-to-line:after,.fad.fa-arrow-alt-to-bottom:after,.fad.fa-down-to-line:after{content:"\f34a\f34a"}.fa-duotone.fa-filter:after,.fad.fa-filter:after{content:"\f0b0\f0b0"}.fa-duotone.fa-square-g:after,.fad.fa-square-g:after{content:"\e271\e271"}.fa-duotone.fa-circle-phone:after,.fa-duotone.fa-phone-circle:after,.fad.fa-circle-phone:after,.fad.fa-phone-circle:after{content:"\e11b\e11b"}.fa-duotone.fa-clipboard-prescription:after,.fad.fa-clipboard-prescription:after{content:"\f5e8\f5e8"}.fa-duotone.fa-user-nurse-hair:after,.fad.fa-user-nurse-hair:after{content:"\e45d\e45d"}.fa-duotone.fa-question:after,.fad.fa-question:after{content:"\3f\3f"}.fa-duotone.fa-file-signature:after,.fad.fa-file-signature:after{content:"\f573\f573"}.fa-duotone.fa-toggle-large-on:after,.fad.fa-toggle-large-on:after{content:"\e5b1\e5b1"}.fa-duotone.fa-arrows-alt:after,.fa-duotone.fa-up-down-left-right:after,.fad.fa-arrows-alt:after,.fad.fa-up-down-left-right:after{content:"\f0b2\f0b2"}.fa-duotone.fa-dryer-alt:after,.fa-duotone.fa-dryer-heat:after,.fad.fa-dryer-alt:after,.fad.fa-dryer-heat:after{content:"\f862\f862"}.fa-duotone.fa-house-chimney-user:after,.fad.fa-house-chimney-user:after{content:"\e065\e065"}.fa-duotone.fa-hand-holding-heart:after,.fad.fa-hand-holding-heart:after{content:"\f4be\f4be"}.fa-duotone.fa-arrow-up-small-big:after,.fa-duotone.fa-sort-size-up-alt:after,.fad.fa-arrow-up-small-big:after,.fad.fa-sort-size-up-alt:after{content:"\f88f\f88f"}.fa-duotone.fa-train-track:after,.fad.fa-train-track:after{content:"\e453\e453"}.fa-duotone.fa-puzzle-piece:after,.fad.fa-puzzle-piece:after{content:"\f12e\f12e"}.fa-duotone.fa-money-check:after,.fad.fa-money-check:after{content:"\f53c\f53c"}.fa-duotone.fa-star-half-alt:after,.fa-duotone.fa-star-half-stroke:after,.fad.fa-star-half-alt:after,.fad.fa-star-half-stroke:after{content:"\f5c0\f5c0"}.fa-duotone.fa-file-exclamation:after,.fad.fa-file-exclamation:after{content:"\f31a\f31a"}.fa-duotone.fa-code:after,.fad.fa-code:after{content:"\f121\f121"}.fa-duotone.fa-glass-whiskey:after,.fa-duotone.fa-whiskey-glass:after,.fad.fa-glass-whiskey:after,.fad.fa-whiskey-glass:after{content:"\f7a0\f7a0"}.fa-duotone.fa-moon-stars:after,.fad.fa-moon-stars:after{content:"\f755\f755"}.fa-duotone.fa-building-circle-exclamation:after,.fad.fa-building-circle-exclamation:after{content:"\e4d3\e4d3"}.fa-duotone.fa-clothes-hanger:after,.fad.fa-clothes-hanger:after{content:"\e136\e136"}.fa-duotone.fa-mobile-iphone:after,.fa-duotone.fa-mobile-notch:after,.fad.fa-mobile-iphone:after,.fad.fa-mobile-notch:after{content:"\e1ee\e1ee"}.fa-duotone.fa-magnifying-glass-chart:after,.fad.fa-magnifying-glass-chart:after{content:"\e522\e522"}.fa-duotone.fa-arrow-up-right-from-square:after,.fa-duotone.fa-external-link:after,.fad.fa-arrow-up-right-from-square:after,.fad.fa-external-link:after{content:"\f08e\f08e"}.fa-duotone.fa-cubes-stacked:after,.fad.fa-cubes-stacked:after{content:"\e4e6\e4e6"}.fa-duotone.fa-images-user:after,.fad.fa-images-user:after{content:"\e1b9\e1b9"}.fa-duotone.fa-krw:after,.fa-duotone.fa-won-sign:after,.fa-duotone.fa-won:after,.fad.fa-krw:after,.fad.fa-won-sign:after,.fad.fa-won:after{content:"\f159\f159"}.fa-duotone.fa-image-polaroid-user:after,.fad.fa-image-polaroid-user:after{content:"\e1b6\e1b6"}.fa-duotone.fa-virus-covid:after,.fad.fa-virus-covid:after{content:"\e4a8\e4a8"}.fa-duotone.fa-square-ellipsis:after,.fad.fa-square-ellipsis:after{content:"\e26e\e26e"}.fa-duotone.fa-pie:after,.fad.fa-pie:after{content:"\f705\f705"}.fa-duotone.fa-chess-knight-alt:after,.fa-duotone.fa-chess-knight-piece:after,.fad.fa-chess-knight-alt:after,.fad.fa-chess-knight-piece:after{content:"\f442\f442"}.fa-duotone.fa-austral-sign:after,.fad.fa-austral-sign:after{content:"\e0a9\e0a9"}.fa-duotone.fa-cloud-plus:after,.fad.fa-cloud-plus:after{content:"\e35e\e35e"}.fa-duotone.fa-f:after,.fad.fa-f:after{content:"\46\46"}.fa-duotone.fa-leaf:after,.fad.fa-leaf:after{content:"\f06c\f06c"}.fa-duotone.fa-bed-bunk:after,.fad.fa-bed-bunk:after{content:"\f8f8\f8f8"}.fa-duotone.fa-road:after,.fad.fa-road:after{content:"\f018\f018"}.fa-duotone.fa-cab:after,.fa-duotone.fa-taxi:after,.fad.fa-cab:after,.fad.fa-taxi:after{content:"\f1ba\f1ba"}.fa-duotone.fa-person-circle-plus:after,.fad.fa-person-circle-plus:after{content:"\e541\e541"}.fa-duotone.fa-chart-pie:after,.fa-duotone.fa-pie-chart:after,.fad.fa-chart-pie:after,.fad.fa-pie-chart:after{content:"\f200\f200"}.fa-duotone.fa-bolt-lightning:after,.fad.fa-bolt-lightning:after{content:"\e0b7\e0b7"}.fa-duotone.fa-clock-eight:after,.fad.fa-clock-eight:after{content:"\e345\e345"}.fa-duotone.fa-sack-xmark:after,.fad.fa-sack-xmark:after{content:"\e56a\e56a"}.fa-duotone.fa-file-xls:after,.fad.fa-file-xls:after{content:"\e64d\e64d"}.fa-duotone.fa-file-excel:after,.fad.fa-file-excel:after{content:"\f1c3\f1c3"}.fa-duotone.fa-file-contract:after,.fad.fa-file-contract:after{content:"\f56c\f56c"}.fa-duotone.fa-fish-fins:after,.fad.fa-fish-fins:after{content:"\e4f2\e4f2"}.fa-duotone.fa-circle-q:after,.fad.fa-circle-q:after{content:"\e11e\e11e"}.fa-duotone.fa-building-flag:after,.fad.fa-building-flag:after{content:"\e4d5\e4d5"}.fa-duotone.fa-face-grin-beam:after,.fa-duotone.fa-grin-beam:after,.fad.fa-face-grin-beam:after,.fad.fa-grin-beam:after{content:"\f582\f582"}.fa-duotone.fa-object-ungroup:after,.fad.fa-object-ungroup:after{content:"\f248\f248"}.fa-duotone.fa-face-disguise:after,.fad.fa-face-disguise:after{content:"\e370\e370"}.fa-duotone.fa-circle-arrow-down-right:after,.fad.fa-circle-arrow-down-right:after{content:"\e0fa\e0fa"}.fa-duotone.fa-alien-8bit:after,.fa-duotone.fa-alien-monster:after,.fad.fa-alien-8bit:after,.fad.fa-alien-monster:after{content:"\f8f6\f8f6"}.fa-duotone.fa-hand-point-ribbon:after,.fad.fa-hand-point-ribbon:after{content:"\e1a6\e1a6"}.fa-duotone.fa-poop:after,.fad.fa-poop:after{content:"\f619\f619"}.fa-duotone.fa-object-exclude:after,.fad.fa-object-exclude:after{content:"\e49c\e49c"}.fa-duotone.fa-telescope:after,.fad.fa-telescope:after{content:"\e03e\e03e"}.fa-duotone.fa-location-pin:after,.fa-duotone.fa-map-marker:after,.fad.fa-location-pin:after,.fad.fa-map-marker:after{content:"\f041\f041"}.fa-duotone.fa-square-list:after,.fad.fa-square-list:after{content:"\e489\e489"}.fa-duotone.fa-kaaba:after,.fad.fa-kaaba:after{content:"\f66b\f66b"}.fa-duotone.fa-toilet-paper:after,.fad.fa-toilet-paper:after{content:"\f71e\f71e"}.fa-duotone.fa-hard-hat:after,.fa-duotone.fa-hat-hard:after,.fa-duotone.fa-helmet-safety:after,.fad.fa-hard-hat:after,.fad.fa-hat-hard:after,.fad.fa-helmet-safety:after{content:"\f807\f807"}.fa-duotone.fa-comment-code:after,.fad.fa-comment-code:after{content:"\e147\e147"}.fa-duotone.fa-sim-cards:after,.fad.fa-sim-cards:after{content:"\e251\e251"}.fa-duotone.fa-starship:after,.fad.fa-starship:after{content:"\e039\e039"}.fa-duotone.fa-eject:after,.fad.fa-eject:after{content:"\f052\f052"}.fa-duotone.fa-arrow-alt-circle-right:after,.fa-duotone.fa-circle-right:after,.fad.fa-arrow-alt-circle-right:after,.fad.fa-circle-right:after{content:"\f35a\f35a"}.fa-duotone.fa-plane-circle-check:after,.fad.fa-plane-circle-check:after{content:"\e555\e555"}.fa-duotone.fa-seal:after,.fad.fa-seal:after{content:"\e241\e241"}.fa-duotone.fa-user-cowboy:after,.fad.fa-user-cowboy:after{content:"\f8ea\f8ea"}.fa-duotone.fa-hexagon-vertical-nft:after,.fad.fa-hexagon-vertical-nft:after{content:"\e505\e505"}.fa-duotone.fa-face-rolling-eyes:after,.fa-duotone.fa-meh-rolling-eyes:after,.fad.fa-face-rolling-eyes:after,.fad.fa-meh-rolling-eyes:after{content:"\f5a5\f5a5"}.fa-duotone.fa-bread-loaf:after,.fad.fa-bread-loaf:after{content:"\f7eb\f7eb"}.fa-duotone.fa-rings-wedding:after,.fad.fa-rings-wedding:after{content:"\f81b\f81b"}.fa-duotone.fa-object-group:after,.fad.fa-object-group:after{content:"\f247\f247"}.fa-duotone.fa-french-fries:after,.fad.fa-french-fries:after{content:"\f803\f803"}.fa-duotone.fa-chart-line:after,.fa-duotone.fa-line-chart:after,.fad.fa-chart-line:after,.fad.fa-line-chart:after{content:"\f201\f201"}.fa-duotone.fa-calendar-arrow-down:after,.fa-duotone.fa-calendar-download:after,.fad.fa-calendar-arrow-down:after,.fad.fa-calendar-download:after{content:"\e0d0\e0d0"}.fa-duotone.fa-send-back:after,.fad.fa-send-back:after{content:"\f87e\f87e"}.fa-duotone.fa-mask-ventilator:after,.fad.fa-mask-ventilator:after{content:"\e524\e524"}.fa-duotone.fa-tickets:after,.fad.fa-tickets:after{content:"\e658\e658"}.fa-duotone.fa-signature-lock:after,.fad.fa-signature-lock:after{content:"\e3ca\e3ca"}.fa-duotone.fa-arrow-right:after,.fad.fa-arrow-right:after{content:"\f061\f061"}.fa-duotone.fa-map-signs:after,.fa-duotone.fa-signs-post:after,.fad.fa-map-signs:after,.fad.fa-signs-post:after{content:"\f277\f277"}.fa-duotone.fa-octagon-plus:after,.fa-duotone.fa-plus-octagon:after,.fad.fa-octagon-plus:after,.fad.fa-plus-octagon:after{content:"\f301\f301"}.fa-duotone.fa-cash-register:after,.fad.fa-cash-register:after{content:"\f788\f788"}.fa-duotone.fa-person-circle-question:after,.fad.fa-person-circle-question:after{content:"\e542\e542"}.fa-duotone.fa-melon-slice:after,.fad.fa-melon-slice:after{content:"\e311\e311"}.fa-duotone.fa-space-station-moon:after,.fad.fa-space-station-moon:after{content:"\e033\e033"}.fa-duotone.fa-comment-alt-smile:after,.fa-duotone.fa-message-smile:after,.fad.fa-comment-alt-smile:after,.fad.fa-message-smile:after{content:"\f4aa\f4aa"}.fa-duotone.fa-cup-straw:after,.fad.fa-cup-straw:after{content:"\e363\e363"}.fa-duotone.fa-arrow-alt-from-right:after,.fa-duotone.fa-left-from-line:after,.fad.fa-arrow-alt-from-right:after,.fad.fa-left-from-line:after{content:"\f348\f348"}.fa-duotone.fa-h:after,.fad.fa-h:after{content:"\48\48"}.fa-duotone.fa-basket-shopping-simple:after,.fa-duotone.fa-shopping-basket-alt:after,.fad.fa-basket-shopping-simple:after,.fad.fa-shopping-basket-alt:after{content:"\e0af\e0af"}.fa-duotone.fa-hands-heart:after,.fa-duotone.fa-hands-holding-heart:after,.fad.fa-hands-heart:after,.fad.fa-hands-holding-heart:after{content:"\f4c3\f4c3"}.fa-duotone.fa-clock-nine:after,.fad.fa-clock-nine:after{content:"\e34c\e34c"}.fa-duotone.fa-hammer-brush:after,.fad.fa-hammer-brush:after{content:"\e620\e620"}.fa-duotone.fa-tarp:after,.fad.fa-tarp:after{content:"\e57b\e57b"}.fa-duotone.fa-face-sleepy:after,.fad.fa-face-sleepy:after{content:"\e38e\e38e"}.fa-duotone.fa-hand-horns:after,.fad.fa-hand-horns:after{content:"\e1a9\e1a9"}.fa-duotone.fa-screwdriver-wrench:after,.fa-duotone.fa-tools:after,.fad.fa-screwdriver-wrench:after,.fad.fa-tools:after{content:"\f7d9\f7d9"}.fa-duotone.fa-arrows-to-eye:after,.fad.fa-arrows-to-eye:after{content:"\e4bf\e4bf"}.fa-duotone.fa-circle-three-quarters:after,.fad.fa-circle-three-quarters:after{content:"\e125\e125"}.fa-duotone.fa-trophy-alt:after,.fa-duotone.fa-trophy-star:after,.fad.fa-trophy-alt:after,.fad.fa-trophy-star:after{content:"\f2eb\f2eb"}.fa-duotone.fa-plug-circle-bolt:after,.fad.fa-plug-circle-bolt:after{content:"\e55b\e55b"}.fa-duotone.fa-face-thermometer:after,.fad.fa-face-thermometer:after{content:"\e39a\e39a"}.fa-duotone.fa-grid-round-4:after,.fad.fa-grid-round-4:after{content:"\e5dd\e5dd"}.fa-duotone.fa-sign-posts-wrench:after,.fad.fa-sign-posts-wrench:after{content:"\e626\e626"}.fa-duotone.fa-shirt-running:after,.fad.fa-shirt-running:after{content:"\e3c8\e3c8"}.fa-duotone.fa-book-circle-arrow-up:after,.fad.fa-book-circle-arrow-up:after{content:"\e0bd\e0bd"}.fa-duotone.fa-face-nauseated:after,.fad.fa-face-nauseated:after{content:"\e381\e381"}.fa-duotone.fa-heart:after,.fad.fa-heart:after{content:"\f004\f004"}.fa-duotone.fa-file-chart-pie:after,.fad.fa-file-chart-pie:after{content:"\f65a\f65a"}.fa-duotone.fa-mars-and-venus:after,.fad.fa-mars-and-venus:after{content:"\f224\f224"}.fa-duotone.fa-home-user:after,.fa-duotone.fa-house-user:after,.fad.fa-home-user:after,.fad.fa-house-user:after{content:"\e1b0\e1b0"}.fa-duotone.fa-circle-arrow-down-left:after,.fad.fa-circle-arrow-down-left:after{content:"\e0f9\e0f9"}.fa-duotone.fa-dumpster-fire:after,.fad.fa-dumpster-fire:after{content:"\f794\f794"}.fa-duotone.fa-hexagon-minus:after,.fa-duotone.fa-minus-hexagon:after,.fad.fa-hexagon-minus:after,.fad.fa-minus-hexagon:after{content:"\f307\f307"}.fa-duotone.fa-arrow-alt-to-left:after,.fa-duotone.fa-left-to-line:after,.fad.fa-arrow-alt-to-left:after,.fad.fa-left-to-line:after{content:"\f34b\f34b"}.fa-duotone.fa-house-crack:after,.fad.fa-house-crack:after{content:"\e3b1\e3b1"}.fa-duotone.fa-paw-alt:after,.fa-duotone.fa-paw-simple:after,.fad.fa-paw-alt:after,.fad.fa-paw-simple:after{content:"\f701\f701"}.fa-duotone.fa-arrow-left-long-to-line:after,.fad.fa-arrow-left-long-to-line:after{content:"\e3d4\e3d4"}.fa-duotone.fa-brackets-round:after,.fa-duotone.fa-parentheses:after,.fad.fa-brackets-round:after,.fad.fa-parentheses:after{content:"\e0c5\e0c5"}.fa-duotone.fa-cocktail:after,.fa-duotone.fa-martini-glass-citrus:after,.fad.fa-cocktail:after,.fad.fa-martini-glass-citrus:after{content:"\f561\f561"}.fa-duotone.fa-user-shakespeare:after,.fad.fa-user-shakespeare:after{content:"\e2c2\e2c2"}.fa-duotone.fa-arrow-right-to-arc:after,.fad.fa-arrow-right-to-arc:after{content:"\e4b2\e4b2"}.fa-duotone.fa-face-surprise:after,.fa-duotone.fa-surprise:after,.fad.fa-face-surprise:after,.fad.fa-surprise:after{content:"\f5c2\f5c2"}.fa-duotone.fa-bottle-water:after,.fad.fa-bottle-water:after{content:"\e4c5\e4c5"}.fa-duotone.fa-circle-pause:after,.fa-duotone.fa-pause-circle:after,.fad.fa-circle-pause:after,.fad.fa-pause-circle:after{content:"\f28b\f28b"}.fa-duotone.fa-gauge-circle-plus:after,.fad.fa-gauge-circle-plus:after{content:"\e498\e498"}.fa-duotone.fa-folders:after,.fad.fa-folders:after{content:"\f660\f660"}.fa-duotone.fa-angel:after,.fad.fa-angel:after{content:"\f779\f779"}.fa-duotone.fa-value-absolute:after,.fad.fa-value-absolute:after{content:"\f6a6\f6a6"}.fa-duotone.fa-rabbit:after,.fad.fa-rabbit:after{content:"\f708\f708"}.fa-duotone.fa-toilet-paper-slash:after,.fad.fa-toilet-paper-slash:after{content:"\e072\e072"}.fa-duotone.fa-circle-euro:after,.fad.fa-circle-euro:after{content:"\e5ce\e5ce"}.fa-duotone.fa-apple-alt:after,.fa-duotone.fa-apple-whole:after,.fad.fa-apple-alt:after,.fad.fa-apple-whole:after{content:"\f5d1\f5d1"}.fa-duotone.fa-kitchen-set:after,.fad.fa-kitchen-set:after{content:"\e51a\e51a"}.fa-duotone.fa-diamond-half:after,.fad.fa-diamond-half:after{content:"\e5b7\e5b7"}.fa-duotone.fa-lock-alt:after,.fa-duotone.fa-lock-keyhole:after,.fad.fa-lock-alt:after,.fad.fa-lock-keyhole:after{content:"\f30d\f30d"}.fa-duotone.fa-r:after,.fad.fa-r:after{content:"\52\52"}.fa-duotone.fa-temperature-1:after,.fa-duotone.fa-temperature-quarter:after,.fa-duotone.fa-thermometer-1:after,.fa-duotone.fa-thermometer-quarter:after,.fad.fa-temperature-1:after,.fad.fa-temperature-quarter:after,.fad.fa-thermometer-1:after,.fad.fa-thermometer-quarter:after{content:"\f2ca\f2ca"}.fa-duotone.fa-info-square:after,.fa-duotone.fa-square-info:after,.fad.fa-info-square:after,.fad.fa-square-info:after{content:"\f30f\f30f"}.fa-duotone.fa-wifi-slash:after,.fad.fa-wifi-slash:after{content:"\f6ac\f6ac"}.fa-duotone.fa-toilet-paper-xmark:after,.fad.fa-toilet-paper-xmark:after{content:"\e5b3\e5b3"}.fa-duotone.fa-hands-holding-dollar:after,.fa-duotone.fa-hands-usd:after,.fad.fa-hands-holding-dollar:after,.fad.fa-hands-usd:after{content:"\f4c5\f4c5"}.fa-duotone.fa-cube:after,.fad.fa-cube:after{content:"\f1b2\f1b2"}.fa-duotone.fa-arrow-down-triangle-square:after,.fa-duotone.fa-sort-shapes-down:after,.fad.fa-arrow-down-triangle-square:after,.fad.fa-sort-shapes-down:after{content:"\f888\f888"}.fa-duotone.fa-bitcoin-sign:after,.fad.fa-bitcoin-sign:after{content:"\e0b4\e0b4"}.fa-duotone.fa-shutters:after,.fad.fa-shutters:after{content:"\e449\e449"}.fa-duotone.fa-shield-dog:after,.fad.fa-shield-dog:after{content:"\e573\e573"}.fa-duotone.fa-solar-panel:after,.fad.fa-solar-panel:after{content:"\f5ba\f5ba"}.fa-duotone.fa-lock-open:after,.fad.fa-lock-open:after{content:"\f3c1\f3c1"}.fa-duotone.fa-table-tree:after,.fad.fa-table-tree:after{content:"\e293\e293"}.fa-duotone.fa-house-chimney-heart:after,.fad.fa-house-chimney-heart:after{content:"\e1b2\e1b2"}.fa-duotone.fa-tally-3:after,.fad.fa-tally-3:after{content:"\e296\e296"}.fa-duotone.fa-elevator:after,.fad.fa-elevator:after{content:"\e16d\e16d"}.fa-duotone.fa-money-bill-transfer:after,.fad.fa-money-bill-transfer:after{content:"\e528\e528"}.fa-duotone.fa-money-bill-trend-up:after,.fad.fa-money-bill-trend-up:after{content:"\e529\e529"}.fa-duotone.fa-house-flood-water-circle-arrow-right:after,.fad.fa-house-flood-water-circle-arrow-right:after{content:"\e50f\e50f"}.fa-duotone.fa-poll-h:after,.fa-duotone.fa-square-poll-horizontal:after,.fad.fa-poll-h:after,.fad.fa-square-poll-horizontal:after{content:"\f682\f682"}.fa-duotone.fa-circle:after,.fad.fa-circle:after{content:"\f111\f111"}.fa-duotone.fa-left-to-bracket:after,.fad.fa-left-to-bracket:after{content:"\e66d\e66d"}.fa-duotone.fa-cart-circle-exclamation:after,.fad.fa-cart-circle-exclamation:after{content:"\e3f2\e3f2"}.fa-duotone.fa-sword:after,.fad.fa-sword:after{content:"\f71c\f71c"}.fa-duotone.fa-backward-fast:after,.fa-duotone.fa-fast-backward:after,.fad.fa-backward-fast:after,.fad.fa-fast-backward:after{content:"\f049\f049"}.fa-duotone.fa-recycle:after,.fad.fa-recycle:after{content:"\f1b8\f1b8"}.fa-duotone.fa-user-astronaut:after,.fad.fa-user-astronaut:after{content:"\f4fb\f4fb"}.fa-duotone.fa-interrobang:after,.fad.fa-interrobang:after{content:"\e5ba\e5ba"}.fa-duotone.fa-plane-slash:after,.fad.fa-plane-slash:after{content:"\e069\e069"}.fa-duotone.fa-circle-dashed:after,.fad.fa-circle-dashed:after{content:"\e105\e105"}.fa-duotone.fa-trademark:after,.fad.fa-trademark:after{content:"\f25c\f25c"}.fa-duotone.fa-basketball-ball:after,.fa-duotone.fa-basketball:after,.fad.fa-basketball-ball:after,.fad.fa-basketball:after{content:"\f434\f434"}.fa-duotone.fa-fork-knife:after,.fa-duotone.fa-utensils-alt:after,.fad.fa-fork-knife:after,.fad.fa-utensils-alt:after{content:"\f2e6\f2e6"}.fa-duotone.fa-satellite-dish:after,.fad.fa-satellite-dish:after{content:"\f7c0\f7c0"}.fa-duotone.fa-badge-check:after,.fad.fa-badge-check:after{content:"\f336\f336"}.fa-duotone.fa-arrow-alt-circle-up:after,.fa-duotone.fa-circle-up:after,.fad.fa-arrow-alt-circle-up:after,.fad.fa-circle-up:after{content:"\f35b\f35b"}.fa-duotone.fa-slider:after,.fad.fa-slider:after{content:"\e252\e252"}.fa-duotone.fa-mobile-alt:after,.fa-duotone.fa-mobile-screen-button:after,.fad.fa-mobile-alt:after,.fad.fa-mobile-screen-button:after{content:"\f3cd\f3cd"}.fa-duotone.fa-clock-one-thirty:after,.fad.fa-clock-one-thirty:after{content:"\e34f\e34f"}.fa-duotone.fa-inbox-arrow-up:after,.fa-duotone.fa-inbox-out:after,.fad.fa-inbox-arrow-up:after,.fad.fa-inbox-out:after{content:"\f311\f311"}.fa-duotone.fa-cloud-slash:after,.fad.fa-cloud-slash:after{content:"\e137\e137"}.fa-duotone.fa-volume-high:after,.fa-duotone.fa-volume-up:after,.fad.fa-volume-high:after,.fad.fa-volume-up:after{content:"\f028\f028"}.fa-duotone.fa-users-rays:after,.fad.fa-users-rays:after{content:"\e593\e593"}.fa-duotone.fa-wallet:after,.fad.fa-wallet:after{content:"\f555\f555"}.fa-duotone.fa-octagon-check:after,.fad.fa-octagon-check:after{content:"\e426\e426"}.fa-duotone.fa-flatbread-stuffed:after,.fad.fa-flatbread-stuffed:after{content:"\e40c\e40c"}.fa-duotone.fa-clipboard-check:after,.fad.fa-clipboard-check:after{content:"\f46c\f46c"}.fa-duotone.fa-cart-circle-plus:after,.fad.fa-cart-circle-plus:after{content:"\e3f3\e3f3"}.fa-duotone.fa-shipping-timed:after,.fa-duotone.fa-truck-clock:after,.fad.fa-shipping-timed:after,.fad.fa-truck-clock:after{content:"\f48c\f48c"}.fa-duotone.fa-pool-8-ball:after,.fad.fa-pool-8-ball:after{content:"\e3c5\e3c5"}.fa-duotone.fa-file-audio:after,.fad.fa-file-audio:after{content:"\f1c7\f1c7"}.fa-duotone.fa-turn-down-left:after,.fad.fa-turn-down-left:after{content:"\e331\e331"}.fa-duotone.fa-lock-hashtag:after,.fad.fa-lock-hashtag:after{content:"\e423\e423"}.fa-duotone.fa-chart-radar:after,.fad.fa-chart-radar:after{content:"\e0e7\e0e7"}.fa-duotone.fa-staff:after,.fad.fa-staff:after{content:"\f71b\f71b"}.fa-duotone.fa-burger:after,.fa-duotone.fa-hamburger:after,.fad.fa-burger:after,.fad.fa-hamburger:after{content:"\f805\f805"}.fa-duotone.fa-utility-pole:after,.fad.fa-utility-pole:after{content:"\e2c3\e2c3"}.fa-duotone.fa-transporter-6:after,.fad.fa-transporter-6:after{content:"\e2a7\e2a7"}.fa-duotone.fa-arrow-turn-left:after,.fad.fa-arrow-turn-left:after{content:"\e632\e632"}.fa-duotone.fa-wrench:after,.fad.fa-wrench:after{content:"\f0ad\f0ad"}.fa-duotone.fa-bugs:after,.fad.fa-bugs:after{content:"\e4d0\e4d0"}.fa-duotone.fa-vector-polygon:after,.fad.fa-vector-polygon:after{content:"\e2c7\e2c7"}.fa-duotone.fa-diagram-nested:after,.fad.fa-diagram-nested:after{content:"\e157\e157"}.fa-duotone.fa-rupee-sign:after,.fa-duotone.fa-rupee:after,.fad.fa-rupee-sign:after,.fad.fa-rupee:after{content:"\f156\f156"}.fa-duotone.fa-file-image:after,.fad.fa-file-image:after{content:"\f1c5\f1c5"}.fa-duotone.fa-circle-question:after,.fa-duotone.fa-question-circle:after,.fad.fa-circle-question:after,.fad.fa-question-circle:after{content:"\f059\f059"}.fa-duotone.fa-tickets-perforated:after,.fad.fa-tickets-perforated:after{content:"\e63f\e63f"}.fa-duotone.fa-image-user:after,.fad.fa-image-user:after{content:"\e1b8\e1b8"}.fa-duotone.fa-buoy:after,.fad.fa-buoy:after{content:"\e5b5\e5b5"}.fa-duotone.fa-plane-departure:after,.fad.fa-plane-departure:after{content:"\f5b0\f5b0"}.fa-duotone.fa-handshake-slash:after,.fad.fa-handshake-slash:after{content:"\e060\e060"}.fa-duotone.fa-book-bookmark:after,.fad.fa-book-bookmark:after{content:"\e0bb\e0bb"}.fa-duotone.fa-border-center-h:after,.fad.fa-border-center-h:after{content:"\f89c\f89c"}.fa-duotone.fa-can-food:after,.fad.fa-can-food:after{content:"\e3e6\e3e6"}.fa-duotone.fa-typewriter:after,.fad.fa-typewriter:after{content:"\f8e7\f8e7"}.fa-duotone.fa-arrow-right-from-arc:after,.fad.fa-arrow-right-from-arc:after{content:"\e4b1\e4b1"}.fa-duotone.fa-circle-k:after,.fad.fa-circle-k:after{content:"\e113\e113"}.fa-duotone.fa-face-hand-over-mouth:after,.fad.fa-face-hand-over-mouth:after{content:"\e378\e378"}.fa-duotone.fa-popcorn:after,.fad.fa-popcorn:after{content:"\f819\f819"}.fa-duotone.fa-house-flood:after,.fa-duotone.fa-house-water:after,.fad.fa-house-flood:after,.fad.fa-house-water:after{content:"\f74f\f74f"}.fa-duotone.fa-object-subtract:after,.fad.fa-object-subtract:after{content:"\e49e\e49e"}.fa-duotone.fa-code-branch:after,.fad.fa-code-branch:after{content:"\f126\f126"}.fa-duotone.fa-warehouse-alt:after,.fa-duotone.fa-warehouse-full:after,.fad.fa-warehouse-alt:after,.fad.fa-warehouse-full:after{content:"\f495\f495"}.fa-duotone.fa-hat-cowboy:after,.fad.fa-hat-cowboy:after{content:"\f8c0\f8c0"}.fa-duotone.fa-bridge:after,.fad.fa-bridge:after{content:"\e4c8\e4c8"}.fa-duotone.fa-phone-alt:after,.fa-duotone.fa-phone-flip:after,.fad.fa-phone-alt:after,.fad.fa-phone-flip:after{content:"\f879\f879"}.fa-duotone.fa-arrow-down-from-dotted-line:after,.fad.fa-arrow-down-from-dotted-line:after{content:"\e090\e090"}.fa-duotone.fa-file-doc:after,.fad.fa-file-doc:after{content:"\e5ed\e5ed"}.fa-duotone.fa-square-quarters:after,.fad.fa-square-quarters:after{content:"\e44e\e44e"}.fa-duotone.fa-truck-front:after,.fad.fa-truck-front:after{content:"\e2b7\e2b7"}.fa-duotone.fa-cat:after,.fad.fa-cat:after{content:"\f6be\f6be"}.fa-duotone.fa-trash-xmark:after,.fad.fa-trash-xmark:after{content:"\e2b4\e2b4"}.fa-duotone.fa-caret-circle-left:after,.fa-duotone.fa-circle-caret-left:after,.fad.fa-caret-circle-left:after,.fad.fa-circle-caret-left:after{content:"\f32e\f32e"}.fa-duotone.fa-files:after,.fad.fa-files:after{content:"\e178\e178"}.fa-duotone.fa-anchor-circle-exclamation:after,.fad.fa-anchor-circle-exclamation:after{content:"\e4ab\e4ab"}.fa-duotone.fa-face-clouds:after,.fad.fa-face-clouds:after{content:"\e47d\e47d"}.fa-duotone.fa-user-crown:after,.fad.fa-user-crown:after{content:"\f6a4\f6a4"}.fa-duotone.fa-basket-shopping-plus:after,.fad.fa-basket-shopping-plus:after{content:"\e653\e653"}.fa-duotone.fa-truck-field:after,.fad.fa-truck-field:after{content:"\e58d\e58d"}.fa-duotone.fa-route:after,.fad.fa-route:after{content:"\f4d7\f4d7"}.fa-duotone.fa-cart-circle-check:after,.fad.fa-cart-circle-check:after{content:"\e3f1\e3f1"}.fa-duotone.fa-clipboard-question:after,.fad.fa-clipboard-question:after{content:"\e4e3\e4e3"}.fa-duotone.fa-panorama:after,.fad.fa-panorama:after{content:"\e209\e209"}.fa-duotone.fa-comment-medical:after,.fad.fa-comment-medical:after{content:"\f7f5\f7f5"}.fa-duotone.fa-teeth-open:after,.fad.fa-teeth-open:after{content:"\f62f\f62f"}.fa-duotone.fa-user-tie-hair-long:after,.fad.fa-user-tie-hair-long:after{content:"\e460\e460"}.fa-duotone.fa-file-circle-minus:after,.fad.fa-file-circle-minus:after{content:"\e4ed\e4ed"}.fa-duotone.fa-head-side-medical:after,.fad.fa-head-side-medical:after{content:"\f809\f809"}.fa-duotone.fa-arrow-turn-right:after,.fad.fa-arrow-turn-right:after{content:"\e635\e635"}.fa-duotone.fa-tags:after,.fad.fa-tags:after{content:"\f02c\f02c"}.fa-duotone.fa-wine-glass:after,.fad.fa-wine-glass:after{content:"\f4e3\f4e3"}.fa-duotone.fa-fast-forward:after,.fa-duotone.fa-forward-fast:after,.fad.fa-fast-forward:after,.fad.fa-forward-fast:after{content:"\f050\f050"}.fa-duotone.fa-face-meh-blank:after,.fa-duotone.fa-meh-blank:after,.fad.fa-face-meh-blank:after,.fad.fa-meh-blank:after{content:"\f5a4\f5a4"}.fa-duotone.fa-user-robot:after,.fad.fa-user-robot:after{content:"\e04b\e04b"}.fa-duotone.fa-parking:after,.fa-duotone.fa-square-parking:after,.fad.fa-parking:after,.fad.fa-square-parking:after{content:"\f540\f540"}.fa-duotone.fa-card-diamond:after,.fad.fa-card-diamond:after{content:"\e3ea\e3ea"}.fa-duotone.fa-face-zipper:after,.fad.fa-face-zipper:after{content:"\e3a5\e3a5"}.fa-duotone.fa-face-raised-eyebrow:after,.fad.fa-face-raised-eyebrow:after{content:"\e388\e388"}.fa-duotone.fa-house-signal:after,.fad.fa-house-signal:after{content:"\e012\e012"}.fa-duotone.fa-chevron-square-up:after,.fa-duotone.fa-square-chevron-up:after,.fad.fa-chevron-square-up:after,.fad.fa-square-chevron-up:after{content:"\f32c\f32c"}.fa-duotone.fa-bars-progress:after,.fa-duotone.fa-tasks-alt:after,.fad.fa-bars-progress:after,.fad.fa-tasks-alt:after{content:"\f828\f828"}.fa-duotone.fa-faucet-drip:after,.fad.fa-faucet-drip:after{content:"\e006\e006"}.fa-duotone.fa-arrows-to-line:after,.fad.fa-arrows-to-line:after{content:"\e0a7\e0a7"}.fa-duotone.fa-dolphin:after,.fad.fa-dolphin:after{content:"\e168\e168"}.fa-duotone.fa-arrow-up-right:after,.fad.fa-arrow-up-right:after{content:"\e09f\e09f"}.fa-duotone.fa-circle-r:after,.fad.fa-circle-r:after{content:"\e120\e120"}.fa-duotone.fa-cart-flatbed:after,.fa-duotone.fa-dolly-flatbed:after,.fad.fa-cart-flatbed:after,.fad.fa-dolly-flatbed:after{content:"\f474\f474"}.fa-duotone.fa-ban-smoking:after,.fa-duotone.fa-smoking-ban:after,.fad.fa-ban-smoking:after,.fad.fa-smoking-ban:after{content:"\f54d\f54d"}.fa-duotone.fa-circle-sort-up:after,.fa-duotone.fa-sort-circle-up:after,.fad.fa-circle-sort-up:after,.fad.fa-sort-circle-up:after{content:"\e032\e032"}.fa-duotone.fa-terminal:after,.fad.fa-terminal:after{content:"\f120\f120"}.fa-duotone.fa-mobile-button:after,.fad.fa-mobile-button:after{content:"\f10b\f10b"}.fa-duotone.fa-house-medical-flag:after,.fad.fa-house-medical-flag:after{content:"\e514\e514"}.fa-duotone.fa-basket-shopping:after,.fa-duotone.fa-shopping-basket:after,.fad.fa-basket-shopping:after,.fad.fa-shopping-basket:after{content:"\f291\f291"}.fa-duotone.fa-tape:after,.fad.fa-tape:after{content:"\f4db\f4db"}.fa-duotone.fa-chestnut:after,.fad.fa-chestnut:after{content:"\e3f6\e3f6"}.fa-duotone.fa-bus-alt:after,.fa-duotone.fa-bus-simple:after,.fad.fa-bus-alt:after,.fad.fa-bus-simple:after{content:"\f55e\f55e"}.fa-duotone.fa-eye:after,.fad.fa-eye:after{content:"\f06e\f06e"}.fa-duotone.fa-face-sad-cry:after,.fa-duotone.fa-sad-cry:after,.fad.fa-face-sad-cry:after,.fad.fa-sad-cry:after{content:"\f5b3\f5b3"}.fa-duotone.fa-heat:after,.fad.fa-heat:after{content:"\e00c\e00c"}.fa-duotone.fa-ticket-airline:after,.fa-duotone.fa-ticket-perforated-plane:after,.fa-duotone.fa-ticket-plane:after,.fad.fa-ticket-airline:after,.fad.fa-ticket-perforated-plane:after,.fad.fa-ticket-plane:after{content:"\e29a\e29a"}.fa-duotone.fa-boot-heeled:after,.fad.fa-boot-heeled:after{content:"\e33f\e33f"}.fa-duotone.fa-arrows-minimize:after,.fa-duotone.fa-compress-arrows:after,.fad.fa-arrows-minimize:after,.fad.fa-compress-arrows:after{content:"\e0a5\e0a5"}.fa-duotone.fa-audio-description:after,.fad.fa-audio-description:after{content:"\f29e\f29e"}.fa-duotone.fa-person-military-to-person:after,.fad.fa-person-military-to-person:after{content:"\e54c\e54c"}.fa-duotone.fa-file-shield:after,.fad.fa-file-shield:after{content:"\e4f0\e4f0"}.fa-duotone.fa-hexagon:after,.fad.fa-hexagon:after{content:"\f312\f312"}.fa-duotone.fa-manhole:after,.fad.fa-manhole:after{content:"\e1d6\e1d6"}.fa-duotone.fa-user-slash:after,.fad.fa-user-slash:after{content:"\f506\f506"}.fa-duotone.fa-pen:after,.fad.fa-pen:after{content:"\f304\f304"}.fa-duotone.fa-tower-observation:after,.fad.fa-tower-observation:after{content:"\e586\e586"}.fa-duotone.fa-floppy-disks:after,.fad.fa-floppy-disks:after{content:"\e183\e183"}.fa-duotone.fa-toilet-paper-blank-under:after,.fa-duotone.fa-toilet-paper-reverse-alt:after,.fad.fa-toilet-paper-blank-under:after,.fad.fa-toilet-paper-reverse-alt:after{content:"\e29f\e29f"}.fa-duotone.fa-file-code:after,.fad.fa-file-code:after{content:"\f1c9\f1c9"}.fa-duotone.fa-signal-5:after,.fa-duotone.fa-signal-perfect:after,.fa-duotone.fa-signal:after,.fad.fa-signal-5:after,.fad.fa-signal-perfect:after,.fad.fa-signal:after{content:"\f012\f012"}.fa-duotone.fa-pump:after,.fad.fa-pump:after{content:"\e442\e442"}.fa-duotone.fa-bus:after,.fad.fa-bus:after{content:"\f207\f207"}.fa-duotone.fa-heart-circle-xmark:after,.fad.fa-heart-circle-xmark:after{content:"\e501\e501"}.fa-duotone.fa-arrow-up-left-from-circle:after,.fad.fa-arrow-up-left-from-circle:after{content:"\e09e\e09e"}.fa-duotone.fa-home-lg:after,.fa-duotone.fa-house-chimney:after,.fad.fa-home-lg:after,.fad.fa-house-chimney:after{content:"\e3af\e3af"}.fa-duotone.fa-window-maximize:after,.fad.fa-window-maximize:after{content:"\f2d0\f2d0"}.fa-duotone.fa-dryer:after,.fad.fa-dryer:after{content:"\f861\f861"}.fa-duotone.fa-face-frown:after,.fa-duotone.fa-frown:after,.fad.fa-face-frown:after,.fad.fa-frown:after{content:"\f119\f119"}.fa-duotone.fa-chess-bishop-alt:after,.fa-duotone.fa-chess-bishop-piece:after,.fad.fa-chess-bishop-alt:after,.fad.fa-chess-bishop-piece:after{content:"\f43b\f43b"}.fa-duotone.fa-shirt-tank-top:after,.fad.fa-shirt-tank-top:after{content:"\e3c9\e3c9"}.fa-duotone.fa-diploma:after,.fa-duotone.fa-scroll-ribbon:after,.fad.fa-diploma:after,.fad.fa-scroll-ribbon:after{content:"\f5ea\f5ea"}.fa-duotone.fa-screencast:after,.fad.fa-screencast:after{content:"\e23e\e23e"}.fa-duotone.fa-walker:after,.fad.fa-walker:after{content:"\f831\f831"}.fa-duotone.fa-prescription:after,.fad.fa-prescription:after{content:"\f5b1\f5b1"}.fa-duotone.fa-shop:after,.fa-duotone.fa-store-alt:after,.fad.fa-shop:after,.fad.fa-store-alt:after{content:"\f54f\f54f"}.fa-duotone.fa-floppy-disk:after,.fa-duotone.fa-save:after,.fad.fa-floppy-disk:after,.fad.fa-save:after{content:"\f0c7\f0c7"}.fa-duotone.fa-vihara:after,.fad.fa-vihara:after{content:"\f6a7\f6a7"}.fa-duotone.fa-face-kiss-closed-eyes:after,.fad.fa-face-kiss-closed-eyes:after{content:"\e37d\e37d"}.fa-duotone.fa-balance-scale-left:after,.fa-duotone.fa-scale-unbalanced:after,.fad.fa-balance-scale-left:after,.fad.fa-scale-unbalanced:after{content:"\f515\f515"}.fa-duotone.fa-file-user:after,.fad.fa-file-user:after{content:"\f65c\f65c"}.fa-duotone.fa-user-police-tie:after,.fad.fa-user-police-tie:after{content:"\e334\e334"}.fa-duotone.fa-face-tongue-money:after,.fad.fa-face-tongue-money:after{content:"\e39d\e39d"}.fa-duotone.fa-tennis-ball:after,.fad.fa-tennis-ball:after{content:"\f45e\f45e"}.fa-duotone.fa-square-l:after,.fad.fa-square-l:after{content:"\e275\e275"}.fa-duotone.fa-sort-asc:after,.fa-duotone.fa-sort-up:after,.fad.fa-sort-asc:after,.fad.fa-sort-up:after{content:"\f0de\f0de"}.fa-duotone.fa-calendar-arrow-up:after,.fa-duotone.fa-calendar-upload:after,.fad.fa-calendar-arrow-up:after,.fad.fa-calendar-upload:after{content:"\e0d1\e0d1"}.fa-duotone.fa-comment-dots:after,.fa-duotone.fa-commenting:after,.fad.fa-comment-dots:after,.fad.fa-commenting:after{content:"\f4ad\f4ad"}.fa-duotone.fa-plant-wilt:after,.fad.fa-plant-wilt:after{content:"\e5aa\e5aa"}.fa-duotone.fa-scarf:after,.fad.fa-scarf:after{content:"\f7c1\f7c1"}.fa-duotone.fa-album-circle-plus:after,.fad.fa-album-circle-plus:after{content:"\e48c\e48c"}.fa-duotone.fa-user-nurse-hair-long:after,.fad.fa-user-nurse-hair-long:after{content:"\e45e\e45e"}.fa-duotone.fa-diamond:after,.fad.fa-diamond:after{content:"\f219\f219"}.fa-duotone.fa-arrow-alt-square-left:after,.fa-duotone.fa-square-left:after,.fad.fa-arrow-alt-square-left:after,.fad.fa-square-left:after{content:"\f351\f351"}.fa-duotone.fa-face-grin-squint:after,.fa-duotone.fa-grin-squint:after,.fad.fa-face-grin-squint:after,.fad.fa-grin-squint:after{content:"\f585\f585"}.fa-duotone.fa-circle-ellipsis-vertical:after,.fad.fa-circle-ellipsis-vertical:after{content:"\e10b\e10b"}.fa-duotone.fa-hand-holding-dollar:after,.fa-duotone.fa-hand-holding-usd:after,.fad.fa-hand-holding-dollar:after,.fad.fa-hand-holding-usd:after{content:"\f4c0\f4c0"}.fa-duotone.fa-grid-dividers:after,.fad.fa-grid-dividers:after{content:"\e3ad\e3ad"}.fa-duotone.fa-bacterium:after,.fad.fa-bacterium:after{content:"\e05a\e05a"}.fa-duotone.fa-hand-pointer:after,.fad.fa-hand-pointer:after{content:"\f25a\f25a"}.fa-duotone.fa-drum-steelpan:after,.fad.fa-drum-steelpan:after{content:"\f56a\f56a"}.fa-duotone.fa-hand-scissors:after,.fad.fa-hand-scissors:after{content:"\f257\f257"}.fa-duotone.fa-hands-praying:after,.fa-duotone.fa-praying-hands:after,.fad.fa-hands-praying:after,.fad.fa-praying-hands:after{content:"\f684\f684"}.fa-duotone.fa-face-pensive:after,.fad.fa-face-pensive:after{content:"\e384\e384"}.fa-duotone.fa-user-music:after,.fad.fa-user-music:after{content:"\f8eb\f8eb"}.fa-duotone.fa-arrow-right-rotate:after,.fa-duotone.fa-arrow-rotate-forward:after,.fa-duotone.fa-arrow-rotate-right:after,.fa-duotone.fa-redo:after,.fad.fa-arrow-right-rotate:after,.fad.fa-arrow-rotate-forward:after,.fad.fa-arrow-rotate-right:after,.fad.fa-redo:after{content:"\f01e\f01e"}.fa-duotone.fa-comments-alt-dollar:after,.fa-duotone.fa-messages-dollar:after,.fad.fa-comments-alt-dollar:after,.fad.fa-messages-dollar:after{content:"\f652\f652"}.fa-duotone.fa-sensor-on:after,.fad.fa-sensor-on:after{content:"\e02b\e02b"}.fa-duotone.fa-balloon:after,.fad.fa-balloon:after{content:"\e2e3\e2e3"}.fa-duotone.fa-biohazard:after,.fad.fa-biohazard:after{content:"\f780\f780"}.fa-duotone.fa-chess-queen-alt:after,.fa-duotone.fa-chess-queen-piece:after,.fad.fa-chess-queen-alt:after,.fad.fa-chess-queen-piece:after{content:"\f446\f446"}.fa-duotone.fa-location-crosshairs:after,.fa-duotone.fa-location:after,.fad.fa-location-crosshairs:after,.fad.fa-location:after{content:"\f601\f601"}.fa-duotone.fa-mars-double:after,.fad.fa-mars-double:after{content:"\f227\f227"}.fa-duotone.fa-left-from-bracket:after,.fad.fa-left-from-bracket:after{content:"\e66c\e66c"}.fa-duotone.fa-house-leave:after,.fa-duotone.fa-house-person-depart:after,.fa-duotone.fa-house-person-leave:after,.fad.fa-house-leave:after,.fad.fa-house-person-depart:after,.fad.fa-house-person-leave:after{content:"\e00f\e00f"}.fa-duotone.fa-ruler-triangle:after,.fad.fa-ruler-triangle:after{content:"\f61c\f61c"}.fa-duotone.fa-card-club:after,.fad.fa-card-club:after{content:"\e3e9\e3e9"}.fa-duotone.fa-child-dress:after,.fad.fa-child-dress:after{content:"\e59c\e59c"}.fa-duotone.fa-users-between-lines:after,.fad.fa-users-between-lines:after{content:"\e591\e591"}.fa-duotone.fa-lungs-virus:after,.fad.fa-lungs-virus:after{content:"\e067\e067"}.fa-duotone.fa-spinner-third:after,.fad.fa-spinner-third:after{content:"\f3f4\f3f4"}.fa-duotone.fa-face-grin-tears:after,.fa-duotone.fa-grin-tears:after,.fad.fa-face-grin-tears:after,.fad.fa-grin-tears:after{content:"\f588\f588"}.fa-duotone.fa-phone:after,.fad.fa-phone:after{content:"\f095\f095"}.fa-duotone.fa-computer-mouse-scrollwheel:after,.fa-duotone.fa-mouse-alt:after,.fad.fa-computer-mouse-scrollwheel:after,.fad.fa-mouse-alt:after{content:"\f8cd\f8cd"}.fa-duotone.fa-calendar-times:after,.fa-duotone.fa-calendar-xmark:after,.fad.fa-calendar-times:after,.fad.fa-calendar-xmark:after{content:"\f273\f273"}.fa-duotone.fa-child-reaching:after,.fad.fa-child-reaching:after{content:"\e59d\e59d"}.fa-duotone.fa-table-layout:after,.fad.fa-table-layout:after{content:"\e290\e290"}.fa-duotone.fa-narwhal:after,.fad.fa-narwhal:after{content:"\f6fe\f6fe"}.fa-duotone.fa-ramp-loading:after,.fad.fa-ramp-loading:after{content:"\f4d4\f4d4"}.fa-duotone.fa-calendar-circle-plus:after,.fad.fa-calendar-circle-plus:after{content:"\e470\e470"}.fa-duotone.fa-toothbrush:after,.fad.fa-toothbrush:after{content:"\f635\f635"}.fa-duotone.fa-border-inner:after,.fad.fa-border-inner:after{content:"\f84e\f84e"}.fa-duotone.fa-paw-claws:after,.fad.fa-paw-claws:after{content:"\f702\f702"}.fa-duotone.fa-kiwi-fruit:after,.fad.fa-kiwi-fruit:after{content:"\e30c\e30c"}.fa-duotone.fa-traffic-light-slow:after,.fad.fa-traffic-light-slow:after{content:"\f639\f639"}.fa-duotone.fa-rectangle-code:after,.fad.fa-rectangle-code:after{content:"\e322\e322"}.fa-duotone.fa-head-side-virus:after,.fad.fa-head-side-virus:after{content:"\e064\e064"}.fa-duotone.fa-keyboard-brightness:after,.fad.fa-keyboard-brightness:after{content:"\e1c0\e1c0"}.fa-duotone.fa-books-medical:after,.fad.fa-books-medical:after{content:"\f7e8\f7e8"}.fa-duotone.fa-lightbulb-slash:after,.fad.fa-lightbulb-slash:after{content:"\f673\f673"}.fa-duotone.fa-home-blank:after,.fa-duotone.fa-house-blank:after,.fad.fa-home-blank:after,.fad.fa-house-blank:after{content:"\e487\e487"}.fa-duotone.fa-square-5:after,.fad.fa-square-5:after{content:"\e25a\e25a"}.fa-duotone.fa-heart-square:after,.fa-duotone.fa-square-heart:after,.fad.fa-heart-square:after,.fad.fa-square-heart:after{content:"\f4c8\f4c8"}.fa-duotone.fa-puzzle:after,.fad.fa-puzzle:after{content:"\e443\e443"}.fa-duotone.fa-user-cog:after,.fa-duotone.fa-user-gear:after,.fad.fa-user-cog:after,.fad.fa-user-gear:after{content:"\f4fe\f4fe"}.fa-duotone.fa-pipe-circle-check:after,.fad.fa-pipe-circle-check:after{content:"\e436\e436"}.fa-duotone.fa-arrow-up-1-9:after,.fa-duotone.fa-sort-numeric-up:after,.fad.fa-arrow-up-1-9:after,.fad.fa-sort-numeric-up:after{content:"\f163\f163"}.fa-duotone.fa-octagon-exclamation:after,.fad.fa-octagon-exclamation:after{content:"\e204\e204"}.fa-duotone.fa-dial-low:after,.fad.fa-dial-low:after{content:"\e15d\e15d"}.fa-duotone.fa-door-closed:after,.fad.fa-door-closed:after{content:"\f52a\f52a"}.fa-duotone.fa-laptop-mobile:after,.fa-duotone.fa-phone-laptop:after,.fad.fa-laptop-mobile:after,.fad.fa-phone-laptop:after{content:"\f87a\f87a"}.fa-duotone.fa-conveyor-belt-alt:after,.fa-duotone.fa-conveyor-belt-boxes:after,.fad.fa-conveyor-belt-alt:after,.fad.fa-conveyor-belt-boxes:after{content:"\f46f\f46f"}.fa-duotone.fa-shield-virus:after,.fad.fa-shield-virus:after{content:"\e06c\e06c"}.fa-duotone.fa-starfighter-alt-advanced:after,.fa-duotone.fa-starfighter-twin-ion-engine-advanced:after,.fad.fa-starfighter-alt-advanced:after,.fad.fa-starfighter-twin-ion-engine-advanced:after{content:"\e28e\e28e"}.fa-duotone.fa-dice-six:after,.fad.fa-dice-six:after{content:"\f526\f526"}.fa-duotone.fa-starfighter-alt:after,.fa-duotone.fa-starfighter-twin-ion-engine:after,.fad.fa-starfighter-alt:after,.fad.fa-starfighter-twin-ion-engine:after{content:"\e038\e038"}.fa-duotone.fa-rocket-launch:after,.fad.fa-rocket-launch:after{content:"\e027\e027"}.fa-duotone.fa-mosquito-net:after,.fad.fa-mosquito-net:after{content:"\e52c\e52c"}.fa-duotone.fa-vent-damper:after,.fad.fa-vent-damper:after{content:"\e465\e465"}.fa-duotone.fa-bridge-water:after,.fad.fa-bridge-water:after{content:"\e4ce\e4ce"}.fa-duotone.fa-ban-bug:after,.fa-duotone.fa-debug:after,.fad.fa-ban-bug:after,.fad.fa-debug:after{content:"\f7f9\f7f9"}.fa-duotone.fa-person-booth:after,.fad.fa-person-booth:after{content:"\f756\f756"}.fa-duotone.fa-text-width:after,.fad.fa-text-width:after{content:"\f035\f035"}.fa-duotone.fa-garage-car:after,.fad.fa-garage-car:after{content:"\e00a\e00a"}.fa-duotone.fa-square-kanban:after,.fad.fa-square-kanban:after{content:"\e488\e488"}.fa-duotone.fa-hat-wizard:after,.fad.fa-hat-wizard:after{content:"\f6e8\f6e8"}.fa-duotone.fa-chart-kanban:after,.fad.fa-chart-kanban:after{content:"\e64f\e64f"}.fa-duotone.fa-pen-fancy:after,.fad.fa-pen-fancy:after{content:"\f5ac\f5ac"}.fa-duotone.fa-coffee-pot:after,.fad.fa-coffee-pot:after{content:"\e002\e002"}.fa-duotone.fa-mouse-field:after,.fad.fa-mouse-field:after{content:"\e5a8\e5a8"}.fa-duotone.fa-digging:after,.fa-duotone.fa-person-digging:after,.fad.fa-digging:after,.fad.fa-person-digging:after{content:"\f85e\f85e"}.fa-duotone.fa-shower-alt:after,.fa-duotone.fa-shower-down:after,.fad.fa-shower-alt:after,.fad.fa-shower-down:after{content:"\e24d\e24d"}.fa-duotone.fa-box-circle-check:after,.fad.fa-box-circle-check:after{content:"\e0c4\e0c4"}.fa-duotone.fa-brightness:after,.fad.fa-brightness:after{content:"\e0c9\e0c9"}.fa-duotone.fa-car-side-bolt:after,.fad.fa-car-side-bolt:after{content:"\e344\e344"}.fa-duotone.fa-file-xml:after,.fad.fa-file-xml:after{content:"\e654\e654"}.fa-duotone.fa-ornament:after,.fad.fa-ornament:after{content:"\f7b8\f7b8"}.fa-duotone.fa-phone-arrow-down-left:after,.fa-duotone.fa-phone-arrow-down:after,.fa-duotone.fa-phone-incoming:after,.fad.fa-phone-arrow-down-left:after,.fad.fa-phone-arrow-down:after,.fad.fa-phone-incoming:after{content:"\e223\e223"}.fa-duotone.fa-cloud-word:after,.fad.fa-cloud-word:after{content:"\e138\e138"}.fa-duotone.fa-hand-fingers-crossed:after,.fad.fa-hand-fingers-crossed:after{content:"\e1a3\e1a3"}.fa-duotone.fa-trash:after,.fad.fa-trash:after{content:"\f1f8\f1f8"}.fa-duotone.fa-gauge-simple-med:after,.fa-duotone.fa-gauge-simple:after,.fa-duotone.fa-tachometer-average:after,.fad.fa-gauge-simple-med:after,.fad.fa-gauge-simple:after,.fad.fa-tachometer-average:after{content:"\f629\f629"}.fa-duotone.fa-arrow-down-small-big:after,.fa-duotone.fa-sort-size-down-alt:after,.fad.fa-arrow-down-small-big:after,.fad.fa-sort-size-down-alt:after{content:"\f88d\f88d"}.fa-duotone.fa-book-medical:after,.fad.fa-book-medical:after{content:"\f7e6\f7e6"}.fa-duotone.fa-face-melting:after,.fad.fa-face-melting:after{content:"\e483\e483"}.fa-duotone.fa-poo:after,.fad.fa-poo:after{content:"\f2fe\f2fe"}.fa-duotone.fa-pen-alt-slash:after,.fa-duotone.fa-pen-clip-slash:after,.fad.fa-pen-alt-slash:after,.fad.fa-pen-clip-slash:after{content:"\e20f\e20f"}.fa-duotone.fa-quote-right-alt:after,.fa-duotone.fa-quote-right:after,.fad.fa-quote-right-alt:after,.fad.fa-quote-right:after{content:"\f10e\f10e"}.fa-duotone.fa-scroll-old:after,.fad.fa-scroll-old:after{content:"\f70f\f70f"}.fa-duotone.fa-guitars:after,.fad.fa-guitars:after{content:"\f8bf\f8bf"}.fa-duotone.fa-phone-xmark:after,.fad.fa-phone-xmark:after{content:"\e227\e227"}.fa-duotone.fa-hose:after,.fad.fa-hose:after{content:"\e419\e419"}.fa-duotone.fa-clock-six:after,.fad.fa-clock-six:after{content:"\e352\e352"}.fa-duotone.fa-shirt:after,.fa-duotone.fa-t-shirt:after,.fa-duotone.fa-tshirt:after,.fad.fa-shirt:after,.fad.fa-t-shirt:after,.fad.fa-tshirt:after{content:"\f553\f553"}.fa-duotone.fa-billboard:after,.fad.fa-billboard:after{content:"\e5cd\e5cd"}.fa-duotone.fa-square-r:after,.fad.fa-square-r:after{content:"\e27c\e27c"}.fa-duotone.fa-cubes:after,.fad.fa-cubes:after{content:"\f1b3\f1b3"}.fa-duotone.fa-envelope-open-dollar:after,.fad.fa-envelope-open-dollar:after{content:"\f657\f657"}.fa-duotone.fa-divide:after,.fad.fa-divide:after{content:"\f529\f529"}.fa-duotone.fa-sun-cloud:after,.fad.fa-sun-cloud:after{content:"\f763\f763"}.fa-duotone.fa-lamp-floor:after,.fad.fa-lamp-floor:after{content:"\e015\e015"}.fa-duotone.fa-square-7:after,.fad.fa-square-7:after{content:"\e25c\e25c"}.fa-duotone.fa-tenge-sign:after,.fa-duotone.fa-tenge:after,.fad.fa-tenge-sign:after,.fad.fa-tenge:after{content:"\f7d7\f7d7"}.fa-duotone.fa-headphones:after,.fad.fa-headphones:after{content:"\f025\f025"}.fa-duotone.fa-hands-holding:after,.fad.fa-hands-holding:after{content:"\f4c2\f4c2"}.fa-duotone.fa-campfire:after,.fad.fa-campfire:after{content:"\f6ba\f6ba"}.fa-duotone.fa-circle-ampersand:after,.fad.fa-circle-ampersand:after{content:"\e0f8\e0f8"}.fa-duotone.fa-snowflakes:after,.fad.fa-snowflakes:after{content:"\f7cf\f7cf"}.fa-duotone.fa-hands-clapping:after,.fad.fa-hands-clapping:after{content:"\e1a8\e1a8"}.fa-duotone.fa-republican:after,.fad.fa-republican:after{content:"\f75e\f75e"}.fa-duotone.fa-leaf-maple:after,.fad.fa-leaf-maple:after{content:"\f6f6\f6f6"}.fa-duotone.fa-arrow-left:after,.fad.fa-arrow-left:after{content:"\f060\f060"}.fa-duotone.fa-person-circle-xmark:after,.fad.fa-person-circle-xmark:after{content:"\e543\e543"}.fa-duotone.fa-ruler:after,.fad.fa-ruler:after{content:"\f545\f545"}.fa-duotone.fa-arrow-left-from-bracket:after,.fad.fa-arrow-left-from-bracket:after{content:"\e668\e668"}.fa-duotone.fa-cup-straw-swoosh:after,.fad.fa-cup-straw-swoosh:after{content:"\e364\e364"}.fa-duotone.fa-temperature-hot:after,.fa-duotone.fa-temperature-sun:after,.fad.fa-temperature-hot:after,.fad.fa-temperature-sun:after{content:"\f76a\f76a"}.fa-duotone.fa-align-left:after,.fad.fa-align-left:after{content:"\f036\f036"}.fa-duotone.fa-dice-d6:after,.fad.fa-dice-d6:after{content:"\f6d1\f6d1"}.fa-duotone.fa-restroom:after,.fad.fa-restroom:after{content:"\f7bd\f7bd"}.fa-duotone.fa-high-definition:after,.fa-duotone.fa-rectangle-hd:after,.fad.fa-high-definition:after,.fad.fa-rectangle-hd:after{content:"\e1ae\e1ae"}.fa-duotone.fa-j:after,.fad.fa-j:after{content:"\4a\4a"}.fa-duotone.fa-galaxy:after,.fad.fa-galaxy:after{content:"\e008\e008"}.fa-duotone.fa-users-viewfinder:after,.fad.fa-users-viewfinder:after{content:"\e595\e595"}.fa-duotone.fa-file-video:after,.fad.fa-file-video:after{content:"\f1c8\f1c8"}.fa-duotone.fa-cherries:after,.fad.fa-cherries:after{content:"\e0ec\e0ec"}.fa-duotone.fa-external-link-alt:after,.fa-duotone.fa-up-right-from-square:after,.fad.fa-external-link-alt:after,.fad.fa-up-right-from-square:after{content:"\f35d\f35d"}.fa-duotone.fa-circle-sort:after,.fa-duotone.fa-sort-circle:after,.fad.fa-circle-sort:after,.fad.fa-sort-circle:after{content:"\e030\e030"}.fa-duotone.fa-table-cells:after,.fa-duotone.fa-th:after,.fad.fa-table-cells:after,.fad.fa-th:after{content:"\f00a\f00a"}.fa-duotone.fa-bag-shopping-minus:after,.fad.fa-bag-shopping-minus:after{content:"\e650\e650"}.fa-duotone.fa-file-pdf:after,.fad.fa-file-pdf:after{content:"\f1c1\f1c1"}.fa-duotone.fa-siren:after,.fad.fa-siren:after{content:"\e02d\e02d"}.fa-duotone.fa-arrow-up-to-dotted-line:after,.fad.fa-arrow-up-to-dotted-line:after{content:"\e0a1\e0a1"}.fa-duotone.fa-image-landscape:after,.fa-duotone.fa-landscape:after,.fad.fa-image-landscape:after,.fad.fa-landscape:after{content:"\e1b5\e1b5"}.fa-duotone.fa-tank-water:after,.fad.fa-tank-water:after{content:"\e452\e452"}.fa-duotone.fa-curling-stone:after,.fa-duotone.fa-curling:after,.fad.fa-curling-stone:after,.fad.fa-curling:after{content:"\f44a\f44a"}.fa-duotone.fa-gamepad-alt:after,.fa-duotone.fa-gamepad-modern:after,.fad.fa-gamepad-alt:after,.fad.fa-gamepad-modern:after{content:"\e5a2\e5a2"}.fa-duotone.fa-messages-question:after,.fad.fa-messages-question:after{content:"\e1e7\e1e7"}.fa-duotone.fa-bible:after,.fa-duotone.fa-book-bible:after,.fad.fa-bible:after,.fad.fa-book-bible:after{content:"\f647\f647"}.fa-duotone.fa-o:after,.fad.fa-o:after{content:"\4f\4f"}.fa-duotone.fa-medkit:after,.fa-duotone.fa-suitcase-medical:after,.fad.fa-medkit:after,.fad.fa-suitcase-medical:after{content:"\f0fa\f0fa"}.fa-duotone.fa-briefcase-arrow-right:after,.fad.fa-briefcase-arrow-right:after{content:"\e2f2\e2f2"}.fa-duotone.fa-expand-wide:after,.fad.fa-expand-wide:after{content:"\f320\f320"}.fa-duotone.fa-clock-eleven-thirty:after,.fad.fa-clock-eleven-thirty:after{content:"\e348\e348"}.fa-duotone.fa-rv:after,.fad.fa-rv:after{content:"\f7be\f7be"}.fa-duotone.fa-user-secret:after,.fad.fa-user-secret:after{content:"\f21b\f21b"}.fa-duotone.fa-otter:after,.fad.fa-otter:after{content:"\f700\f700"}.fa-duotone.fa-dreidel:after,.fad.fa-dreidel:after{content:"\f792\f792"}.fa-duotone.fa-female:after,.fa-duotone.fa-person-dress:after,.fad.fa-female:after,.fad.fa-person-dress:after{content:"\f182\f182"}.fa-duotone.fa-comment-dollar:after,.fad.fa-comment-dollar:after{content:"\f651\f651"}.fa-duotone.fa-briefcase-clock:after,.fa-duotone.fa-business-time:after,.fad.fa-briefcase-clock:after,.fad.fa-business-time:after{content:"\f64a\f64a"}.fa-duotone.fa-flower-tulip:after,.fad.fa-flower-tulip:after{content:"\f801\f801"}.fa-duotone.fa-people-pants-simple:after,.fad.fa-people-pants-simple:after{content:"\e21a\e21a"}.fa-duotone.fa-cloud-drizzle:after,.fad.fa-cloud-drizzle:after{content:"\f738\f738"}.fa-duotone.fa-table-cells-large:after,.fa-duotone.fa-th-large:after,.fad.fa-table-cells-large:after,.fad.fa-th-large:after{content:"\f009\f009"}.fa-duotone.fa-book-tanakh:after,.fa-duotone.fa-tanakh:after,.fad.fa-book-tanakh:after,.fad.fa-tanakh:after{content:"\f827\f827"}.fa-duotone.fa-solar-system:after,.fad.fa-solar-system:after{content:"\e02f\e02f"}.fa-duotone.fa-seal-question:after,.fad.fa-seal-question:after{content:"\e243\e243"}.fa-duotone.fa-phone-volume:after,.fa-duotone.fa-volume-control-phone:after,.fad.fa-phone-volume:after,.fad.fa-volume-control-phone:after{content:"\f2a0\f2a0"}.fa-duotone.fa-disc-drive:after,.fad.fa-disc-drive:after{content:"\f8b5\f8b5"}.fa-duotone.fa-hat-cowboy-side:after,.fad.fa-hat-cowboy-side:after{content:"\f8c1\f8c1"}.fa-duotone.fa-rows:after,.fa-duotone.fa-table-rows:after,.fad.fa-rows:after,.fad.fa-table-rows:after{content:"\e292\e292"}.fa-duotone.fa-location-exclamation:after,.fa-duotone.fa-map-marker-exclamation:after,.fad.fa-location-exclamation:after,.fad.fa-map-marker-exclamation:after{content:"\f608\f608"}.fa-duotone.fa-face-fearful:after,.fad.fa-face-fearful:after{content:"\e375\e375"}.fa-duotone.fa-clipboard-user:after,.fad.fa-clipboard-user:after{content:"\f7f3\f7f3"}.fa-duotone.fa-bus-school:after,.fad.fa-bus-school:after{content:"\f5dd\f5dd"}.fa-duotone.fa-film-slash:after,.fad.fa-film-slash:after{content:"\e179\e179"}.fa-duotone.fa-square-arrow-down-right:after,.fad.fa-square-arrow-down-right:after{content:"\e262\e262"}.fa-duotone.fa-book-sparkles:after,.fa-duotone.fa-book-spells:after,.fad.fa-book-sparkles:after,.fad.fa-book-spells:after{content:"\f6b8\f6b8"}.fa-duotone.fa-washer:after,.fa-duotone.fa-washing-machine:after,.fad.fa-washer:after,.fad.fa-washing-machine:after{content:"\f898\f898"}.fa-duotone.fa-child:after,.fad.fa-child:after{content:"\f1ae\f1ae"}.fa-duotone.fa-lira-sign:after,.fad.fa-lira-sign:after{content:"\f195\f195"}.fa-duotone.fa-user-visor:after,.fad.fa-user-visor:after{content:"\e04c\e04c"}.fa-duotone.fa-file-plus-minus:after,.fad.fa-file-plus-minus:after{content:"\e177\e177"}.fa-duotone.fa-chess-clock-alt:after,.fa-duotone.fa-chess-clock-flip:after,.fad.fa-chess-clock-alt:after,.fad.fa-chess-clock-flip:after{content:"\f43e\f43e"}.fa-duotone.fa-satellite:after,.fad.fa-satellite:after{content:"\f7bf\f7bf"}.fa-duotone.fa-truck-fire:after,.fad.fa-truck-fire:after{content:"\e65a\e65a"}.fa-duotone.fa-plane-lock:after,.fad.fa-plane-lock:after{content:"\e558\e558"}.fa-duotone.fa-steering-wheel:after,.fad.fa-steering-wheel:after{content:"\f622\f622"}.fa-duotone.fa-tag:after,.fad.fa-tag:after{content:"\f02b\f02b"}.fa-duotone.fa-stretcher:after,.fad.fa-stretcher:after{content:"\f825\f825"}.fa-duotone.fa-book-law:after,.fa-duotone.fa-book-section:after,.fad.fa-book-law:after,.fad.fa-book-section:after{content:"\e0c1\e0c1"}.fa-duotone.fa-inboxes:after,.fad.fa-inboxes:after{content:"\e1bb\e1bb"}.fa-duotone.fa-coffee-bean:after,.fad.fa-coffee-bean:after{content:"\e13e\e13e"}.fa-duotone.fa-circle-yen:after,.fad.fa-circle-yen:after{content:"\e5d0\e5d0"}.fa-duotone.fa-brackets-curly:after,.fad.fa-brackets-curly:after{content:"\f7ea\f7ea"}.fa-duotone.fa-ellipsis-stroke-vertical:after,.fa-duotone.fa-ellipsis-v-alt:after,.fad.fa-ellipsis-stroke-vertical:after,.fad.fa-ellipsis-v-alt:after{content:"\f39c\f39c"}.fa-duotone.fa-comment:after,.fad.fa-comment:after{content:"\f075\f075"}.fa-duotone.fa-square-1:after,.fad.fa-square-1:after{content:"\e256\e256"}.fa-duotone.fa-birthday-cake:after,.fa-duotone.fa-cake-candles:after,.fa-duotone.fa-cake:after,.fad.fa-birthday-cake:after,.fad.fa-cake-candles:after,.fad.fa-cake:after{content:"\f1fd\f1fd"}.fa-duotone.fa-head-side:after,.fad.fa-head-side:after{content:"\f6e9\f6e9"}.fa-duotone.fa-truck-ladder:after,.fad.fa-truck-ladder:after{content:"\e657\e657"}.fa-duotone.fa-envelope:after,.fad.fa-envelope:after{content:"\f0e0\f0e0"}.fa-duotone.fa-dolly-empty:after,.fad.fa-dolly-empty:after{content:"\f473\f473"}.fa-duotone.fa-face-tissue:after,.fad.fa-face-tissue:after{content:"\e39c\e39c"}.fa-duotone.fa-angle-double-up:after,.fa-duotone.fa-angles-up:after,.fad.fa-angle-double-up:after,.fad.fa-angles-up:after{content:"\f102\f102"}.fa-duotone.fa-bin-recycle:after,.fad.fa-bin-recycle:after{content:"\e5f7\e5f7"}.fa-duotone.fa-paperclip:after,.fad.fa-paperclip:after{content:"\f0c6\f0c6"}.fa-duotone.fa-chart-line-down:after,.fad.fa-chart-line-down:after{content:"\f64d\f64d"}.fa-duotone.fa-arrow-right-to-city:after,.fad.fa-arrow-right-to-city:after{content:"\e4b3\e4b3"}.fa-duotone.fa-lock-a:after,.fad.fa-lock-a:after{content:"\e422\e422"}.fa-duotone.fa-ribbon:after,.fad.fa-ribbon:after{content:"\f4d6\f4d6"}.fa-duotone.fa-lungs:after,.fad.fa-lungs:after{content:"\f604\f604"}.fa-duotone.fa-person-pinball:after,.fad.fa-person-pinball:after{content:"\e21d\e21d"}.fa-duotone.fa-arrow-up-9-1:after,.fa-duotone.fa-sort-numeric-up-alt:after,.fad.fa-arrow-up-9-1:after,.fad.fa-sort-numeric-up-alt:after{content:"\f887\f887"}.fa-duotone.fa-apple-core:after,.fad.fa-apple-core:after{content:"\e08f\e08f"}.fa-duotone.fa-circle-y:after,.fad.fa-circle-y:after{content:"\e12f\e12f"}.fa-duotone.fa-h6:after,.fad.fa-h6:after{content:"\e413\e413"}.fa-duotone.fa-litecoin-sign:after,.fad.fa-litecoin-sign:after{content:"\e1d3\e1d3"}.fa-duotone.fa-bottle-baby:after,.fad.fa-bottle-baby:after{content:"\e673\e673"}.fa-duotone.fa-circle-small:after,.fad.fa-circle-small:after{content:"\e122\e122"}.fa-duotone.fa-border-none:after,.fad.fa-border-none:after{content:"\f850\f850"}.fa-duotone.fa-arrow-turn-down-left:after,.fad.fa-arrow-turn-down-left:after{content:"\e2e1\e2e1"}.fa-duotone.fa-circle-wifi-circle-wifi:after,.fa-duotone.fa-circle-wifi-group:after,.fad.fa-circle-wifi-circle-wifi:after,.fad.fa-circle-wifi-group:after{content:"\e67e\e67e"}.fa-duotone.fa-circle-nodes:after,.fad.fa-circle-nodes:after{content:"\e4e2\e4e2"}.fa-duotone.fa-parachute-box:after,.fad.fa-parachute-box:after{content:"\f4cd\f4cd"}.fa-duotone.fa-reflect-horizontal:after,.fad.fa-reflect-horizontal:after{content:"\e664\e664"}.fa-duotone.fa-comment-alt-medical:after,.fa-duotone.fa-message-medical:after,.fad.fa-comment-alt-medical:after,.fad.fa-message-medical:after{content:"\f7f4\f7f4"}.fa-duotone.fa-rugby-ball:after,.fad.fa-rugby-ball:after{content:"\e3c6\e3c6"}.fa-duotone.fa-comment-music:after,.fad.fa-comment-music:after{content:"\f8b0\f8b0"}.fa-duotone.fa-indent:after,.fad.fa-indent:after{content:"\f03c\f03c"}.fa-duotone.fa-tree-alt:after,.fa-duotone.fa-tree-deciduous:after,.fad.fa-tree-alt:after,.fad.fa-tree-deciduous:after{content:"\f400\f400"}.fa-duotone.fa-puzzle-piece-alt:after,.fa-duotone.fa-puzzle-piece-simple:after,.fad.fa-puzzle-piece-alt:after,.fad.fa-puzzle-piece-simple:after{content:"\e231\e231"}.fa-duotone.fa-truck-field-un:after,.fad.fa-truck-field-un:after{content:"\e58e\e58e"}.fa-duotone.fa-nfc-trash:after,.fad.fa-nfc-trash:after{content:"\e1fd\e1fd"}.fa-duotone.fa-hourglass-empty:after,.fa-duotone.fa-hourglass:after,.fad.fa-hourglass-empty:after,.fad.fa-hourglass:after{content:"\f254\f254"}.fa-duotone.fa-mountain:after,.fad.fa-mountain:after{content:"\f6fc\f6fc"}.fa-duotone.fa-file-times:after,.fa-duotone.fa-file-xmark:after,.fad.fa-file-times:after,.fad.fa-file-xmark:after{content:"\f317\f317"}.fa-duotone.fa-home-heart:after,.fa-duotone.fa-house-heart:after,.fad.fa-home-heart:after,.fad.fa-house-heart:after{content:"\f4c9\f4c9"}.fa-duotone.fa-house-chimney-blank:after,.fad.fa-house-chimney-blank:after{content:"\e3b0\e3b0"}.fa-duotone.fa-meter-bolt:after,.fad.fa-meter-bolt:after{content:"\e1e9\e1e9"}.fa-duotone.fa-user-doctor:after,.fa-duotone.fa-user-md:after,.fad.fa-user-doctor:after,.fad.fa-user-md:after{content:"\f0f0\f0f0"}.fa-duotone.fa-slash-back:after,.fad.fa-slash-back:after{content:"\5c\5c"}.fa-duotone.fa-circle-info:after,.fa-duotone.fa-info-circle:after,.fad.fa-circle-info:after,.fad.fa-info-circle:after{content:"\f05a\f05a"}.fa-duotone.fa-fishing-rod:after,.fad.fa-fishing-rod:after{content:"\e3a8\e3a8"}.fa-duotone.fa-hammer-crash:after,.fad.fa-hammer-crash:after{content:"\e414\e414"}.fa-duotone.fa-message-heart:after,.fad.fa-message-heart:after{content:"\e5c9\e5c9"}.fa-duotone.fa-cloud-meatball:after,.fad.fa-cloud-meatball:after{content:"\f73b\f73b"}.fa-duotone.fa-camera-polaroid:after,.fad.fa-camera-polaroid:after{content:"\f8aa\f8aa"}.fa-duotone.fa-camera-alt:after,.fa-duotone.fa-camera:after,.fad.fa-camera-alt:after,.fad.fa-camera:after{content:"\f030\f030"}.fa-duotone.fa-square-virus:after,.fad.fa-square-virus:after{content:"\e578\e578"}.fa-duotone.fa-cart-arrow-up:after,.fad.fa-cart-arrow-up:after{content:"\e3ee\e3ee"}.fa-duotone.fa-meteor:after,.fad.fa-meteor:after{content:"\f753\f753"}.fa-duotone.fa-car-on:after,.fad.fa-car-on:after{content:"\e4dd\e4dd"}.fa-duotone.fa-sleigh:after,.fad.fa-sleigh:after{content:"\f7cc\f7cc"}.fa-duotone.fa-arrow-down-1-9:after,.fa-duotone.fa-sort-numeric-asc:after,.fa-duotone.fa-sort-numeric-down:after,.fad.fa-arrow-down-1-9:after,.fad.fa-sort-numeric-asc:after,.fad.fa-sort-numeric-down:after{content:"\f162\f162"}.fa-duotone.fa-buoy-mooring:after,.fad.fa-buoy-mooring:after{content:"\e5b6\e5b6"}.fa-duotone.fa-square-4:after,.fad.fa-square-4:after{content:"\e259\e259"}.fa-duotone.fa-hand-holding-droplet:after,.fa-duotone.fa-hand-holding-water:after,.fad.fa-hand-holding-droplet:after,.fad.fa-hand-holding-water:after{content:"\f4c1\f4c1"}.fa-duotone.fa-file-eps:after,.fad.fa-file-eps:after{content:"\e644\e644"}.fa-duotone.fa-tricycle-adult:after,.fad.fa-tricycle-adult:after{content:"\e5c4\e5c4"}.fa-duotone.fa-waveform:after,.fad.fa-waveform:after{content:"\f8f1\f8f1"}.fa-duotone.fa-water:after,.fad.fa-water:after{content:"\f773\f773"}.fa-duotone.fa-star-sharp-half-alt:after,.fa-duotone.fa-star-sharp-half-stroke:after,.fad.fa-star-sharp-half-alt:after,.fad.fa-star-sharp-half-stroke:after{content:"\e28d\e28d"}.fa-duotone.fa-nfc-signal:after,.fad.fa-nfc-signal:after{content:"\e1fb\e1fb"}.fa-duotone.fa-plane-prop:after,.fad.fa-plane-prop:after{content:"\e22b\e22b"}.fa-duotone.fa-calendar-check:after,.fad.fa-calendar-check:after{content:"\f274\f274"}.fa-duotone.fa-clock-desk:after,.fad.fa-clock-desk:after{content:"\e134\e134"}.fa-duotone.fa-calendar-clock:after,.fa-duotone.fa-calendar-time:after,.fad.fa-calendar-clock:after,.fad.fa-calendar-time:after{content:"\e0d2\e0d2"}.fa-duotone.fa-braille:after,.fad.fa-braille:after{content:"\f2a1\f2a1"}.fa-duotone.fa-prescription-bottle-alt:after,.fa-duotone.fa-prescription-bottle-medical:after,.fad.fa-prescription-bottle-alt:after,.fad.fa-prescription-bottle-medical:after{content:"\f486\f486"}.fa-duotone.fa-plate-utensils:after,.fad.fa-plate-utensils:after{content:"\e43b\e43b"}.fa-duotone.fa-family-pants:after,.fad.fa-family-pants:after{content:"\e302\e302"}.fa-duotone.fa-hose-reel:after,.fad.fa-hose-reel:after{content:"\e41a\e41a"}.fa-duotone.fa-house-window:after,.fad.fa-house-window:after{content:"\e3b3\e3b3"}.fa-duotone.fa-landmark:after,.fad.fa-landmark:after{content:"\f66f\f66f"}.fa-duotone.fa-truck:after,.fad.fa-truck:after{content:"\f0d1\f0d1"}.fa-duotone.fa-music-magnifying-glass:after,.fad.fa-music-magnifying-glass:after{content:"\e662\e662"}.fa-duotone.fa-crosshairs:after,.fad.fa-crosshairs:after{content:"\f05b\f05b"}.fa-duotone.fa-cloud-rainbow:after,.fad.fa-cloud-rainbow:after{content:"\f73e\f73e"}.fa-duotone.fa-person-cane:after,.fad.fa-person-cane:after{content:"\e53c\e53c"}.fa-duotone.fa-alien:after,.fad.fa-alien:after{content:"\f8f5\f8f5"}.fa-duotone.fa-tent:after,.fad.fa-tent:after{content:"\e57d\e57d"}.fa-duotone.fa-laptop-binary:after,.fad.fa-laptop-binary:after{content:"\e5e7\e5e7"}.fa-duotone.fa-vest-patches:after,.fad.fa-vest-patches:after{content:"\e086\e086"}.fa-duotone.fa-people-dress-simple:after,.fad.fa-people-dress-simple:after{content:"\e218\e218"}.fa-duotone.fa-check-double:after,.fad.fa-check-double:after{content:"\f560\f560"}.fa-duotone.fa-arrow-down-a-z:after,.fa-duotone.fa-sort-alpha-asc:after,.fa-duotone.fa-sort-alpha-down:after,.fad.fa-arrow-down-a-z:after,.fad.fa-sort-alpha-asc:after,.fad.fa-sort-alpha-down:after{content:"\f15d\f15d"}.fa-duotone.fa-bowling-ball-pin:after,.fad.fa-bowling-ball-pin:after{content:"\e0c3\e0c3"}.fa-duotone.fa-bell-school-slash:after,.fad.fa-bell-school-slash:after{content:"\f5d6\f5d6"}.fa-duotone.fa-plus-large:after,.fad.fa-plus-large:after{content:"\e59e\e59e"}.fa-duotone.fa-money-bill-wheat:after,.fad.fa-money-bill-wheat:after{content:"\e52a\e52a"}.fa-duotone.fa-camera-viewfinder:after,.fa-duotone.fa-screenshot:after,.fad.fa-camera-viewfinder:after,.fad.fa-screenshot:after{content:"\e0da\e0da"}.fa-duotone.fa-comment-alt-music:after,.fa-duotone.fa-message-music:after,.fad.fa-comment-alt-music:after,.fad.fa-message-music:after{content:"\f8af\f8af"}.fa-duotone.fa-car-building:after,.fad.fa-car-building:after{content:"\f859\f859"}.fa-duotone.fa-border-bottom-right:after,.fa-duotone.fa-border-style-alt:after,.fad.fa-border-bottom-right:after,.fad.fa-border-style-alt:after{content:"\f854\f854"}.fa-duotone.fa-octagon:after,.fad.fa-octagon:after{content:"\f306\f306"}.fa-duotone.fa-comment-arrow-up-right:after,.fad.fa-comment-arrow-up-right:after{content:"\e145\e145"}.fa-duotone.fa-octagon-divide:after,.fad.fa-octagon-divide:after{content:"\e203\e203"}.fa-duotone.fa-cookie:after,.fad.fa-cookie:after{content:"\f563\f563"}.fa-duotone.fa-arrow-left-rotate:after,.fa-duotone.fa-arrow-rotate-back:after,.fa-duotone.fa-arrow-rotate-backward:after,.fa-duotone.fa-arrow-rotate-left:after,.fa-duotone.fa-undo:after,.fad.fa-arrow-left-rotate:after,.fad.fa-arrow-rotate-back:after,.fad.fa-arrow-rotate-backward:after,.fad.fa-arrow-rotate-left:after,.fad.fa-undo:after{content:"\f0e2\f0e2"}.fa-duotone.fa-tv-music:after,.fad.fa-tv-music:after{content:"\f8e6\f8e6"}.fa-duotone.fa-hard-drive:after,.fa-duotone.fa-hdd:after,.fad.fa-hard-drive:after,.fad.fa-hdd:after{content:"\f0a0\f0a0"}.fa-duotone.fa-reel:after,.fad.fa-reel:after{content:"\e238\e238"}.fa-duotone.fa-face-grin-squint-tears:after,.fa-duotone.fa-grin-squint-tears:after,.fad.fa-face-grin-squint-tears:after,.fad.fa-grin-squint-tears:after{content:"\f586\f586"}.fa-duotone.fa-dumbbell:after,.fad.fa-dumbbell:after{content:"\f44b\f44b"}.fa-duotone.fa-list-alt:after,.fa-duotone.fa-rectangle-list:after,.fad.fa-list-alt:after,.fad.fa-rectangle-list:after{content:"\f022\f022"}.fa-duotone.fa-tarp-droplet:after,.fad.fa-tarp-droplet:after{content:"\e57c\e57c"}.fa-duotone.fa-alarm-exclamation:after,.fad.fa-alarm-exclamation:after{content:"\f843\f843"}.fa-duotone.fa-house-medical-circle-check:after,.fad.fa-house-medical-circle-check:after{content:"\e511\e511"}.fa-duotone.fa-traffic-cone:after,.fad.fa-traffic-cone:after{content:"\f636\f636"}.fa-duotone.fa-grate:after,.fad.fa-grate:after{content:"\e193\e193"}.fa-duotone.fa-arrow-down-right:after,.fad.fa-arrow-down-right:after{content:"\e093\e093"}.fa-duotone.fa-person-skiing-nordic:after,.fa-duotone.fa-skiing-nordic:after,.fad.fa-person-skiing-nordic:after,.fad.fa-skiing-nordic:after{content:"\f7ca\f7ca"}.fa-duotone.fa-calendar-plus:after,.fad.fa-calendar-plus:after{content:"\f271\f271"}.fa-duotone.fa-person-from-portal:after,.fa-duotone.fa-portal-exit:after,.fad.fa-person-from-portal:after,.fad.fa-portal-exit:after{content:"\e023\e023"}.fa-duotone.fa-plane-arrival:after,.fad.fa-plane-arrival:after{content:"\f5af\f5af"}.fa-duotone.fa-cowbell-circle-plus:after,.fa-duotone.fa-cowbell-more:after,.fad.fa-cowbell-circle-plus:after,.fad.fa-cowbell-more:after{content:"\f8b4\f8b4"}.fa-duotone.fa-arrow-alt-circle-left:after,.fa-duotone.fa-circle-left:after,.fad.fa-arrow-alt-circle-left:after,.fad.fa-circle-left:after{content:"\f359\f359"}.fa-duotone.fa-distribute-spacing-vertical:after,.fad.fa-distribute-spacing-vertical:after{content:"\e366\e366"}.fa-duotone.fa-signal-alt-2:after,.fa-duotone.fa-signal-bars-fair:after,.fad.fa-signal-alt-2:after,.fad.fa-signal-bars-fair:after{content:"\f692\f692"}.fa-duotone.fa-sportsball:after,.fad.fa-sportsball:after{content:"\e44b\e44b"}.fa-duotone.fa-game-console-handheld-crank:after,.fad.fa-game-console-handheld-crank:after{content:"\e5b9\e5b9"}.fa-duotone.fa-subway:after,.fa-duotone.fa-train-subway:after,.fad.fa-subway:after,.fad.fa-train-subway:after{content:"\f239\f239"}.fa-duotone.fa-chart-gantt:after,.fad.fa-chart-gantt:after{content:"\e0e4\e0e4"}.fa-duotone.fa-face-smile-upside-down:after,.fad.fa-face-smile-upside-down:after{content:"\e395\e395"}.fa-duotone.fa-ball-pile:after,.fad.fa-ball-pile:after{content:"\f77e\f77e"}.fa-duotone.fa-badge-dollar:after,.fad.fa-badge-dollar:after{content:"\f645\f645"}.fa-duotone.fa-money-bills-alt:after,.fa-duotone.fa-money-bills-simple:after,.fad.fa-money-bills-alt:after,.fad.fa-money-bills-simple:after{content:"\e1f4\e1f4"}.fa-duotone.fa-list-timeline:after,.fad.fa-list-timeline:after{content:"\e1d1\e1d1"}.fa-duotone.fa-indian-rupee-sign:after,.fa-duotone.fa-indian-rupee:after,.fa-duotone.fa-inr:after,.fad.fa-indian-rupee-sign:after,.fad.fa-indian-rupee:after,.fad.fa-inr:after{content:"\e1bc\e1bc"}.fa-duotone.fa-crop-alt:after,.fa-duotone.fa-crop-simple:after,.fad.fa-crop-alt:after,.fad.fa-crop-simple:after{content:"\f565\f565"}.fa-duotone.fa-money-bill-1:after,.fa-duotone.fa-money-bill-alt:after,.fad.fa-money-bill-1:after,.fad.fa-money-bill-alt:after{content:"\f3d1\f3d1"}.fa-duotone.fa-left-long:after,.fa-duotone.fa-long-arrow-alt-left:after,.fad.fa-left-long:after,.fad.fa-long-arrow-alt-left:after{content:"\f30a\f30a"}.fa-duotone.fa-keyboard-down:after,.fad.fa-keyboard-down:after{content:"\e1c2\e1c2"}.fa-duotone.fa-circle-up-right:after,.fad.fa-circle-up-right:after{content:"\e129\e129"}.fa-duotone.fa-cloud-bolt-moon:after,.fa-duotone.fa-thunderstorm-moon:after,.fad.fa-cloud-bolt-moon:after,.fad.fa-thunderstorm-moon:after{content:"\f76d\f76d"}.fa-duotone.fa-turn-left-up:after,.fad.fa-turn-left-up:after{content:"\e638\e638"}.fa-duotone.fa-dna:after,.fad.fa-dna:after{content:"\f471\f471"}.fa-duotone.fa-virus-slash:after,.fad.fa-virus-slash:after{content:"\e075\e075"}.fa-duotone.fa-bracket-round-right:after,.fad.fa-bracket-round-right:after{content:"\29\29"}.fa-duotone.fa-circle-sterling:after,.fad.fa-circle-sterling:after{content:"\e5cf\e5cf"}.fa-duotone.fa-circle-5:after,.fad.fa-circle-5:after{content:"\e0f2\e0f2"}.fa-duotone.fa-minus:after,.fa-duotone.fa-subtract:after,.fad.fa-minus:after,.fad.fa-subtract:after{content:"\f068\f068"}.fa-duotone.fa-fire-flame:after,.fa-duotone.fa-flame:after,.fad.fa-fire-flame:after,.fad.fa-flame:after{content:"\f6df\f6df"}.fa-duotone.fa-arrow-alt-to-right:after,.fa-duotone.fa-right-to-line:after,.fad.fa-arrow-alt-to-right:after,.fad.fa-right-to-line:after{content:"\f34c\f34c"}.fa-duotone.fa-gif:after,.fad.fa-gif:after{content:"\e190\e190"}.fa-duotone.fa-chess:after,.fad.fa-chess:after{content:"\f439\f439"}.fa-duotone.fa-trash-slash:after,.fad.fa-trash-slash:after{content:"\e2b3\e2b3"}.fa-duotone.fa-arrow-left-long:after,.fa-duotone.fa-long-arrow-left:after,.fad.fa-arrow-left-long:after,.fad.fa-long-arrow-left:after{content:"\f177\f177"}.fa-duotone.fa-plug-circle-check:after,.fad.fa-plug-circle-check:after{content:"\e55c\e55c"}.fa-duotone.fa-font-case:after,.fad.fa-font-case:after{content:"\f866\f866"}.fa-duotone.fa-street-view:after,.fad.fa-street-view:after{content:"\f21d\f21d"}.fa-duotone.fa-arrow-down-left:after,.fad.fa-arrow-down-left:after{content:"\e091\e091"}.fa-duotone.fa-franc-sign:after,.fad.fa-franc-sign:after{content:"\e18f\e18f"}.fa-duotone.fa-flask-poison:after,.fa-duotone.fa-flask-round-poison:after,.fad.fa-flask-poison:after,.fad.fa-flask-round-poison:after{content:"\f6e0\f6e0"}.fa-duotone.fa-volume-off:after,.fad.fa-volume-off:after{content:"\f026\f026"}.fa-duotone.fa-book-circle-arrow-right:after,.fad.fa-book-circle-arrow-right:after{content:"\e0bc\e0bc"}.fa-duotone.fa-chart-user:after,.fa-duotone.fa-user-chart:after,.fad.fa-chart-user:after,.fad.fa-user-chart:after{content:"\f6a3\f6a3"}.fa-duotone.fa-american-sign-language-interpreting:after,.fa-duotone.fa-asl-interpreting:after,.fa-duotone.fa-hands-american-sign-language-interpreting:after,.fa-duotone.fa-hands-asl-interpreting:after,.fad.fa-american-sign-language-interpreting:after,.fad.fa-asl-interpreting:after,.fad.fa-hands-american-sign-language-interpreting:after,.fad.fa-hands-asl-interpreting:after{content:"\f2a3\f2a3"}.fa-duotone.fa-presentation-screen:after,.fa-duotone.fa-presentation:after,.fad.fa-presentation-screen:after,.fad.fa-presentation:after{content:"\f685\f685"}.fa-duotone.fa-circle-bolt:after,.fad.fa-circle-bolt:after{content:"\e0fe\e0fe"}.fa-duotone.fa-face-smile-halo:after,.fad.fa-face-smile-halo:after{content:"\e38f\e38f"}.fa-duotone.fa-cart-circle-arrow-down:after,.fad.fa-cart-circle-arrow-down:after{content:"\e3ef\e3ef"}.fa-duotone.fa-house-person-arrive:after,.fa-duotone.fa-house-person-return:after,.fa-duotone.fa-house-return:after,.fad.fa-house-person-arrive:after,.fad.fa-house-person-return:after,.fad.fa-house-return:after{content:"\e011\e011"}.fa-duotone.fa-comment-alt-times:after,.fa-duotone.fa-message-times:after,.fa-duotone.fa-message-xmark:after,.fad.fa-comment-alt-times:after,.fad.fa-message-times:after,.fad.fa-message-xmark:after{content:"\f4ab\f4ab"}.fa-duotone.fa-file-award:after,.fa-duotone.fa-file-certificate:after,.fad.fa-file-award:after,.fad.fa-file-certificate:after{content:"\f5f3\f5f3"}.fa-duotone.fa-user-doctor-hair-long:after,.fad.fa-user-doctor-hair-long:after{content:"\e459\e459"}.fa-duotone.fa-camera-home:after,.fa-duotone.fa-camera-security:after,.fad.fa-camera-home:after,.fad.fa-camera-security:after{content:"\f8fe\f8fe"}.fa-duotone.fa-cog:after,.fa-duotone.fa-gear:after,.fad.fa-cog:after,.fad.fa-gear:after{content:"\f013\f013"}.fa-duotone.fa-droplet-slash:after,.fa-duotone.fa-tint-slash:after,.fad.fa-droplet-slash:after,.fad.fa-tint-slash:after{content:"\f5c7\f5c7"}.fa-duotone.fa-book-heart:after,.fad.fa-book-heart:after{content:"\f499\f499"}.fa-duotone.fa-mosque:after,.fad.fa-mosque:after{content:"\f678\f678"}.fa-duotone.fa-duck:after,.fad.fa-duck:after{content:"\f6d8\f6d8"}.fa-duotone.fa-mosquito:after,.fad.fa-mosquito:after{content:"\e52b\e52b"}.fa-duotone.fa-star-of-david:after,.fad.fa-star-of-david:after{content:"\f69a\f69a"}.fa-duotone.fa-flag-alt:after,.fa-duotone.fa-flag-swallowtail:after,.fad.fa-flag-alt:after,.fad.fa-flag-swallowtail:after{content:"\f74c\f74c"}.fa-duotone.fa-person-military-rifle:after,.fad.fa-person-military-rifle:after{content:"\e54b\e54b"}.fa-duotone.fa-car-garage:after,.fad.fa-car-garage:after{content:"\f5e2\f5e2"}.fa-duotone.fa-cart-shopping:after,.fa-duotone.fa-shopping-cart:after,.fad.fa-cart-shopping:after,.fad.fa-shopping-cart:after{content:"\f07a\f07a"}.fa-duotone.fa-book-font:after,.fad.fa-book-font:after{content:"\e0bf\e0bf"}.fa-duotone.fa-shield-plus:after,.fad.fa-shield-plus:after{content:"\e24a\e24a"}.fa-duotone.fa-vials:after,.fad.fa-vials:after{content:"\f493\f493"}.fa-duotone.fa-eye-dropper-full:after,.fad.fa-eye-dropper-full:after{content:"\e172\e172"}.fa-duotone.fa-distribute-spacing-horizontal:after,.fad.fa-distribute-spacing-horizontal:after{content:"\e365\e365"}.fa-duotone.fa-tablet-rugged:after,.fad.fa-tablet-rugged:after{content:"\f48f\f48f"}.fa-duotone.fa-temperature-frigid:after,.fa-duotone.fa-temperature-snow:after,.fad.fa-temperature-frigid:after,.fad.fa-temperature-snow:after{content:"\f768\f768"}.fa-duotone.fa-moped:after,.fad.fa-moped:after{content:"\e3b9\e3b9"}.fa-duotone.fa-face-smile-plus:after,.fa-duotone.fa-smile-plus:after,.fad.fa-face-smile-plus:after,.fad.fa-smile-plus:after{content:"\f5b9\f5b9"}.fa-duotone.fa-radio-alt:after,.fa-duotone.fa-radio-tuner:after,.fad.fa-radio-alt:after,.fad.fa-radio-tuner:after{content:"\f8d8\f8d8"}.fa-duotone.fa-face-swear:after,.fad.fa-face-swear:after{content:"\e399\e399"}.fa-duotone.fa-water-arrow-down:after,.fa-duotone.fa-water-lower:after,.fad.fa-water-arrow-down:after,.fad.fa-water-lower:after{content:"\f774\f774"}.fa-duotone.fa-scanner-touchscreen:after,.fad.fa-scanner-touchscreen:after{content:"\f48a\f48a"}.fa-duotone.fa-circle-7:after,.fad.fa-circle-7:after{content:"\e0f4\e0f4"}.fa-duotone.fa-plug-circle-plus:after,.fad.fa-plug-circle-plus:after{content:"\e55f\e55f"}.fa-duotone.fa-person-ski-jumping:after,.fa-duotone.fa-ski-jump:after,.fad.fa-person-ski-jumping:after,.fad.fa-ski-jump:after{content:"\f7c7\f7c7"}.fa-duotone.fa-place-of-worship:after,.fad.fa-place-of-worship:after{content:"\f67f\f67f"}.fa-duotone.fa-water-arrow-up:after,.fa-duotone.fa-water-rise:after,.fad.fa-water-arrow-up:after,.fad.fa-water-rise:after{content:"\f775\f775"}.fa-duotone.fa-waveform-lines:after,.fa-duotone.fa-waveform-path:after,.fad.fa-waveform-lines:after,.fad.fa-waveform-path:after{content:"\f8f2\f8f2"}.fa-duotone.fa-split:after,.fad.fa-split:after{content:"\e254\e254"}.fa-duotone.fa-film-canister:after,.fa-duotone.fa-film-cannister:after,.fad.fa-film-canister:after,.fad.fa-film-cannister:after{content:"\f8b7\f8b7"}.fa-duotone.fa-folder-times:after,.fa-duotone.fa-folder-xmark:after,.fad.fa-folder-times:after,.fad.fa-folder-xmark:after{content:"\f65f\f65f"}.fa-duotone.fa-toilet-paper-alt:after,.fa-duotone.fa-toilet-paper-blank:after,.fad.fa-toilet-paper-alt:after,.fad.fa-toilet-paper-blank:after{content:"\f71f\f71f"}.fa-duotone.fa-tablet-android-alt:after,.fa-duotone.fa-tablet-screen:after,.fad.fa-tablet-android-alt:after,.fad.fa-tablet-screen:after{content:"\f3fc\f3fc"}.fa-duotone.fa-hexagon-vertical-nft-slanted:after,.fad.fa-hexagon-vertical-nft-slanted:after{content:"\e506\e506"}.fa-duotone.fa-folder-music:after,.fad.fa-folder-music:after{content:"\e18d\e18d"}.fa-duotone.fa-desktop-medical:after,.fa-duotone.fa-display-medical:after,.fad.fa-desktop-medical:after,.fad.fa-display-medical:after{content:"\e166\e166"}.fa-duotone.fa-share-all:after,.fad.fa-share-all:after{content:"\f367\f367"}.fa-duotone.fa-peapod:after,.fad.fa-peapod:after{content:"\e31c\e31c"}.fa-duotone.fa-chess-clock:after,.fad.fa-chess-clock:after{content:"\f43d\f43d"}.fa-duotone.fa-axe:after,.fad.fa-axe:after{content:"\f6b2\f6b2"}.fa-duotone.fa-square-d:after,.fad.fa-square-d:after{content:"\e268\e268"}.fa-duotone.fa-grip-vertical:after,.fad.fa-grip-vertical:after{content:"\f58e\f58e"}.fa-duotone.fa-mobile-signal-out:after,.fad.fa-mobile-signal-out:after{content:"\e1f0\e1f0"}.fa-duotone.fa-arrow-turn-up:after,.fa-duotone.fa-level-up:after,.fad.fa-arrow-turn-up:after,.fad.fa-level-up:after{content:"\f148\f148"}.fa-duotone.fa-u:after,.fad.fa-u:after{content:"\55\55"}.fa-duotone.fa-arrow-up-from-dotted-line:after,.fad.fa-arrow-up-from-dotted-line:after{content:"\e09b\e09b"}.fa-duotone.fa-square-root-alt:after,.fa-duotone.fa-square-root-variable:after,.fad.fa-square-root-alt:after,.fad.fa-square-root-variable:after{content:"\f698\f698"}.fa-duotone.fa-light-switch-on:after,.fad.fa-light-switch-on:after{content:"\e019\e019"}.fa-duotone.fa-arrow-down-arrow-up:after,.fa-duotone.fa-sort-alt:after,.fad.fa-arrow-down-arrow-up:after,.fad.fa-sort-alt:after{content:"\f883\f883"}.fa-duotone.fa-raindrops:after,.fad.fa-raindrops:after{content:"\f75c\f75c"}.fa-duotone.fa-dash:after,.fa-duotone.fa-minus-large:after,.fad.fa-dash:after,.fad.fa-minus-large:after{content:"\e404\e404"}.fa-duotone.fa-clock-four:after,.fa-duotone.fa-clock:after,.fad.fa-clock-four:after,.fad.fa-clock:after{content:"\f017\f017"}.fa-duotone.fa-input-numeric:after,.fad.fa-input-numeric:after{content:"\e1bd\e1bd"}.fa-duotone.fa-truck-tow:after,.fad.fa-truck-tow:after{content:"\e2b8\e2b8"}.fa-duotone.fa-backward-step:after,.fa-duotone.fa-step-backward:after,.fad.fa-backward-step:after,.fad.fa-step-backward:after{content:"\f048\f048"}.fa-duotone.fa-pallet:after,.fad.fa-pallet:after{content:"\f482\f482"}.fa-duotone.fa-car-bolt:after,.fad.fa-car-bolt:after{content:"\e341\e341"}.fa-duotone.fa-arrows-maximize:after,.fa-duotone.fa-expand-arrows:after,.fad.fa-arrows-maximize:after,.fad.fa-expand-arrows:after{content:"\f31d\f31d"}.fa-duotone.fa-faucet:after,.fad.fa-faucet:after{content:"\e005\e005"}.fa-duotone.fa-cloud-sleet:after,.fad.fa-cloud-sleet:after{content:"\f741\f741"}.fa-duotone.fa-lamp-street:after,.fad.fa-lamp-street:after{content:"\e1c5\e1c5"}.fa-duotone.fa-list-radio:after,.fad.fa-list-radio:after{content:"\e1d0\e1d0"}.fa-duotone.fa-pen-nib-slash:after,.fad.fa-pen-nib-slash:after{content:"\e4a1\e4a1"}.fa-duotone.fa-baseball-bat-ball:after,.fad.fa-baseball-bat-ball:after{content:"\f432\f432"}.fa-duotone.fa-square-up-left:after,.fad.fa-square-up-left:after{content:"\e282\e282"}.fa-duotone.fa-overline:after,.fad.fa-overline:after{content:"\f876\f876"}.fa-duotone.fa-s:after,.fad.fa-s:after{content:"\53\53"}.fa-duotone.fa-timeline:after,.fad.fa-timeline:after{content:"\e29c\e29c"}.fa-duotone.fa-keyboard:after,.fad.fa-keyboard:after{content:"\f11c\f11c"}.fa-duotone.fa-arrows-from-dotted-line:after,.fad.fa-arrows-from-dotted-line:after{content:"\e0a3\e0a3"}.fa-duotone.fa-usb-drive:after,.fad.fa-usb-drive:after{content:"\f8e9\f8e9"}.fa-duotone.fa-ballot:after,.fad.fa-ballot:after{content:"\f732\f732"}.fa-duotone.fa-caret-down:after,.fad.fa-caret-down:after{content:"\f0d7\f0d7"}.fa-duotone.fa-location-dot-slash:after,.fa-duotone.fa-map-marker-alt-slash:after,.fad.fa-location-dot-slash:after,.fad.fa-map-marker-alt-slash:after{content:"\f605\f605"}.fa-duotone.fa-cards:after,.fad.fa-cards:after{content:"\e3ed\e3ed"}.fa-duotone.fa-clinic-medical:after,.fa-duotone.fa-house-chimney-medical:after,.fad.fa-clinic-medical:after,.fad.fa-house-chimney-medical:after{content:"\f7f2\f7f2"}.fa-duotone.fa-boxing-glove:after,.fa-duotone.fa-glove-boxing:after,.fad.fa-boxing-glove:after,.fad.fa-glove-boxing:after{content:"\f438\f438"}.fa-duotone.fa-temperature-3:after,.fa-duotone.fa-temperature-three-quarters:after,.fa-duotone.fa-thermometer-3:after,.fa-duotone.fa-thermometer-three-quarters:after,.fad.fa-temperature-3:after,.fad.fa-temperature-three-quarters:after,.fad.fa-thermometer-3:after,.fad.fa-thermometer-three-quarters:after{content:"\f2c8\f2c8"}.fa-duotone.fa-bell-school:after,.fad.fa-bell-school:after{content:"\f5d5\f5d5"}.fa-duotone.fa-mobile-android-alt:after,.fa-duotone.fa-mobile-screen:after,.fad.fa-mobile-android-alt:after,.fad.fa-mobile-screen:after{content:"\f3cf\f3cf"}.fa-duotone.fa-plane-up:after,.fad.fa-plane-up:after{content:"\e22d\e22d"}.fa-duotone.fa-folder-heart:after,.fad.fa-folder-heart:after{content:"\e189\e189"}.fa-duotone.fa-circle-location-arrow:after,.fa-duotone.fa-location-circle:after,.fad.fa-circle-location-arrow:after,.fad.fa-location-circle:after{content:"\f602\f602"}.fa-duotone.fa-face-head-bandage:after,.fad.fa-face-head-bandage:after{content:"\e37a\e37a"}.fa-duotone.fa-maki-roll:after,.fa-duotone.fa-makizushi:after,.fa-duotone.fa-sushi-roll:after,.fad.fa-maki-roll:after,.fad.fa-makizushi:after,.fad.fa-sushi-roll:after{content:"\e48b\e48b"}.fa-duotone.fa-car-bump:after,.fad.fa-car-bump:after{content:"\f5e0\f5e0"}.fa-duotone.fa-piggy-bank:after,.fad.fa-piggy-bank:after{content:"\f4d3\f4d3"}.fa-duotone.fa-racquet:after,.fad.fa-racquet:after{content:"\f45a\f45a"}.fa-duotone.fa-car-mirrors:after,.fad.fa-car-mirrors:after{content:"\e343\e343"}.fa-duotone.fa-industry-alt:after,.fa-duotone.fa-industry-windows:after,.fad.fa-industry-alt:after,.fad.fa-industry-windows:after{content:"\f3b3\f3b3"}.fa-duotone.fa-bolt-auto:after,.fad.fa-bolt-auto:after{content:"\e0b6\e0b6"}.fa-duotone.fa-battery-3:after,.fa-duotone.fa-battery-half:after,.fad.fa-battery-3:after,.fad.fa-battery-half:after{content:"\f242\f242"}.fa-duotone.fa-flux-capacitor:after,.fad.fa-flux-capacitor:after{content:"\f8ba\f8ba"}.fa-duotone.fa-mountain-city:after,.fad.fa-mountain-city:after{content:"\e52e\e52e"}.fa-duotone.fa-coins:after,.fad.fa-coins:after{content:"\f51e\f51e"}.fa-duotone.fa-honey-pot:after,.fad.fa-honey-pot:after{content:"\e418\e418"}.fa-duotone.fa-olive:after,.fad.fa-olive:after{content:"\e316\e316"}.fa-duotone.fa-khanda:after,.fad.fa-khanda:after{content:"\f66d\f66d"}.fa-duotone.fa-filter-list:after,.fad.fa-filter-list:after{content:"\e17c\e17c"}.fa-duotone.fa-outlet:after,.fad.fa-outlet:after{content:"\e01c\e01c"}.fa-duotone.fa-sliders-h:after,.fa-duotone.fa-sliders:after,.fad.fa-sliders-h:after,.fad.fa-sliders:after{content:"\f1de\f1de"}.fa-duotone.fa-cauldron:after,.fad.fa-cauldron:after{content:"\f6bf\f6bf"}.fa-duotone.fa-people:after,.fad.fa-people:after{content:"\e216\e216"}.fa-duotone.fa-folder-tree:after,.fad.fa-folder-tree:after{content:"\f802\f802"}.fa-duotone.fa-network-wired:after,.fad.fa-network-wired:after{content:"\f6ff\f6ff"}.fa-duotone.fa-croissant:after,.fad.fa-croissant:after{content:"\f7f6\f7f6"}.fa-duotone.fa-map-pin:after,.fad.fa-map-pin:after{content:"\f276\f276"}.fa-duotone.fa-hamsa:after,.fad.fa-hamsa:after{content:"\f665\f665"}.fa-duotone.fa-cent-sign:after,.fad.fa-cent-sign:after{content:"\e3f5\e3f5"}.fa-duotone.fa-swords-laser:after,.fad.fa-swords-laser:after{content:"\e03d\e03d"}.fa-duotone.fa-flask:after,.fad.fa-flask:after{content:"\f0c3\f0c3"}.fa-duotone.fa-person-pregnant:after,.fad.fa-person-pregnant:after{content:"\e31e\e31e"}.fa-duotone.fa-square-u:after,.fad.fa-square-u:after{content:"\e281\e281"}.fa-duotone.fa-wand-sparkles:after,.fad.fa-wand-sparkles:after{content:"\f72b\f72b"}.fa-duotone.fa-router:after,.fad.fa-router:after{content:"\f8da\f8da"}.fa-duotone.fa-ellipsis-v:after,.fa-duotone.fa-ellipsis-vertical:after,.fad.fa-ellipsis-v:after,.fad.fa-ellipsis-vertical:after{content:"\f142\f142"}.fa-duotone.fa-sword-laser-alt:after,.fad.fa-sword-laser-alt:after{content:"\e03c\e03c"}.fa-duotone.fa-ticket:after,.fad.fa-ticket:after{content:"\f145\f145"}.fa-duotone.fa-power-off:after,.fad.fa-power-off:after{content:"\f011\f011"}.fa-duotone.fa-coin:after,.fad.fa-coin:after{content:"\f85c\f85c"}.fa-duotone.fa-laptop-slash:after,.fad.fa-laptop-slash:after{content:"\e1c7\e1c7"}.fa-duotone.fa-long-arrow-alt-right:after,.fa-duotone.fa-right-long:after,.fad.fa-long-arrow-alt-right:after,.fad.fa-right-long:after{content:"\f30b\f30b"}.fa-duotone.fa-circle-b:after,.fad.fa-circle-b:after{content:"\e0fd\e0fd"}.fa-duotone.fa-person-dress-simple:after,.fad.fa-person-dress-simple:after{content:"\e21c\e21c"}.fa-duotone.fa-pipe-collar:after,.fad.fa-pipe-collar:after{content:"\e437\e437"}.fa-duotone.fa-lights-holiday:after,.fad.fa-lights-holiday:after{content:"\f7b2\f7b2"}.fa-duotone.fa-citrus:after,.fad.fa-citrus:after{content:"\e2f4\e2f4"}.fa-duotone.fa-flag-usa:after,.fad.fa-flag-usa:after{content:"\f74d\f74d"}.fa-duotone.fa-laptop-file:after,.fad.fa-laptop-file:after{content:"\e51d\e51d"}.fa-duotone.fa-teletype:after,.fa-duotone.fa-tty:after,.fad.fa-teletype:after,.fad.fa-tty:after{content:"\f1e4\f1e4"}.fa-duotone.fa-chart-tree-map:after,.fad.fa-chart-tree-map:after{content:"\e0ea\e0ea"}.fa-duotone.fa-diagram-next:after,.fad.fa-diagram-next:after{content:"\e476\e476"}.fa-duotone.fa-person-rifle:after,.fad.fa-person-rifle:after{content:"\e54e\e54e"}.fa-duotone.fa-clock-five-thirty:after,.fad.fa-clock-five-thirty:after{content:"\e34a\e34a"}.fa-duotone.fa-pipe-valve:after,.fad.fa-pipe-valve:after{content:"\e439\e439"}.fa-duotone.fa-arrow-up-from-arc:after,.fad.fa-arrow-up-from-arc:after{content:"\e4b4\e4b4"}.fa-duotone.fa-face-spiral-eyes:after,.fad.fa-face-spiral-eyes:after{content:"\e485\e485"}.fa-duotone.fa-compress-wide:after,.fad.fa-compress-wide:after{content:"\f326\f326"}.fa-duotone.fa-circle-phone-hangup:after,.fa-duotone.fa-phone-circle-down:after,.fad.fa-circle-phone-hangup:after,.fad.fa-phone-circle-down:after{content:"\e11d\e11d"}.fa-duotone.fa-gear-complex-code:after,.fad.fa-gear-complex-code:after{content:"\e5eb\e5eb"}.fa-duotone.fa-house-medical-circle-exclamation:after,.fad.fa-house-medical-circle-exclamation:after{content:"\e512\e512"}.fa-duotone.fa-badminton:after,.fad.fa-badminton:after{content:"\e33a\e33a"}.fa-duotone.fa-closed-captioning:after,.fad.fa-closed-captioning:after{content:"\f20a\f20a"}.fa-duotone.fa-hiking:after,.fa-duotone.fa-person-hiking:after,.fad.fa-hiking:after,.fad.fa-person-hiking:after{content:"\f6ec\f6ec"}.fa-duotone.fa-arrow-alt-from-left:after,.fa-duotone.fa-right-from-line:after,.fad.fa-arrow-alt-from-left:after,.fad.fa-right-from-line:after{content:"\f347\f347"}.fa-duotone.fa-venus-double:after,.fad.fa-venus-double:after{content:"\f226\f226"}.fa-duotone.fa-images:after,.fad.fa-images:after{content:"\f302\f302"}.fa-duotone.fa-calculator:after,.fad.fa-calculator:after{content:"\f1ec\f1ec"}.fa-duotone.fa-shuttlecock:after,.fad.fa-shuttlecock:after{content:"\f45b\f45b"}.fa-duotone.fa-user-hair:after,.fad.fa-user-hair:after{content:"\e45a\e45a"}.fa-duotone.fa-eye-evil:after,.fad.fa-eye-evil:after{content:"\f6db\f6db"}.fa-duotone.fa-people-pulling:after,.fad.fa-people-pulling:after{content:"\e535\e535"}.fa-duotone.fa-n:after,.fad.fa-n:after{content:"\4e\4e"}.fa-duotone.fa-swap:after,.fad.fa-swap:after{content:"\e609\e609"}.fa-duotone.fa-garage:after,.fad.fa-garage:after{content:"\e009\e009"}.fa-duotone.fa-cable-car:after,.fa-duotone.fa-tram:after,.fad.fa-cable-car:after,.fad.fa-tram:after{content:"\f7da\f7da"}.fa-duotone.fa-shovel-snow:after,.fad.fa-shovel-snow:after{content:"\f7c3\f7c3"}.fa-duotone.fa-cloud-rain:after,.fad.fa-cloud-rain:after{content:"\f73d\f73d"}.fa-duotone.fa-face-lying:after,.fad.fa-face-lying:after{content:"\e37e\e37e"}.fa-duotone.fa-sprinkler:after,.fad.fa-sprinkler:after{content:"\e035\e035"}.fa-duotone.fa-building-circle-xmark:after,.fad.fa-building-circle-xmark:after{content:"\e4d4\e4d4"}.fa-duotone.fa-person-sledding:after,.fa-duotone.fa-sledding:after,.fad.fa-person-sledding:after,.fad.fa-sledding:after{content:"\f7cb\f7cb"}.fa-duotone.fa-game-console-handheld:after,.fad.fa-game-console-handheld:after{content:"\f8bb\f8bb"}.fa-duotone.fa-ship:after,.fad.fa-ship:after{content:"\f21a\f21a"}.fa-duotone.fa-clock-six-thirty:after,.fad.fa-clock-six-thirty:after{content:"\e353\e353"}.fa-duotone.fa-battery-slash:after,.fad.fa-battery-slash:after{content:"\f377\f377"}.fa-duotone.fa-tugrik-sign:after,.fad.fa-tugrik-sign:after{content:"\e2ba\e2ba"}.fa-duotone.fa-arrows-down-to-line:after,.fad.fa-arrows-down-to-line:after{content:"\e4b8\e4b8"}.fa-duotone.fa-download:after,.fad.fa-download:after{content:"\f019\f019"}.fa-duotone.fa-angles-up-down:after,.fad.fa-angles-up-down:after{content:"\e60d\e60d"}.fa-duotone.fa-inventory:after,.fa-duotone.fa-shelves:after,.fad.fa-inventory:after,.fad.fa-shelves:after{content:"\f480\f480"}.fa-duotone.fa-cloud-snow:after,.fad.fa-cloud-snow:after{content:"\f742\f742"}.fa-duotone.fa-face-grin:after,.fa-duotone.fa-grin:after,.fad.fa-face-grin:after,.fad.fa-grin:after{content:"\f580\f580"}.fa-duotone.fa-backspace:after,.fa-duotone.fa-delete-left:after,.fad.fa-backspace:after,.fad.fa-delete-left:after{content:"\f55a\f55a"}.fa-duotone.fa-oven:after,.fad.fa-oven:after{content:"\e01d\e01d"}.fa-duotone.fa-cloud-binary:after,.fad.fa-cloud-binary:after{content:"\e601\e601"}.fa-duotone.fa-eye-dropper-empty:after,.fa-duotone.fa-eye-dropper:after,.fa-duotone.fa-eyedropper:after,.fad.fa-eye-dropper-empty:after,.fad.fa-eye-dropper:after,.fad.fa-eyedropper:after{content:"\f1fb\f1fb"}.fa-duotone.fa-comment-captions:after,.fad.fa-comment-captions:after{content:"\e146\e146"}.fa-duotone.fa-comments-question:after,.fad.fa-comments-question:after{content:"\e14e\e14e"}.fa-duotone.fa-scribble:after,.fad.fa-scribble:after{content:"\e23f\e23f"}.fa-duotone.fa-rotate-exclamation:after,.fad.fa-rotate-exclamation:after{content:"\e23c\e23c"}.fa-duotone.fa-file-circle-check:after,.fad.fa-file-circle-check:after{content:"\e5a0\e5a0"}.fa-duotone.fa-glass:after,.fad.fa-glass:after{content:"\f804\f804"}.fa-duotone.fa-loader:after,.fad.fa-loader:after{content:"\e1d4\e1d4"}.fa-duotone.fa-forward:after,.fad.fa-forward:after{content:"\f04e\f04e"}.fa-duotone.fa-user-pilot:after,.fad.fa-user-pilot:after{content:"\e2c0\e2c0"}.fa-duotone.fa-mobile-android:after,.fa-duotone.fa-mobile-phone:after,.fa-duotone.fa-mobile:after,.fad.fa-mobile-android:after,.fad.fa-mobile-phone:after,.fad.fa-mobile:after{content:"\f3ce\f3ce"}.fa-duotone.fa-code-pull-request-closed:after,.fad.fa-code-pull-request-closed:after{content:"\e3f9\e3f9"}.fa-duotone.fa-face-meh:after,.fa-duotone.fa-meh:after,.fad.fa-face-meh:after,.fad.fa-meh:after{content:"\f11a\f11a"}.fa-duotone.fa-align-center:after,.fad.fa-align-center:after{content:"\f037\f037"}.fa-duotone.fa-book-dead:after,.fa-duotone.fa-book-skull:after,.fad.fa-book-dead:after,.fad.fa-book-skull:after{content:"\f6b7\f6b7"}.fa-duotone.fa-drivers-license:after,.fa-duotone.fa-id-card:after,.fad.fa-drivers-license:after,.fad.fa-id-card:after{content:"\f2c2\f2c2"}.fa-duotone.fa-face-dotted:after,.fad.fa-face-dotted:after{content:"\e47f\e47f"}.fa-duotone.fa-face-worried:after,.fad.fa-face-worried:after{content:"\e3a3\e3a3"}.fa-duotone.fa-dedent:after,.fa-duotone.fa-outdent:after,.fad.fa-dedent:after,.fad.fa-outdent:after{content:"\f03b\f03b"}.fa-duotone.fa-court-sport:after,.fad.fa-court-sport:after{content:"\e643\e643"}.fa-duotone.fa-heart-circle-exclamation:after,.fad.fa-heart-circle-exclamation:after{content:"\e4fe\e4fe"}.fa-duotone.fa-home-alt:after,.fa-duotone.fa-home-lg-alt:after,.fa-duotone.fa-home:after,.fa-duotone.fa-house:after,.fad.fa-home-alt:after,.fad.fa-home-lg-alt:after,.fad.fa-home:after,.fad.fa-house:after{content:"\f015\f015"}.fa-duotone.fa-vector-circle:after,.fad.fa-vector-circle:after{content:"\e2c6\e2c6"}.fa-duotone.fa-car-circle-bolt:after,.fad.fa-car-circle-bolt:after{content:"\e342\e342"}.fa-duotone.fa-calendar-week:after,.fad.fa-calendar-week:after{content:"\f784\f784"}.fa-duotone.fa-flying-disc:after,.fad.fa-flying-disc:after{content:"\e3a9\e3a9"}.fa-duotone.fa-laptop-medical:after,.fad.fa-laptop-medical:after{content:"\f812\f812"}.fa-duotone.fa-square-down-right:after,.fad.fa-square-down-right:after{content:"\e26c\e26c"}.fa-duotone.fa-b:after,.fad.fa-b:after{content:"\42\42"}.fa-duotone.fa-seat-airline:after,.fad.fa-seat-airline:after{content:"\e244\e244"}.fa-duotone.fa-eclipse-alt:after,.fa-duotone.fa-moon-over-sun:after,.fad.fa-eclipse-alt:after,.fad.fa-moon-over-sun:after{content:"\f74a\f74a"}.fa-duotone.fa-pipe:after,.fad.fa-pipe:after{content:"\7c\7c"}.fa-duotone.fa-file-medical:after,.fad.fa-file-medical:after{content:"\f477\f477"}.fa-duotone.fa-potato:after,.fad.fa-potato:after{content:"\e440\e440"}.fa-duotone.fa-dice-one:after,.fad.fa-dice-one:after{content:"\f525\f525"}.fa-duotone.fa-circle-a:after,.fad.fa-circle-a:after{content:"\e0f7\e0f7"}.fa-duotone.fa-helmet-battle:after,.fad.fa-helmet-battle:after{content:"\f6eb\f6eb"}.fa-duotone.fa-butter:after,.fad.fa-butter:after{content:"\e3e4\e3e4"}.fa-duotone.fa-blanket-fire:after,.fad.fa-blanket-fire:after{content:"\e3da\e3da"}.fa-duotone.fa-kiwi-bird:after,.fad.fa-kiwi-bird:after{content:"\f535\f535"}.fa-duotone.fa-castle:after,.fad.fa-castle:after{content:"\e0de\e0de"}.fa-duotone.fa-golf-club:after,.fad.fa-golf-club:after{content:"\f451\f451"}.fa-duotone.fa-arrow-right-arrow-left:after,.fa-duotone.fa-exchange:after,.fad.fa-arrow-right-arrow-left:after,.fad.fa-exchange:after{content:"\f0ec\f0ec"}.fa-duotone.fa-redo-alt:after,.fa-duotone.fa-rotate-forward:after,.fa-duotone.fa-rotate-right:after,.fad.fa-redo-alt:after,.fad.fa-rotate-forward:after,.fad.fa-rotate-right:after{content:"\f2f9\f2f9"}.fa-duotone.fa-cutlery:after,.fa-duotone.fa-utensils:after,.fad.fa-cutlery:after,.fad.fa-utensils:after{content:"\f2e7\f2e7"}.fa-duotone.fa-arrow-up-wide-short:after,.fa-duotone.fa-sort-amount-up:after,.fad.fa-arrow-up-wide-short:after,.fad.fa-sort-amount-up:after{content:"\f161\f161"}.fa-duotone.fa-chart-pie-simple-circle-dollar:after,.fad.fa-chart-pie-simple-circle-dollar:after{content:"\e605\e605"}.fa-duotone.fa-balloons:after,.fad.fa-balloons:after{content:"\e2e4\e2e4"}.fa-duotone.fa-mill-sign:after,.fad.fa-mill-sign:after{content:"\e1ed\e1ed"}.fa-duotone.fa-bowl-rice:after,.fad.fa-bowl-rice:after{content:"\e2eb\e2eb"}.fa-duotone.fa-timeline-arrow:after,.fad.fa-timeline-arrow:after{content:"\e29d\e29d"}.fa-duotone.fa-skull:after,.fad.fa-skull:after{content:"\f54c\f54c"}.fa-duotone.fa-game-board-alt:after,.fa-duotone.fa-game-board-simple:after,.fad.fa-game-board-alt:after,.fad.fa-game-board-simple:after{content:"\f868\f868"}.fa-duotone.fa-circle-video:after,.fa-duotone.fa-video-circle:after,.fad.fa-circle-video:after,.fad.fa-video-circle:after{content:"\e12b\e12b"}.fa-duotone.fa-chart-scatter-bubble:after,.fad.fa-chart-scatter-bubble:after{content:"\e0e9\e0e9"}.fa-duotone.fa-house-turret:after,.fad.fa-house-turret:after{content:"\e1b4\e1b4"}.fa-duotone.fa-banana:after,.fad.fa-banana:after{content:"\e2e5\e2e5"}.fa-duotone.fa-hand-holding-skull:after,.fad.fa-hand-holding-skull:after{content:"\e1a4\e1a4"}.fa-duotone.fa-people-dress:after,.fad.fa-people-dress:after{content:"\e217\e217"}.fa-duotone.fa-couch-small:after,.fa-duotone.fa-loveseat:after,.fad.fa-couch-small:after,.fad.fa-loveseat:after{content:"\f4cc\f4cc"}.fa-duotone.fa-broadcast-tower:after,.fa-duotone.fa-tower-broadcast:after,.fad.fa-broadcast-tower:after,.fad.fa-tower-broadcast:after{content:"\f519\f519"}.fa-duotone.fa-truck-pickup:after,.fad.fa-truck-pickup:after{content:"\f63c\f63c"}.fa-duotone.fa-block-quote:after,.fad.fa-block-quote:after{content:"\e0b5\e0b5"}.fa-duotone.fa-long-arrow-alt-up:after,.fa-duotone.fa-up-long:after,.fad.fa-long-arrow-alt-up:after,.fad.fa-up-long:after{content:"\f30c\f30c"}.fa-duotone.fa-stop:after,.fad.fa-stop:after{content:"\f04d\f04d"}.fa-duotone.fa-code-merge:after,.fad.fa-code-merge:after{content:"\f387\f387"}.fa-duotone.fa-money-check-dollar-pen:after,.fa-duotone.fa-money-check-edit-alt:after,.fad.fa-money-check-dollar-pen:after,.fad.fa-money-check-edit-alt:after{content:"\f873\f873"}.fa-duotone.fa-arrow-alt-from-bottom:after,.fa-duotone.fa-up-from-line:after,.fad.fa-arrow-alt-from-bottom:after,.fad.fa-up-from-line:after{content:"\f346\f346"}.fa-duotone.fa-upload:after,.fad.fa-upload:after{content:"\f093\f093"}.fa-duotone.fa-hurricane:after,.fad.fa-hurricane:after{content:"\f751\f751"}.fa-duotone.fa-grid-round-2-plus:after,.fad.fa-grid-round-2-plus:after{content:"\e5dc\e5dc"}.fa-duotone.fa-people-pants:after,.fad.fa-people-pants:after{content:"\e219\e219"}.fa-duotone.fa-mound:after,.fad.fa-mound:after{content:"\e52d\e52d"}.fa-duotone.fa-windsock:after,.fad.fa-windsock:after{content:"\f777\f777"}.fa-duotone.fa-circle-half:after,.fad.fa-circle-half:after{content:"\e110\e110"}.fa-duotone.fa-brake-warning:after,.fad.fa-brake-warning:after{content:"\e0c7\e0c7"}.fa-duotone.fa-toilet-portable:after,.fad.fa-toilet-portable:after{content:"\e583\e583"}.fa-duotone.fa-compact-disc:after,.fad.fa-compact-disc:after{content:"\f51f\f51f"}.fa-duotone.fa-file-arrow-down:after,.fa-duotone.fa-file-download:after,.fad.fa-file-arrow-down:after,.fad.fa-file-download:after{content:"\f56d\f56d"}.fa-duotone.fa-sax-hot:after,.fa-duotone.fa-saxophone-fire:after,.fad.fa-sax-hot:after,.fad.fa-saxophone-fire:after{content:"\f8db\f8db"}.fa-duotone.fa-camera-web-slash:after,.fa-duotone.fa-webcam-slash:after,.fad.fa-camera-web-slash:after,.fad.fa-webcam-slash:after{content:"\f833\f833"}.fa-duotone.fa-folder-medical:after,.fad.fa-folder-medical:after{content:"\e18c\e18c"}.fa-duotone.fa-folder-cog:after,.fa-duotone.fa-folder-gear:after,.fad.fa-folder-cog:after,.fad.fa-folder-gear:after{content:"\e187\e187"}.fa-duotone.fa-hand-wave:after,.fad.fa-hand-wave:after{content:"\e1a7\e1a7"}.fa-duotone.fa-arrow-up-arrow-down:after,.fa-duotone.fa-sort-up-down:after,.fad.fa-arrow-up-arrow-down:after,.fad.fa-sort-up-down:after{content:"\e099\e099"}.fa-duotone.fa-caravan:after,.fad.fa-caravan:after{content:"\f8ff\f8ff"}.fa-duotone.fa-shield-cat:after,.fad.fa-shield-cat:after{content:"\e572\e572"}.fa-duotone.fa-comment-alt-slash:after,.fa-duotone.fa-message-slash:after,.fad.fa-comment-alt-slash:after,.fad.fa-message-slash:after{content:"\f4a9\f4a9"}.fa-duotone.fa-bolt:after,.fa-duotone.fa-zap:after,.fad.fa-bolt:after,.fad.fa-zap:after{content:"\f0e7\f0e7"}.fa-duotone.fa-trash-can-check:after,.fad.fa-trash-can-check:after{content:"\e2a9\e2a9"}.fa-duotone.fa-glass-water:after,.fad.fa-glass-water:after{content:"\e4f4\e4f4"}.fa-duotone.fa-oil-well:after,.fad.fa-oil-well:after{content:"\e532\e532"}.fa-duotone.fa-person-simple:after,.fad.fa-person-simple:after{content:"\e220\e220"}.fa-duotone.fa-arrow-turn-left-up:after,.fad.fa-arrow-turn-left-up:after{content:"\e634\e634"}.fa-duotone.fa-vault:after,.fad.fa-vault:after{content:"\e2c5\e2c5"}.fa-duotone.fa-mars:after,.fad.fa-mars:after{content:"\f222\f222"}.fa-duotone.fa-toilet:after,.fad.fa-toilet:after{content:"\f7d8\f7d8"}.fa-duotone.fa-plane-circle-xmark:after,.fad.fa-plane-circle-xmark:after{content:"\e557\e557"}.fa-duotone.fa-cny:after,.fa-duotone.fa-jpy:after,.fa-duotone.fa-rmb:after,.fa-duotone.fa-yen-sign:after,.fa-duotone.fa-yen:after,.fad.fa-cny:after,.fad.fa-jpy:after,.fad.fa-rmb:after,.fad.fa-yen-sign:after,.fad.fa-yen:after{content:"\f157\f157"}.fa-duotone.fa-gear-code:after,.fad.fa-gear-code:after{content:"\e5e8\e5e8"}.fa-duotone.fa-notes:after,.fad.fa-notes:after{content:"\e202\e202"}.fa-duotone.fa-rouble:after,.fa-duotone.fa-rub:after,.fa-duotone.fa-ruble-sign:after,.fa-duotone.fa-ruble:after,.fad.fa-rouble:after,.fad.fa-rub:after,.fad.fa-ruble-sign:after,.fad.fa-ruble:after{content:"\f158\f158"}.fa-duotone.fa-trash-arrow-turn-left:after,.fa-duotone.fa-trash-undo:after,.fad.fa-trash-arrow-turn-left:after,.fad.fa-trash-undo:after{content:"\f895\f895"}.fa-duotone.fa-champagne-glass:after,.fa-duotone.fa-glass-champagne:after,.fad.fa-champagne-glass:after,.fad.fa-glass-champagne:after{content:"\f79e\f79e"}.fa-duotone.fa-objects-align-center-horizontal:after,.fad.fa-objects-align-center-horizontal:after{content:"\e3bc\e3bc"}.fa-duotone.fa-sun:after,.fad.fa-sun:after{content:"\f185\f185"}.fa-duotone.fa-trash-alt-slash:after,.fa-duotone.fa-trash-can-slash:after,.fad.fa-trash-alt-slash:after,.fad.fa-trash-can-slash:after{content:"\e2ad\e2ad"}.fa-duotone.fa-screen-users:after,.fa-duotone.fa-users-class:after,.fad.fa-screen-users:after,.fad.fa-users-class:after{content:"\f63d\f63d"}.fa-duotone.fa-guitar:after,.fad.fa-guitar:after{content:"\f7a6\f7a6"}.fa-duotone.fa-arrow-square-left:after,.fa-duotone.fa-square-arrow-left:after,.fad.fa-arrow-square-left:after,.fad.fa-square-arrow-left:after{content:"\f33a\f33a"}.fa-duotone.fa-square-8:after,.fad.fa-square-8:after{content:"\e25d\e25d"}.fa-duotone.fa-face-smile-hearts:after,.fad.fa-face-smile-hearts:after{content:"\e390\e390"}.fa-duotone.fa-brackets-square:after,.fa-duotone.fa-brackets:after,.fad.fa-brackets-square:after,.fad.fa-brackets:after{content:"\f7e9\f7e9"}.fa-duotone.fa-laptop-arrow-down:after,.fad.fa-laptop-arrow-down:after{content:"\e1c6\e1c6"}.fa-duotone.fa-hockey-stick-puck:after,.fad.fa-hockey-stick-puck:after{content:"\e3ae\e3ae"}.fa-duotone.fa-house-tree:after,.fad.fa-house-tree:after{content:"\e1b3\e1b3"}.fa-duotone.fa-signal-2:after,.fa-duotone.fa-signal-fair:after,.fad.fa-signal-2:after,.fad.fa-signal-fair:after{content:"\f68d\f68d"}.fa-duotone.fa-face-laugh-wink:after,.fa-duotone.fa-laugh-wink:after,.fad.fa-face-laugh-wink:after,.fad.fa-laugh-wink:after{content:"\f59c\f59c"}.fa-duotone.fa-circle-dollar:after,.fa-duotone.fa-dollar-circle:after,.fa-duotone.fa-usd-circle:after,.fad.fa-circle-dollar:after,.fad.fa-dollar-circle:after,.fad.fa-usd-circle:after{content:"\f2e8\f2e8"}.fa-duotone.fa-horse-head:after,.fad.fa-horse-head:after{content:"\f7ab\f7ab"}.fa-duotone.fa-arrows-repeat:after,.fa-duotone.fa-repeat-alt:after,.fad.fa-arrows-repeat:after,.fad.fa-repeat-alt:after{content:"\f364\f364"}.fa-duotone.fa-bore-hole:after,.fad.fa-bore-hole:after{content:"\e4c3\e4c3"}.fa-duotone.fa-industry:after,.fad.fa-industry:after{content:"\f275\f275"}.fa-duotone.fa-image-polaroid:after,.fad.fa-image-polaroid:after{content:"\f8c4\f8c4"}.fa-duotone.fa-wave-triangle:after,.fad.fa-wave-triangle:after{content:"\f89a\f89a"}.fa-duotone.fa-turn-left-down:after,.fad.fa-turn-left-down:after{content:"\e637\e637"}.fa-duotone.fa-person-running-fast:after,.fad.fa-person-running-fast:after{content:"\e5ff\e5ff"}.fa-duotone.fa-arrow-alt-circle-down:after,.fa-duotone.fa-circle-down:after,.fad.fa-arrow-alt-circle-down:after,.fad.fa-circle-down:after{content:"\f358\f358"}.fa-duotone.fa-grill:after,.fad.fa-grill:after{content:"\e5a3\e5a3"}.fa-duotone.fa-arrows-turn-to-dots:after,.fad.fa-arrows-turn-to-dots:after{content:"\e4c1\e4c1"}.fa-duotone.fa-analytics:after,.fa-duotone.fa-chart-mixed:after,.fad.fa-analytics:after,.fad.fa-chart-mixed:after{content:"\f643\f643"}.fa-duotone.fa-florin-sign:after,.fad.fa-florin-sign:after{content:"\e184\e184"}.fa-duotone.fa-arrow-down-short-wide:after,.fa-duotone.fa-sort-amount-desc:after,.fa-duotone.fa-sort-amount-down-alt:after,.fad.fa-arrow-down-short-wide:after,.fad.fa-sort-amount-desc:after,.fad.fa-sort-amount-down-alt:after{content:"\f884\f884"}.fa-duotone.fa-less-than:after,.fad.fa-less-than:after{content:"\3c\3c"}.fa-duotone.fa-desktop-code:after,.fa-duotone.fa-display-code:after,.fad.fa-desktop-code:after,.fad.fa-display-code:after{content:"\e165\e165"}.fa-duotone.fa-face-drooling:after,.fad.fa-face-drooling:after{content:"\e372\e372"}.fa-duotone.fa-oil-temp:after,.fa-duotone.fa-oil-temperature:after,.fad.fa-oil-temp:after,.fad.fa-oil-temperature:after{content:"\f614\f614"}.fa-duotone.fa-question-square:after,.fa-duotone.fa-square-question:after,.fad.fa-question-square:after,.fad.fa-square-question:after{content:"\f2fd\f2fd"}.fa-duotone.fa-air-conditioner:after,.fad.fa-air-conditioner:after{content:"\f8f4\f8f4"}.fa-duotone.fa-angle-down:after,.fad.fa-angle-down:after{content:"\f107\f107"}.fa-duotone.fa-mountains:after,.fad.fa-mountains:after{content:"\f6fd\f6fd"}.fa-duotone.fa-omega:after,.fad.fa-omega:after{content:"\f67a\f67a"}.fa-duotone.fa-car-tunnel:after,.fad.fa-car-tunnel:after{content:"\e4de\e4de"}.fa-duotone.fa-person-dolly-empty:after,.fad.fa-person-dolly-empty:after{content:"\f4d1\f4d1"}.fa-duotone.fa-pan-food:after,.fad.fa-pan-food:after{content:"\e42b\e42b"}.fa-duotone.fa-head-side-cough:after,.fad.fa-head-side-cough:after{content:"\e061\e061"}.fa-duotone.fa-grip-lines:after,.fad.fa-grip-lines:after{content:"\f7a4\f7a4"}.fa-duotone.fa-thumbs-down:after,.fad.fa-thumbs-down:after{content:"\f165\f165"}.fa-duotone.fa-user-lock:after,.fad.fa-user-lock:after{content:"\f502\f502"}.fa-duotone.fa-arrow-right-long:after,.fa-duotone.fa-long-arrow-right:after,.fad.fa-arrow-right-long:after,.fad.fa-long-arrow-right:after{content:"\f178\f178"}.fa-duotone.fa-tickets-airline:after,.fa-duotone.fa-tickets-perforated-plane:after,.fa-duotone.fa-tickets-plane:after,.fad.fa-tickets-airline:after,.fad.fa-tickets-perforated-plane:after,.fad.fa-tickets-plane:after{content:"\e29b\e29b"}.fa-duotone.fa-tent-double-peak:after,.fad.fa-tent-double-peak:after{content:"\e627\e627"}.fa-duotone.fa-anchor-circle-xmark:after,.fad.fa-anchor-circle-xmark:after{content:"\e4ac\e4ac"}.fa-duotone.fa-ellipsis-h:after,.fa-duotone.fa-ellipsis:after,.fad.fa-ellipsis-h:after,.fad.fa-ellipsis:after{content:"\f141\f141"}.fa-duotone.fa-nfc-slash:after,.fad.fa-nfc-slash:after{content:"\e1fc\e1fc"}.fa-duotone.fa-chess-pawn:after,.fad.fa-chess-pawn:after{content:"\f443\f443"}.fa-duotone.fa-first-aid:after,.fa-duotone.fa-kit-medical:after,.fad.fa-first-aid:after,.fad.fa-kit-medical:after{content:"\f479\f479"}.fa-duotone.fa-grid-2-plus:after,.fad.fa-grid-2-plus:after{content:"\e197\e197"}.fa-duotone.fa-bells:after,.fad.fa-bells:after{content:"\f77f\f77f"}.fa-duotone.fa-person-through-window:after,.fad.fa-person-through-window:after{content:"\e5a9\e5a9"}.fa-duotone.fa-toolbox:after,.fad.fa-toolbox:after{content:"\f552\f552"}.fa-duotone.fa-envelope-badge:after,.fa-duotone.fa-envelope-dot:after,.fad.fa-envelope-badge:after,.fad.fa-envelope-dot:after{content:"\e16f\e16f"}.fa-duotone.fa-magnifying-glass-waveform:after,.fad.fa-magnifying-glass-waveform:after{content:"\e661\e661"}.fa-duotone.fa-hands-holding-circle:after,.fad.fa-hands-holding-circle:after{content:"\e4fb\e4fb"}.fa-duotone.fa-bug:after,.fad.fa-bug:after{content:"\f188\f188"}.fa-duotone.fa-bowl-chopsticks:after,.fad.fa-bowl-chopsticks:after{content:"\e2e9\e2e9"}.fa-duotone.fa-credit-card-alt:after,.fa-duotone.fa-credit-card:after,.fad.fa-credit-card-alt:after,.fad.fa-credit-card:after{content:"\f09d\f09d"}.fa-duotone.fa-circle-s:after,.fad.fa-circle-s:after{content:"\e121\e121"}.fa-duotone.fa-box-ballot:after,.fad.fa-box-ballot:after{content:"\f735\f735"}.fa-duotone.fa-automobile:after,.fa-duotone.fa-car:after,.fad.fa-automobile:after,.fad.fa-car:after{content:"\f1b9\f1b9"}.fa-duotone.fa-hand-holding-hand:after,.fad.fa-hand-holding-hand:after{content:"\e4f7\e4f7"}.fa-duotone.fa-user-tie-hair:after,.fad.fa-user-tie-hair:after{content:"\e45f\e45f"}.fa-duotone.fa-podium-star:after,.fad.fa-podium-star:after{content:"\f758\f758"}.fa-duotone.fa-business-front:after,.fa-duotone.fa-party-back:after,.fa-duotone.fa-trian-balbot:after,.fa-duotone.fa-user-hair-mullet:after,.fad.fa-business-front:after,.fad.fa-party-back:after,.fad.fa-trian-balbot:after,.fad.fa-user-hair-mullet:after{content:"\e45c\e45c"}.fa-duotone.fa-microphone-stand:after,.fad.fa-microphone-stand:after{content:"\f8cb\f8cb"}.fa-duotone.fa-book-open-reader:after,.fa-duotone.fa-book-reader:after,.fad.fa-book-open-reader:after,.fad.fa-book-reader:after{content:"\f5da\f5da"}.fa-duotone.fa-family-dress:after,.fad.fa-family-dress:after{content:"\e301\e301"}.fa-duotone.fa-circle-x:after,.fad.fa-circle-x:after{content:"\e12e\e12e"}.fa-duotone.fa-cabin:after,.fad.fa-cabin:after{content:"\e46d\e46d"}.fa-duotone.fa-mountain-sun:after,.fad.fa-mountain-sun:after{content:"\e52f\e52f"}.fa-duotone.fa-chart-simple-horizontal:after,.fad.fa-chart-simple-horizontal:after{content:"\e474\e474"}.fa-duotone.fa-arrows-left-right-to-line:after,.fad.fa-arrows-left-right-to-line:after{content:"\e4ba\e4ba"}.fa-duotone.fa-hand-back-point-left:after,.fad.fa-hand-back-point-left:after{content:"\e19f\e19f"}.fa-duotone.fa-comment-alt-dots:after,.fa-duotone.fa-message-dots:after,.fa-duotone.fa-messaging:after,.fad.fa-comment-alt-dots:after,.fad.fa-message-dots:after,.fad.fa-messaging:after{content:"\f4a3\f4a3"}.fa-duotone.fa-file-heart:after,.fad.fa-file-heart:after{content:"\e176\e176"}.fa-duotone.fa-beer-foam:after,.fa-duotone.fa-beer-mug:after,.fad.fa-beer-foam:after,.fad.fa-beer-mug:after{content:"\e0b3\e0b3"}.fa-duotone.fa-dice-d20:after,.fad.fa-dice-d20:after{content:"\f6cf\f6cf"}.fa-duotone.fa-drone:after,.fad.fa-drone:after{content:"\f85f\f85f"}.fa-duotone.fa-truck-droplet:after,.fad.fa-truck-droplet:after{content:"\e58c\e58c"}.fa-duotone.fa-file-circle-xmark:after,.fad.fa-file-circle-xmark:after{content:"\e5a1\e5a1"}.fa-duotone.fa-temperature-arrow-up:after,.fa-duotone.fa-temperature-up:after,.fad.fa-temperature-arrow-up:after,.fad.fa-temperature-up:after{content:"\e040\e040"}.fa-duotone.fa-medal:after,.fad.fa-medal:after{content:"\f5a2\f5a2"}.fa-duotone.fa-person-fairy:after,.fad.fa-person-fairy:after{content:"\e608\e608"}.fa-duotone.fa-bed:after,.fad.fa-bed:after{content:"\f236\f236"}.fa-duotone.fa-book-copy:after,.fad.fa-book-copy:after{content:"\e0be\e0be"}.fa-duotone.fa-h-square:after,.fa-duotone.fa-square-h:after,.fad.fa-h-square:after,.fad.fa-square-h:after{content:"\f0fd\f0fd"}.fa-duotone.fa-square-c:after,.fad.fa-square-c:after{content:"\e266\e266"}.fa-duotone.fa-clock-two:after,.fad.fa-clock-two:after{content:"\e35a\e35a"}.fa-duotone.fa-square-ellipsis-vertical:after,.fad.fa-square-ellipsis-vertical:after{content:"\e26f\e26f"}.fa-duotone.fa-calendar-users:after,.fad.fa-calendar-users:after{content:"\e5e2\e5e2"}.fa-duotone.fa-podcast:after,.fad.fa-podcast:after{content:"\f2ce\f2ce"}.fa-duotone.fa-bee:after,.fad.fa-bee:after{content:"\e0b2\e0b2"}.fa-duotone.fa-temperature-4:after,.fa-duotone.fa-temperature-full:after,.fa-duotone.fa-thermometer-4:after,.fa-duotone.fa-thermometer-full:after,.fad.fa-temperature-4:after,.fad.fa-temperature-full:after,.fad.fa-thermometer-4:after,.fad.fa-thermometer-full:after{content:"\f2c7\f2c7"}.fa-duotone.fa-bell:after,.fad.fa-bell:after{content:"\f0f3\f0f3"}.fa-duotone.fa-candy-bar:after,.fa-duotone.fa-chocolate-bar:after,.fad.fa-candy-bar:after,.fad.fa-chocolate-bar:after{content:"\e3e8\e3e8"}.fa-duotone.fa-xmark-large:after,.fad.fa-xmark-large:after{content:"\e59b\e59b"}.fa-duotone.fa-pinata:after,.fad.fa-pinata:after{content:"\e3c3\e3c3"}.fa-duotone.fa-file-ppt:after,.fad.fa-file-ppt:after{content:"\e64a\e64a"}.fa-duotone.fa-arrows-from-line:after,.fad.fa-arrows-from-line:after{content:"\e0a4\e0a4"}.fa-duotone.fa-superscript:after,.fad.fa-superscript:after{content:"\f12b\f12b"}.fa-duotone.fa-bowl-spoon:after,.fad.fa-bowl-spoon:after{content:"\e3e0\e3e0"}.fa-duotone.fa-hexagon-check:after,.fad.fa-hexagon-check:after{content:"\e416\e416"}.fa-duotone.fa-plug-circle-xmark:after,.fad.fa-plug-circle-xmark:after{content:"\e560\e560"}.fa-duotone.fa-star-of-life:after,.fad.fa-star-of-life:after{content:"\f621\f621"}.fa-duotone.fa-phone-slash:after,.fad.fa-phone-slash:after{content:"\f3dd\f3dd"}.fa-duotone.fa-traffic-light-stop:after,.fad.fa-traffic-light-stop:after{content:"\f63a\f63a"}.fa-duotone.fa-paint-roller:after,.fad.fa-paint-roller:after{content:"\f5aa\f5aa"}.fa-duotone.fa-accent-grave:after,.fad.fa-accent-grave:after{content:"\60\60"}.fa-duotone.fa-hands-helping:after,.fa-duotone.fa-handshake-angle:after,.fad.fa-hands-helping:after,.fad.fa-handshake-angle:after{content:"\f4c4\f4c4"}.fa-duotone.fa-circle-0:after,.fad.fa-circle-0:after{content:"\e0ed\e0ed"}.fa-duotone.fa-dial-med-low:after,.fad.fa-dial-med-low:after{content:"\e160\e160"}.fa-duotone.fa-location-dot:after,.fa-duotone.fa-map-marker-alt:after,.fad.fa-location-dot:after,.fad.fa-map-marker-alt:after{content:"\f3c5\f3c5"}.fa-duotone.fa-crab:after,.fad.fa-crab:after{content:"\e3ff\e3ff"}.fa-duotone.fa-box-full:after,.fa-duotone.fa-box-open-full:after,.fad.fa-box-full:after,.fad.fa-box-open-full:after{content:"\f49c\f49c"}.fa-duotone.fa-file:after,.fad.fa-file:after{content:"\f15b\f15b"}.fa-duotone.fa-greater-than:after,.fad.fa-greater-than:after{content:"\3e\3e"}.fa-duotone.fa-quotes:after,.fad.fa-quotes:after{content:"\e234\e234"}.fa-duotone.fa-pretzel:after,.fad.fa-pretzel:after{content:"\e441\e441"}.fa-duotone.fa-t-rex:after,.fad.fa-t-rex:after{content:"\e629\e629"}.fa-duotone.fa-person-swimming:after,.fa-duotone.fa-swimmer:after,.fad.fa-person-swimming:after,.fad.fa-swimmer:after{content:"\f5c4\f5c4"}.fa-duotone.fa-arrow-down:after,.fad.fa-arrow-down:after{content:"\f063\f063"}.fa-duotone.fa-user-robot-xmarks:after,.fad.fa-user-robot-xmarks:after{content:"\e4a7\e4a7"}.fa-duotone.fa-comment-alt-quote:after,.fa-duotone.fa-message-quote:after,.fad.fa-comment-alt-quote:after,.fad.fa-message-quote:after{content:"\e1e4\e1e4"}.fa-duotone.fa-candy-corn:after,.fad.fa-candy-corn:after{content:"\f6bd\f6bd"}.fa-duotone.fa-folder-magnifying-glass:after,.fa-duotone.fa-folder-search:after,.fad.fa-folder-magnifying-glass:after,.fad.fa-folder-search:after{content:"\e18b\e18b"}.fa-duotone.fa-notebook:after,.fad.fa-notebook:after{content:"\e201\e201"}.fa-duotone.fa-circle-wifi:after,.fad.fa-circle-wifi:after{content:"\e67d\e67d"}.fa-duotone.fa-droplet:after,.fa-duotone.fa-tint:after,.fad.fa-droplet:after,.fad.fa-tint:after{content:"\f043\f043"}.fa-duotone.fa-bullseye-pointer:after,.fad.fa-bullseye-pointer:after{content:"\f649\f649"}.fa-duotone.fa-eraser:after,.fad.fa-eraser:after{content:"\f12d\f12d"}.fa-duotone.fa-hexagon-image:after,.fad.fa-hexagon-image:after{content:"\e504\e504"}.fa-duotone.fa-earth-america:after,.fa-duotone.fa-earth-americas:after,.fa-duotone.fa-earth:after,.fa-duotone.fa-globe-americas:after,.fad.fa-earth-america:after,.fad.fa-earth-americas:after,.fad.fa-earth:after,.fad.fa-globe-americas:after{content:"\f57d\f57d"}.fa-duotone.fa-file-svg:after,.fad.fa-file-svg:after{content:"\e64b\e64b"}.fa-duotone.fa-crate-apple:after,.fad.fa-crate-apple:after{content:"\f6b1\f6b1"}.fa-duotone.fa-apple-crate:after,.fad.fa-apple-crate:after{content:"\f6b1\f6b1"}.fa-duotone.fa-person-burst:after,.fad.fa-person-burst:after{content:"\e53b\e53b"}.fa-duotone.fa-game-board:after,.fad.fa-game-board:after{content:"\f867\f867"}.fa-duotone.fa-hat-chef:after,.fad.fa-hat-chef:after{content:"\f86b\f86b"}.fa-duotone.fa-hand-back-point-right:after,.fad.fa-hand-back-point-right:after{content:"\e1a1\e1a1"}.fa-duotone.fa-dove:after,.fad.fa-dove:after{content:"\f4ba\f4ba"}.fa-duotone.fa-snowflake-droplets:after,.fad.fa-snowflake-droplets:after{content:"\e5c1\e5c1"}.fa-duotone.fa-battery-0:after,.fa-duotone.fa-battery-empty:after,.fad.fa-battery-0:after,.fad.fa-battery-empty:after{content:"\f244\f244"}.fa-duotone.fa-grid-4:after,.fad.fa-grid-4:after{content:"\e198\e198"}.fa-duotone.fa-socks:after,.fad.fa-socks:after{content:"\f696\f696"}.fa-duotone.fa-face-sunglasses:after,.fad.fa-face-sunglasses:after{content:"\e398\e398"}.fa-duotone.fa-inbox:after,.fad.fa-inbox:after{content:"\f01c\f01c"}.fa-duotone.fa-square-0:after,.fad.fa-square-0:after{content:"\e255\e255"}.fa-duotone.fa-section:after,.fad.fa-section:after{content:"\e447\e447"}.fa-duotone.fa-box-up:after,.fa-duotone.fa-square-this-way-up:after,.fad.fa-box-up:after,.fad.fa-square-this-way-up:after{content:"\f49f\f49f"}.fa-duotone.fa-gauge-high:after,.fa-duotone.fa-tachometer-alt-fast:after,.fa-duotone.fa-tachometer-alt:after,.fad.fa-gauge-high:after,.fad.fa-tachometer-alt-fast:after,.fad.fa-tachometer-alt:after{content:"\f625\f625"}.fa-duotone.fa-square-ampersand:after,.fad.fa-square-ampersand:after{content:"\e260\e260"}.fa-duotone.fa-envelope-open-text:after,.fad.fa-envelope-open-text:after{content:"\f658\f658"}.fa-duotone.fa-lamp-desk:after,.fad.fa-lamp-desk:after{content:"\e014\e014"}.fa-duotone.fa-hospital-alt:after,.fa-duotone.fa-hospital-wide:after,.fa-duotone.fa-hospital:after,.fad.fa-hospital-alt:after,.fad.fa-hospital-wide:after,.fad.fa-hospital:after{content:"\f0f8\f0f8"}.fa-duotone.fa-poll-people:after,.fad.fa-poll-people:after{content:"\f759\f759"}.fa-duotone.fa-glass-whiskey-rocks:after,.fa-duotone.fa-whiskey-glass-ice:after,.fad.fa-glass-whiskey-rocks:after,.fad.fa-whiskey-glass-ice:after{content:"\f7a1\f7a1"}.fa-duotone.fa-wine-bottle:after,.fad.fa-wine-bottle:after{content:"\f72f\f72f"}.fa-duotone.fa-chess-rook:after,.fad.fa-chess-rook:after{content:"\f447\f447"}.fa-duotone.fa-user-bounty-hunter:after,.fad.fa-user-bounty-hunter:after{content:"\e2bf\e2bf"}.fa-duotone.fa-bars-staggered:after,.fa-duotone.fa-reorder:after,.fa-duotone.fa-stream:after,.fad.fa-bars-staggered:after,.fad.fa-reorder:after,.fad.fa-stream:after{content:"\f550\f550"}.fa-duotone.fa-diagram-sankey:after,.fad.fa-diagram-sankey:after{content:"\e158\e158"}.fa-duotone.fa-cloud-hail-mixed:after,.fad.fa-cloud-hail-mixed:after{content:"\f73a\f73a"}.fa-duotone.fa-circle-up-left:after,.fad.fa-circle-up-left:after{content:"\e128\e128"}.fa-duotone.fa-dharmachakra:after,.fad.fa-dharmachakra:after{content:"\f655\f655"}.fa-duotone.fa-objects-align-left:after,.fad.fa-objects-align-left:after{content:"\e3be\e3be"}.fa-duotone.fa-oil-can-drip:after,.fad.fa-oil-can-drip:after{content:"\e205\e205"}.fa-duotone.fa-face-smiling-hands:after,.fad.fa-face-smiling-hands:after{content:"\e396\e396"}.fa-duotone.fa-broccoli:after,.fad.fa-broccoli:after{content:"\e3e2\e3e2"}.fa-duotone.fa-route-interstate:after,.fad.fa-route-interstate:after{content:"\f61b\f61b"}.fa-duotone.fa-ear-muffs:after,.fad.fa-ear-muffs:after{content:"\f795\f795"}.fa-duotone.fa-hotdog:after,.fad.fa-hotdog:after{content:"\f80f\f80f"}.fa-duotone.fa-transporter-empty:after,.fad.fa-transporter-empty:after{content:"\e046\e046"}.fa-duotone.fa-blind:after,.fa-duotone.fa-person-walking-with-cane:after,.fad.fa-blind:after,.fad.fa-person-walking-with-cane:after{content:"\f29d\f29d"}.fa-duotone.fa-angle-90:after,.fad.fa-angle-90:after{content:"\e08d\e08d"}.fa-duotone.fa-rectangle-terminal:after,.fad.fa-rectangle-terminal:after{content:"\e236\e236"}.fa-duotone.fa-kite:after,.fad.fa-kite:after{content:"\f6f4\f6f4"}.fa-duotone.fa-drum:after,.fad.fa-drum:after{content:"\f569\f569"}.fa-duotone.fa-scrubber:after,.fad.fa-scrubber:after{content:"\f2f8\f2f8"}.fa-duotone.fa-ice-cream:after,.fad.fa-ice-cream:after{content:"\f810\f810"}.fa-duotone.fa-heart-circle-bolt:after,.fad.fa-heart-circle-bolt:after{content:"\e4fc\e4fc"}.fa-duotone.fa-fish-bones:after,.fad.fa-fish-bones:after{content:"\e304\e304"}.fa-duotone.fa-deer-rudolph:after,.fad.fa-deer-rudolph:after{content:"\f78f\f78f"}.fa-duotone.fa-fax:after,.fad.fa-fax:after{content:"\f1ac\f1ac"}.fa-duotone.fa-paragraph:after,.fad.fa-paragraph:after{content:"\f1dd\f1dd"}.fa-duotone.fa-head-side-heart:after,.fad.fa-head-side-heart:after{content:"\e1aa\e1aa"}.fa-duotone.fa-square-e:after,.fad.fa-square-e:after{content:"\e26d\e26d"}.fa-duotone.fa-meter-fire:after,.fad.fa-meter-fire:after{content:"\e1eb\e1eb"}.fa-duotone.fa-cloud-hail:after,.fad.fa-cloud-hail:after{content:"\f739\f739"}.fa-duotone.fa-check-to-slot:after,.fa-duotone.fa-vote-yea:after,.fad.fa-check-to-slot:after,.fad.fa-vote-yea:after{content:"\f772\f772"}.fa-duotone.fa-money-from-bracket:after,.fad.fa-money-from-bracket:after{content:"\e312\e312"}.fa-duotone.fa-star-half:after,.fad.fa-star-half:after{content:"\f089\f089"}.fa-duotone.fa-car-bus:after,.fad.fa-car-bus:after{content:"\f85a\f85a"}.fa-duotone.fa-speaker:after,.fad.fa-speaker:after{content:"\f8df\f8df"}.fa-duotone.fa-timer:after,.fad.fa-timer:after{content:"\e29e\e29e"}.fa-duotone.fa-boxes-alt:after,.fa-duotone.fa-boxes-stacked:after,.fa-duotone.fa-boxes:after,.fad.fa-boxes-alt:after,.fad.fa-boxes-stacked:after,.fad.fa-boxes:after{content:"\f468\f468"}.fa-duotone.fa-landmark-magnifying-glass:after,.fad.fa-landmark-magnifying-glass:after{content:"\e622\e622"}.fa-duotone.fa-grill-hot:after,.fad.fa-grill-hot:after{content:"\e5a5\e5a5"}.fa-duotone.fa-ballot-check:after,.fad.fa-ballot-check:after{content:"\f733\f733"}.fa-duotone.fa-chain:after,.fa-duotone.fa-link:after,.fad.fa-chain:after,.fad.fa-link:after{content:"\f0c1\f0c1"}.fa-duotone.fa-assistive-listening-systems:after,.fa-duotone.fa-ear-listen:after,.fad.fa-assistive-listening-systems:after,.fad.fa-ear-listen:after{content:"\f2a2\f2a2"}.fa-duotone.fa-file-minus:after,.fad.fa-file-minus:after{content:"\f318\f318"}.fa-duotone.fa-tree-city:after,.fad.fa-tree-city:after{content:"\e587\e587"}.fa-duotone.fa-play:after,.fad.fa-play:after{content:"\f04b\f04b"}.fa-duotone.fa-font:after,.fad.fa-font:after{content:"\f031\f031"}.fa-duotone.fa-coffee-togo:after,.fa-duotone.fa-cup-togo:after,.fad.fa-coffee-togo:after,.fad.fa-cup-togo:after{content:"\f6c5\f6c5"}.fa-duotone.fa-square-down-left:after,.fad.fa-square-down-left:after{content:"\e26b\e26b"}.fa-duotone.fa-burger-lettuce:after,.fad.fa-burger-lettuce:after{content:"\e3e3\e3e3"}.fa-duotone.fa-table-cells-row-lock:after,.fad.fa-table-cells-row-lock:after{content:"\e67a\e67a"}.fa-duotone.fa-rupiah-sign:after,.fad.fa-rupiah-sign:after{content:"\e23d\e23d"}.fa-duotone.fa-magnifying-glass:after,.fa-duotone.fa-search:after,.fad.fa-magnifying-glass:after,.fad.fa-search:after{content:"\f002\f002"}.fa-duotone.fa-ping-pong-paddle-ball:after,.fa-duotone.fa-table-tennis-paddle-ball:after,.fa-duotone.fa-table-tennis:after,.fad.fa-ping-pong-paddle-ball:after,.fad.fa-table-tennis-paddle-ball:after,.fad.fa-table-tennis:after{content:"\f45d\f45d"}.fa-duotone.fa-diagnoses:after,.fa-duotone.fa-person-dots-from-line:after,.fad.fa-diagnoses:after,.fad.fa-person-dots-from-line:after{content:"\f470\f470"}.fa-duotone.fa-chevron-double-down:after,.fa-duotone.fa-chevrons-down:after,.fad.fa-chevron-double-down:after,.fad.fa-chevrons-down:after{content:"\f322\f322"}.fa-duotone.fa-trash-can-arrow-up:after,.fa-duotone.fa-trash-restore-alt:after,.fad.fa-trash-can-arrow-up:after,.fad.fa-trash-restore-alt:after{content:"\f82a\f82a"}.fa-duotone.fa-signal-3:after,.fa-duotone.fa-signal-good:after,.fad.fa-signal-3:after,.fad.fa-signal-good:after{content:"\f68e\f68e"}.fa-duotone.fa-location-question:after,.fa-duotone.fa-map-marker-question:after,.fad.fa-location-question:after,.fad.fa-map-marker-question:after{content:"\f60b\f60b"}.fa-duotone.fa-floppy-disk-circle-xmark:after,.fa-duotone.fa-floppy-disk-times:after,.fa-duotone.fa-save-circle-xmark:after,.fa-duotone.fa-save-times:after,.fad.fa-floppy-disk-circle-xmark:after,.fad.fa-floppy-disk-times:after,.fad.fa-save-circle-xmark:after,.fad.fa-save-times:after{content:"\e181\e181"}.fa-duotone.fa-naira-sign:after,.fad.fa-naira-sign:after{content:"\e1f6\e1f6"}.fa-duotone.fa-peach:after,.fad.fa-peach:after{content:"\e20b\e20b"}.fa-duotone.fa-taxi-bus:after,.fad.fa-taxi-bus:after{content:"\e298\e298"}.fa-duotone.fa-bracket-curly-left:after,.fa-duotone.fa-bracket-curly:after,.fad.fa-bracket-curly-left:after,.fad.fa-bracket-curly:after{content:"\7b\7b"}.fa-duotone.fa-lobster:after,.fad.fa-lobster:after{content:"\e421\e421"}.fa-duotone.fa-cart-flatbed-empty:after,.fa-duotone.fa-dolly-flatbed-empty:after,.fad.fa-cart-flatbed-empty:after,.fad.fa-dolly-flatbed-empty:after{content:"\f476\f476"}.fa-duotone.fa-colon:after,.fad.fa-colon:after{content:"\3a\3a"}.fa-duotone.fa-cart-arrow-down:after,.fad.fa-cart-arrow-down:after{content:"\f218\f218"}.fa-duotone.fa-wand:after,.fad.fa-wand:after{content:"\f72a\f72a"}.fa-duotone.fa-walkie-talkie:after,.fad.fa-walkie-talkie:after{content:"\f8ef\f8ef"}.fa-duotone.fa-file-edit:after,.fa-duotone.fa-file-pen:after,.fad.fa-file-edit:after,.fad.fa-file-pen:after{content:"\f31c\f31c"}.fa-duotone.fa-receipt:after,.fad.fa-receipt:after{content:"\f543\f543"}.fa-duotone.fa-table-picnic:after,.fad.fa-table-picnic:after{content:"\e32d\e32d"}.fa-duotone.fa-pen-square:after,.fa-duotone.fa-pencil-square:after,.fa-duotone.fa-square-pen:after,.fad.fa-pen-square:after,.fad.fa-pencil-square:after,.fad.fa-square-pen:after{content:"\f14b\f14b"}.fa-duotone.fa-circle-microphone-lines:after,.fa-duotone.fa-microphone-circle-alt:after,.fad.fa-circle-microphone-lines:after,.fad.fa-microphone-circle-alt:after{content:"\e117\e117"}.fa-duotone.fa-desktop-slash:after,.fa-duotone.fa-display-slash:after,.fad.fa-desktop-slash:after,.fad.fa-display-slash:after{content:"\e2fa\e2fa"}.fa-duotone.fa-suitcase-rolling:after,.fad.fa-suitcase-rolling:after{content:"\f5c1\f5c1"}.fa-duotone.fa-person-circle-exclamation:after,.fad.fa-person-circle-exclamation:after{content:"\e53f\e53f"}.fa-duotone.fa-transporter-2:after,.fad.fa-transporter-2:after{content:"\e044\e044"}.fa-duotone.fa-hand-receiving:after,.fa-duotone.fa-hands-holding-diamond:after,.fad.fa-hand-receiving:after,.fad.fa-hands-holding-diamond:after{content:"\f47c\f47c"}.fa-duotone.fa-money-bill-simple-wave:after,.fad.fa-money-bill-simple-wave:after{content:"\e1f2\e1f2"}.fa-duotone.fa-chevron-down:after,.fad.fa-chevron-down:after{content:"\f078\f078"}.fa-duotone.fa-battery-5:after,.fa-duotone.fa-battery-full:after,.fa-duotone.fa-battery:after,.fad.fa-battery-5:after,.fad.fa-battery-full:after,.fad.fa-battery:after{content:"\f240\f240"}.fa-duotone.fa-bell-plus:after,.fad.fa-bell-plus:after{content:"\f849\f849"}.fa-duotone.fa-book-arrow-right:after,.fad.fa-book-arrow-right:after{content:"\e0b9\e0b9"}.fa-duotone.fa-hospitals:after,.fad.fa-hospitals:after{content:"\f80e\f80e"}.fa-duotone.fa-club:after,.fad.fa-club:after{content:"\f327\f327"}.fa-duotone.fa-skull-crossbones:after,.fad.fa-skull-crossbones:after{content:"\f714\f714"}.fa-duotone.fa-dewpoint:after,.fa-duotone.fa-droplet-degree:after,.fad.fa-dewpoint:after,.fad.fa-droplet-degree:after{content:"\f748\f748"}.fa-duotone.fa-code-compare:after,.fad.fa-code-compare:after{content:"\e13a\e13a"}.fa-duotone.fa-list-dots:after,.fa-duotone.fa-list-ul:after,.fad.fa-list-dots:after,.fad.fa-list-ul:after{content:"\f0ca\f0ca"}.fa-duotone.fa-hand-holding-magic:after,.fad.fa-hand-holding-magic:after{content:"\f6e5\f6e5"}.fa-duotone.fa-watermelon-slice:after,.fad.fa-watermelon-slice:after{content:"\e337\e337"}.fa-duotone.fa-circle-ellipsis:after,.fad.fa-circle-ellipsis:after{content:"\e10a\e10a"}.fa-duotone.fa-school-lock:after,.fad.fa-school-lock:after{content:"\e56f\e56f"}.fa-duotone.fa-tower-cell:after,.fad.fa-tower-cell:after{content:"\e585\e585"}.fa-duotone.fa-sd-cards:after,.fad.fa-sd-cards:after{content:"\e240\e240"}.fa-duotone.fa-jug-bottle:after,.fad.fa-jug-bottle:after{content:"\e5fb\e5fb"}.fa-duotone.fa-down-long:after,.fa-duotone.fa-long-arrow-alt-down:after,.fad.fa-down-long:after,.fad.fa-long-arrow-alt-down:after{content:"\f309\f309"}.fa-duotone.fa-envelopes:after,.fad.fa-envelopes:after{content:"\e170\e170"}.fa-duotone.fa-phone-office:after,.fad.fa-phone-office:after{content:"\f67d\f67d"}.fa-duotone.fa-ranking-star:after,.fad.fa-ranking-star:after{content:"\e561\e561"}.fa-duotone.fa-chess-king:after,.fad.fa-chess-king:after{content:"\f43f\f43f"}.fa-duotone.fa-nfc-pen:after,.fad.fa-nfc-pen:after{content:"\e1fa\e1fa"}.fa-duotone.fa-person-harassing:after,.fad.fa-person-harassing:after{content:"\e549\e549"}.fa-duotone.fa-magnifying-glass-play:after,.fad.fa-magnifying-glass-play:after{content:"\e660\e660"}.fa-duotone.fa-hat-winter:after,.fad.fa-hat-winter:after{content:"\f7a8\f7a8"}.fa-duotone.fa-brazilian-real-sign:after,.fad.fa-brazilian-real-sign:after{content:"\e46c\e46c"}.fa-duotone.fa-landmark-alt:after,.fa-duotone.fa-landmark-dome:after,.fad.fa-landmark-alt:after,.fad.fa-landmark-dome:after{content:"\f752\f752"}.fa-duotone.fa-bone-break:after,.fad.fa-bone-break:after{content:"\f5d8\f5d8"}.fa-duotone.fa-arrow-up:after,.fad.fa-arrow-up:after{content:"\f062\f062"}.fa-duotone.fa-down-from-dotted-line:after,.fad.fa-down-from-dotted-line:after{content:"\e407\e407"}.fa-duotone.fa-television:after,.fa-duotone.fa-tv-alt:after,.fa-duotone.fa-tv:after,.fad.fa-television:after,.fad.fa-tv-alt:after,.fad.fa-tv:after{content:"\f26c\f26c"}.fa-duotone.fa-border-left:after,.fad.fa-border-left:after{content:"\f84f\f84f"}.fa-duotone.fa-circle-divide:after,.fad.fa-circle-divide:after{content:"\e106\e106"}.fa-duotone.fa-shrimp:after,.fad.fa-shrimp:after{content:"\e448\e448"}.fa-duotone.fa-list-check:after,.fa-duotone.fa-tasks:after,.fad.fa-list-check:after,.fad.fa-tasks:after{content:"\f0ae\f0ae"}.fa-duotone.fa-diagram-subtask:after,.fad.fa-diagram-subtask:after{content:"\e479\e479"}.fa-duotone.fa-jug-detergent:after,.fad.fa-jug-detergent:after{content:"\e519\e519"}.fa-duotone.fa-circle-user:after,.fa-duotone.fa-user-circle:after,.fad.fa-circle-user:after,.fad.fa-user-circle:after{content:"\f2bd\f2bd"}.fa-duotone.fa-square-y:after,.fad.fa-square-y:after{content:"\e287\e287"}.fa-duotone.fa-user-doctor-hair:after,.fad.fa-user-doctor-hair:after{content:"\e458\e458"}.fa-duotone.fa-planet-ringed:after,.fad.fa-planet-ringed:after{content:"\e020\e020"}.fa-duotone.fa-mushroom:after,.fad.fa-mushroom:after{content:"\e425\e425"}.fa-duotone.fa-user-shield:after,.fad.fa-user-shield:after{content:"\f505\f505"}.fa-duotone.fa-megaphone:after,.fad.fa-megaphone:after{content:"\f675\f675"}.fa-duotone.fa-wreath-laurel:after,.fad.fa-wreath-laurel:after{content:"\e5d2\e5d2"}.fa-duotone.fa-circle-exclamation-check:after,.fad.fa-circle-exclamation-check:after{content:"\e10d\e10d"}.fa-duotone.fa-wind:after,.fad.fa-wind:after{content:"\f72e\f72e"}.fa-duotone.fa-box-dollar:after,.fa-duotone.fa-box-usd:after,.fad.fa-box-dollar:after,.fad.fa-box-usd:after{content:"\f4a0\f4a0"}.fa-duotone.fa-car-burst:after,.fa-duotone.fa-car-crash:after,.fad.fa-car-burst:after,.fad.fa-car-crash:after{content:"\f5e1\f5e1"}.fa-duotone.fa-y:after,.fad.fa-y:after{content:"\59\59"}.fa-duotone.fa-user-headset:after,.fad.fa-user-headset:after{content:"\f82d\f82d"}.fa-duotone.fa-arrows-retweet:after,.fa-duotone.fa-retweet-alt:after,.fad.fa-arrows-retweet:after,.fad.fa-retweet-alt:after{content:"\f361\f361"}.fa-duotone.fa-person-snowboarding:after,.fa-duotone.fa-snowboarding:after,.fad.fa-person-snowboarding:after,.fad.fa-snowboarding:after{content:"\f7ce\f7ce"}.fa-duotone.fa-chevron-square-right:after,.fa-duotone.fa-square-chevron-right:after,.fad.fa-chevron-square-right:after,.fad.fa-square-chevron-right:after{content:"\f32b\f32b"}.fa-duotone.fa-lacrosse-stick-ball:after,.fad.fa-lacrosse-stick-ball:after{content:"\e3b6\e3b6"}.fa-duotone.fa-shipping-fast:after,.fa-duotone.fa-truck-fast:after,.fad.fa-shipping-fast:after,.fad.fa-truck-fast:after{content:"\f48b\f48b"}.fa-duotone.fa-user-magnifying-glass:after,.fad.fa-user-magnifying-glass:after{content:"\e5c5\e5c5"}.fa-duotone.fa-star-sharp:after,.fad.fa-star-sharp:after{content:"\e28b\e28b"}.fa-duotone.fa-comment-heart:after,.fad.fa-comment-heart:after{content:"\e5c8\e5c8"}.fa-duotone.fa-circle-1:after,.fad.fa-circle-1:after{content:"\e0ee\e0ee"}.fa-duotone.fa-circle-star:after,.fa-duotone.fa-star-circle:after,.fad.fa-circle-star:after,.fad.fa-star-circle:after{content:"\e123\e123"}.fa-duotone.fa-fish:after,.fad.fa-fish:after{content:"\f578\f578"}.fa-duotone.fa-cloud-fog:after,.fa-duotone.fa-fog:after,.fad.fa-cloud-fog:after,.fad.fa-fog:after{content:"\f74e\f74e"}.fa-duotone.fa-waffle:after,.fad.fa-waffle:after{content:"\e466\e466"}.fa-duotone.fa-music-alt:after,.fa-duotone.fa-music-note:after,.fad.fa-music-alt:after,.fad.fa-music-note:after{content:"\f8cf\f8cf"}.fa-duotone.fa-hexagon-exclamation:after,.fad.fa-hexagon-exclamation:after{content:"\e417\e417"}.fa-duotone.fa-cart-shopping-fast:after,.fad.fa-cart-shopping-fast:after{content:"\e0dc\e0dc"}.fa-duotone.fa-object-union:after,.fad.fa-object-union:after{content:"\e49f\e49f"}.fa-duotone.fa-user-graduate:after,.fad.fa-user-graduate:after{content:"\f501\f501"}.fa-duotone.fa-starfighter:after,.fad.fa-starfighter:after{content:"\e037\e037"}.fa-duotone.fa-adjust:after,.fa-duotone.fa-circle-half-stroke:after,.fad.fa-adjust:after,.fad.fa-circle-half-stroke:after{content:"\f042\f042"}.fa-duotone.fa-arrow-right-long-to-line:after,.fad.fa-arrow-right-long-to-line:after{content:"\e3d5\e3d5"}.fa-duotone.fa-arrow-square-down:after,.fa-duotone.fa-square-arrow-down:after,.fad.fa-arrow-square-down:after,.fad.fa-square-arrow-down:after{content:"\f339\f339"}.fa-duotone.fa-diamond-half-stroke:after,.fad.fa-diamond-half-stroke:after{content:"\e5b8\e5b8"}.fa-duotone.fa-clapperboard:after,.fad.fa-clapperboard:after{content:"\e131\e131"}.fa-duotone.fa-chevron-square-left:after,.fa-duotone.fa-square-chevron-left:after,.fad.fa-chevron-square-left:after,.fad.fa-square-chevron-left:after{content:"\f32a\f32a"}.fa-duotone.fa-phone-intercom:after,.fad.fa-phone-intercom:after{content:"\e434\e434"}.fa-duotone.fa-chain-horizontal:after,.fa-duotone.fa-link-horizontal:after,.fad.fa-chain-horizontal:after,.fad.fa-link-horizontal:after{content:"\e1cb\e1cb"}.fa-duotone.fa-mango:after,.fad.fa-mango:after{content:"\e30f\e30f"}.fa-duotone.fa-music-alt-slash:after,.fa-duotone.fa-music-note-slash:after,.fad.fa-music-alt-slash:after,.fad.fa-music-note-slash:after{content:"\f8d0\f8d0"}.fa-duotone.fa-circle-radiation:after,.fa-duotone.fa-radiation-alt:after,.fad.fa-circle-radiation:after,.fad.fa-radiation-alt:after{content:"\f7ba\f7ba"}.fa-duotone.fa-face-tongue-sweat:after,.fad.fa-face-tongue-sweat:after{content:"\e39e\e39e"}.fa-duotone.fa-globe-stand:after,.fad.fa-globe-stand:after{content:"\f5f6\f5f6"}.fa-duotone.fa-baseball-ball:after,.fa-duotone.fa-baseball:after,.fad.fa-baseball-ball:after,.fad.fa-baseball:after{content:"\f433\f433"}.fa-duotone.fa-circle-p:after,.fad.fa-circle-p:after{content:"\e11a\e11a"}.fa-duotone.fa-award-simple:after,.fad.fa-award-simple:after{content:"\e0ab\e0ab"}.fa-duotone.fa-jet-fighter-up:after,.fad.fa-jet-fighter-up:after{content:"\e518\e518"}.fa-duotone.fa-diagram-project:after,.fa-duotone.fa-project-diagram:after,.fad.fa-diagram-project:after,.fad.fa-project-diagram:after{content:"\f542\f542"}.fa-duotone.fa-pedestal:after,.fad.fa-pedestal:after{content:"\e20d\e20d"}.fa-duotone.fa-chart-pyramid:after,.fad.fa-chart-pyramid:after{content:"\e0e6\e0e6"}.fa-duotone.fa-sidebar:after,.fad.fa-sidebar:after{content:"\e24e\e24e"}.fa-duotone.fa-frosty-head:after,.fa-duotone.fa-snowman-head:after,.fad.fa-frosty-head:after,.fad.fa-snowman-head:after{content:"\f79b\f79b"}.fa-duotone.fa-copy:after,.fad.fa-copy:after{content:"\f0c5\f0c5"}.fa-duotone.fa-burger-glass:after,.fad.fa-burger-glass:after{content:"\e0ce\e0ce"}.fa-duotone.fa-volume-mute:after,.fa-duotone.fa-volume-times:after,.fa-duotone.fa-volume-xmark:after,.fad.fa-volume-mute:after,.fad.fa-volume-times:after,.fad.fa-volume-xmark:after{content:"\f6a9\f6a9"}.fa-duotone.fa-hand-sparkles:after,.fad.fa-hand-sparkles:after{content:"\e05d\e05d"}.fa-duotone.fa-bars-filter:after,.fad.fa-bars-filter:after{content:"\e0ad\e0ad"}.fa-duotone.fa-paintbrush-pencil:after,.fad.fa-paintbrush-pencil:after{content:"\e206\e206"}.fa-duotone.fa-party-bell:after,.fad.fa-party-bell:after{content:"\e31a\e31a"}.fa-duotone.fa-user-vneck-hair:after,.fad.fa-user-vneck-hair:after{content:"\e462\e462"}.fa-duotone.fa-jack-o-lantern:after,.fad.fa-jack-o-lantern:after{content:"\f30e\f30e"}.fa-duotone.fa-grip-horizontal:after,.fa-duotone.fa-grip:after,.fad.fa-grip-horizontal:after,.fad.fa-grip:after{content:"\f58d\f58d"}.fa-duotone.fa-share-from-square:after,.fa-duotone.fa-share-square:after,.fad.fa-share-from-square:after,.fad.fa-share-square:after{content:"\f14d\f14d"}.fa-duotone.fa-keynote:after,.fad.fa-keynote:after{content:"\f66c\f66c"}.fa-duotone.fa-child-combatant:after,.fa-duotone.fa-child-rifle:after,.fad.fa-child-combatant:after,.fad.fa-child-rifle:after{content:"\e4e0\e4e0"}.fa-duotone.fa-gun:after,.fad.fa-gun:after{content:"\e19b\e19b"}.fa-duotone.fa-phone-square:after,.fa-duotone.fa-square-phone:after,.fad.fa-phone-square:after,.fad.fa-square-phone:after{content:"\f098\f098"}.fa-duotone.fa-hat-beach:after,.fad.fa-hat-beach:after{content:"\e606\e606"}.fa-duotone.fa-add:after,.fa-duotone.fa-plus:after,.fad.fa-add:after,.fad.fa-plus:after{content:"\2b\2b"}.fa-duotone.fa-expand:after,.fad.fa-expand:after{content:"\f065\f065"}.fa-duotone.fa-computer:after,.fad.fa-computer:after{content:"\e4e5\e4e5"}.fa-duotone.fa-fort:after,.fad.fa-fort:after{content:"\e486\e486"}.fa-duotone.fa-cloud-check:after,.fad.fa-cloud-check:after{content:"\e35c\e35c"}.fa-duotone.fa-close:after,.fa-duotone.fa-multiply:after,.fa-duotone.fa-remove:after,.fa-duotone.fa-times:after,.fa-duotone.fa-xmark:after,.fad.fa-close:after,.fad.fa-multiply:after,.fad.fa-remove:after,.fad.fa-times:after,.fad.fa-xmark:after{content:"\f00d\f00d"}.fa-duotone.fa-face-smirking:after,.fad.fa-face-smirking:after{content:"\e397\e397"}.fa-duotone.fa-arrows-up-down-left-right:after,.fa-duotone.fa-arrows:after,.fad.fa-arrows-up-down-left-right:after,.fad.fa-arrows:after{content:"\f047\f047"}.fa-duotone.fa-chalkboard-teacher:after,.fa-duotone.fa-chalkboard-user:after,.fad.fa-chalkboard-teacher:after,.fad.fa-chalkboard-user:after{content:"\f51c\f51c"}.fa-duotone.fa-rhombus:after,.fad.fa-rhombus:after{content:"\e23b\e23b"}.fa-duotone.fa-claw-marks:after,.fad.fa-claw-marks:after{content:"\f6c2\f6c2"}.fa-duotone.fa-peso-sign:after,.fad.fa-peso-sign:after{content:"\e222\e222"}.fa-duotone.fa-face-smile-tongue:after,.fad.fa-face-smile-tongue:after{content:"\e394\e394"}.fa-duotone.fa-cart-circle-xmark:after,.fad.fa-cart-circle-xmark:after{content:"\e3f4\e3f4"}.fa-duotone.fa-building-shield:after,.fad.fa-building-shield:after{content:"\e4d8\e4d8"}.fa-duotone.fa-circle-phone-flip:after,.fa-duotone.fa-phone-circle-alt:after,.fad.fa-circle-phone-flip:after,.fad.fa-phone-circle-alt:after{content:"\e11c\e11c"}.fa-duotone.fa-baby:after,.fad.fa-baby:after{content:"\f77c\f77c"}.fa-duotone.fa-users-line:after,.fad.fa-users-line:after{content:"\e592\e592"}.fa-duotone.fa-quote-left-alt:after,.fa-duotone.fa-quote-left:after,.fad.fa-quote-left-alt:after,.fad.fa-quote-left:after{content:"\f10d\f10d"}.fa-duotone.fa-tractor:after,.fad.fa-tractor:after{content:"\f722\f722"}.fa-duotone.fa-down-from-bracket:after,.fad.fa-down-from-bracket:after{content:"\e66b\e66b"}.fa-duotone.fa-key-skeleton:after,.fad.fa-key-skeleton:after{content:"\f6f3\f6f3"}.fa-duotone.fa-trash-arrow-up:after,.fa-duotone.fa-trash-restore:after,.fad.fa-trash-arrow-up:after,.fad.fa-trash-restore:after{content:"\f829\f829"}.fa-duotone.fa-arrow-down-up-lock:after,.fad.fa-arrow-down-up-lock:after{content:"\e4b0\e4b0"}.fa-duotone.fa-arrow-down-to-bracket:after,.fad.fa-arrow-down-to-bracket:after{content:"\e094\e094"}.fa-duotone.fa-lines-leaning:after,.fad.fa-lines-leaning:after{content:"\e51e\e51e"}.fa-duotone.fa-square-q:after,.fad.fa-square-q:after{content:"\e27b\e27b"}.fa-duotone.fa-ruler-combined:after,.fad.fa-ruler-combined:after{content:"\f546\f546"}.fa-duotone.fa-icons-alt:after,.fa-duotone.fa-symbols:after,.fad.fa-icons-alt:after,.fad.fa-symbols:after{content:"\f86e\f86e"}.fa-duotone.fa-copyright:after,.fad.fa-copyright:after{content:"\f1f9\f1f9"}.fa-duotone.fa-flask-gear:after,.fad.fa-flask-gear:after{content:"\e5f1\e5f1"}.fa-duotone.fa-highlighter-line:after,.fad.fa-highlighter-line:after{content:"\e1af\e1af"}.fa-duotone.fa-bracket-left:after,.fa-duotone.fa-bracket-square:after,.fa-duotone.fa-bracket:after,.fad.fa-bracket-left:after,.fad.fa-bracket-square:after,.fad.fa-bracket:after{content:"\5b\5b"}.fa-duotone.fa-island-tree-palm:after,.fa-duotone.fa-island-tropical:after,.fad.fa-island-tree-palm:after,.fad.fa-island-tropical:after{content:"\f811\f811"}.fa-duotone.fa-arrow-from-left:after,.fa-duotone.fa-arrow-right-from-line:after,.fad.fa-arrow-from-left:after,.fad.fa-arrow-right-from-line:after{content:"\f343\f343"}.fa-duotone.fa-h2:after,.fad.fa-h2:after{content:"\f314\f314"}.fa-duotone.fa-equals:after,.fad.fa-equals:after{content:"\3d\3d"}.fa-duotone.fa-cake-slice:after,.fa-duotone.fa-shortcake:after,.fad.fa-cake-slice:after,.fad.fa-shortcake:after{content:"\e3e5\e3e5"}.fa-duotone.fa-building-magnifying-glass:after,.fad.fa-building-magnifying-glass:after{content:"\e61c\e61c"}.fa-duotone.fa-peanut:after,.fad.fa-peanut:after{content:"\e430\e430"}.fa-duotone.fa-wrench-simple:after,.fad.fa-wrench-simple:after{content:"\e2d1\e2d1"}.fa-duotone.fa-blender:after,.fad.fa-blender:after{content:"\f517\f517"}.fa-duotone.fa-teeth:after,.fad.fa-teeth:after{content:"\f62e\f62e"}.fa-duotone.fa-tally-2:after,.fad.fa-tally-2:after{content:"\e295\e295"}.fa-duotone.fa-ils:after,.fa-duotone.fa-shekel-sign:after,.fa-duotone.fa-shekel:after,.fa-duotone.fa-sheqel-sign:after,.fa-duotone.fa-sheqel:after,.fad.fa-ils:after,.fad.fa-shekel-sign:after,.fad.fa-shekel:after,.fad.fa-sheqel-sign:after,.fad.fa-sheqel:after{content:"\f20b\f20b"}.fa-duotone.fa-cars:after,.fad.fa-cars:after{content:"\f85b\f85b"}.fa-duotone.fa-axe-battle:after,.fad.fa-axe-battle:after{content:"\f6b3\f6b3"}.fa-duotone.fa-user-hair-long:after,.fad.fa-user-hair-long:after{content:"\e45b\e45b"}.fa-duotone.fa-map:after,.fad.fa-map:after{content:"\f279\f279"}.fa-duotone.fa-arrow-left-from-arc:after,.fad.fa-arrow-left-from-arc:after{content:"\e615\e615"}.fa-duotone.fa-file-circle-info:after,.fad.fa-file-circle-info:after{content:"\e493\e493"}.fa-duotone.fa-face-disappointed:after,.fad.fa-face-disappointed:after{content:"\e36f\e36f"}.fa-duotone.fa-lasso-sparkles:after,.fad.fa-lasso-sparkles:after{content:"\e1c9\e1c9"}.fa-duotone.fa-clock-eleven:after,.fad.fa-clock-eleven:after{content:"\e347\e347"}.fa-duotone.fa-rocket:after,.fad.fa-rocket:after{content:"\f135\f135"}.fa-duotone.fa-siren-on:after,.fad.fa-siren-on:after{content:"\e02e\e02e"}.fa-duotone.fa-clock-ten:after,.fad.fa-clock-ten:after{content:"\e354\e354"}.fa-duotone.fa-candle-holder:after,.fad.fa-candle-holder:after{content:"\f6bc\f6bc"}.fa-duotone.fa-video-arrow-down-left:after,.fad.fa-video-arrow-down-left:after{content:"\e2c8\e2c8"}.fa-duotone.fa-photo-film:after,.fa-duotone.fa-photo-video:after,.fad.fa-photo-film:after,.fad.fa-photo-video:after{content:"\f87c\f87c"}.fa-duotone.fa-floppy-disk-circle-arrow-right:after,.fa-duotone.fa-save-circle-arrow-right:after,.fad.fa-floppy-disk-circle-arrow-right:after,.fad.fa-save-circle-arrow-right:after{content:"\e180\e180"}.fa-duotone.fa-folder-minus:after,.fad.fa-folder-minus:after{content:"\f65d\f65d"}.fa-duotone.fa-planet-moon:after,.fad.fa-planet-moon:after{content:"\e01f\e01f"}.fa-duotone.fa-face-eyes-xmarks:after,.fad.fa-face-eyes-xmarks:after{content:"\e374\e374"}.fa-duotone.fa-chart-scatter:after,.fad.fa-chart-scatter:after{content:"\f7ee\f7ee"}.fa-duotone.fa-circle-gf:after,.fad.fa-circle-gf:after{content:"\e67f\e67f"}.fa-duotone.fa-display-arrow-down:after,.fad.fa-display-arrow-down:after{content:"\e164\e164"}.fa-duotone.fa-store:after,.fad.fa-store:after{content:"\f54e\f54e"}.fa-duotone.fa-arrow-trend-up:after,.fad.fa-arrow-trend-up:after{content:"\e098\e098"}.fa-duotone.fa-plug-circle-minus:after,.fad.fa-plug-circle-minus:after{content:"\e55e\e55e"}.fa-duotone.fa-olive-branch:after,.fad.fa-olive-branch:after{content:"\e317\e317"}.fa-duotone.fa-angle:after,.fad.fa-angle:after{content:"\e08c\e08c"}.fa-duotone.fa-vacuum-robot:after,.fad.fa-vacuum-robot:after{content:"\e04e\e04e"}.fa-duotone.fa-sign-hanging:after,.fa-duotone.fa-sign:after,.fad.fa-sign-hanging:after,.fad.fa-sign:after{content:"\f4d9\f4d9"}.fa-duotone.fa-square-divide:after,.fad.fa-square-divide:after{content:"\e26a\e26a"}.fa-duotone.fa-folder-check:after,.fad.fa-folder-check:after{content:"\e64e\e64e"}.fa-duotone.fa-signal-stream-slash:after,.fad.fa-signal-stream-slash:after{content:"\e250\e250"}.fa-duotone.fa-bezier-curve:after,.fad.fa-bezier-curve:after{content:"\f55b\f55b"}.fa-duotone.fa-eye-dropper-half:after,.fad.fa-eye-dropper-half:after{content:"\e173\e173"}.fa-duotone.fa-store-lock:after,.fad.fa-store-lock:after{content:"\e4a6\e4a6"}.fa-duotone.fa-bell-slash:after,.fad.fa-bell-slash:after{content:"\f1f6\f1f6"}.fa-duotone.fa-cloud-bolt-sun:after,.fa-duotone.fa-thunderstorm-sun:after,.fad.fa-cloud-bolt-sun:after,.fad.fa-thunderstorm-sun:after{content:"\f76e\f76e"}.fa-duotone.fa-camera-slash:after,.fad.fa-camera-slash:after{content:"\e0d9\e0d9"}.fa-duotone.fa-comment-quote:after,.fad.fa-comment-quote:after{content:"\e14c\e14c"}.fa-duotone.fa-tablet-android:after,.fa-duotone.fa-tablet:after,.fad.fa-tablet-android:after,.fad.fa-tablet:after{content:"\f3fb\f3fb"}.fa-duotone.fa-school-flag:after,.fad.fa-school-flag:after{content:"\e56e\e56e"}.fa-duotone.fa-message-code:after,.fad.fa-message-code:after{content:"\e1df\e1df"}.fa-duotone.fa-glass-half-empty:after,.fa-duotone.fa-glass-half-full:after,.fa-duotone.fa-glass-half:after,.fad.fa-glass-half-empty:after,.fad.fa-glass-half-full:after,.fad.fa-glass-half:after{content:"\e192\e192"}.fa-duotone.fa-fill:after,.fad.fa-fill:after{content:"\f575\f575"}.fa-duotone.fa-comment-alt-minus:after,.fa-duotone.fa-message-minus:after,.fad.fa-comment-alt-minus:after,.fad.fa-message-minus:after{content:"\f4a7\f4a7"}.fa-duotone.fa-angle-up:after,.fad.fa-angle-up:after{content:"\f106\f106"}.fa-duotone.fa-dinosaur:after,.fad.fa-dinosaur:after{content:"\e5fe\e5fe"}.fa-duotone.fa-drumstick-bite:after,.fad.fa-drumstick-bite:after{content:"\f6d7\f6d7"}.fa-duotone.fa-chain-horizontal-slash:after,.fa-duotone.fa-link-horizontal-slash:after,.fad.fa-chain-horizontal-slash:after,.fad.fa-link-horizontal-slash:after{content:"\e1cc\e1cc"}.fa-duotone.fa-holly-berry:after,.fad.fa-holly-berry:after{content:"\f7aa\f7aa"}.fa-duotone.fa-nose:after,.fad.fa-nose:after{content:"\e5bd\e5bd"}.fa-duotone.fa-arrow-left-to-arc:after,.fad.fa-arrow-left-to-arc:after{content:"\e616\e616"}.fa-duotone.fa-chevron-left:after,.fad.fa-chevron-left:after{content:"\f053\f053"}.fa-duotone.fa-bacteria:after,.fad.fa-bacteria:after{content:"\e059\e059"}.fa-duotone.fa-clouds:after,.fad.fa-clouds:after{content:"\f744\f744"}.fa-duotone.fa-money-bill-simple:after,.fad.fa-money-bill-simple:after{content:"\e1f1\e1f1"}.fa-duotone.fa-hand-lizard:after,.fad.fa-hand-lizard:after{content:"\f258\f258"}.fa-duotone.fa-table-pivot:after,.fad.fa-table-pivot:after{content:"\e291\e291"}.fa-duotone.fa-filter-slash:after,.fad.fa-filter-slash:after{content:"\e17d\e17d"}.fa-duotone.fa-trash-can-arrow-turn-left:after,.fa-duotone.fa-trash-can-undo:after,.fa-duotone.fa-trash-undo-alt:after,.fad.fa-trash-can-arrow-turn-left:after,.fad.fa-trash-can-undo:after,.fad.fa-trash-undo-alt:after{content:"\f896\f896"}.fa-duotone.fa-notdef:after,.fad.fa-notdef:after{content:"\e1fe\e1fe"}.fa-duotone.fa-disease:after,.fad.fa-disease:after{content:"\f7fa\f7fa"}.fa-duotone.fa-person-to-door:after,.fad.fa-person-to-door:after{content:"\e433\e433"}.fa-duotone.fa-turntable:after,.fad.fa-turntable:after{content:"\f8e4\f8e4"}.fa-duotone.fa-briefcase-medical:after,.fad.fa-briefcase-medical:after{content:"\f469\f469"}.fa-duotone.fa-genderless:after,.fad.fa-genderless:after{content:"\f22d\f22d"}.fa-duotone.fa-chevron-right:after,.fad.fa-chevron-right:after{content:"\f054\f054"}.fa-duotone.fa-signal-1:after,.fa-duotone.fa-signal-weak:after,.fad.fa-signal-1:after,.fad.fa-signal-weak:after{content:"\f68c\f68c"}.fa-duotone.fa-clock-five:after,.fad.fa-clock-five:after{content:"\e349\e349"}.fa-duotone.fa-retweet:after,.fad.fa-retweet:after{content:"\f079\f079"}.fa-duotone.fa-car-alt:after,.fa-duotone.fa-car-rear:after,.fad.fa-car-alt:after,.fad.fa-car-rear:after{content:"\f5de\f5de"}.fa-duotone.fa-pump-soap:after,.fad.fa-pump-soap:after{content:"\e06b\e06b"}.fa-duotone.fa-computer-classic:after,.fad.fa-computer-classic:after{content:"\f8b1\f8b1"}.fa-duotone.fa-frame:after,.fad.fa-frame:after{content:"\e495\e495"}.fa-duotone.fa-video-slash:after,.fad.fa-video-slash:after{content:"\f4e2\f4e2"}.fa-duotone.fa-battery-2:after,.fa-duotone.fa-battery-quarter:after,.fad.fa-battery-2:after,.fad.fa-battery-quarter:after{content:"\f243\f243"}.fa-duotone.fa-ellipsis-h-alt:after,.fa-duotone.fa-ellipsis-stroke:after,.fad.fa-ellipsis-h-alt:after,.fad.fa-ellipsis-stroke:after{content:"\f39b\f39b"}.fa-duotone.fa-radio:after,.fad.fa-radio:after{content:"\f8d7\f8d7"}.fa-duotone.fa-baby-carriage:after,.fa-duotone.fa-carriage-baby:after,.fad.fa-baby-carriage:after,.fad.fa-carriage-baby:after{content:"\f77d\f77d"}.fa-duotone.fa-face-expressionless:after,.fad.fa-face-expressionless:after{content:"\e373\e373"}.fa-duotone.fa-down-to-dotted-line:after,.fad.fa-down-to-dotted-line:after{content:"\e408\e408"}.fa-duotone.fa-cloud-music:after,.fad.fa-cloud-music:after{content:"\f8ae\f8ae"}.fa-duotone.fa-traffic-light:after,.fad.fa-traffic-light:after{content:"\f637\f637"}.fa-duotone.fa-cloud-minus:after,.fad.fa-cloud-minus:after{content:"\e35d\e35d"}.fa-duotone.fa-thermometer:after,.fad.fa-thermometer:after{content:"\f491\f491"}.fa-duotone.fa-shield-minus:after,.fad.fa-shield-minus:after{content:"\e249\e249"}.fa-duotone.fa-vr-cardboard:after,.fad.fa-vr-cardboard:after{content:"\f729\f729"}.fa-duotone.fa-car-tilt:after,.fad.fa-car-tilt:after{content:"\f5e5\f5e5"}.fa-duotone.fa-gauge-circle-minus:after,.fad.fa-gauge-circle-minus:after{content:"\e497\e497"}.fa-duotone.fa-brightness-low:after,.fad.fa-brightness-low:after{content:"\e0ca\e0ca"}.fa-duotone.fa-hand-middle-finger:after,.fad.fa-hand-middle-finger:after{content:"\f806\f806"}.fa-duotone.fa-percent:after,.fa-duotone.fa-percentage:after,.fad.fa-percent:after,.fad.fa-percentage:after{content:"\25\25"}.fa-duotone.fa-truck-moving:after,.fad.fa-truck-moving:after{content:"\f4df\f4df"}.fa-duotone.fa-glass-water-droplet:after,.fad.fa-glass-water-droplet:after{content:"\e4f5\e4f5"}.fa-duotone.fa-conveyor-belt:after,.fad.fa-conveyor-belt:after{content:"\f46e\f46e"}.fa-duotone.fa-location-check:after,.fa-duotone.fa-map-marker-check:after,.fad.fa-location-check:after,.fad.fa-map-marker-check:after{content:"\f606\f606"}.fa-duotone.fa-coin-vertical:after,.fad.fa-coin-vertical:after{content:"\e3fd\e3fd"}.fa-duotone.fa-display:after,.fad.fa-display:after{content:"\e163\e163"}.fa-duotone.fa-person-sign:after,.fad.fa-person-sign:after{content:"\f757\f757"}.fa-duotone.fa-face-smile:after,.fa-duotone.fa-smile:after,.fad.fa-face-smile:after,.fad.fa-smile:after{content:"\f118\f118"}.fa-duotone.fa-phone-hangup:after,.fad.fa-phone-hangup:after{content:"\e225\e225"}.fa-duotone.fa-signature-slash:after,.fad.fa-signature-slash:after{content:"\e3cb\e3cb"}.fa-duotone.fa-thumb-tack:after,.fa-duotone.fa-thumbtack:after,.fad.fa-thumb-tack:after,.fad.fa-thumbtack:after{content:"\f08d\f08d"}.fa-duotone.fa-wheat-slash:after,.fad.fa-wheat-slash:after{content:"\e339\e339"}.fa-duotone.fa-trophy:after,.fad.fa-trophy:after{content:"\f091\f091"}.fa-duotone.fa-clouds-sun:after,.fad.fa-clouds-sun:after{content:"\f746\f746"}.fa-duotone.fa-person-praying:after,.fa-duotone.fa-pray:after,.fad.fa-person-praying:after,.fad.fa-pray:after{content:"\f683\f683"}.fa-duotone.fa-hammer:after,.fad.fa-hammer:after{content:"\f6e3\f6e3"}.fa-duotone.fa-face-vomit:after,.fad.fa-face-vomit:after{content:"\e3a0\e3a0"}.fa-duotone.fa-speakers:after,.fad.fa-speakers:after{content:"\f8e0\f8e0"}.fa-duotone.fa-teletype-answer:after,.fa-duotone.fa-tty-answer:after,.fad.fa-teletype-answer:after,.fad.fa-tty-answer:after{content:"\e2b9\e2b9"}.fa-duotone.fa-mug-tea-saucer:after,.fad.fa-mug-tea-saucer:after{content:"\e1f5\e1f5"}.fa-duotone.fa-diagram-lean-canvas:after,.fad.fa-diagram-lean-canvas:after{content:"\e156\e156"}.fa-duotone.fa-alt:after,.fad.fa-alt:after{content:"\e08a\e08a"}.fa-duotone.fa-dial-med-high:after,.fa-duotone.fa-dial:after,.fad.fa-dial-med-high:after,.fad.fa-dial:after{content:"\e15b\e15b"}.fa-duotone.fa-hand-peace:after,.fad.fa-hand-peace:after{content:"\f25b\f25b"}.fa-duotone.fa-circle-trash:after,.fa-duotone.fa-trash-circle:after,.fad.fa-circle-trash:after,.fad.fa-trash-circle:after{content:"\e126\e126"}.fa-duotone.fa-rotate:after,.fa-duotone.fa-sync-alt:after,.fad.fa-rotate:after,.fad.fa-sync-alt:after{content:"\f2f1\f2f1"}.fa-duotone.fa-circle-quarters:after,.fad.fa-circle-quarters:after{content:"\e3f8\e3f8"}.fa-duotone.fa-spinner:after,.fad.fa-spinner:after{content:"\f110\f110"}.fa-duotone.fa-tower-control:after,.fad.fa-tower-control:after{content:"\e2a2\e2a2"}.fa-duotone.fa-arrow-up-triangle-square:after,.fa-duotone.fa-sort-shapes-up:after,.fad.fa-arrow-up-triangle-square:after,.fad.fa-sort-shapes-up:after{content:"\f88a\f88a"}.fa-duotone.fa-whale:after,.fad.fa-whale:after{content:"\f72c\f72c"}.fa-duotone.fa-robot:after,.fad.fa-robot:after{content:"\f544\f544"}.fa-duotone.fa-peace:after,.fad.fa-peace:after{content:"\f67c\f67c"}.fa-duotone.fa-party-horn:after,.fad.fa-party-horn:after{content:"\e31b\e31b"}.fa-duotone.fa-cogs:after,.fa-duotone.fa-gears:after,.fad.fa-cogs:after,.fad.fa-gears:after{content:"\f085\f085"}.fa-duotone.fa-sun-alt:after,.fa-duotone.fa-sun-bright:after,.fad.fa-sun-alt:after,.fad.fa-sun-bright:after{content:"\e28f\e28f"}.fa-duotone.fa-warehouse:after,.fad.fa-warehouse:after{content:"\f494\f494"}.fa-duotone.fa-conveyor-belt-arm:after,.fad.fa-conveyor-belt-arm:after{content:"\e5f8\e5f8"}.fa-duotone.fa-lock-keyhole-open:after,.fa-duotone.fa-lock-open-alt:after,.fad.fa-lock-keyhole-open:after,.fad.fa-lock-open-alt:after{content:"\f3c2\f3c2"}.fa-duotone.fa-box-fragile:after,.fa-duotone.fa-square-fragile:after,.fa-duotone.fa-square-wine-glass-crack:after,.fad.fa-box-fragile:after,.fad.fa-square-fragile:after,.fad.fa-square-wine-glass-crack:after{content:"\f49b\f49b"}.fa-duotone.fa-arrow-up-right-dots:after,.fad.fa-arrow-up-right-dots:after{content:"\e4b7\e4b7"}.fa-duotone.fa-square-n:after,.fad.fa-square-n:after{content:"\e277\e277"}.fa-duotone.fa-splotch:after,.fad.fa-splotch:after{content:"\f5bc\f5bc"}.fa-duotone.fa-face-grin-hearts:after,.fa-duotone.fa-grin-hearts:after,.fad.fa-face-grin-hearts:after,.fad.fa-grin-hearts:after{content:"\f584\f584"}.fa-duotone.fa-meter:after,.fad.fa-meter:after{content:"\e1e8\e1e8"}.fa-duotone.fa-mandolin:after,.fad.fa-mandolin:after{content:"\f6f9\f6f9"}.fa-duotone.fa-dice-four:after,.fad.fa-dice-four:after{content:"\f524\f524"}.fa-duotone.fa-sim-card:after,.fad.fa-sim-card:after{content:"\f7c4\f7c4"}.fa-duotone.fa-transgender-alt:after,.fa-duotone.fa-transgender:after,.fad.fa-transgender-alt:after,.fad.fa-transgender:after{content:"\f225\f225"}.fa-duotone.fa-mercury:after,.fad.fa-mercury:after{content:"\f223\f223"}.fa-duotone.fa-up-from-bracket:after,.fad.fa-up-from-bracket:after{content:"\e590\e590"}.fa-duotone.fa-knife-kitchen:after,.fad.fa-knife-kitchen:after{content:"\f6f5\f6f5"}.fa-duotone.fa-border-right:after,.fad.fa-border-right:after{content:"\f852\f852"}.fa-duotone.fa-arrow-turn-down:after,.fa-duotone.fa-level-down:after,.fad.fa-arrow-turn-down:after,.fad.fa-level-down:after{content:"\f149\f149"}.fa-duotone.fa-spade:after,.fad.fa-spade:after{content:"\f2f4\f2f4"}.fa-duotone.fa-card-spade:after,.fad.fa-card-spade:after{content:"\e3ec\e3ec"}.fa-duotone.fa-line-columns:after,.fad.fa-line-columns:after{content:"\f870\f870"}.fa-duotone.fa-ant:after,.fad.fa-ant:after{content:"\e680\e680"}.fa-duotone.fa-arrow-right-to-line:after,.fa-duotone.fa-arrow-to-right:after,.fad.fa-arrow-right-to-line:after,.fad.fa-arrow-to-right:after{content:"\f340\f340"}.fa-duotone.fa-person-falling-burst:after,.fad.fa-person-falling-burst:after{content:"\e547\e547"}.fa-duotone.fa-flag-pennant:after,.fa-duotone.fa-pennant:after,.fad.fa-flag-pennant:after,.fad.fa-pennant:after{content:"\f456\f456"}.fa-duotone.fa-conveyor-belt-empty:after,.fad.fa-conveyor-belt-empty:after{content:"\e150\e150"}.fa-duotone.fa-user-group-simple:after,.fad.fa-user-group-simple:after{content:"\e603\e603"}.fa-duotone.fa-award:after,.fad.fa-award:after{content:"\f559\f559"}.fa-duotone.fa-ticket-alt:after,.fa-duotone.fa-ticket-simple:after,.fad.fa-ticket-alt:after,.fad.fa-ticket-simple:after{content:"\f3ff\f3ff"}.fa-duotone.fa-building:after,.fad.fa-building:after{content:"\f1ad\f1ad"}.fa-duotone.fa-angle-double-left:after,.fa-duotone.fa-angles-left:after,.fad.fa-angle-double-left:after,.fad.fa-angles-left:after{content:"\f100\f100"}.fa-duotone.fa-camcorder:after,.fa-duotone.fa-video-handheld:after,.fad.fa-camcorder:after,.fad.fa-video-handheld:after{content:"\f8a8\f8a8"}.fa-duotone.fa-pancakes:after,.fad.fa-pancakes:after{content:"\e42d\e42d"}.fa-duotone.fa-album-circle-user:after,.fad.fa-album-circle-user:after{content:"\e48d\e48d"}.fa-duotone.fa-subtitles-slash:after,.fad.fa-subtitles-slash:after{content:"\e610\e610"}.fa-duotone.fa-qrcode:after,.fad.fa-qrcode:after{content:"\f029\f029"}.fa-duotone.fa-dice-d10:after,.fad.fa-dice-d10:after{content:"\f6cd\f6cd"}.fa-duotone.fa-fireplace:after,.fad.fa-fireplace:after{content:"\f79a\f79a"}.fa-duotone.fa-browser:after,.fad.fa-browser:after{content:"\f37e\f37e"}.fa-duotone.fa-pen-paintbrush:after,.fa-duotone.fa-pencil-paintbrush:after,.fad.fa-pen-paintbrush:after,.fad.fa-pencil-paintbrush:after{content:"\f618\f618"}.fa-duotone.fa-fish-cooked:after,.fad.fa-fish-cooked:after{content:"\f7fe\f7fe"}.fa-duotone.fa-chair-office:after,.fad.fa-chair-office:after{content:"\f6c1\f6c1"}.fa-duotone.fa-magnifying-glass-music:after,.fad.fa-magnifying-glass-music:after{content:"\e65f\e65f"}.fa-duotone.fa-nesting-dolls:after,.fad.fa-nesting-dolls:after{content:"\e3ba\e3ba"}.fa-duotone.fa-clock-rotate-left:after,.fa-duotone.fa-history:after,.fad.fa-clock-rotate-left:after,.fad.fa-history:after{content:"\f1da\f1da"}.fa-duotone.fa-trumpet:after,.fad.fa-trumpet:after{content:"\f8e3\f8e3"}.fa-duotone.fa-face-grin-beam-sweat:after,.fa-duotone.fa-grin-beam-sweat:after,.fad.fa-face-grin-beam-sweat:after,.fad.fa-grin-beam-sweat:after{content:"\f583\f583"}.fa-duotone.fa-fire-smoke:after,.fad.fa-fire-smoke:after{content:"\f74b\f74b"}.fa-duotone.fa-phone-missed:after,.fad.fa-phone-missed:after{content:"\e226\e226"}.fa-duotone.fa-arrow-right-from-file:after,.fa-duotone.fa-file-export:after,.fad.fa-arrow-right-from-file:after,.fad.fa-file-export:after{content:"\f56e\f56e"}.fa-duotone.fa-shield-blank:after,.fa-duotone.fa-shield:after,.fad.fa-shield-blank:after,.fad.fa-shield:after{content:"\f132\f132"}.fa-duotone.fa-arrow-up-short-wide:after,.fa-duotone.fa-sort-amount-up-alt:after,.fad.fa-arrow-up-short-wide:after,.fad.fa-sort-amount-up-alt:after{content:"\f885\f885"}.fa-duotone.fa-arrows-repeat-1:after,.fa-duotone.fa-repeat-1-alt:after,.fad.fa-arrows-repeat-1:after,.fad.fa-repeat-1-alt:after{content:"\f366\f366"}.fa-duotone.fa-gun-slash:after,.fad.fa-gun-slash:after{content:"\e19c\e19c"}.fa-duotone.fa-avocado:after,.fad.fa-avocado:after{content:"\e0aa\e0aa"}.fa-duotone.fa-binary:after,.fad.fa-binary:after{content:"\e33b\e33b"}.fa-duotone.fa-glasses-alt:after,.fa-duotone.fa-glasses-round:after,.fad.fa-glasses-alt:after,.fad.fa-glasses-round:after{content:"\f5f5\f5f5"}.fa-duotone.fa-phone-plus:after,.fad.fa-phone-plus:after{content:"\f4d2\f4d2"}.fa-duotone.fa-ditto:after,.fad.fa-ditto:after{content:"\22\22"}.fa-duotone.fa-person-seat:after,.fad.fa-person-seat:after{content:"\e21e\e21e"}.fa-duotone.fa-house-medical:after,.fad.fa-house-medical:after{content:"\e3b2\e3b2"}.fa-duotone.fa-golf-ball-tee:after,.fa-duotone.fa-golf-ball:after,.fad.fa-golf-ball-tee:after,.fad.fa-golf-ball:after{content:"\f450\f450"}.fa-duotone.fa-chevron-circle-left:after,.fa-duotone.fa-circle-chevron-left:after,.fad.fa-chevron-circle-left:after,.fad.fa-circle-chevron-left:after{content:"\f137\f137"}.fa-duotone.fa-house-chimney-window:after,.fad.fa-house-chimney-window:after{content:"\e00d\e00d"}.fa-duotone.fa-scythe:after,.fad.fa-scythe:after{content:"\f710\f710"}.fa-duotone.fa-pen-nib:after,.fad.fa-pen-nib:after{content:"\f5ad\f5ad"}.fa-duotone.fa-ban-parking:after,.fa-duotone.fa-parking-circle-slash:after,.fad.fa-ban-parking:after,.fad.fa-parking-circle-slash:after{content:"\f616\f616"}.fa-duotone.fa-tent-arrow-turn-left:after,.fad.fa-tent-arrow-turn-left:after{content:"\e580\e580"}.fa-duotone.fa-face-diagonal-mouth:after,.fad.fa-face-diagonal-mouth:after{content:"\e47e\e47e"}.fa-duotone.fa-diagram-cells:after,.fad.fa-diagram-cells:after{content:"\e475\e475"}.fa-duotone.fa-cricket-bat-ball:after,.fa-duotone.fa-cricket:after,.fad.fa-cricket-bat-ball:after,.fad.fa-cricket:after{content:"\f449\f449"}.fa-duotone.fa-tents:after,.fad.fa-tents:after{content:"\e582\e582"}.fa-duotone.fa-magic:after,.fa-duotone.fa-wand-magic:after,.fad.fa-magic:after,.fad.fa-wand-magic:after{content:"\f0d0\f0d0"}.fa-duotone.fa-dog:after,.fad.fa-dog:after{content:"\f6d3\f6d3"}.fa-duotone.fa-pen-line:after,.fad.fa-pen-line:after{content:"\e212\e212"}.fa-duotone.fa-atom-alt:after,.fa-duotone.fa-atom-simple:after,.fad.fa-atom-alt:after,.fad.fa-atom-simple:after{content:"\f5d3\f5d3"}.fa-duotone.fa-ampersand:after,.fad.fa-ampersand:after{content:"\26\26"}.fa-duotone.fa-carrot:after,.fad.fa-carrot:after{content:"\f787\f787"}.fa-duotone.fa-arrow-from-bottom:after,.fa-duotone.fa-arrow-up-from-line:after,.fad.fa-arrow-from-bottom:after,.fad.fa-arrow-up-from-line:after{content:"\f342\f342"}.fa-duotone.fa-moon:after,.fad.fa-moon:after{content:"\f186\f186"}.fa-duotone.fa-pen-slash:after,.fad.fa-pen-slash:after{content:"\e213\e213"}.fa-duotone.fa-wine-glass-alt:after,.fa-duotone.fa-wine-glass-empty:after,.fad.fa-wine-glass-alt:after,.fad.fa-wine-glass-empty:after{content:"\f5ce\f5ce"}.fa-duotone.fa-square-star:after,.fad.fa-square-star:after{content:"\e27f\e27f"}.fa-duotone.fa-cheese:after,.fad.fa-cheese:after{content:"\f7ef\f7ef"}.fa-duotone.fa-send-backward:after,.fad.fa-send-backward:after{content:"\f87f\f87f"}.fa-duotone.fa-yin-yang:after,.fad.fa-yin-yang:after{content:"\f6ad\f6ad"}.fa-duotone.fa-music:after,.fad.fa-music:after{content:"\f001\f001"}.fa-duotone.fa-compass-slash:after,.fad.fa-compass-slash:after{content:"\f5e9\f5e9"}.fa-duotone.fa-clock-one:after,.fad.fa-clock-one:after{content:"\e34e\e34e"}.fa-duotone.fa-file-music:after,.fad.fa-file-music:after{content:"\f8b6\f8b6"}.fa-duotone.fa-code-commit:after,.fad.fa-code-commit:after{content:"\f386\f386"}.fa-duotone.fa-temperature-low:after,.fad.fa-temperature-low:after{content:"\f76b\f76b"}.fa-duotone.fa-biking:after,.fa-duotone.fa-person-biking:after,.fad.fa-biking:after,.fad.fa-person-biking:after{content:"\f84a\f84a"}.fa-duotone.fa-display-chart-up-circle-currency:after,.fad.fa-display-chart-up-circle-currency:after{content:"\e5e5\e5e5"}.fa-duotone.fa-skeleton:after,.fad.fa-skeleton:after{content:"\f620\f620"}.fa-duotone.fa-circle-g:after,.fad.fa-circle-g:after{content:"\e10f\e10f"}.fa-duotone.fa-circle-arrow-up-left:after,.fad.fa-circle-arrow-up-left:after{content:"\e0fb\e0fb"}.fa-duotone.fa-coin-blank:after,.fad.fa-coin-blank:after{content:"\e3fb\e3fb"}.fa-duotone.fa-broom:after,.fad.fa-broom:after{content:"\f51a\f51a"}.fa-duotone.fa-vacuum:after,.fad.fa-vacuum:after{content:"\e04d\e04d"}.fa-duotone.fa-shield-heart:after,.fad.fa-shield-heart:after{content:"\e574\e574"}.fa-duotone.fa-card-heart:after,.fad.fa-card-heart:after{content:"\e3eb\e3eb"}.fa-duotone.fa-lightbulb-cfl-on:after,.fad.fa-lightbulb-cfl-on:after{content:"\e5a7\e5a7"}.fa-duotone.fa-melon:after,.fad.fa-melon:after{content:"\e310\e310"}.fa-duotone.fa-gopuram:after,.fad.fa-gopuram:after{content:"\f664\f664"}.fa-duotone.fa-earth-oceania:after,.fa-duotone.fa-globe-oceania:after,.fad.fa-earth-oceania:after,.fad.fa-globe-oceania:after{content:"\e47b\e47b"}.fa-duotone.fa-container-storage:after,.fad.fa-container-storage:after{content:"\f4b7\f4b7"}.fa-duotone.fa-face-pouting:after,.fad.fa-face-pouting:after{content:"\e387\e387"}.fa-duotone.fa-square-xmark:after,.fa-duotone.fa-times-square:after,.fa-duotone.fa-xmark-square:after,.fad.fa-square-xmark:after,.fad.fa-times-square:after,.fad.fa-xmark-square:after{content:"\f2d3\f2d3"}.fa-duotone.fa-exploding-head:after,.fa-duotone.fa-face-explode:after,.fad.fa-exploding-head:after,.fad.fa-face-explode:after{content:"\e2fe\e2fe"}.fa-duotone.fa-hashtag:after,.fad.fa-hashtag:after{content:"\23\23"}.fa-duotone.fa-expand-alt:after,.fa-duotone.fa-up-right-and-down-left-from-center:after,.fad.fa-expand-alt:after,.fad.fa-up-right-and-down-left-from-center:after{content:"\f424\f424"}.fa-duotone.fa-oil-can:after,.fad.fa-oil-can:after{content:"\f613\f613"}.fa-duotone.fa-t:after,.fad.fa-t:after{content:"\54\54"}.fa-duotone.fa-transformer-bolt:after,.fad.fa-transformer-bolt:after{content:"\e2a4\e2a4"}.fa-duotone.fa-hippo:after,.fad.fa-hippo:after{content:"\f6ed\f6ed"}.fa-duotone.fa-chart-column:after,.fad.fa-chart-column:after{content:"\e0e3\e0e3"}.fa-duotone.fa-cassette-vhs:after,.fa-duotone.fa-vhs:after,.fad.fa-cassette-vhs:after,.fad.fa-vhs:after{content:"\f8ec\f8ec"}.fa-duotone.fa-infinity:after,.fad.fa-infinity:after{content:"\f534\f534"}.fa-duotone.fa-vial-circle-check:after,.fad.fa-vial-circle-check:after{content:"\e596\e596"}.fa-duotone.fa-chimney:after,.fad.fa-chimney:after{content:"\f78b\f78b"}.fa-duotone.fa-object-intersect:after,.fad.fa-object-intersect:after{content:"\e49d\e49d"}.fa-duotone.fa-person-arrow-down-to-line:after,.fad.fa-person-arrow-down-to-line:after{content:"\e538\e538"}.fa-duotone.fa-voicemail:after,.fad.fa-voicemail:after{content:"\f897\f897"}.fa-duotone.fa-block-brick:after,.fa-duotone.fa-wall-brick:after,.fad.fa-block-brick:after,.fad.fa-wall-brick:after{content:"\e3db\e3db"}.fa-duotone.fa-fan:after,.fad.fa-fan:after{content:"\f863\f863"}.fa-duotone.fa-bags-shopping:after,.fad.fa-bags-shopping:after{content:"\f847\f847"}.fa-duotone.fa-paragraph-left:after,.fa-duotone.fa-paragraph-rtl:after,.fad.fa-paragraph-left:after,.fad.fa-paragraph-rtl:after{content:"\f878\f878"}.fa-duotone.fa-person-walking-luggage:after,.fad.fa-person-walking-luggage:after{content:"\e554\e554"}.fa-duotone.fa-caravan-alt:after,.fa-duotone.fa-caravan-simple:after,.fad.fa-caravan-alt:after,.fad.fa-caravan-simple:after{content:"\e000\e000"}.fa-duotone.fa-turtle:after,.fad.fa-turtle:after{content:"\f726\f726"}.fa-duotone.fa-pencil-mechanical:after,.fad.fa-pencil-mechanical:after{content:"\e5ca\e5ca"}.fa-duotone.fa-arrows-alt-v:after,.fa-duotone.fa-up-down:after,.fad.fa-arrows-alt-v:after,.fad.fa-up-down:after{content:"\f338\f338"}.fa-duotone.fa-cloud-moon-rain:after,.fad.fa-cloud-moon-rain:after{content:"\f73c\f73c"}.fa-duotone.fa-booth-curtain:after,.fad.fa-booth-curtain:after{content:"\f734\f734"}.fa-duotone.fa-calendar:after,.fad.fa-calendar:after{content:"\f133\f133"}.fa-duotone.fa-box-heart:after,.fad.fa-box-heart:after{content:"\f49d\f49d"}.fa-duotone.fa-trailer:after,.fad.fa-trailer:after{content:"\e041\e041"}.fa-duotone.fa-user-doctor-message:after,.fa-duotone.fa-user-md-chat:after,.fad.fa-user-doctor-message:after,.fad.fa-user-md-chat:after{content:"\f82e\f82e"}.fa-duotone.fa-bahai:after,.fa-duotone.fa-haykal:after,.fad.fa-bahai:after,.fad.fa-haykal:after{content:"\f666\f666"}.fa-duotone.fa-lighthouse:after,.fad.fa-lighthouse:after{content:"\e612\e612"}.fa-duotone.fa-amp-guitar:after,.fad.fa-amp-guitar:after{content:"\f8a1\f8a1"}.fa-duotone.fa-sd-card:after,.fad.fa-sd-card:after{content:"\f7c2\f7c2"}.fa-duotone.fa-volume-slash:after,.fad.fa-volume-slash:after{content:"\f2e2\f2e2"}.fa-duotone.fa-border-bottom:after,.fad.fa-border-bottom:after{content:"\f84d\f84d"}.fa-duotone.fa-wifi-1:after,.fa-duotone.fa-wifi-weak:after,.fad.fa-wifi-1:after,.fad.fa-wifi-weak:after{content:"\f6aa\f6aa"}.fa-duotone.fa-dragon:after,.fad.fa-dragon:after{content:"\f6d5\f6d5"}.fa-duotone.fa-shoe-prints:after,.fad.fa-shoe-prints:after{content:"\f54b\f54b"}.fa-duotone.fa-circle-plus:after,.fa-duotone.fa-plus-circle:after,.fad.fa-circle-plus:after,.fad.fa-plus-circle:after{content:"\f055\f055"}.fa-duotone.fa-face-grin-tongue-wink:after,.fa-duotone.fa-grin-tongue-wink:after,.fad.fa-face-grin-tongue-wink:after,.fad.fa-grin-tongue-wink:after{content:"\f58b\f58b"}.fa-duotone.fa-hand-holding:after,.fad.fa-hand-holding:after{content:"\f4bd\f4bd"}.fa-duotone.fa-plug-circle-exclamation:after,.fad.fa-plug-circle-exclamation:after{content:"\e55d\e55d"}.fa-duotone.fa-chain-broken:after,.fa-duotone.fa-chain-slash:after,.fa-duotone.fa-link-slash:after,.fa-duotone.fa-unlink:after,.fad.fa-chain-broken:after,.fad.fa-chain-slash:after,.fad.fa-link-slash:after,.fad.fa-unlink:after{content:"\f127\f127"}.fa-duotone.fa-clone:after,.fad.fa-clone:after{content:"\f24d\f24d"}.fa-duotone.fa-person-walking-arrow-loop-left:after,.fad.fa-person-walking-arrow-loop-left:after{content:"\e551\e551"}.fa-duotone.fa-arrow-up-z-a:after,.fa-duotone.fa-sort-alpha-up-alt:after,.fad.fa-arrow-up-z-a:after,.fad.fa-sort-alpha-up-alt:after{content:"\f882\f882"}.fa-duotone.fa-fire-alt:after,.fa-duotone.fa-fire-flame-curved:after,.fad.fa-fire-alt:after,.fad.fa-fire-flame-curved:after{content:"\f7e4\f7e4"}.fa-duotone.fa-tornado:after,.fad.fa-tornado:after{content:"\f76f\f76f"}.fa-duotone.fa-file-circle-plus:after,.fad.fa-file-circle-plus:after{content:"\e494\e494"}.fa-duotone.fa-delete-right:after,.fad.fa-delete-right:after{content:"\e154\e154"}.fa-duotone.fa-book-quran:after,.fa-duotone.fa-quran:after,.fad.fa-book-quran:after,.fad.fa-quran:after{content:"\f687\f687"}.fa-duotone.fa-circle-quarter:after,.fad.fa-circle-quarter:after{content:"\e11f\e11f"}.fa-duotone.fa-anchor:after,.fad.fa-anchor:after{content:"\f13d\f13d"}.fa-duotone.fa-border-all:after,.fad.fa-border-all:after{content:"\f84c\f84c"}.fa-duotone.fa-function:after,.fad.fa-function:after{content:"\f661\f661"}.fa-duotone.fa-angry:after,.fa-duotone.fa-face-angry:after,.fad.fa-angry:after,.fad.fa-face-angry:after{content:"\f556\f556"}.fa-duotone.fa-people-simple:after,.fad.fa-people-simple:after{content:"\e21b\e21b"}.fa-duotone.fa-cookie-bite:after,.fad.fa-cookie-bite:after{content:"\f564\f564"}.fa-duotone.fa-arrow-trend-down:after,.fad.fa-arrow-trend-down:after{content:"\e097\e097"}.fa-duotone.fa-feed:after,.fa-duotone.fa-rss:after,.fad.fa-feed:after,.fad.fa-rss:after{content:"\f09e\f09e"}.fa-duotone.fa-face-monocle:after,.fad.fa-face-monocle:after{content:"\e380\e380"}.fa-duotone.fa-draw-polygon:after,.fad.fa-draw-polygon:after{content:"\f5ee\f5ee"}.fa-duotone.fa-balance-scale:after,.fa-duotone.fa-scale-balanced:after,.fad.fa-balance-scale:after,.fad.fa-scale-balanced:after{content:"\f24e\f24e"}.fa-duotone.fa-calendar-lines:after,.fa-duotone.fa-calendar-note:after,.fad.fa-calendar-lines:after,.fad.fa-calendar-note:after{content:"\e0d5\e0d5"}.fa-duotone.fa-arrow-down-big-small:after,.fa-duotone.fa-sort-size-down:after,.fad.fa-arrow-down-big-small:after,.fad.fa-sort-size-down:after{content:"\f88c\f88c"}.fa-duotone.fa-gauge-simple-high:after,.fa-duotone.fa-tachometer-fast:after,.fa-duotone.fa-tachometer:after,.fad.fa-gauge-simple-high:after,.fad.fa-tachometer-fast:after,.fad.fa-tachometer:after{content:"\f62a\f62a"}.fa-duotone.fa-do-not-enter:after,.fad.fa-do-not-enter:after{content:"\f5ec\f5ec"}.fa-duotone.fa-shower:after,.fad.fa-shower:after{content:"\f2cc\f2cc"}.fa-duotone.fa-dice-d8:after,.fad.fa-dice-d8:after{content:"\f6d2\f6d2"}.fa-duotone.fa-desktop-alt:after,.fa-duotone.fa-desktop:after,.fad.fa-desktop-alt:after,.fad.fa-desktop:after{content:"\f390\f390"}.fa-duotone.fa-m:after,.fad.fa-m:after{content:"\4d\4d"}.fa-duotone.fa-spinner-scale:after,.fad.fa-spinner-scale:after{content:"\e62a\e62a"}.fa-duotone.fa-grip-dots-vertical:after,.fad.fa-grip-dots-vertical:after{content:"\e411\e411"}.fa-duotone.fa-face-viewfinder:after,.fad.fa-face-viewfinder:after{content:"\e2ff\e2ff"}.fa-duotone.fa-creemee:after,.fa-duotone.fa-soft-serve:after,.fad.fa-creemee:after,.fad.fa-soft-serve:after{content:"\e400\e400"}.fa-duotone.fa-h5:after,.fad.fa-h5:after{content:"\e412\e412"}.fa-duotone.fa-hand-back-point-down:after,.fad.fa-hand-back-point-down:after{content:"\e19e\e19e"}.fa-duotone.fa-table-list:after,.fa-duotone.fa-th-list:after,.fad.fa-table-list:after,.fad.fa-th-list:after{content:"\f00b\f00b"}.fa-duotone.fa-basket-shopping-minus:after,.fad.fa-basket-shopping-minus:after{content:"\e652\e652"}.fa-duotone.fa-comment-sms:after,.fa-duotone.fa-sms:after,.fad.fa-comment-sms:after,.fad.fa-sms:after{content:"\f7cd\f7cd"}.fa-duotone.fa-rectangle-landscape:after,.fa-duotone.fa-rectangle:after,.fad.fa-rectangle-landscape:after,.fad.fa-rectangle:after{content:"\f2fa\f2fa"}.fa-duotone.fa-clipboard-list-check:after,.fad.fa-clipboard-list-check:after{content:"\f737\f737"}.fa-duotone.fa-turkey:after,.fad.fa-turkey:after{content:"\f725\f725"}.fa-duotone.fa-book:after,.fad.fa-book:after{content:"\f02d\f02d"}.fa-duotone.fa-user-plus:after,.fad.fa-user-plus:after{content:"\f234\f234"}.fa-duotone.fa-ice-skate:after,.fad.fa-ice-skate:after{content:"\f7ac\f7ac"}.fa-duotone.fa-check:after,.fad.fa-check:after{content:"\f00c\f00c"}.fa-duotone.fa-battery-4:after,.fa-duotone.fa-battery-three-quarters:after,.fad.fa-battery-4:after,.fad.fa-battery-three-quarters:after{content:"\f241\f241"}.fa-duotone.fa-tomato:after,.fad.fa-tomato:after{content:"\e330\e330"}.fa-duotone.fa-sword-laser:after,.fad.fa-sword-laser:after{content:"\e03b\e03b"}.fa-duotone.fa-house-circle-check:after,.fad.fa-house-circle-check:after{content:"\e509\e509"}.fa-duotone.fa-buildings:after,.fad.fa-buildings:after{content:"\e0cc\e0cc"}.fa-duotone.fa-angle-left:after,.fad.fa-angle-left:after{content:"\f104\f104"}.fa-duotone.fa-cart-flatbed-boxes:after,.fa-duotone.fa-dolly-flatbed-alt:after,.fad.fa-cart-flatbed-boxes:after,.fad.fa-dolly-flatbed-alt:after{content:"\f475\f475"}.fa-duotone.fa-diagram-successor:after,.fad.fa-diagram-successor:after{content:"\e47a\e47a"}.fa-duotone.fa-truck-arrow-right:after,.fad.fa-truck-arrow-right:after{content:"\e58b\e58b"}.fa-duotone.fa-square-w:after,.fad.fa-square-w:after{content:"\e285\e285"}.fa-duotone.fa-arrows-split-up-and-left:after,.fad.fa-arrows-split-up-and-left:after{content:"\e4bc\e4bc"}.fa-duotone.fa-lamp:after,.fad.fa-lamp:after{content:"\f4ca\f4ca"}.fa-duotone.fa-airplay:after,.fad.fa-airplay:after{content:"\e089\e089"}.fa-duotone.fa-fist-raised:after,.fa-duotone.fa-hand-fist:after,.fad.fa-fist-raised:after,.fad.fa-hand-fist:after{content:"\f6de\f6de"}.fa-duotone.fa-shield-quartered:after,.fad.fa-shield-quartered:after{content:"\e575\e575"}.fa-duotone.fa-slash-forward:after,.fad.fa-slash-forward:after{content:"\2f\2f"}.fa-duotone.fa-location-pen:after,.fa-duotone.fa-map-marker-edit:after,.fad.fa-location-pen:after,.fad.fa-map-marker-edit:after{content:"\f607\f607"}.fa-duotone.fa-cloud-moon:after,.fad.fa-cloud-moon:after{content:"\f6c3\f6c3"}.fa-duotone.fa-pot-food:after,.fad.fa-pot-food:after{content:"\e43f\e43f"}.fa-duotone.fa-briefcase:after,.fad.fa-briefcase:after{content:"\f0b1\f0b1"}.fa-duotone.fa-person-falling:after,.fad.fa-person-falling:after{content:"\e546\e546"}.fa-duotone.fa-image-portrait:after,.fa-duotone.fa-portrait:after,.fad.fa-image-portrait:after,.fad.fa-portrait:after{content:"\f3e0\f3e0"}.fa-duotone.fa-user-tag:after,.fad.fa-user-tag:after{content:"\f507\f507"}.fa-duotone.fa-rug:after,.fad.fa-rug:after{content:"\e569\e569"}.fa-duotone.fa-print-slash:after,.fad.fa-print-slash:after{content:"\f686\f686"}.fa-duotone.fa-earth-europe:after,.fa-duotone.fa-globe-europe:after,.fad.fa-earth-europe:after,.fad.fa-globe-europe:after{content:"\f7a2\f7a2"}.fa-duotone.fa-cart-flatbed-suitcase:after,.fa-duotone.fa-luggage-cart:after,.fad.fa-cart-flatbed-suitcase:after,.fad.fa-luggage-cart:after{content:"\f59d\f59d"}.fa-duotone.fa-hand-back-point-ribbon:after,.fad.fa-hand-back-point-ribbon:after{content:"\e1a0\e1a0"}.fa-duotone.fa-rectangle-times:after,.fa-duotone.fa-rectangle-xmark:after,.fa-duotone.fa-times-rectangle:after,.fa-duotone.fa-window-close:after,.fad.fa-rectangle-times:after,.fad.fa-rectangle-xmark:after,.fad.fa-times-rectangle:after,.fad.fa-window-close:after{content:"\f410\f410"}.fa-duotone.fa-tire-rugged:after,.fad.fa-tire-rugged:after{content:"\f634\f634"}.fa-duotone.fa-lightbulb-dollar:after,.fad.fa-lightbulb-dollar:after{content:"\f670\f670"}.fa-duotone.fa-cowbell:after,.fad.fa-cowbell:after{content:"\f8b3\f8b3"}.fa-duotone.fa-baht-sign:after,.fad.fa-baht-sign:after{content:"\e0ac\e0ac"}.fa-duotone.fa-corner:after,.fad.fa-corner:after{content:"\e3fe\e3fe"}.fa-duotone.fa-chevron-double-right:after,.fa-duotone.fa-chevrons-right:after,.fad.fa-chevron-double-right:after,.fad.fa-chevrons-right:after{content:"\f324\f324"}.fa-duotone.fa-book-open:after,.fad.fa-book-open:after{content:"\f518\f518"}.fa-duotone.fa-book-journal-whills:after,.fa-duotone.fa-journal-whills:after,.fad.fa-book-journal-whills:after,.fad.fa-journal-whills:after{content:"\f66a\f66a"}.fa-duotone.fa-inhaler:after,.fad.fa-inhaler:after{content:"\f5f9\f5f9"}.fa-duotone.fa-handcuffs:after,.fad.fa-handcuffs:after{content:"\e4f8\e4f8"}.fa-duotone.fa-snake:after,.fad.fa-snake:after{content:"\f716\f716"}.fa-duotone.fa-exclamation-triangle:after,.fa-duotone.fa-triangle-exclamation:after,.fa-duotone.fa-warning:after,.fad.fa-exclamation-triangle:after,.fad.fa-triangle-exclamation:after,.fad.fa-warning:after{content:"\f071\f071"}.fa-duotone.fa-note-medical:after,.fad.fa-note-medical:after{content:"\e200\e200"}.fa-duotone.fa-database:after,.fad.fa-database:after{content:"\f1c0\f1c0"}.fa-duotone.fa-down-left:after,.fad.fa-down-left:after{content:"\e16a\e16a"}.fa-duotone.fa-mail-forward:after,.fa-duotone.fa-share:after,.fad.fa-mail-forward:after,.fad.fa-share:after{content:"\f064\f064"}.fa-duotone.fa-face-thinking:after,.fad.fa-face-thinking:after{content:"\e39b\e39b"}.fa-duotone.fa-turn-down-right:after,.fad.fa-turn-down-right:after{content:"\e455\e455"}.fa-duotone.fa-bottle-droplet:after,.fad.fa-bottle-droplet:after{content:"\e4c4\e4c4"}.fa-duotone.fa-mask-face:after,.fad.fa-mask-face:after{content:"\e1d7\e1d7"}.fa-duotone.fa-hill-rockslide:after,.fad.fa-hill-rockslide:after{content:"\e508\e508"}.fa-duotone.fa-scanner-keyboard:after,.fad.fa-scanner-keyboard:after{content:"\f489\f489"}.fa-duotone.fa-circle-o:after,.fad.fa-circle-o:after{content:"\e119\e119"}.fa-duotone.fa-grid-horizontal:after,.fad.fa-grid-horizontal:after{content:"\e307\e307"}.fa-duotone.fa-comment-alt-dollar:after,.fa-duotone.fa-message-dollar:after,.fad.fa-comment-alt-dollar:after,.fad.fa-message-dollar:after{content:"\f650\f650"}.fa-duotone.fa-exchange-alt:after,.fa-duotone.fa-right-left:after,.fad.fa-exchange-alt:after,.fad.fa-right-left:after{content:"\f362\f362"}.fa-duotone.fa-columns-3:after,.fad.fa-columns-3:after{content:"\e361\e361"}.fa-duotone.fa-paper-plane:after,.fad.fa-paper-plane:after{content:"\f1d8\f1d8"}.fa-duotone.fa-road-circle-exclamation:after,.fad.fa-road-circle-exclamation:after{content:"\e565\e565"}.fa-duotone.fa-dungeon:after,.fad.fa-dungeon:after{content:"\f6d9\f6d9"}.fa-duotone.fa-hand-holding-box:after,.fad.fa-hand-holding-box:after{content:"\f47b\f47b"}.fa-duotone.fa-input-text:after,.fad.fa-input-text:after{content:"\e1bf\e1bf"}.fa-duotone.fa-window-alt:after,.fa-duotone.fa-window-flip:after,.fad.fa-window-alt:after,.fad.fa-window-flip:after{content:"\f40f\f40f"}.fa-duotone.fa-align-right:after,.fad.fa-align-right:after{content:"\f038\f038"}.fa-duotone.fa-scanner-gun:after,.fa-duotone.fa-scanner:after,.fad.fa-scanner-gun:after,.fad.fa-scanner:after{content:"\f488\f488"}.fa-duotone.fa-tire:after,.fad.fa-tire:after{content:"\f631\f631"}.fa-duotone.fa-engine:after,.fad.fa-engine:after{content:"\e16e\e16e"}.fa-duotone.fa-money-bill-1-wave:after,.fa-duotone.fa-money-bill-wave-alt:after,.fad.fa-money-bill-1-wave:after,.fad.fa-money-bill-wave-alt:after{content:"\f53b\f53b"}.fa-duotone.fa-life-ring:after,.fad.fa-life-ring:after{content:"\f1cd\f1cd"}.fa-duotone.fa-hands:after,.fa-duotone.fa-sign-language:after,.fa-duotone.fa-signing:after,.fad.fa-hands:after,.fad.fa-sign-language:after,.fad.fa-signing:after{content:"\f2a7\f2a7"}.fa-duotone.fa-caret-circle-right:after,.fa-duotone.fa-circle-caret-right:after,.fad.fa-caret-circle-right:after,.fad.fa-circle-caret-right:after{content:"\f330\f330"}.fa-duotone.fa-turn-left:after,.fad.fa-turn-left:after{content:"\e636\e636"}.fa-duotone.fa-wheat:after,.fad.fa-wheat:after{content:"\f72d\f72d"}.fa-duotone.fa-file-spreadsheet:after,.fad.fa-file-spreadsheet:after{content:"\f65b\f65b"}.fa-duotone.fa-audio-description-slash:after,.fad.fa-audio-description-slash:after{content:"\e0a8\e0a8"}.fa-duotone.fa-bell-ring:after,.fad.fa-bell-ring:after{content:"\e62c\e62c"}.fa-duotone.fa-calendar-day:after,.fad.fa-calendar-day:after{content:"\f783\f783"}.fa-duotone.fa-ladder-water:after,.fa-duotone.fa-swimming-pool:after,.fa-duotone.fa-water-ladder:after,.fad.fa-ladder-water:after,.fad.fa-swimming-pool:after,.fad.fa-water-ladder:after{content:"\f5c5\f5c5"}.fa-duotone.fa-arrows-up-down:after,.fa-duotone.fa-arrows-v:after,.fad.fa-arrows-up-down:after,.fad.fa-arrows-v:after{content:"\f07d\f07d"}.fa-duotone.fa-chess-pawn-alt:after,.fa-duotone.fa-chess-pawn-piece:after,.fad.fa-chess-pawn-alt:after,.fad.fa-chess-pawn-piece:after{content:"\f444\f444"}.fa-duotone.fa-face-grimace:after,.fa-duotone.fa-grimace:after,.fad.fa-face-grimace:after,.fad.fa-grimace:after{content:"\f57f\f57f"}.fa-duotone.fa-wheelchair-alt:after,.fa-duotone.fa-wheelchair-move:after,.fad.fa-wheelchair-alt:after,.fad.fa-wheelchair-move:after{content:"\e2ce\e2ce"}.fa-duotone.fa-level-down-alt:after,.fa-duotone.fa-turn-down:after,.fad.fa-level-down-alt:after,.fad.fa-turn-down:after{content:"\f3be\f3be"}.fa-duotone.fa-square-s:after,.fad.fa-square-s:after{content:"\e27d\e27d"}.fa-duotone.fa-barcode-alt:after,.fa-duotone.fa-rectangle-barcode:after,.fad.fa-barcode-alt:after,.fad.fa-rectangle-barcode:after{content:"\f463\f463"}.fa-duotone.fa-person-walking-arrow-right:after,.fad.fa-person-walking-arrow-right:after{content:"\e552\e552"}.fa-duotone.fa-envelope-square:after,.fa-duotone.fa-square-envelope:after,.fad.fa-envelope-square:after,.fad.fa-square-envelope:after{content:"\f199\f199"}.fa-duotone.fa-dice:after,.fad.fa-dice:after{content:"\f522\f522"}.fa-duotone.fa-unicorn:after,.fad.fa-unicorn:after{content:"\f727\f727"}.fa-duotone.fa-bowling-ball:after,.fad.fa-bowling-ball:after{content:"\f436\f436"}.fa-duotone.fa-pompebled:after,.fad.fa-pompebled:after{content:"\e43d\e43d"}.fa-duotone.fa-brain:after,.fad.fa-brain:after{content:"\f5dc\f5dc"}.fa-duotone.fa-watch-smart:after,.fad.fa-watch-smart:after{content:"\e2cc\e2cc"}.fa-duotone.fa-book-user:after,.fad.fa-book-user:after{content:"\f7e7\f7e7"}.fa-duotone.fa-sensor-cloud:after,.fa-duotone.fa-sensor-smoke:after,.fad.fa-sensor-cloud:after,.fad.fa-sensor-smoke:after{content:"\e02c\e02c"}.fa-duotone.fa-clapperboard-play:after,.fad.fa-clapperboard-play:after{content:"\e132\e132"}.fa-duotone.fa-band-aid:after,.fa-duotone.fa-bandage:after,.fad.fa-band-aid:after,.fad.fa-bandage:after{content:"\f462\f462"}.fa-duotone.fa-calendar-minus:after,.fad.fa-calendar-minus:after{content:"\f272\f272"}.fa-duotone.fa-circle-xmark:after,.fa-duotone.fa-times-circle:after,.fa-duotone.fa-xmark-circle:after,.fad.fa-circle-xmark:after,.fad.fa-times-circle:after,.fad.fa-xmark-circle:after{content:"\f057\f057"}.fa-duotone.fa-circle-4:after,.fad.fa-circle-4:after{content:"\e0f1\e0f1"}.fa-duotone.fa-gifts:after,.fad.fa-gifts:after{content:"\f79c\f79c"}.fa-duotone.fa-album-collection:after,.fad.fa-album-collection:after{content:"\f8a0\f8a0"}.fa-duotone.fa-hotel:after,.fad.fa-hotel:after{content:"\f594\f594"}.fa-duotone.fa-earth-asia:after,.fa-duotone.fa-globe-asia:after,.fad.fa-earth-asia:after,.fad.fa-globe-asia:after{content:"\f57e\f57e"}.fa-duotone.fa-id-card-alt:after,.fa-duotone.fa-id-card-clip:after,.fad.fa-id-card-alt:after,.fad.fa-id-card-clip:after{content:"\f47f\f47f"}.fa-duotone.fa-magnifying-glass-plus:after,.fa-duotone.fa-search-plus:after,.fad.fa-magnifying-glass-plus:after,.fad.fa-search-plus:after{content:"\f00e\f00e"}.fa-duotone.fa-thumbs-up:after,.fad.fa-thumbs-up:after{content:"\f164\f164"}.fa-duotone.fa-cloud-showers:after,.fad.fa-cloud-showers:after{content:"\f73f\f73f"}.fa-duotone.fa-user-clock:after,.fad.fa-user-clock:after{content:"\f4fd\f4fd"}.fa-duotone.fa-onion:after,.fad.fa-onion:after{content:"\e427\e427"}.fa-duotone.fa-clock-twelve-thirty:after,.fad.fa-clock-twelve-thirty:after{content:"\e359\e359"}.fa-duotone.fa-arrow-down-to-dotted-line:after,.fad.fa-arrow-down-to-dotted-line:after{content:"\e095\e095"}.fa-duotone.fa-allergies:after,.fa-duotone.fa-hand-dots:after,.fad.fa-allergies:after,.fad.fa-hand-dots:after{content:"\f461\f461"}.fa-duotone.fa-file-invoice:after,.fad.fa-file-invoice:after{content:"\f570\f570"}.fa-duotone.fa-window-minimize:after,.fad.fa-window-minimize:after{content:"\f2d1\f2d1"}.fa-duotone.fa-rectangle-wide:after,.fad.fa-rectangle-wide:after{content:"\f2fc\f2fc"}.fa-duotone.fa-comment-arrow-up:after,.fad.fa-comment-arrow-up:after{content:"\e144\e144"}.fa-duotone.fa-garlic:after,.fad.fa-garlic:after{content:"\e40e\e40e"}.fa-duotone.fa-coffee:after,.fa-duotone.fa-mug-saucer:after,.fad.fa-coffee:after,.fad.fa-mug-saucer:after{content:"\f0f4\f0f4"}.fa-duotone.fa-brush:after,.fad.fa-brush:after{content:"\f55d\f55d"}.fa-duotone.fa-tree-decorated:after,.fad.fa-tree-decorated:after{content:"\f7dc\f7dc"}.fa-duotone.fa-mask:after,.fad.fa-mask:after{content:"\f6fa\f6fa"}.fa-duotone.fa-calendar-heart:after,.fad.fa-calendar-heart:after{content:"\e0d3\e0d3"}.fa-duotone.fa-magnifying-glass-minus:after,.fa-duotone.fa-search-minus:after,.fad.fa-magnifying-glass-minus:after,.fad.fa-search-minus:after{content:"\f010\f010"}.fa-duotone.fa-flower:after,.fad.fa-flower:after{content:"\f7ff\f7ff"}.fa-duotone.fa-arrow-down-from-arc:after,.fad.fa-arrow-down-from-arc:after{content:"\e614\e614"}.fa-duotone.fa-right-left-large:after,.fad.fa-right-left-large:after{content:"\e5e1\e5e1"}.fa-duotone.fa-ruler-vertical:after,.fad.fa-ruler-vertical:after{content:"\f548\f548"}.fa-duotone.fa-circles-overlap:after,.fad.fa-circles-overlap:after{content:"\e600\e600"}.fa-duotone.fa-user-alt:after,.fa-duotone.fa-user-large:after,.fad.fa-user-alt:after,.fad.fa-user-large:after{content:"\f406\f406"}.fa-duotone.fa-starship-freighter:after,.fad.fa-starship-freighter:after{content:"\e03a\e03a"}.fa-duotone.fa-train-tram:after,.fad.fa-train-tram:after{content:"\e5b4\e5b4"}.fa-duotone.fa-bridge-suspension:after,.fad.fa-bridge-suspension:after{content:"\e4cd\e4cd"}.fa-duotone.fa-trash-check:after,.fad.fa-trash-check:after{content:"\e2af\e2af"}.fa-duotone.fa-user-nurse:after,.fad.fa-user-nurse:after{content:"\f82f\f82f"}.fa-duotone.fa-boombox:after,.fad.fa-boombox:after{content:"\f8a5\f8a5"}.fa-duotone.fa-syringe:after,.fad.fa-syringe:after{content:"\f48e\f48e"}.fa-duotone.fa-cloud-sun:after,.fad.fa-cloud-sun:after{content:"\f6c4\f6c4"}.fa-duotone.fa-shield-exclamation:after,.fad.fa-shield-exclamation:after{content:"\e247\e247"}.fa-duotone.fa-stopwatch-20:after,.fad.fa-stopwatch-20:after{content:"\e06f\e06f"}.fa-duotone.fa-square-full:after,.fad.fa-square-full:after{content:"\f45c\f45c"}.fa-duotone.fa-grip-dots:after,.fad.fa-grip-dots:after{content:"\e410\e410"}.fa-duotone.fa-comment-exclamation:after,.fad.fa-comment-exclamation:after{content:"\f4af\f4af"}.fa-duotone.fa-pen-swirl:after,.fad.fa-pen-swirl:after{content:"\e214\e214"}.fa-duotone.fa-falafel:after,.fad.fa-falafel:after{content:"\e40a\e40a"}.fa-duotone.fa-circle-2:after,.fad.fa-circle-2:after{content:"\e0ef\e0ef"}.fa-duotone.fa-magnet:after,.fad.fa-magnet:after{content:"\f076\f076"}.fa-duotone.fa-jar:after,.fad.fa-jar:after{content:"\e516\e516"}.fa-duotone.fa-gramophone:after,.fad.fa-gramophone:after{content:"\f8bd\f8bd"}.fa-duotone.fa-dice-d12:after,.fad.fa-dice-d12:after{content:"\f6ce\f6ce"}.fa-duotone.fa-note-sticky:after,.fa-duotone.fa-sticky-note:after,.fad.fa-note-sticky:after,.fad.fa-sticky-note:after{content:"\f249\f249"}.fa-duotone.fa-arrow-alt-down:after,.fa-duotone.fa-down:after,.fad.fa-arrow-alt-down:after,.fad.fa-down:after{content:"\f354\f354"}.fa-duotone.fa-100:after,.fa-duotone.fa-hundred-points:after,.fad.fa-100:after,.fad.fa-hundred-points:after{content:"\e41c\e41c"}.fa-duotone.fa-paperclip-vertical:after,.fad.fa-paperclip-vertical:after{content:"\e3c2\e3c2"}.fa-duotone.fa-wind-circle-exclamation:after,.fa-duotone.fa-wind-warning:after,.fad.fa-wind-circle-exclamation:after,.fad.fa-wind-warning:after{content:"\f776\f776"}.fa-duotone.fa-location-pin-slash:after,.fa-duotone.fa-map-marker-slash:after,.fad.fa-location-pin-slash:after,.fad.fa-map-marker-slash:after{content:"\f60c\f60c"}.fa-duotone.fa-face-sad-sweat:after,.fad.fa-face-sad-sweat:after{content:"\e38a\e38a"}.fa-duotone.fa-bug-slash:after,.fad.fa-bug-slash:after{content:"\e490\e490"}.fa-duotone.fa-cupcake:after,.fad.fa-cupcake:after{content:"\e402\e402"}.fa-duotone.fa-light-switch-off:after,.fad.fa-light-switch-off:after{content:"\e018\e018"}.fa-duotone.fa-toggle-large-off:after,.fad.fa-toggle-large-off:after{content:"\e5b0\e5b0"}.fa-duotone.fa-pen-fancy-slash:after,.fad.fa-pen-fancy-slash:after{content:"\e210\e210"}.fa-duotone.fa-truck-container:after,.fad.fa-truck-container:after{content:"\f4dc\f4dc"}.fa-duotone.fa-boot:after,.fad.fa-boot:after{content:"\f782\f782"}.fa-duotone.fa-arrow-up-from-water-pump:after,.fad.fa-arrow-up-from-water-pump:after{content:"\e4b6\e4b6"}.fa-duotone.fa-file-check:after,.fad.fa-file-check:after{content:"\f316\f316"}.fa-duotone.fa-bone:after,.fad.fa-bone:after{content:"\f5d7\f5d7"}.fa-duotone.fa-cards-blank:after,.fad.fa-cards-blank:after{content:"\e4df\e4df"}.fa-duotone.fa-circle-3:after,.fad.fa-circle-3:after{content:"\e0f0\e0f0"}.fa-duotone.fa-bench-tree:after,.fad.fa-bench-tree:after{content:"\e2e7\e2e7"}.fa-duotone.fa-keyboard-brightness-low:after,.fad.fa-keyboard-brightness-low:after{content:"\e1c1\e1c1"}.fa-duotone.fa-ski-boot-ski:after,.fad.fa-ski-boot-ski:after{content:"\e3cd\e3cd"}.fa-duotone.fa-brain-circuit:after,.fad.fa-brain-circuit:after{content:"\e0c6\e0c6"}.fa-duotone.fa-user-injured:after,.fad.fa-user-injured:after{content:"\f728\f728"}.fa-duotone.fa-block-brick-fire:after,.fa-duotone.fa-firewall:after,.fad.fa-block-brick-fire:after,.fad.fa-firewall:after{content:"\e3dc\e3dc"}.fa-duotone.fa-face-sad-tear:after,.fa-duotone.fa-sad-tear:after,.fad.fa-face-sad-tear:after,.fad.fa-sad-tear:after{content:"\f5b4\f5b4"}.fa-duotone.fa-plane:after,.fad.fa-plane:after{content:"\f072\f072"}.fa-duotone.fa-tent-arrows-down:after,.fad.fa-tent-arrows-down:after{content:"\e581\e581"}.fa-duotone.fa-exclamation:after,.fad.fa-exclamation:after{content:"\21\21"}.fa-duotone.fa-arrows-spin:after,.fad.fa-arrows-spin:after{content:"\e4bb\e4bb"}.fa-duotone.fa-face-smile-relaxed:after,.fad.fa-face-smile-relaxed:after{content:"\e392\e392"}.fa-duotone.fa-comment-times:after,.fa-duotone.fa-comment-xmark:after,.fad.fa-comment-times:after,.fad.fa-comment-xmark:after{content:"\f4b5\f4b5"}.fa-duotone.fa-print:after,.fad.fa-print:after{content:"\f02f\f02f"}.fa-duotone.fa-try:after,.fa-duotone.fa-turkish-lira-sign:after,.fa-duotone.fa-turkish-lira:after,.fad.fa-try:after,.fad.fa-turkish-lira-sign:after,.fad.fa-turkish-lira:after{content:"\e2bb\e2bb"}.fa-duotone.fa-face-nose-steam:after,.fad.fa-face-nose-steam:after{content:"\e382\e382"}.fa-duotone.fa-circle-waveform-lines:after,.fa-duotone.fa-waveform-circle:after,.fad.fa-circle-waveform-lines:after,.fad.fa-waveform-circle:after{content:"\e12d\e12d"}.fa-duotone.fa-dollar-sign:after,.fa-duotone.fa-dollar:after,.fa-duotone.fa-usd:after,.fad.fa-dollar-sign:after,.fad.fa-dollar:after,.fad.fa-usd:after{content:"\24\24"}.fa-duotone.fa-ferris-wheel:after,.fad.fa-ferris-wheel:after{content:"\e174\e174"}.fa-duotone.fa-computer-speaker:after,.fad.fa-computer-speaker:after{content:"\f8b2\f8b2"}.fa-duotone.fa-skull-cow:after,.fad.fa-skull-cow:after{content:"\f8de\f8de"}.fa-duotone.fa-x:after,.fad.fa-x:after{content:"\58\58"}.fa-duotone.fa-magnifying-glass-dollar:after,.fa-duotone.fa-search-dollar:after,.fad.fa-magnifying-glass-dollar:after,.fad.fa-search-dollar:after{content:"\f688\f688"}.fa-duotone.fa-users-cog:after,.fa-duotone.fa-users-gear:after,.fad.fa-users-cog:after,.fad.fa-users-gear:after{content:"\f509\f509"}.fa-duotone.fa-person-military-pointing:after,.fad.fa-person-military-pointing:after{content:"\e54a\e54a"}.fa-duotone.fa-bank:after,.fa-duotone.fa-building-columns:after,.fa-duotone.fa-institution:after,.fa-duotone.fa-museum:after,.fa-duotone.fa-university:after,.fad.fa-bank:after,.fad.fa-building-columns:after,.fad.fa-institution:after,.fad.fa-museum:after,.fad.fa-university:after{content:"\f19c\f19c"}.fa-duotone.fa-circle-t:after,.fad.fa-circle-t:after{content:"\e124\e124"}.fa-duotone.fa-sack:after,.fad.fa-sack:after{content:"\f81c\f81c"}.fa-duotone.fa-grid-2:after,.fad.fa-grid-2:after{content:"\e196\e196"}.fa-duotone.fa-camera-cctv:after,.fa-duotone.fa-cctv:after,.fad.fa-camera-cctv:after,.fad.fa-cctv:after{content:"\f8ac\f8ac"}.fa-duotone.fa-umbrella:after,.fad.fa-umbrella:after{content:"\f0e9\f0e9"}.fa-duotone.fa-trowel:after,.fad.fa-trowel:after{content:"\e589\e589"}.fa-duotone.fa-horizontal-rule:after,.fad.fa-horizontal-rule:after{content:"\f86c\f86c"}.fa-duotone.fa-bed-alt:after,.fa-duotone.fa-bed-front:after,.fad.fa-bed-alt:after,.fad.fa-bed-front:after{content:"\f8f7\f8f7"}.fa-duotone.fa-d:after,.fad.fa-d:after{content:"\44\44"}.fa-duotone.fa-stapler:after,.fad.fa-stapler:after{content:"\e5af\e5af"}.fa-duotone.fa-masks-theater:after,.fa-duotone.fa-theater-masks:after,.fad.fa-masks-theater:after,.fad.fa-theater-masks:after{content:"\f630\f630"}.fa-duotone.fa-file-gif:after,.fad.fa-file-gif:after{content:"\e645\e645"}.fa-duotone.fa-kip-sign:after,.fad.fa-kip-sign:after{content:"\e1c4\e1c4"}.fa-duotone.fa-face-woozy:after,.fad.fa-face-woozy:after{content:"\e3a2\e3a2"}.fa-duotone.fa-cloud-question:after,.fad.fa-cloud-question:after{content:"\e492\e492"}.fa-duotone.fa-pineapple:after,.fad.fa-pineapple:after{content:"\e31f\e31f"}.fa-duotone.fa-hand-point-left:after,.fad.fa-hand-point-left:after{content:"\f0a5\f0a5"}.fa-duotone.fa-gallery-thumbnails:after,.fad.fa-gallery-thumbnails:after{content:"\e3aa\e3aa"}.fa-duotone.fa-circle-j:after,.fad.fa-circle-j:after{content:"\e112\e112"}.fa-duotone.fa-eyes:after,.fad.fa-eyes:after{content:"\e367\e367"}.fa-duotone.fa-handshake-alt:after,.fa-duotone.fa-handshake-simple:after,.fad.fa-handshake-alt:after,.fad.fa-handshake-simple:after{content:"\f4c6\f4c6"}.fa-duotone.fa-file-caret-up:after,.fa-duotone.fa-page-caret-up:after,.fad.fa-file-caret-up:after,.fad.fa-page-caret-up:after{content:"\e42a\e42a"}.fa-duotone.fa-fighter-jet:after,.fa-duotone.fa-jet-fighter:after,.fad.fa-fighter-jet:after,.fad.fa-jet-fighter:after{content:"\f0fb\f0fb"}.fa-duotone.fa-comet:after,.fad.fa-comet:after{content:"\e003\e003"}.fa-duotone.fa-share-alt-square:after,.fa-duotone.fa-square-share-nodes:after,.fad.fa-share-alt-square:after,.fad.fa-square-share-nodes:after{content:"\f1e1\f1e1"}.fa-duotone.fa-reflect-vertical:after,.fad.fa-reflect-vertical:after{content:"\e665\e665"}.fa-duotone.fa-shield-keyhole:after,.fad.fa-shield-keyhole:after{content:"\e248\e248"}.fa-duotone.fa-file-mp4:after,.fad.fa-file-mp4:after{content:"\e649\e649"}.fa-duotone.fa-barcode:after,.fad.fa-barcode:after{content:"\f02a\f02a"}.fa-duotone.fa-bulldozer:after,.fad.fa-bulldozer:after{content:"\e655\e655"}.fa-duotone.fa-plus-minus:after,.fad.fa-plus-minus:after{content:"\e43c\e43c"}.fa-duotone.fa-sliders-v-square:after,.fa-duotone.fa-square-sliders-vertical:after,.fad.fa-sliders-v-square:after,.fad.fa-square-sliders-vertical:after{content:"\f3f2\f3f2"}.fa-duotone.fa-video-camera:after,.fa-duotone.fa-video:after,.fad.fa-video-camera:after,.fad.fa-video:after{content:"\f03d\f03d"}.fa-duotone.fa-comment-middle-alt:after,.fa-duotone.fa-message-middle:after,.fad.fa-comment-middle-alt:after,.fad.fa-message-middle:after{content:"\e1e1\e1e1"}.fa-duotone.fa-graduation-cap:after,.fa-duotone.fa-mortar-board:after,.fad.fa-graduation-cap:after,.fad.fa-mortar-board:after{content:"\f19d\f19d"}.fa-duotone.fa-hand-holding-medical:after,.fad.fa-hand-holding-medical:after{content:"\e05c\e05c"}.fa-duotone.fa-person-circle-check:after,.fad.fa-person-circle-check:after{content:"\e53e\e53e"}.fa-duotone.fa-square-z:after,.fad.fa-square-z:after{content:"\e288\e288"}.fa-duotone.fa-comment-alt-text:after,.fa-duotone.fa-message-text:after,.fad.fa-comment-alt-text:after,.fad.fa-message-text:after{content:"\e1e6\e1e6"}.fa-duotone.fa-level-up-alt:after,.fa-duotone.fa-turn-up:after,.fad.fa-level-up-alt:after,.fad.fa-turn-up:after{content:"\f3bf\f3bf"}:host,:root{--fa-font-light:normal 300 1em/1 "Font Awesome 6 Pro"}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-light,.fal{font-weight:300}:host,:root{--fa-font-regular:normal 400 1em/1 "Font Awesome 6 Pro"}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-regular,.far{font-weight:400}:host,:root{--fa-font-solid:normal 900 1em/1 "Font Awesome 6 Pro"}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-solid,.fas{font-weight:900}:host,:root{--fa-style-family-classic:"Font Awesome 6 Pro";--fa-font-thin:normal 100 1em/1 "Font Awesome 6 Pro"}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Pro";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-thin-100-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-thin,.fat{font-weight:100}@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.ttf) format("truetype");unicode-range:u+e007,u+e013,u+e01a,u+e01e,u+e049,u+e052,u+e055-e057,u+e077-e084,u+e087-e088,u+f081-f082,u+f08c,u+f092,u+f099-f09b,u+f0d2-f0d5,u+f0e1,u+f113,u+f136,u+f13b-f13c,u+f15a,u+f167-f169,u+f16b-f16e,u+f170-f171,u+f173-f174,u+f179-f17e,u+f180-f181,u+f184,u+f189-f18d,u+f194,u+f198,u+f19a-f19b,u+f19e,u+f1a0-f1a9,u+f1b4,u+f1bc,u+f1be,u+f1e8,u+f1ed,u+f1f0-f1f1,u+f20e,u+f210,u+f213-f214,u+f232,u+f23a,u+f26b,u+f270,u+f288,u+f299,u+f2a6,u+f2b0,u+f2c5-f2c6,u+f2e0,u+f368,u+f379,u+f392-f393,u+f39f,u+f3a9,u+f3ab-f3ac,u+f3c0,u+f3c7,u+f3ca,u+f3e2,u+f3eb-f3ec,u+f3ef,u+f3f8,u+f3fe,u+f419,u+f41b,u+f4d5,u+f4e4,u+f4f8-f4f9,u+f514,u+f5b5,u+f6c9,u+f731,u+f77b,u+f7af,u+f7e1,u+f83b}@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.ttf) format("truetype");unicode-range:u+f1aa,u+f1b5-f1b7,u+f1bd,u+f1ca-f1cc,u+f1d0-f1d7,u+f1e7,u+f1e9,u+f1ee,u+f1f2-f1f5,u+f202-f203,u+f208-f209,u+f20d,u+f211-f212,u+f215-f216,u+f231,u+f237,u+f23b-f23e,u+f24b-f24c,u+f25e,u+f260-f261,u+f263-f26a,u+f26d-f26e,u+f27c-f27e,u+f280-f282,u+f284-f287,u+f289-f28a,u+f293-f294,u+f296-f298,u+f2a5,u+f2a9-f2ae,u+f2b1-f2b4,u+f2b8,u+f2c4,u+f2d5-f2da,u+f2dd-f2de,u+f35c,u+f369-f375,u+f378,u+f37a-f37d,u+f37f-f380,u+f383-f385,u+f388,u+f38b-f38f,u+f391,u+f394-f397,u+f399-f39a,u+f39d-f39e,u+f3a1-f3a4,u+f3a6-f3a8,u+f3aa,u+f3ad-f3b2,u+f3b4-f3bd,u+f3c3-f3c4,u+f3c6,u+f3c8,u+f3cb-f3cc,u+f3d0,u+f3d2-f3dc,u+f3df,u+f3e1,u+f3e3-f3e4,u+f3e6-f3e7,u+f425,u+f4e6}@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.ttf) format("truetype");unicode-range:u+f3e8-f3ea,u+f3ee,u+f3f3,u+f3f5-f3f7,u+f3f9,u+f402-f405,u+f407-f40d,u+f411-f417,u+f41a,u+f41c-f421,u+f423,u+f426-f431,u+f44d,u+f452,u+f457,u+f459,u+f4e5,u+f4e7-f4f7,u+f50a-f513,u+f592,u+f59e,u+f5a3,u+f5a8,u+f5b2,u+f5be,u+f5c6,u+f5cc,u+f5cf,u+f5f1,u+f5f7,u+f5fa,u+f60f,u+f612,u+f63f,u+f642,u+f69d,u+f6ca,u+f6cc,u+f6dc,u+f730,u+f75d,u+f77a,u+f785,u+f789,u+f78d,u+f790-f791,u+f797-f799,u+f7b0-f7b1,u+f7b3,u+f7bb-f7bc,u+f7c6,u+f7d3,u+f7d6,u+f7df-f7e0,u+f7e3,u+f834-f83a,u+f83c-f83d,u+f83f-f842,u+f89e,u+f8a6,u+f8ca,u+f8d2,u+f8e1,u+f8e8}@font-face{font-family:"Font Awesome 5 Brands";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400.ttf) format("truetype");unicode-range:u+a}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.ttf) format("truetype");unicode-range:u+f040,u+f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f108-f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-2.ttf) format("truetype");unicode-range:u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047-e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-4.ttf) format("truetype");unicode-range:u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-8.ttf) format("truetype");unicode-range:u+f80b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-11.ttf) format("truetype");unicode-range:u+f8bc}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.ttf) format("truetype");unicode-range:u+f0d9-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221,u+f381-f382}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.ttf) format("truetype");unicode-range:u+f222-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f319,u+f31c,u+f4e6,u+f8e5}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-15.ttf) format("truetype");unicode-range:u+f31d-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-16.ttf) format("truetype");unicode-range:u+f45a-f47c,u+f47e-f480,u+f482-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f507,u+f509,u+f515-f51a,u+f51c,u+f51f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-17.ttf) format("truetype");unicode-range:u+f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-18.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f5c3-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f66f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-19.ttf) format("truetype");unicode-range:u+f670-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f710}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-20.ttf) format("truetype");unicode-range:u+f711-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-21.ttf) format("truetype");unicode-range:u+f7c3-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f87b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-22.ttf) format("truetype");unicode-range:u+f87c-f892,u+f895-f896,u+f898-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.ttf) format("truetype");unicode-range:u+f040,u+f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f108-f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-2.ttf) format("truetype");unicode-range:u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-3.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047-e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-4.ttf) format("truetype");unicode-range:u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-8.ttf) format("truetype");unicode-range:u+f80b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-11.ttf) format("truetype");unicode-range:u+f8bc}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.ttf) format("truetype");unicode-range:u+f0d9-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221,u+f381-f382}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.ttf) format("truetype");unicode-range:u+f222-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f319,u+f31c,u+f4e6,u+f8e5}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.ttf) format("truetype");unicode-range:u+f31d-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.ttf) format("truetype");unicode-range:u+f45a-f47c,u+f47e-f480,u+f482-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f507,u+f509,u+f515-f51a,u+f51c,u+f51f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-17.ttf) format("truetype");unicode-range:u+f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-18.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f5c3-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f66f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-19.ttf) format("truetype");unicode-range:u+f670-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f710}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-20.ttf) format("truetype");unicode-range:u+f711-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-21.ttf) format("truetype");unicode-range:u+f7c3-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f87b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:400;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-22.ttf) format("truetype");unicode-range:u+f87c-f892,u+f895-f896,u+f898-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-1.ttf) format("truetype");unicode-range:u+f040,u+f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f108-f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-2.ttf) format("truetype");unicode-range:u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-3.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047-e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-4.ttf) format("truetype");unicode-range:u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-8.ttf) format("truetype");unicode-range:u+f80b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-11.ttf) format("truetype");unicode-range:u+f8bc}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-12.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-13.ttf) format("truetype");unicode-range:u+f0d9-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221,u+f381-f382}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-14.ttf) format("truetype");unicode-range:u+f222-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f319,u+f31c,u+f4e6,u+f8e5}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-15.ttf) format("truetype");unicode-range:u+f31d-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-16.ttf) format("truetype");unicode-range:u+f45a-f47c,u+f47e-f480,u+f482-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f507,u+f509,u+f515-f51a,u+f51c,u+f51f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-17.ttf) format("truetype");unicode-range:u+f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-18.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f5c3-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f66f}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-19.ttf) format("truetype");unicode-range:u+f670-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f710}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-20.ttf) format("truetype");unicode-range:u+f711-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c2}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-21.ttf) format("truetype");unicode-range:u+f7c3-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f87b}@font-face{font-family:"Font Awesome 5 Pro";font-display:block;font-weight:300;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-light-300-22.ttf) format("truetype");unicode-range:u+f87c-f892,u+f895-f896,u+f898-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-0.ttf) format("truetype");unicode-range:u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c-f09d,u+f0a3,u+f128,u+f12a,u+f155,u+f292,u+f295,u+f332,u+f541,u+f80a,u+f80c,u+10e010,u+10e017,u+10e01b,u+10e01f-10e021,u+10e024,u+10e02f,u+10e03a,u+10e042,u+10e045-10e046,u+10e060,u+10e068,u+10e06e,u+10e074,u+10e076,u+10f001,u+10f004-10f005,u+10f007-10f008,u+10f00c,u+10f011-10f012,u+10f015,u+10f017-10f019,u+10f01c,u+10f023-10f025,u+10f02a,u+10f02c-10f031,u+10f03a,u+10f03d-10f03e,u+10f041,u+10f04a-10f04e,u+10f05b,u+10f060-10f065,u+10f067-10f068,u+10f06b-10f06e,u+10f072,u+10f075,u+10f077-10f078,u+10f07b,u+10f084,u+10f086,u+10f091,u+10f093,u+10f095,u+10f09c-10f09d,u+10f0a3,u+10f128,u+10f12a,u+10f155,u+10f292,u+10f295,u+10f332,u+10f541,u+10f80a,u+10f80c}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-1.ttf) format("truetype");unicode-range:u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8-f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d6-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f8,u+f106,u+f109,u+f10e,u+f110-f111,u+f11c,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0-f1c3,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1ea-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e,u+f47d,u+10f0a6,u+10f0ac-10f0ad,u+10f0b0-10f0b1,u+10f0c0-10f0c2,u+10f0c5-10f0c6,u+10f0c8-10f0c9,u+10f0cc,u+10f0ce,u+10f0d1,u+10f0d6-10f0d7,u+10f0dc,u+10f0e0,u+10f0e7-10f0e8,u+10f0eb,u+10f0f3,u+10f0f8,u+10f106,u+10f109,u+10f10e,u+10f110-10f111,u+10f11c,u+10f11e,u+10f121,u+10f126,u+10f129,u+10f12c-10f12e,u+10f130-10f133,u+10f135,u+10f13d,u+10f140,u+10f145,u+10f14e,u+10f15b,u+10f164,u+10f186,u+10f188,u+10f1ab,u+10f1ad-10f1ae,u+10f1b2,u+10f1b8,u+10f1bb,u+10f1c0-10f1c3,u+10f1ce,u+10f1d8,u+10f1dc,u+10f1e4-10f1e6,u+10f1ea-10f1ec,u+10f1f8-10f1f9,u+10f205,u+10f20a,u+10f217,u+10f219-10f21d,u+10f22d,u+10f233-10f234,u+10f238,u+10f246,u+10f24d,u+10f251,u+10f25d,u+10f275,u+10f29e,u+10f47d}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-2.ttf) format("truetype");unicode-range:u+f040,u+f108,u+f2a0,u+f2a7,u+f2b5,u+f2bb,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+10f040,u+10f108,u+10f2a0,u+10f2a7,u+10f2b5,u+10f2bb,u+10f2cc-10f2cd,u+10f2d2,u+10f2db,u+10f2e1,u+10f2ec,u+10f2f7,u+10f2fc,u+10f302-10f303,u+10f316,u+10f31a,u+10f328,u+10f335,u+10f363,u+10f37e,u+10f390,u+10f3c5,u+10f3ce,u+10f3e5,u+10f3f4,u+10f3fb,u+10f40e,u+10f435,u+10f44b,u+10f481,u+10f48a,u+10f48f-10f490,u+10f493-10f494,u+10f498,u+10f4b7,u+10f4ca,u+10f4cc,u+10f4d1,u+10f4d7-10f4d8,u+10f4e2,u+10f503,u+10f508,u+10f51b,u+10f51d-10f51e,u+10f521-10f522,u+10f52b,u+10f530,u+10f535,u+10f53e,u+10f543-10f545,u+10f548-10f549,u+10f54e,u+10f555,u+10f559,u+10f55d,u+10f564,u+10f56c,u+10f56e-10f570,u+10f577-10f578,u+10f590,u+10f594-10f595,u+10f5a1-10f5a2,u+10f5aa-10f5ab,u+10f5b0,u+10f5b7,u+10f5ba,u+10f5bf,u+10f5ca,u+10f5db-10f5dc,u+10f5ef,u+10f5f2,u+10f5f6,u+10f5fb}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-3.ttf) format("truetype");unicode-range:u+f069,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+10f069,u+10f5fd,u+10f621,u+10f63b,u+10f649-10f64a,u+10f64f,u+10f653-10f654,u+10f656,u+10f65b,u+10f664,u+10f673,u+10f675,u+10f67d,u+10f67f,u+10f695,u+10f69c,u+10f6a8,u+10f6bf-10f6c0,u+10f6d5,u+10f6e3,u+10f6e9,u+10f6f5,u+10f6fa,u+10f6ff-10f700,u+10f70b,u+10f70e,u+10f715,u+10f71b,u+10f72e-10f72f,u+10f733-10f734,u+10f747,u+10f755,u+10f757,u+10f75c,u+10f762,u+10f773,u+10f77c,u+10f781,u+10f784,u+10f788,u+10f7b2,u+10f7b6,u+10f7bd,u+10f7d5,u+10f7ee,u+10f7ff,u+10f801,u+10f804,u+10f813-10f814,u+10f82f-10f830,u+10f845-10f846,u+10f850,u+10f855,u+10f858-10f859,u+10f85c,u+10f866,u+10f86d,u+10f871,u+10f875,u+10f893-10f894,u+10f897,u+10f89f,u+10f8a9,u+10f8b1-10f8b2,u+10f8bb,u+10f8c7,u+10f8d6-10f8d7,u+10f8d9,u+10f8df-10f8e0,u+10f8e7,u+10f8ee-10f8ef,u+10f8fd}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-4.ttf) format("truetype");unicode-range:u+e000-e006,u+e008-e00f,u+e011-e012,u+e014-e016,u+e018-e019,u+e01c-e01d,u+e022-e023,u+e025-e02e,u+e030-e039,u+e03b-e041,u+e043-e044,u+e047,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+10e000-10e006,u+10e008-10e00f,u+10e011-10e012,u+10e014-10e016,u+10e018-10e019,u+10e01c-10e01d,u+10e022-10e023,u+10e025-10e02e,u+10e030-10e039,u+10e03b-10e041,u+10e043-10e044,u+10e047,u+10f1fa,u+10f52c,u+10f531,u+10f536,u+10f69f}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-5.ttf) format("truetype");unicode-range:u+e048,u+e04a-e051,u+e053-e054,u+e058-e05f,u+e061-e067,u+e069-e06d,u+e06f-e073,u+e075,u+e085-e086,u+10e048,u+10e04a-10e051,u+10e053-10e054,u+10e058-10e05f,u+10e061-10e067,u+10e069-10e06d,u+10e06f-10e073,u+10e075,u+10e085-10e086}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-9.ttf) format("truetype");unicode-range:u+f80b,u+10f80b}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-12.ttf) format("truetype");unicode-range:u+f8bc,u+10f8bc}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-13.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021-f022,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f044,u+f047-f049,u+f050-f05a,u+f05e,u+f066,u+f06a,u+f070-f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c-f07e,u+f080,u+f083,u+f085,u+f089,u+f08b,u+f08d-f08e,u+f090,u+10f000,u+10f002,u+10f009-10f00b,u+10f00d-10f00e,u+10f010,u+10f013,u+10f01e,u+10f021-10f022,u+10f026-10f029,u+10f02b,u+10f032-10f039,u+10f03b-10f03c,u+10f042-10f044,u+10f047-10f049,u+10f050-10f05a,u+10f05e,u+10f066,u+10f06a,u+10f070-10f071,u+10f073-10f074,u+10f076,u+10f079-10f07a,u+10f07c-10f07e,u+10f080,u+10f083,u+10f085,u+10f089,u+10f08b,u+10f08d-10f08e,u+10f090}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-14.ttf) format("truetype");unicode-range:u+f094,u+f098,u+f09e,u+f0a0-f0a1,u+f0a4-f0a5,u+f0a7-f0ab,u+f0ae,u+f0b2,u+f0c3-f0c4,u+f0c7,u+f0ca-f0cb,u+f0cd,u+f0d0,u+f0d8-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ec-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10a-f10b,u+f10d,u+f118-f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f13e,u+f141-f144,u+f146,u+f148-f14d,u+f150-f154,u+f156-f157,u+f381-f382,u+10f094,u+10f098,u+10f09e,u+10f0a0-10f0a1,u+10f0a4-10f0a5,u+10f0a7-10f0ab,u+10f0ae,u+10f0b2,u+10f0c3-10f0c4,u+10f0c7,u+10f0ca-10f0cb,u+10f0cd,u+10f0d0,u+10f0d8-10f0db,u+10f0dd-10f0de,u+10f0e2-10f0e3,u+10f0e9-10f0ea,u+10f0ec-10f0ee,u+10f0f0-10f0f2,u+10f0f4,u+10f0f9-10f0fe,u+10f100-10f105,u+10f107,u+10f10a-10f10b,u+10f10d,u+10f118-10f11b,u+10f120,u+10f122,u+10f124-10f125,u+10f127,u+10f12b,u+10f134,u+10f137-10f13a,u+10f13e,u+10f141-10f144,u+10f146,u+10f148-10f14d,u+10f150-10f154,u+10f156-10f157,u+10f381-10f382}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-15.ttf) format("truetype");unicode-range:u+f158-f159,u+f15c-f15e,u+f160-f163,u+f165,u+f175-f178,u+f182-f183,u+f185,u+f187,u+f191-f193,u+f195,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1c4-f1c9,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e3,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221-f22c,u+f235-f236,u+f239,u+f240-f245,u+f247-f249,u+f24e,u+f252-f258,u+10f158-10f159,u+10f15c-10f15e,u+10f160-10f163,u+10f165,u+10f175-10f178,u+10f182-10f183,u+10f185,u+10f187,u+10f191-10f193,u+10f195,u+10f197,u+10f199,u+10f19c-10f19d,u+10f1ac,u+10f1b0,u+10f1b3,u+10f1b9-10f1ba,u+10f1c4-10f1c9,u+10f1cd,u+10f1da,u+10f1dd-10f1de,u+10f1e0-10f1e3,u+10f1f6,u+10f1fb-10f1fe,u+10f200-10f201,u+10f204,u+10f206-10f207,u+10f20b,u+10f218,u+10f21e,u+10f221-10f22c,u+10f235-10f236,u+10f239,u+10f240-10f245,u+10f247-10f249,u+10f24e,u+10f252-10f258}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-16.ttf) format("truetype");unicode-range:u+f259-f25c,u+f26c,u+f271-f274,u+f276-f277,u+f279-f27a,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c1-f2c2,u+f2c7-f2cb,u+f2ce,u+f2d0-f2d1,u+f2d3,u+f2dc,u+f2e2-f2eb,u+f2ed-f2ee,u+f2f0-f2f6,u+f2f8-f2fb,u+f2fd-f2fe,u+f300-f301,u+f304-f315,u+f317-f318,u+f4e6,u+f8e5,u+10f259-10f25c,u+10f26c,u+10f271-10f274,u+10f276-10f277,u+10f279-10f27a,u+10f28b,u+10f28d,u+10f290-10f291,u+10f29a,u+10f29d,u+10f2a1-10f2a4,u+10f2a8,u+10f2b6,u+10f2b9,u+10f2bd,u+10f2c1-10f2c2,u+10f2c7-10f2cb,u+10f2ce,u+10f2d0-10f2d1,u+10f2d3,u+10f2dc,u+10f2e2-10f2eb,u+10f2ed-10f2ee,u+10f2f0-10f2f6,u+10f2f8-10f2fb,u+10f2fd-10f2fe,u+10f300-10f301,u+10f304-10f315,u+10f317-10f318,u+10f4e6,u+10f8e5}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-17.ttf) format("truetype");unicode-range:u+f319,u+f31c-f31e,u+f320-f327,u+f329-f32e,u+f330-f331,u+f333-f334,u+f336-f33e,u+f340-f34e,u+f350-f35b,u+f35d,u+f360-f362,u+f364-f367,u+f376-f377,u+f386-f387,u+f389-f38a,u+f39b-f39c,u+f3a0,u+f3a5,u+f3b3,u+f3be-f3bf,u+f3c1-f3c2,u+f3c9,u+f3cd,u+f3cf,u+f3d1,u+f3dd-f3de,u+f3e0,u+f3ed,u+f3f0,u+10f319,u+10f31c-10f31e,u+10f320-10f327,u+10f329-10f32e,u+10f330-10f331,u+10f333-10f334,u+10f336-10f33e,u+10f340-10f34e,u+10f350-10f35b,u+10f35d,u+10f360-10f362,u+10f364-10f367,u+10f376-10f377,u+10f386-10f387,u+10f389-10f38a,u+10f39b-10f39c,u+10f3a0,u+10f3a5,u+10f3b3,u+10f3be-10f3bf,u+10f3c1-10f3c2,u+10f3c9,u+10f3cd,u+10f3cf,u+10f3d1,u+10f3dd-10f3de,u+10f3e0,u+10f3ed,u+10f3f0}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-18.ttf) format("truetype");unicode-range:u+f3f1-f3f2,u+f3fa,u+f3fc,u+f3ff-f401,u+f406,u+f40f-f410,u+f422,u+f424,u+f432-f434,u+f436-f44a,u+f44c,u+f44e-f451,u+f453-f456,u+f458,u+f45a-f47c,u+f47e-f480,u+f482-f485,u+f4a1,u+10f3f1-10f3f2,u+10f3fa,u+10f3fc,u+10f3ff-10f401,u+10f406,u+10f40f-10f410,u+10f422,u+10f424,u+10f432-10f434,u+10f436-10f44a,u+10f44c,u+10f44e-10f451,u+10f453-10f456,u+10f458,u+10f45a-10f47c,u+10f47e-10f480,u+10f482-10f485,u+10f4a1}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-19.ttf) format("truetype");unicode-range:u+f486-f489,u+f48b-f48e,u+f491-f492,u+f495-f497,u+f499-f4a0,u+f4a2-f4b6,u+f4b8-f4c9,u+f4cb,u+f4cd-f4d0,u+f4d2-f4d4,u+f4d6,u+f4d9-f4e1,u+f4e3,u+f4fa-f502,u+f504-f505,u+10f486-10f489,u+10f48b-10f48e,u+10f491-10f492,u+10f495-10f497,u+10f499-10f4a0,u+10f4a2-10f4b6,u+10f4b8-10f4c9,u+10f4cb,u+10f4cd-10f4d0,u+10f4d2-10f4d4,u+10f4d6,u+10f4d9-10f4e1,u+10f4e3,u+10f4fa-10f502,u+10f504-10f505}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-20.ttf) format("truetype");unicode-range:u+f506-f507,u+f509,u+f515-f51a,u+f51c,u+f51f-f520,u+f523-f52a,u+f52d-f52f,u+f532-f534,u+f537-f53d,u+f53f-f540,u+f542,u+f546-f547,u+f54a-f54d,u+f54f-f554,u+f556-f558,u+f55a-f55c,u+f55e-f563,u+f565-f56b,u+f56d,u+f571-f576,u+f579-f588,u+10f506-10f507,u+10f509,u+10f515-10f51a,u+10f51c,u+10f51f-10f520,u+10f523-10f52a,u+10f52d-10f52f,u+10f532-10f534,u+10f537-10f53d,u+10f53f-10f540,u+10f542,u+10f546-10f547,u+10f54a-10f54d,u+10f54f-10f554,u+10f556-10f558,u+10f55a-10f55c,u+10f55e-10f563,u+10f565-10f56b,u+10f56d,u+10f571-10f576,u+10f579-10f588}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-21.ttf) format("truetype");unicode-range:u+f589-f58f,u+f591,u+f593,u+f596-f59d,u+f59f-f5a0,u+f5a4-f5a7,u+f5a9,u+f5ac-f5af,u+f5b1,u+f5b3-f5b4,u+f5b6,u+f5b8-f5b9,u+f5bb-f5bd,u+f5c0-f5c5,u+f5c7-f5c9,u+f5cb,u+f5cd-f5ce,u+f5d0-f5da,u+f5dd-f5ee,u+f5f0,u+f5f3-f5f5,u+f5f8-f5f9,u+f5fc,u+f5fe-f602,u+10f589-10f58f,u+10f591,u+10f593,u+10f596-10f59d,u+10f59f-10f5a0,u+10f5a4-10f5a7,u+10f5a9,u+10f5ac-10f5af,u+10f5b1,u+10f5b3-10f5b4,u+10f5b6,u+10f5b8-10f5b9,u+10f5bb-10f5bd,u+10f5c0-10f5c5,u+10f5c7-10f5c9,u+10f5cb,u+10f5cd-10f5ce,u+10f5d0-10f5da,u+10f5dd-10f5ee,u+10f5f0,u+10f5f3-10f5f5,u+10f5f8-10f5f9,u+10f5fc,u+10f5fe-10f602}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-22.ttf) format("truetype");unicode-range:u+f0e4,u+f3fd,u+f603-f60e,u+f610-f611,u+f613-f620,u+f622-f63a,u+f63c-f63e,u+f640-f641,u+f643-f648,u+f64b-f64e,u+f650-f652,u+f655,u+f657-f65a,u+f65c-f663,u+f665-f668,u+10f0e4,u+10f3fd,u+10f603-10f60e,u+10f610-10f611,u+10f613-10f620,u+10f622-10f63a,u+10f63c-10f63e,u+10f640-10f641,u+10f643-10f648,u+10f64b-10f64e,u+10f650-10f652,u+10f655,u+10f657-10f65a,u+10f65c-10f663,u+10f665-10f668}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-23.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-23.ttf) format("truetype");unicode-range:u+f669-f672,u+f674,u+f676-f67c,u+f67e,u+f680-f694,u+f696-f69b,u+f69e,u+f6a0-f6a7,u+f6a9-f6ae,u+f6b0-f6be,u+f6c1-f6c8,u+f6cb,u+f6cd-f6d1,u+10f669-10f672,u+10f674,u+10f676-10f67c,u+10f67e,u+10f680-10f694,u+10f696-10f69b,u+10f69e,u+10f6a0-10f6a7,u+10f6a9-10f6ae,u+10f6b0-10f6be,u+10f6c1-10f6c8,u+10f6cb,u+10f6cd-10f6d1}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-24.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-24.ttf) format("truetype");unicode-range:u+f6d2-f6d4,u+f6d6-f6db,u+f6dd-f6e2,u+f6e4-f6e8,u+f6ea-f6f4,u+f6f6-f6f9,u+f6fb-f6fe,u+f701-f703,u+f705-f70a,u+f70c-f70d,u+f70f-f714,u+f716-f71a,u+f71c-f72d,u+f732,u+f735-f73e,u+10f6d2-10f6d4,u+10f6d6-10f6db,u+10f6dd-10f6e2,u+10f6e4-10f6e8,u+10f6ea-10f6f4,u+10f6f6-10f6f9,u+10f6fb-10f6fe,u+10f701-10f703,u+10f705-10f70a,u+10f70c-10f70d,u+10f70f-10f714,u+10f716-10f71a,u+10f71c-10f72d,u+10f732,u+10f735-10f73e}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-25.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-25.ttf) format("truetype");unicode-range:u+f73f-f746,u+f748-f754,u+f756,u+f758-f75b,u+f75e-f761,u+f763-f772,u+f774-f777,u+f779,u+f77d-f780,u+f782-f783,u+f786-f787,u+f78a-f78c,u+f78e-f78f,u+f792-f796,u+f79a-f7ae,u+10f73f-10f746,u+10f748-10f754,u+10f756,u+10f758-10f75b,u+10f75e-10f761,u+10f763-10f772,u+10f774-10f777,u+10f779,u+10f77d-10f780,u+10f782-10f783,u+10f786-10f787,u+10f78a-10f78c,u+10f78e-10f78f,u+10f792-10f796,u+10f79a-10f7ae}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-26.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-26.ttf) format("truetype");unicode-range:u+f7b4-f7b5,u+f7b7-f7ba,u+f7be-f7c5,u+f7c7-f7d2,u+f7d4,u+f7d7-f7de,u+f7e2,u+f7e4-f7ed,u+f7ef-f7fe,u+f800,u+f802-f803,u+f805-f809,u+f80d-f812,u+f815-f822,u+10f7b4-10f7b5,u+10f7b7-10f7ba,u+10f7be-10f7c5,u+10f7c7-10f7d2,u+10f7d4,u+10f7d7-10f7de,u+10f7e2,u+10f7e4-10f7ed,u+10f7ef-10f7fe,u+10f800,u+10f802-10f803,u+10f805-10f809,u+10f80d-10f812,u+10f815-10f822}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-27.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-27.ttf) format("truetype");unicode-range:u+f823-f82e,u+f831-f833,u+f83e,u+f843-f844,u+f847-f84f,u+f851-f854,u+f856-f857,u+f85a-f85b,u+f85d-f865,u+f867-f86c,u+f86e-f870,u+f872-f874,u+f876-f892,u+f895-f896,u+f898-f89a,u+10f823-10f82e,u+10f831-10f833,u+10f83e,u+10f843-10f844,u+10f847-10f84f,u+10f851-10f854,u+10f856-10f857,u+10f85a-10f85b,u+10f85d-10f865,u+10f867-10f86c,u+10f86e-10f870,u+10f872-10f874,u+10f876-10f892,u+10f895-10f896,u+10f898-10f89a}@font-face{font-family:"Font Awesome 5 Duotone";font-display:block;font-weight:900;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-28.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-duotone-900-28.ttf) format("truetype");unicode-range:u+f89b-f89d,u+f8a0-f8a5,u+f8a7-f8a8,u+f8aa-f8b0,u+f8b3-f8ba,u+f8bd-f8c6,u+f8c8-f8c9,u+f8cb-f8d1,u+f8d3-f8d5,u+f8d8,u+f8da-f8de,u+f8e2-f8e4,u+f8e6,u+f8e9-f8ed,u+f8f0-f8fc,u+f8fe-f8ff,u+10f89b-10f89d,u+10f8a0-10f8a5,u+10f8a7-10f8a8,u+10f8aa-10f8b0,u+10f8b3-10f8ba,u+10f8bd-10f8c6,u+10f8c8-10f8c9,u+10f8cb-10f8d1,u+10f8d3-10f8d5,u+10f8d8,u+10f8da-10f8de,u+10f8e2-10f8e4,u+10f8e6,u+10f8e9-10f8ed,u+10f8f0-10f8fc,u+10f8fe-10f8ff}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-0.ttf) format("truetype");unicode-range:u+f001,u+f004-f005,u+f007-f008,u+f00c,u+f011-f012,u+f015,u+f018-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d,u+f04a-f04e,u+f05b,u+f060-f064,u+f067-f068,u+f06b-f06d,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f091,u+f093,u+f095,u+f09c,u+f0a3,u+f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c6,u+f0c8,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-1.ttf) format("truetype");unicode-range:u+f040,u+f0c9,u+f0cc,u+f0ce,u+f0d1,u+f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0f3,u+f106,u+f108-f109,u+f10e,u+f110-f112,u+f11e,u+f121,u+f126,u+f129,u+f12c-f12e,u+f130-f132,u+f135,u+f13d,u+f140,u+f145,u+f15b,u+f164,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb,u+f1c0,u+f1ce,u+f1d8,u+f1dc,u+f1e4-f1e6,u+f1eb-f1ec,u+f205,u+f217,u+f21a-f21d,u+f22d,u+f233-f234,u+f238,u+f246,u+f251,u+f275,u+f29e,u+f2a0,u+f2a7,u+f2bb,u+f2cc-f2cd,u+f2db}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-3.ttf) format("truetype");unicode-range:u+f069,u+f1fa}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-12.ttf) format("truetype");unicode-range:u+f000,u+f002,u+f009-f00b,u+f00d-f00e,u+f010,u+f013,u+f01e,u+f021,u+f026-f029,u+f02b,u+f032-f039,u+f03b-f03c,u+f042-f043,u+f048-f049,u+f050-f05a,u+f05e,u+f06a,u+f071,u+f073-f074,u+f076,u+f079-f07a,u+f07c,u+f083,u+f085,u+f089,u+f08d,u+f098,u+f09e,u+f0a1,u+f0a8-f0ab,u+f0c3-f0c4,u+f0ca-f0cb,u+f0cd,u+f0d8}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-13.ttf) format("truetype");unicode-range:u+f045,u+f0d9-f0db,u+f0dd-f0de,u+f0e2-f0e3,u+f0e9-f0ea,u+f0ed-f0ee,u+f0f0-f0f2,u+f0f4,u+f0f9-f0fe,u+f100-f105,u+f107,u+f10d,u+f11b,u+f120,u+f122,u+f124-f125,u+f127,u+f12b,u+f134,u+f137-f13a,u+f141-f144,u+f146,u+f14a-f14b,u+f14d,u+f153-f154,u+f157-f159,u+f15c-f15d,u+f162,u+f165,u+f182-f183,u+f187,u+f193,u+f197,u+f199,u+f19c-f19d,u+f1ac,u+f1b0,u+f1b3,u+f1b9-f1ba,u+f1cd,u+f1da,u+f1dd-f1de,u+f1e0-f1e2,u+f1f6,u+f1fb-f1fe,u+f200-f201,u+f204,u+f206-f207,u+f20b,u+f218,u+f21e,u+f221}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-solid-900-14.ttf) format("truetype");unicode-range:u+f0f5,u+f1b1,u+f222-f22c,u+f235-f236,u+f239,u+f240-f245,u+f249,u+f24e,u+f250,u+f252-f254,u+f25c,u+f26c,u+f276-f277,u+f279,u+f28b,u+f28d,u+f290-f291,u+f29a,u+f29d,u+f2a1-f2a4,u+f2a8,u+f2b6,u+f2b9,u+f2bd,u+f2c2,u+f2c7-f2cb,u+f2ce,u+f2d1,u+f2d3}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-0.ttf) format("truetype");unicode-range:u+f081-f082,u+f08c,u+f092,u+f099,u+f09b,u+f0d2-f0d5,u+f0e1,u+f113,u+f136,u+f13b-f13c,u+f15a,u+f167-f16e,u+f170-f174,u+f179-f17e,u+f180-f181,u+f184,u+f189-f18d,u+f194,u+f198,u+f19a-f19b,u+f19e,u+f1a0-f1a9,u+f1b4,u+f1bc,u+f1be,u+f1e8,u+f1ed,u+f1f0-f1f1,u+f20e,u+f210,u+f213-f214,u+f230,u+f232,u+f23a,u+f26b,u+f270,u+f288,u+f299,u+f29b,u+f2a6,u+f2b0,u+f2c5-f2c6,u+f2e0}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-1.ttf) format("truetype");unicode-range:u+f1aa,u+f1b5-f1b7,u+f1bd,u+f1ca-f1cc,u+f1d0-f1d7,u+f1e7,u+f1e9,u+f1ee,u+f1f2-f1f5,u+f202-f203,u+f208-f209,u+f20d,u+f211-f212,u+f215-f216,u+f231,u+f237,u+f23b-f23e,u+f24b-f24c,u+f25e,u+f260-f261,u+f263-f26a,u+f26d-f26e,u+f27c-f27e,u+f280-f282,u+f284-f287,u+f289-f28a,u+f293-f294,u+f296-f298,u+f2a5,u+f2a9-f2ae,u+f2b1-f2b4,u+f2b8,u+f2c4,u+f2d5-f2da,u+f2dd-f2de}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-brands-400-2.ttf) format("truetype");unicode-range:u+f166}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-0.ttf) format("truetype");unicode-range:u+f006,u+f017,u+f03e,u+f06e,u+f08a,u+f096-f097,u+f09d,u+f0a6,u+f0c5,u+f0e5-f0e6,u+f114,u+f11d,u+f2c0}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-1.ttf) format("truetype");unicode-range:u+f003,u+f016,u+f087,u+f0a2,u+f0eb,u+f0f7-f0f8,u+f10c,u+f11c,u+f133,u+f14e,u+f186,u+f1c1-f1c3,u+f1d9,u+f1db,u+f1ea,u+f1f9,u+f20a,u+f24d,u+f25d,u+f2b5,u+f2bc,u+f2d2}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-12.ttf) format("truetype");unicode-range:u+f022,u+f044,u+f05c-f05d,u+f070,u+f094,u+f0a0,u+f0a4-f0a5,u+f0a7,u+f0c7,u+f115,u+f29c}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-13.ttf) format("truetype");unicode-range:u+f01d,u+f046,u+f088,u+f0f6,u+f118-f11a,u+f147,u+f150-f152,u+f185,u+f191-f192,u+f196,u+f1c4-f1c9,u+f1e3,u+f1f7}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-14.ttf) format("truetype");unicode-range:u+f014,u+f247-f248,u+f24a,u+f255-f25b,u+f271-f274,u+f278,u+f28c,u+f28e,u+f2b7,u+f2ba,u+f2be,u+f2c1,u+f2c3,u+f2d0,u+f2dc}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-15.ttf) format("truetype");unicode-range:u+f01a-f01b,u+f18e,u+f190,u+f2d4}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-regular-400-16.ttf) format("truetype");unicode-range:u+f27b}@font-face{font-family:"FontAwesome";font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-v4compatibility.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-v4compatibility.ttf) format("truetype");unicode-range:u+f041,u+f047,u+f065-f066,u+f07d-f07e,u+f080,u+f08b,u+f08e,u+f090,u+f09a,u+f0ac,u+f0ae,u+f0b2,u+f0d0,u+f0d6,u+f0e4,u+f0ec,u+f10a-f10b,u+f123,u+f13e,u+f148-f149,u+f14c,u+f156,u+f15e,u+f160-f161,u+f163,u+f175-f178,u+f195,u+f1f8,u+f219,u+f27a}

/*!
 * Font Awesome Pro 6.5.2 by @fontawesome - https://fontawesome.com
 * License - https://fontawesome.com/license (Commercial License)
 * Copyright 2024 Fonticons, Inc.
 */:host,:root{--fa-font-sharp-solid:normal 900 1em/1 "Font Awesome 6 Sharp"}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:900;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-solid-900-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-solid,.fass{font-weight:900}

/*!
 * Font Awesome Pro 6.5.2 by @fontawesome - https://fontawesome.com
 * License - https://fontawesome.com/license (Commercial License)
 * Copyright 2024 Fonticons, Inc.
 */:host,:root{--fa-font-sharp-regular:normal 400 1em/1 "Font Awesome 6 Sharp"}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:400;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-regular-400-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-regular,.fasr{font-weight:400}

/*!
 * Font Awesome Pro 6.5.2 by @fontawesome - https://fontawesome.com
 * License - https://fontawesome.com/license (Commercial License)
 * Copyright 2024 Fonticons, Inc.
 */:host,:root{--fa-font-sharp-light:normal 300 1em/1 "Font Awesome 6 Sharp"}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:300;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-light-300-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-light,.fasl{font-weight:300}

/*!
 * Font Awesome Pro 6.5.2 by @fontawesome - https://fontawesome.com
 * License - https://fontawesome.com/license (Commercial License)
 * Copyright 2024 Fonticons, Inc.
 */:host,:root{--fa-style-family-sharp:"Font Awesome 6 Sharp";--fa-font-sharp-thin:normal 100 1em/1 "Font Awesome 6 Sharp"}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-22.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-22.ttf) format("truetype")}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-21.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-21.ttf) format("truetype");unicode-range:u+2015-20b8,u+2603,u+2698-26f7,u+e0cf-f87b,u+1f32d-1f334,u+1f336-1f384,u+1f3c2-1f477,u+1f4b0-1f4df,u+1f523,u+1f57d-1f595,u+1f6a1-1f6cd,u+1f950-1f9c0}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-20.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-20.ttf) format("truetype");unicode-range:u+2604,u+2620-2694,u+26c6-26e8,u+26f8,u+f711-f7c2,u+1f305-1f32b,u+1f40b-1f422,u+1f494,u+1f4e1,u+1f571,u+1f577-1f578,u+1f5e1-1f69c,u+1f6f0-1f943,u+1f955,u+1f97e-1f98c,u+1f9e3-1f9fb,u+1faa6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-19.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-19.ttf) format("truetype");unicode-range:u+20b4,u+2211-221a,u+22c3,u+262a-262f,u+26b0,u+26c5,u+26e9-26f0,u+26fa-2721,u+f670-f710,u+1f33d-1f341,u+1f357,u+1f3c3-1f3d4,u+1f3f9-1f408,u+1f40e-1f40f,u+1f412-1f418,u+1f47b,u+1f549-1f54e,u+1f56f,u+1f5dd,u+1f6d0,u+1f967,u+1f986,u+1f99b-1f9ae,u+1f9e6,u+1fa81-1fa93}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-18.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-18.ttf) format("truetype");unicode-range:u+22c2,u+2625,u+262c,u+2638,u+269b,u+f0e4-f66f,u+1f34e-1f34f,u+1f392,u+1f3ad-1f3af,u+1f3ca,u+1f3db,u+1f442-1f453,u+1f4c9-1f4d9,u+1f52c,u+1f54b,u+1f5c4,u+1f5e2,u+1f62b-1f697,u+1f6a6,u+1f6fb,u+1f9b4-1f9b7,u+1f9ee,u+1faa5,u+1fac1}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-17.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-17.ttf) format("truetype");unicode-range:u+f7,u+221e,u+232b,u+267e-2685,u+26fd,u+2711-2712,u+2797,u+f520-f5c2,u+1f17f,u+1f30d-1f30f,u+1f36a-1f378,u+1f3a8,u+1f455,u+1f480,u+1f58b,u+1f600-1f626,u+1f62c-1f690,u+1f6aa-1f6ad,u+1f6b6,u+1f6ce,u+1f6ec,u+1f923-1f941,u+1f9f0,u+1fa9b,u+1fab6}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-16.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-16.ttf) format("truetype");unicode-range:u+24bd,u+2b1b-2b1c,u+e207,u+f27b,u+f45a-f51f,u+1f377,u+1f397,u+1f3be,u+1f3d0-1f3d3,u+1f465,u+1f489,u+1f49f,u+1f4ac,u+1f4bf-1f4c0,u+1f4d6,u+1f4e6,u+1f54a,u+1f56e,u+1f5b8,u+1f609,u+1f6ac,u+1f7e5-1f91d,u+1f977,u+1f9ea-1f9ec,u+1f9f9,u+1fa79}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-15.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-15.ttf) format("truetype");unicode-range:u+2194-21e5,u+23f0,u+265a-2663,u+26be,u+2708,u+27a1-2b0d,u+2b95,u+f01a-f01b,u+f18e-f190,u+f2d4-f3fc,u+f3ff-f458,u+1f333,u+1f399,u+1f3c0,u+1f3c8,u+1f3cf,u+1f3d1,u+1f3ed,u+1f48e,u+1f4fa,u+1f502,u+1f6a9,u+1f94a-1f94e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-14.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-14.ttf) format("truetype");unicode-range:u+2122-2139,u+231b,u+23f1-23f3,u+25ac,u+263f-2642,u+2660,u+2696,u+26a2-26a9,u+26b2,u+270b-270c,u+2744-274e,u+2b23,u+f014,u+f0f5,u+f1b1-f27a,u+f28b-f2d3,u+f2dc-f31c,u+f425,u+f4e6,u+f8e5,u+1f321,u+1f374,u+1f383,u+1f4a9,u+1f4cd,u+1f4dd,u+1f4e4-1f4e5,u+1f504-1f50b,u+1f58a,u+1f596,u+1f5d5-1f5d6,u+1f5fa,u+1f6cc,u+1f6d1,u+1f91a,u+1f944}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-13.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-13.ttf) format("truetype");unicode-range:u+a3-bb,u+201c-20ac,u+20bd,u+21ba-21c4,u+2304,u+2600,u+2611,u+2639,u+2640,u+26bd,u+2705,u+f01d-f0e3,u+f0e9-f0f4,u+f0f6-f187,u+f191-f1b0,u+f1b3-f221,u+f381-f382,u+1f382,u+1f393,u+1f44e,u+1f4a3,u+1f4e0,u+1f515-1f518,u+1f58c,u+1f5b7,u+1f5b9-1f5bb,u+1f5ce,u+1f610,u+1f642,u+1f68d,u+1f691-1f696,u+1f698,u+1f6b2,u+1f9cd,u+1f9ef,u+1f9f3,u+1fa7a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-12.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-12.ttf) format("truetype");unicode-range:u+d7,u+21bb,u+2329-232a,u+23cf-23ee,u+25d0,u+2699,u+26a0,u+2700-2704,u+2715-2716,u+274c,u+e647-f013,u+f01e-f044,u+f047-f085,u+f089-f0d8,u+f115,u+f123,u+f29c,u+1f34b,u+1f3f7,u+1f4a7,u+1f4be,u+1f4c2,u+1f4cc,u+1f4e2,u+1f4f7,u+1f500,u+1f508-1f50a,u+1f50d,u+1f56b,u+1f588,u+1f5aa,u+1f5b4,u+1f5c1,u+1f5d8-1f5d9,u+1f6ab,u+1f6d2,u+1f9f2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-11.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-11.ttf) format("truetype");unicode-range:u+203d,u+e574-e646,u+f8bc,u+1f3ae,u+1f68a}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-10.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-10.ttf) format("truetype");unicode-range:u+e4ba-e573}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-9.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-9.ttf) format("truetype");unicode-range:u+e41e-e4b9,u+e4ec,u+e4ee,u+e550,u+e559,u+1f344,u+1f363,u+1f36d-1f373,u+1f92d,u+1f954,u+1f958,u+1f95c-1f95e,u+1f968,u+1f979,u+1f990-1f991,u+1f99e,u+1f9c2-1f9c7,u+1fad1-1fae5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-8.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-8.ttf) format("truetype");unicode-range:u+e383-e41d,u+f80b,u+1f330,u+1f345,u+1f366-1f367,u+1f369,u+1f36b-1f36c,u+1f36e-1f370,u+1f4af,u+1f952,u+1f956,u+1f959,u+1f963-1f966,u+1f96b-1f96f,u+1f980,u+1f9c1,u+1f9c4,u+1f9c6,u+1f9c8,u+1fad3,u+1fad5}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-7.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-7.ttf) format("truetype");unicode-range:u+e29f-e382,u+1f964,u+1fae2}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-6.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-6.ttf) format("truetype");unicode-range:u+e1e5-e206,u+e208-e29e}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-5.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-5.ttf) format("truetype");unicode-range:u+e12d-e1e4}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-4.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-4.ttf) format("truetype");unicode-range:u+e061-e0ce,u+e0d0-e12c}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-3.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-3.ttf) format("truetype");unicode-range:u+22-7e,u+2731,u+e000-e05f,u+f069,u+f1fa,u+f52c,u+f531,u+f536,u+f69f,u+f8df-f8e0,u+f8e7,u+f8ee-f8ef,u+f8fd,u+1f320,u+1f52d,u+1f680,u+1f6b0,u+1f6f8,u+1fa9f}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-2.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-2.ttf) format("truetype");unicode-range:u+d8,u+2205,u+2615,u+26ea,u+271d,u+273f,u+f44b,u+f481,u+f48a,u+f48f-f490,u+f493-f494,u+f498,u+f4b7,u+f4ca,u+f4cc,u+f4d1,u+f4d7-f4d8,u+f4e2,u+f503,u+f508,u+f51b,u+f51d-f51e,u+f521-f522,u+f52b,u+f530,u+f535,u+f53e,u+f543-f545,u+f548-f549,u+f54e,u+f555,u+f559,u+f55d,u+f564,u+f56c,u+f56e-f570,u+f577-f578,u+f590,u+f594-f595,u+f5a1-f5a2,u+f5aa-f5ab,u+f5b0,u+f5b7,u+f5ba,u+f5bf,u+f5ca,u+f5db-f5dc,u+f5ef,u+f5f2,u+f5f6,u+f5fb,u+f5fd,u+f621,u+f63b,u+f649-f64a,u+f64f,u+f653-f654,u+f656,u+f65b,u+f664,u+f673,u+f675,u+f67d,u+f67f,u+f695,u+f69c,u+f6a8,u+f6bf-f6c0,u+f6d5,u+f6e3,u+f6e9,u+f6f5,u+f6fa,u+f6ff-f700,u+f70b,u+f70e,u+f715,u+f71b,u+f72e-f72f,u+f733-f734,u+f747,u+f755,u+f757,u+f75c,u+f762,u+f773,u+f77c,u+f781,u+f784,u+f788,u+f7b2,u+f7b6,u+f7bd,u+f7d5,u+f7ee,u+f7ff,u+f801,u+f804,u+f813-f814,u+f82f-f830,u+f845-f846,u+f850,u+f855,u+f858-f859,u+f85c,u+f866,u+f86d,u+f871,u+f875,u+f893-f894,u+f897,u+f89f,u+f8a9,u+f8b1-f8b2,u+f8bb,u+f8c7,u+f8d6-f8d7,u+f8d9,u+1f331,u+1f337-1f33c,u+1f356,u+1f3a5-1f3a6,u+1f3b2,u+1f3c5,u+1f3d6-1f3d9,u+1f3e8-1f3eb,u+1f409,u+1f41f,u+1f451,u+1f4bd,u+1f4cf,u+1f4da-1f4dc,u+1f4e3,u+1f4ea,u+1f4fb,u+1f509,u+1f528-1f52a,u+1f547,u+1f6eb,u+1f916,u+1f95b,u+1f9a6,u+1f9e0,u+1f9fe,u+1fa91,u+1fa99}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-1.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-1.ttf) format("truetype");unicode-range:u+a9,u+ae,u+201d,u+2303,u+231a,u+2328,u+23fe,u+25b2-25cf,u+2666-267b,u+2693,u+26a1,u+26aa-26ab,u+26df,u+2709,u+270f,u+2b24,u+f003,u+f016,u+f040,u+f087,u+f0a2,u+f0c9,u+f0cc,u+f0ce,u+f0d1-f0d7,u+f0dc,u+f0e0,u+f0e7-f0e8,u+f0eb,u+f0f3,u+f0f7-f0f8,u+f106,u+f108-f109,u+f10c,u+f10e-f112,u+f11c-f11e,u+f121,u+f126,u+f129,u+f12c-f133,u+f135,u+f13d,u+f140,u+f145,u+f14e,u+f15b,u+f164,u+f186,u+f188,u+f1ab,u+f1ad-f1ae,u+f1b2,u+f1b8,u+f1bb-f1c3,u+f1ce-f1d9,u+f1db-f1dc,u+f1e4-f1ec,u+f1f8-f1f9,u+f205,u+f20a,u+f217,u+f219-f21d,u+f22d-f234,u+f238,u+f246,u+f24d,u+f251,u+f25d,u+f275,u+f29e-f2a0,u+f2a7,u+f2b5,u+f2bb-f2bc,u+f2cc-f2cd,u+f2d2,u+f2db,u+f2e1,u+f2ec,u+f2f7,u+f2fc,u+f302-f303,u+f316,u+f31a,u+f328,u+f335,u+f363,u+f37e,u+f390,u+f3c5,u+f3ce,u+f3e5,u+f3f4,u+f3fb,u+f40e,u+f435,u+f47d,u+1f319,u+1f332,u+1f39f,u+1f3c1,u+1f3cd,u+1f3e2-1f3e5,u+1f44d,u+1f4a1,u+1f4bb,u+1f4c4-1f4c6,u+1f4cb,u+1f4f0-1f4f1,u+1f501,u+1f50c,u+1f514,u+1f534-1f535,u+1f575,u+1f582,u+1f5a5-1f5a9,u+1f5cb,u+1f5d4,u+1f686,u+1f69a,u+1f6a2,u+1f6bf-1f6c1,u+1f6e1,u+1f7e0-1f7e4,u+1f9e9,u+1f9ed}@font-face{font-family:"Font Awesome 6 Sharp";font-style:normal;font-weight:100;font-display:block;src:url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-0.woff2) format("woff2"),url(https://ka-p.fontawesome.com/releases/v6.5.2/webfonts/pro-fa-sharp-thin-100-0.ttf) format("truetype");unicode-range:u+21,u+23-25,u+2b,u+3f,u+2013,u+2190-2193,u+2212,u+2399,u+23e9-23ea,u+23f8-23fb,u+25a0,u+25b6,u+25fb-25fc,u+2601,u+261d,u+2665,u+2713-2714,u+2753-2796,u+2b50,u+e010,u+e017,u+e01b,u+e01f-e021,u+e024,u+e02f,u+e03a,u+e042,u+e045-e046,u+e060,u+e068,u+e06e,u+e074,u+e076,u+f001,u+f004-f008,u+f00c,u+f011-f012,u+f015,u+f017-f019,u+f01c,u+f023-f025,u+f02a,u+f02c-f031,u+f03a,u+f03d-f03e,u+f041,u+f04a-f04e,u+f05b,u+f060-f065,u+f067-f068,u+f06b-f06e,u+f072,u+f075,u+f077-f078,u+f07b,u+f084,u+f086,u+f08a,u+f091-f093,u+f095-f097,u+f09c-f09d,u+f0a3,u+f0a6,u+f0ac-f0ad,u+f0b0-f0b1,u+f0c0-f0c2,u+f0c5-f0c6,u+f0c8,u+f0e5-f0e6,u+f114,u+f11d,u+f128,u+f12a,u+f155,u+f283,u+f292,u+f295,u+f2c0,u+f332,u+f541,u+f80a,u+f80c,u+1f310,u+1f381,u+1f39e,u+1f3a7,u+1f3b5,u+1f3c6,u+1f3e0,u+1f3f4,u+1f441,u+1f464,u+1f499-1f49c,u+1f4b2-1f4b3,u+1f4bc,u+1f4c1,u+1f4ce,u+1f4d4,u+1f4de,u+1f4f6,u+1f511-1f513,u+1f516-1f517,u+1f525,u+1f527,u+1f553,u+1f57b,u+1f5a4,u+1f5a8,u+1f5b6,u+1f5bf,u+1f5e9-1f5ea,u+1f6e3,u+1f90d-1f90e,u+1f9e1,u+1f9fc,u+1fa90}.fa-thin,.fast{font-weight:100}</style>
    <script src="./2024_files/bootstrap-select.min.js.下载"></script>


    <style>
        body {
            font-family: Exo;}
    </style>







     

    
    <link rel="stylesheet" href="./2024_files/css">
    <link rel="stylesheet" href="./2024_files/css(1)">
    <link href="./2024_files/css2" rel="stylesheet">
    <link rel="stylesheet" href="./2024_files/virtual.css">
    <script src="./2024_files/d3.v5.min.js.下载"></script>
    <script src="./2024_files/typeahead.bundle.min.js.下载" integrity="sha512-lEb9Vp/rkl9g2E/LdHIMFTqz21+LA79f84gqP75fbimHqVTu6483JG1AwJlWLLQ8ezTehty78fObKupq3HSHPQ==" crossorigin="anonymous"></script>
    <script src="./2024_files/moment.min.js.下载" integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>
    <script src="./2024_files/js.cookie.min.js.下载"></script>
    <script src="./2024_files/ajax-csrf-snippet.js.下载" type="text/javascript"></script>
    <script src="./2024_files/virtual.js.下载"></script>
    

    <title>CVPR 2024 Accepted Papers</title>
    
    <link rel="canonical" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers">

<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 2px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 2px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: 1em}
.MathJax_MenuRadioCheck.RTL {right: 1em; left: auto}
.MathJax_MenuLabel {padding: 2px 2em 4px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #CCCCCC; margin: 4px 1px 0px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: Highlight; color: HighlightText}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.mjx-chtml {display: inline-block; line-height: 0; text-indent: 0; text-align: left; text-transform: none; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; letter-spacing: normal; word-wrap: normal; word-spacing: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; margin: 0; padding: 1px 0}
.MJXc-display {display: block; text-align: center; margin: 1em 0; padding: 0}
.mjx-chtml[tabindex]:focus, body :focus .mjx-chtml[tabindex] {display: inline-table}
.mjx-full-width {text-align: center; display: table-cell!important; width: 10000em}
.mjx-math {display: inline-block; border-collapse: separate; border-spacing: 0}
.mjx-math * {display: inline-block; -webkit-box-sizing: content-box!important; -moz-box-sizing: content-box!important; box-sizing: content-box!important; text-align: left}
.mjx-numerator {display: block; text-align: center}
.mjx-denominator {display: block; text-align: center}
.MJXc-stacked {height: 0; position: relative}
.MJXc-stacked > * {position: absolute}
.MJXc-bevelled > * {display: inline-block}
.mjx-stack {display: inline-block}
.mjx-op {display: block}
.mjx-under {display: table-cell}
.mjx-over {display: block}
.mjx-over > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-under > * {padding-left: 0px!important; padding-right: 0px!important}
.mjx-stack > .mjx-sup {display: block}
.mjx-stack > .mjx-sub {display: block}
.mjx-prestack > .mjx-presup {display: block}
.mjx-prestack > .mjx-presub {display: block}
.mjx-delim-h > .mjx-char {display: inline-block}
.mjx-surd {vertical-align: top}
.mjx-mphantom * {visibility: hidden}
.mjx-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 2px 3px; font-style: normal; font-size: 90%}
.mjx-annotation-xml {line-height: normal}
.mjx-menclose > svg {fill: none; stroke: currentColor}
.mjx-mtr {display: table-row}
.mjx-mlabeledtr {display: table-row}
.mjx-mtd {display: table-cell; text-align: center}
.mjx-label {display: table-row}
.mjx-box {display: inline-block}
.mjx-block {display: block}
.mjx-span {display: inline}
.mjx-char {display: block; white-space: pre}
.mjx-itable {display: inline-table; width: auto}
.mjx-row {display: table-row}
.mjx-cell {display: table-cell}
.mjx-table {display: table; width: 100%}
.mjx-line {display: block; height: 0}
.mjx-strut {width: 0; padding-top: 1em}
.mjx-vsize {width: 0}
.MJXc-space1 {margin-left: .167em}
.MJXc-space2 {margin-left: .222em}
.MJXc-space3 {margin-left: .278em}
.mjx-chartest {display: block; visibility: hidden; position: absolute; top: 0; line-height: normal; font-size: 500%}
.mjx-chartest .mjx-char {display: inline}
.mjx-chartest .mjx-box {padding-top: 1000px}
.MJXc-processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MJXc-processed {display: none}
.mjx-test {display: block; font-style: normal; font-weight: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow: hidden; height: 1px}
.mjx-ex-box-test {position: absolute; width: 1px; height: 60ex}
.mjx-line-box-test {display: table!important}
.mjx-line-box-test span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
#MathJax_CHTML_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.mjx-chtml .mjx-noError {line-height: 1.2; vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
.MJXc-TeX-unknown-R {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: normal}
.MJXc-TeX-unknown-I {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: normal}
.MJXc-TeX-unknown-B {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: normal; font-weight: bold}
.MJXc-TeX-unknown-BI {font-family: STIXGeneral,'Cambria Math','Arial Unicode MS',serif; font-style: italic; font-weight: bold}
.MJXc-TeX-ams-R {font-family: MJXc-TeX-ams-R,MJXc-TeX-ams-Rw}
.MJXc-TeX-cal-B {font-family: MJXc-TeX-cal-B,MJXc-TeX-cal-Bx,MJXc-TeX-cal-Bw}
.MJXc-TeX-frak-R {font-family: MJXc-TeX-frak-R,MJXc-TeX-frak-Rw}
.MJXc-TeX-frak-B {font-family: MJXc-TeX-frak-B,MJXc-TeX-frak-Bx,MJXc-TeX-frak-Bw}
.MJXc-TeX-math-BI {font-family: MJXc-TeX-math-BI,MJXc-TeX-math-BIx,MJXc-TeX-math-BIw}
.MJXc-TeX-sans-R {font-family: MJXc-TeX-sans-R,MJXc-TeX-sans-Rw}
.MJXc-TeX-sans-B {font-family: MJXc-TeX-sans-B,MJXc-TeX-sans-Bx,MJXc-TeX-sans-Bw}
.MJXc-TeX-sans-I {font-family: MJXc-TeX-sans-I,MJXc-TeX-sans-Ix,MJXc-TeX-sans-Iw}
.MJXc-TeX-script-R {font-family: MJXc-TeX-script-R,MJXc-TeX-script-Rw}
.MJXc-TeX-type-R {font-family: MJXc-TeX-type-R,MJXc-TeX-type-Rw}
.MJXc-TeX-cal-R {font-family: MJXc-TeX-cal-R,MJXc-TeX-cal-Rw}
.MJXc-TeX-main-B {font-family: MJXc-TeX-main-B,MJXc-TeX-main-Bx,MJXc-TeX-main-Bw}
.MJXc-TeX-main-I {font-family: MJXc-TeX-main-I,MJXc-TeX-main-Ix,MJXc-TeX-main-Iw}
.MJXc-TeX-main-R {font-family: MJXc-TeX-main-R,MJXc-TeX-main-Rw}
.MJXc-TeX-math-I {font-family: MJXc-TeX-math-I,MJXc-TeX-math-Ix,MJXc-TeX-math-Iw}
.MJXc-TeX-size1-R {font-family: MJXc-TeX-size1-R,MJXc-TeX-size1-Rw}
.MJXc-TeX-size2-R {font-family: MJXc-TeX-size2-R,MJXc-TeX-size2-Rw}
.MJXc-TeX-size3-R {font-family: MJXc-TeX-size3-R,MJXc-TeX-size3-Rw}
.MJXc-TeX-size4-R {font-family: MJXc-TeX-size4-R,MJXc-TeX-size4-Rw}
@font-face {font-family: MJXc-TeX-ams-R; src: local('MathJax_AMS'), local('MathJax_AMS-Regular')}
@font-face {font-family: MJXc-TeX-ams-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_AMS-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_AMS-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_AMS-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-B; src: local('MathJax_Caligraphic Bold'), local('MathJax_Caligraphic-Bold')}
@font-face {font-family: MJXc-TeX-cal-Bx; src: local('MathJax_Caligraphic'); font-weight: bold}
@font-face {font-family: MJXc-TeX-cal-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-R; src: local('MathJax_Fraktur'), local('MathJax_Fraktur-Regular')}
@font-face {font-family: MJXc-TeX-frak-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-frak-B; src: local('MathJax_Fraktur Bold'), local('MathJax_Fraktur-Bold')}
@font-face {font-family: MJXc-TeX-frak-Bx; src: local('MathJax_Fraktur'); font-weight: bold}
@font-face {font-family: MJXc-TeX-frak-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Fraktur-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Fraktur-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Fraktur-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-BI; src: local('MathJax_Math BoldItalic'), local('MathJax_Math-BoldItalic')}
@font-face {font-family: MJXc-TeX-math-BIx; src: local('MathJax_Math'); font-weight: bold; font-style: italic}
@font-face {font-family: MJXc-TeX-math-BIw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Math-BoldItalic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-BoldItalic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-BoldItalic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-R; src: local('MathJax_SansSerif'), local('MathJax_SansSerif-Regular')}
@font-face {font-family: MJXc-TeX-sans-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-B; src: local('MathJax_SansSerif Bold'), local('MathJax_SansSerif-Bold')}
@font-face {font-family: MJXc-TeX-sans-Bx; src: local('MathJax_SansSerif'); font-weight: bold}
@font-face {font-family: MJXc-TeX-sans-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-sans-I; src: local('MathJax_SansSerif Italic'), local('MathJax_SansSerif-Italic')}
@font-face {font-family: MJXc-TeX-sans-Ix; src: local('MathJax_SansSerif'); font-style: italic}
@font-face {font-family: MJXc-TeX-sans-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_SansSerif-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_SansSerif-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_SansSerif-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-script-R; src: local('MathJax_Script'), local('MathJax_Script-Regular')}
@font-face {font-family: MJXc-TeX-script-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Script-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Script-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Script-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-type-R; src: local('MathJax_Typewriter'), local('MathJax_Typewriter-Regular')}
@font-face {font-family: MJXc-TeX-type-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Typewriter-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Typewriter-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Typewriter-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-cal-R; src: local('MathJax_Caligraphic'), local('MathJax_Caligraphic-Regular')}
@font-face {font-family: MJXc-TeX-cal-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Caligraphic-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Caligraphic-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Caligraphic-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-B; src: local('MathJax_Main Bold'), local('MathJax_Main-Bold')}
@font-face {font-family: MJXc-TeX-main-Bx; src: local('MathJax_Main'); font-weight: bold}
@font-face {font-family: MJXc-TeX-main-Bw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Main-Bold.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Bold.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Bold.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-I; src: local('MathJax_Main Italic'), local('MathJax_Main-Italic')}
@font-face {font-family: MJXc-TeX-main-Ix; src: local('MathJax_Main'); font-style: italic}
@font-face {font-family: MJXc-TeX-main-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Main-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-main-R; src: local('MathJax_Main'), local('MathJax_Main-Regular')}
@font-face {font-family: MJXc-TeX-main-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Main-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Main-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Main-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-math-I; src: local('MathJax_Math Italic'), local('MathJax_Math-Italic')}
@font-face {font-family: MJXc-TeX-math-Ix; src: local('MathJax_Math'); font-style: italic}
@font-face {font-family: MJXc-TeX-math-Iw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Math-Italic.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Math-Italic.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Math-Italic.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size1-R; src: local('MathJax_Size1'), local('MathJax_Size1-Regular')}
@font-face {font-family: MJXc-TeX-size1-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Size1-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size1-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size1-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size2-R; src: local('MathJax_Size2'), local('MathJax_Size2-Regular')}
@font-face {font-family: MJXc-TeX-size2-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Size2-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size2-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size2-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size3-R; src: local('MathJax_Size3'), local('MathJax_Size3-Regular')}
@font-face {font-family: MJXc-TeX-size3-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Size3-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size3-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size3-Regular.otf') format('opentype')}
@font-face {font-family: MJXc-TeX-size4-R; src: local('MathJax_Size4'), local('MathJax_Size4-Regular')}
@font-face {font-family: MJXc-TeX-size4-Rw; src /*1*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/eot/MathJax_Size4-Regular.eot'); src /*2*/: url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/woff/MathJax_Size4-Regular.woff') format('woff'), url('https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/fonts/HTML-CSS/TeX/otf/MathJax_Size4-Regular.otf') format('opentype')}
</style><style></style></head>

<body><div role="dialog" aria-live="polite" aria-label="cookieconsent" aria-describedby="cookieconsent:desc" class="cc-window cc-banner cc-type-info cc-theme-block cc-bottom cc-color-override-170793312  cc-invisible" style="display: none;"><!--googleoff: all--><span id="cookieconsent:desc" class="cc-message">IEEE websites place cookies on your device to give you the best user experience. By using our websites, you agree to the placement of these cookies. To learn more, read our <a aria-label="learn more about cookies" role="button" tabindex="0" class="cc-link" href="https://www.ieee.org/about/help/security_privacy.html" target="_blank">Privacy Policy.</a></span><div class="cc-compliance"><a aria-label="dismiss cookie message" role="button" tabindex="0" class="cc-btn cc-dismiss">Accept &amp; Close</a></div><!--googleon: all--></div><div id="MathJax_Message" style="display: none;"></div>




<div class="noprint">
    
        <!--Navbar start-->
<header>
    <a href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#child-menu" class="off-screen">Skip to yearly menu bar</a>
    <a href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#main" class="off-screen">Skip to main content</a>
    <div id="id_navbar" class="navbar navbar-expand-sm navbar-dark" aria-label="Main Navigation" style="background-color: #1B427D;">
        <h2 class="off-screen">Main Navigation</h2>
        <div class="container-fluid">
            <div><a class="navbar-brand" href="https://cvpr.thecvf.com/">

                <img src="./2024_files/cvpr-navbar-logo.svg" alt="conference_logo" height="40px"></a></div>


            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarToggler1" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler1">
                <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                    
    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            CVPR
        </a>
        <ul class="dropdown-menu dropdown-menu-dark">
            
    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CodeOfConduct">
                    <span>
                        Code of Conduct
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Profile/create">
                    <span>
                        Create Profile
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/public/PrivacyPolicy">
                    <span>
                        Privacy Policy
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Help/Contact">
                    <span>
                        Contact CVPR
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/FAQ">
                    <span>
                        HELP/FAQ
                    </span>
                </a>
            </li>

        

    

    

        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/MyStuff">
                    <span>
                        My Stuff
                    </span>
                </a>
            </li>

        

    

    

                </ul>

                <form class="d-flex mx-2" aria-label="Search" role="search" action="https://cvpr.thecvf.com/search">
                    <div class="input-group" role="search" style="outline-color:green;">
                        <input type="text" class="form-control" placeholder="Search" name="q" value="" aria-label="Search" aria-describedby="btnGroupAddon" id="navbar-search">
                        <div class="input-group-text btn-primary" id="btnGroupAddon">
                            <button style="border: none; background-color: transparent; padding: 0;" type="submit">
                                <i class="fa-solid fa-magnifying-glass" aria-hidden="true"></i>
                            </button>
                        </div>
                    </div>
                </form>

                
                    <a href="https://cvpr.thecvf.com/accounts/login?nextp=/" class="navbar-brand"><span class="fa-solid fa-right-to-bracket" aria-hidden="true"></span> Login</a>
                

            </div>
        </div>
    </div>
</header>
<!--Navbar end-->
    
</div><!--noprint div-->


<!--This holds the whole page including the navbar-->

<main id="main">
    <div class="container-fluid">
        <!--Navbar start-->

<div class="dropdown" id="child-menu">
    <nav class="align-middle navbar navbar-expand-md mx-4 border border-3 border-top-0 rounded-bottom" style="min-height: 57px; background-color: #F6f6f6;">
        <div class="container-fluid">

            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarToggler887" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>

            <div class="collapse navbar-collapse" id="navbarToggler887">
                <ul class="navbar-nav me-auto mb-lg-0">
                    


    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle border-3  btn btn-primary text-white p-1" style="background-color: #070bff; font-size: 1.2 em;" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Select Year: (2024)
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item ">
                <a class="dropdown-item p-1" href="https://cvpr.thecvf.com/Conferences/2024">2024
                </a>
            </li>
        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item ">
                <a class="dropdown-item p-1" href="https://cvpr.thecvf.com/Conferences/2023">2023
                </a>
            </li>
        

    

    

        </ul>
    </li>
    



    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/Dates">
                    <span>
                        Dates
                    </span>
                </a>
            </li>

        

    

    

    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Calls
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CallForPapers">
                    <span>
                        Call for Papers
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CallForTutorials">
                    <span>
                        Call for Tutorial Proposals
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/WorkshopProposals">
                    <span>
                        Call for Workshop Proposals
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CallForMusicalPerformance">
                    <span>
                        Call for Musical Performance
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CallForSocials">
                    <span>
                        Call for Socials
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CallForAIArt">
                    <span>
                        Call for AI Art
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CallForDemos">
                    <span>
                        Call for Demos
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CallForDoctoralConsortium">
                    <span>
                        Call for Participation: Doctoral Consortium
                    </span>
                </a>
            </li>

        

    

    

        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Author &amp; Reviewer Guides
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers">
                    <span>
                        Authors
                    </span>
                </a>
            </li>

        

    

    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/AuthorGuidelines">
                    <span style="font-size: .8em;">
                        &nbsp;&nbsp;&nbsp;&nbsp;Author Guidelines
                    </span>
                </a>
            </li>

        

    

    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/OpenReviewAuthorInstructions">
                    <span style="font-size: .8em;">
                        &nbsp;&nbsp;&nbsp;&nbsp;OpenReview Author Instructions
                    </span>
                </a>
            </li>

        

    

    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/AuthorSuggestedPractices">
                    <span style="font-size: .8em;">
                        &nbsp;&nbsp;&nbsp;&nbsp;Author Suggested Practices
                    </span>
                </a>
            </li>

        

    

    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/EthicsGuidelines">
                    <span style="font-size: .8em;">
                        &nbsp;&nbsp;&nbsp;&nbsp;Author Ethics Guidelines
                    </span>
                </a>
            </li>

        

    

    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers">
                    <span>
                        Reviewers
                    </span>
                </a>
            </li>

        

    

    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/ReviewerGuidelines">
                    <span style="font-size: .8em;">
                        &nbsp;&nbsp;&nbsp;&nbsp;Reviewer Guidelines
                    </span>
                </a>
            </li>

        

    

    

        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Attend
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/Pricing2">
                    <span>
                        Register
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/InvitationLetter">
                    <span>
                        Invitation Letter
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/Hotels">
                    <span>
                        Book Your Hotel
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/CodeOfConduct">
                    <span>
                        Code of Conduct
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/DEI">
                    <span>
                        Diversity Equity and Inclusion
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://media.eventhosts.cc/Conferences/CVPR2024/Accessibility_Guide.pdf">
                    <span>
                        Accessibility Guide
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/tutorial-list">
                    <span>
                        Tutorials
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/workshop-list">
                    <span>
                        Workshops
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://visitseattle.org/visitor-information/getting-around/">
                    <span>
                        Transportation
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://visitseattle.org/visitor-information/visitors-center/">
                    <span>
                        Visitor Information Centers
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://visitseattle.org/food-drink/restaurants/">
                    <span>
                        Restaurants &amp; Eateries
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://visitseattle.org/things-to-do/sightseeing/top-25-attractions/">
                    <span>
                        Top 25 Things To Do
                    </span>
                </a>
            </li>

        

    

    

        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Expo
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/Sponsors">
                    <span>
                        Sponsors
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/ExhibitorInformation">
                    <span>
                        Exhibitor Information
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/ExpoSchedule">
                    <span>
                        Expo Schedule
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://hallerickson.ungerboeck.com/prod/app85.cshtml?aat=bzLelIIqbsBWDDAWAnd5CIhNdGez%2fO2dhNgTaUEGe3g%3d">
                    <span>
                        Sponsor, Exhibitor List &amp; Floor-plan
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/PromotionalOpportunities">
                    <span>
                        Promotional Opportunities
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/ExhibitorManual">
                    <span>
                        2024 Exhibitor Manual
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/PRProfessionals">
                    <span>
                        Exhibitor/Sponsor PR Professionals
                    </span>
                </a>
            </li>

        

    

    

        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Media
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/PressLandingPage">
                    <span>
                        Media Center
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/MediaPass">
                    <span>
                        Get Media Pass
                    </span>
                </a>
            </li>

        

    

    
        <hr class="dropdown-divider" aria-hidden="true">
    

    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/NewsAndResources">
                    <span>
                        News and Resources
                    </span>
                </a>
            </li>

        

    

    

        </ul>
    </li>
    



    <li class="dropdown-item dropdown pe-3">
        <a class="nav-link dropdown-toggle  p-1" href="https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers#" role="button" data-bs-toggle="dropdown" aria-expanded="false">
            Organization
        </a>
        <ul class="dropdown-menu">
            
    
        
            <li class="dropdown-item  pe-2">
                <a class="nav-link p-1" href="https://cvpr.thecvf.com/Conferences/2024/Organizers">
                    <span>
                        Organizing Committee
                    </span>
                </a>
            </li>

        

    

    

        </ul>
    </li>
    



                </ul>
            </div>
        </div>
    </nav>
</div>
    <!--Navbar end-->
    </div>
    <br><br>
    
        
        <div class="container">
    
    

    
    <div class="container">
        
            <!--JUst above the HTML in document-snippet.html-->
            <div> <div>
<h2>CVPR 2024 Accepted Papers</h2>

<h3>&nbsp;</h3>

<h3>Accepted Papers:</h3>

<p>Papers are assigned to poster sessions such that topics are maximally spread over sessions (attendees will find interesting papers at each session) while grouping similar posters within each poster session to minimize walking distances. We used a 1D t-SNE projection of the SPECTER paper embeddings to realize this assignment.</p>

<p><span class="highlight">This page is cached for 1 hour</span>. &nbsp;Changes to affiliation or name in your local&nbsp;<a href="https://cvpr.thecvf.com/EditProfile">profile</a> may take up to 60 minutes to appear here.</p>

<p>
</p><table>
    <thead>
    </thead><colgroup><col style="min-width:50%;">
    <col>
    <col style="width:140px;">
    </colgroup>

    <tbody><tr>
        

        
    </tr>
    
        <tr>

            <td>
                <strong>A theory of volumetric representations for opaque solids</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bailey Miller (Carnegie Mellon University) · Hanyu Chen (Carnegie Mellon University) · Alice Lai (Carnegie Mellon University) · Ioannis Gkioulekas (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ACT-Diffusion: Efficient Adversarial Consistency Training for One-step Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fei Kong (University of Electronic Science and Technology of China) · Jinhao Duan (Drexel University) · Lichao Sun (Lehigh University) · Hao Cheng (Hong Kong University of Science and Technology(Guangzhou)) · Renjing Xu (Hong Kong University of Science and Technology (Guangzhou)) · Heng Tao Shen (University of Electronic Science and Technology of China) · Xiaofeng Zhu (University of Electronic Science and Technology of China) · Xiaoshuang Shi (University of Electronic Science and Technology of China) · Kaidi Xu (Drexel University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>I'M HOI: Inertia-aware Monocular Capture of 3D Human-Object Interactions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengfeng Zhao (ShanghaiTech University) · Juze Zhang (ShanghaiTech University) · Jiashen Du (None) · Ziwei Shan (ShanghaiTech University) · Junye Wang (ShanghaiTech University) · Jingyi Yu (Shanghai Tech University) · Jingya Wang (ShanghaiTech University) · Lan Xu (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HOI-M<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-1-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-1" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-2" class="mjx-mrow"><span id="MJXc-Node-3" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-4" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-5" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">3</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-1">^3</script>: Capture Multiple Humans and Objects Interaction within Contextual Environment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juze Zhang (ShanghaiTech University) · Jingyan Zhang (ShanghaiTech University) · Zining Song (ShanghaiTech University) · Zhanhe Shi (ShanghaiTech University) · Chengfeng Zhao (ShanghaiTech University) · Ye Shi (ShanghaiTech University) · Jingyi Yu (Shanghai Tech University) · Lan Xu (ShanghaiTech University) · Jingya Wang (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OrthCaps: An Orthogonal CapsNet with Sparse Attention Routing and Pruning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Geng Xinyu (None) · Jiaming Wang (Harbin Institute of Technology) · Jiawei Gong (Harbin Institute of Technology) · yuerong xue (Harbin Institute of Technology) · Jun Xu (Harbin Institute of Technology) · Fanglin Chen (Harbin Institute of Technology (Shenzhen)) · Xiaolin Huang (Shanghai Jiao Tong University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unbiased Estimator for Distorted Conic in Camera Calibration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaehyeon Song (Seoul National University) · Jaeho Shin (Seoul National University) · Myung-Hwan Jeon (Seoul National University) · Jongwoo Lim (Seoul National University) · Ayoung Kim (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Posterior Distillation Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juil Koo (KAIST) · Chanho Park (KAIST) · Minhyuk Sung (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Vision Check-up for Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pratyusha Sharma (Massachusetts Institute of Technology) · Tamar Rott Shaham (MIT) · Manel Baradad (Massachusetts Institute of Technology) · Stephanie Fu (University of California, Berkeley) · Adrian Rodriguez-Munoz (Massachusetts Institute of Technology) · Shivam Duggal (Massachusetts Institute of Technology) · Phillip Isola (None) · Antonio Torralba (MIT)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/uzh-rpg/ssms_event_cameras" target="_blank">State Space Models for Event Cameras</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nikola Zubic (Robotics and Perception Group, University of Zurich and ETH Zurich) · Mathias Gehrig (University of Zurich) · Davide Scaramuzza (University of Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Weak-to-Strong 3D Object Detection with X-Ray Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alexander Gambashidze (AIRI) · Aleksandr Dadukin (Higher School of Economics) · Maksim Golyadkin (AIRI) · Maria Razzhivina (Higher School of Economics, Higher School of Economics) · Ilya Makarov (Moscow State Institute of Steel and Alloys)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://astra-vision.github.io/FAMix/" target="_blank">A Simple Recipe for Language-guided Domain Generalized Segmentation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammad Fahes (Inria) · TUAN-HUNG VU (None) · Andrei Bursuc (valeo.ai) · Patrick Pérez (None) · Raoul de Charette (Inria)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://astra-vision.github.io/PaSCo/" target="_blank">PaSCo: Urban 3D Panoptic Scene Completion with Uncertainty Awareness</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anh-Quan Cao (INRIA) · Angela Dai () · Raoul de Charette (Inria)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://vip-llava.github.io/" target="_blank">Making Large Multimodal Models Understand Arbitrary Visual Prompts</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mu Cai (Department of Computer Science, University of Wisconsin, Madison) · Haotian Liu (University of Wisconsin-Madison) · Siva Mustikovela (Heidelberg University) · Gregory P. Meyer (Cruise) · Yuning Chai (Cruise) · Dennis Park (Toyota Research Institute) · Yong Jae Lee (Department of Computer Sciences, University of Wisconsin - Madison)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dynamic Cues-Assisted Transformer for Robust Point Cloud Registration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hong Chen (Huazhong University of Science and Technology) · Pei Yan (Huazhong University of Science and Technology) · sihe xiang (None) · Yihua Tan (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://skawngus1111.github.io/MADGNet_project/" target="_blank">Modality-agnostic Domain Generalizable Medical Image Segmentation by Multi-Frequency in Multi-Scale Attention</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ju-Hyeon Nam (Inha University) · Nur Suriza Syazwany (Inha University) · Su Jung Kim (Inha University) · Sang-Chul Lee (Inha University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kristen Grauman (University of Texas at Austin) · Andrew Westbury (Facebook AI Research) · Lorenzo Torresani (Facebook) · Kris Kitani (Carnegie Mellon University) · Jitendra Malik (University of California at Berkeley) · Triantafyllos Afouras (University of Oxford) · Kumar Ashutosh (UT Austin &amp; FAIR, Meta) · Vijay Baiyya (University of Louisiana at Lafayette) · Siddhant Bansal (University of Bristol, UK) · Bikram Boote (University of Illinois, Urbana Champaign) · Eugene Byrne (Meta) · Zachary Chavis (University of Minnesota) · Joya Chen (National University of Singapore) · Feng Cheng (University of North Carolina at Chapel Hill) · Fu-Jen Chu (Facebook) · Sean Crane (School of Computer Science, Carnegie Mellon University) · Avijit Dasgupta (IIIT Hyderabad) · Jing Dong (Meta) · Maria Escobar (Universidad de Los Andes) · Cristhian David Forigua Diaz (Universidad de Los Andes) · Abrham Gebreselasie (Carnegie Mellon University) · Sanjay Haresh (Qualcomm Inc, QualComm) · Jing Huang (Facebook) · Md Mohaiminul Islam (UNC Chapel Hill) · Suyog Jain (Meta) · Rawal Khirodkar (Meta) · Devansh Kukreja (Carnegie Mellon University) · Kevin Liang (FAIR at Meta) · Jia-Wei Liu (National University of Singapore) · Sagnik Majumder (UT Austin &amp; Meta AI) · Yongsen Mao (Simon Fraser University) · Miguel Martin (Meta Platforms, Inc.) · Effrosyni Mavroudi () · Tushar Nagarajan (Meta) · Francesco Ragusa (None) · Santhosh Kumar Ramakrishnan (University of Texas, Austin) · Luigi Seminara (University of Catania) · Arjun Somayazulu (University of Texas at Austin) · Yale Song (Meta) · Shan Su (University of Pennsylvania) · Zihui Xue (None) · Edward Zhang (University of Pennsylvania, University of Pennsylvania) · Jinxu Zhang (University of Pennsylvania, University of Pennsylvania) · Angela Castillo (Universidad de Los Andes) · Changan Chen (University of Texas at Austin) · Fu Xinzhu (National University of Singapore) · Ryosuke Furuta (The University of Tokyo) · Cristina González (Universidad de Los Andes) · Gupta (None) · Jiabo Hu (Facebook) · Yifei Huang (The University of Tokyo) · Yiming Huang (University of Pennsylvania) · Weslie Khoo (Indiana University) · Anush Kumar (Torc Robotics) · Robert Kuo (Facebook) · Sach Lakhavani (None) · Miao Liu (META AI) · Mi Luo (The University of Texas at Austin) · Zhengyi Luo (Carnegie Mellon University) · Brighid Meredith (meta) · Austin Miller (Meta) · Oluwatumininu Oguntola (University of North Carolina at Chapel Hill) · Xiaqing Pan (Meta) · Penny Peng (Meta) · Shraman Pramanick (None) · Merey Ramazanova (KAUST) · Fiona Ryan (Georgia Institute of Technology) · Wei Shan (University of North Carolina at Chapel Hill) · Kiran Somasundaram (None) · Chenan Song (national university of singaore, National University of Singapore) · Audrey Southerland (Georgia Institute of Technology) · Masatoshi Tateno (AIST, National Institute of Advanced Industrial Science and Technology) · Huiyu Wang (Facebook) · Yuchen Wang (Indiana University) · Takuma Yagi (None) · Mingfei Yan (None) · Xitong Yang (Meta) · Zecheng Yu (University of Tokyo) · Shengxin Zha (Meta GenAI) · Chen Zhao (King Abdullah University of Science and Technology (KAUST)) · Ziwei Zhao (Indiana University) · Zhifan Zhu (University of Bristol) · Jeff Zhuo (University of North Carolina at Chapel Hill) · Pablo ARBELAEZ (Universidad de los Andes) · Gedas Bertasius (UNC Chapel Hill) · Dima Damen (University of Bristol and Google DeepMind) · Jakob Engel (Research, Meta Reality Labs) · Giovanni Maria Farinella (University of Catania, Italy) · Antonino Furnari (University of Catania) · Bernard Ghanem (KAUST) · Judy Hoffman (Georgia Institute of Technology) · C.V. Jawahar (IIIT-Hyderabad) · Richard Newcombe (Meta, Reality Labs Research) · Hyun Soo Park (The University of Minnesota) · James Rehg (None) · Yoichi Sato (University of Tokyo) · Manolis Savva (Simon Fraser University) · Jianbo Shi (None) · Mike Zheng Shou (National University of Singapore) · Michael Wray (University of Bristol)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SoundingActions: Learning How Actions Sound from Narrated Egocentric Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changan Chen (University of Texas at Austin) · Kumar Ashutosh (UT Austin &amp; FAIR, Meta) · Rohit Girdhar (Meta) · David Harwath (University of Texas, Austin) · Kristen Grauman (University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://understanding-visual-datasets.github.io/VisDiff-website/" target="_blank">Describing Differences in Image Sets with Natural Language</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lisa Dunlap (University of California, Berkeley) · Yuhui Zhang (Stanford University) · Xiaohan Wang (Stanford University) · Ruiqi Zhong (University of California Berkeley) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Jacob Steinhardt (University of California Berkeley) · Joseph Gonzalez (University of California - Berkeley) · Serena Yeung (Stanford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Masked Autoencoders for Microscopy are Scalable Learners of Cellular Biology</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Oren Kraus (Recursion) · Kian Kenyon-Dean (Recursion Pharma) · Saber Saberian (Recursion Pharma) · Maryam Fallah (Recursion Pharmaceuticals) · Peter McLean (Recursion) · Jess Leung (Recursion) · Vasudev Sharma (Recursion) · Ayla Khan (University of Utah) · Jia Balakrishnan (Recursion Pharmaceuticals) · Safiye Celik (Recursion) · Dominique Beaini (Valence Labs) · Maciej Sypetkowski (Valence Labs) · Chi Cheng (Boston University, Boston University) · Kristen Morse (Recursion) · Maureen Makes (University of Utah) · Ben Mabey (None) · Berton Earnshaw (University of Utah)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Data-Efficient Multimodal Fusion on a Single GPU</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Noël Vouitsis (Layer 6 AI) · Zhaoyan Liu (Layer6 AI) · Satya Krishna Gorti (Layer6 AI) · Valentin Villecroze (Layer 6) · Jesse C. Cresswell (Layer 6 AI) · Guangwei Yu (Layer6 AI) · Gabriel Loaiza-Ganem (Layer 6 AI) · Maksims Volkovs (Layer6 AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AIDE: An Automatic Data Engine for Object Detection in Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingfu Liang (Northwestern University) · Jong-Chyi Su (None) · Samuel Schulter (NEC Laboratories America) · Sparsh Garg (NEC Laboratories America) · Shiyu Zhao (Rutgers University, New Brunswick) · Ying Wu (Northwestern University) · Manmohan Chandraker (UC San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning from One Continuous Video Stream</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Joao Carreira (DeepMind) · Michael King (Fit) · Viorica Patraucean (DeepMind) · Dilara Gokay (Google DeepMind) · Catalin Ionescu (Google) · Yi Yang (DeepMind) · Daniel Zoran (DeepMind) · Joseph Heyward (Google) · Carl Doersch (DeepMind) · Yusuf Aytar (Google DeepMind) · Dima Damen (University of Bristol and Google DeepMind) · Andrew Zisserman (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TIM: A Time Interval Machine for Audio-Visual Action Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jacob Chalk (None) · Jaesung Huh (University of Oxford) · Evangelos Kazakos (Czech Technical University of Prague) · Andrew Zisserman (University of Oxford) · Dima Damen (University of Bristol and Google DeepMind)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaskirat Singh (Australian National University) · Jianming Zhang (Adobe Systems) · Qing Liu (Adobe Systems) · Cameron Smith (Adobe Systems) · Zhe Lin (Adobe Research) · Liang Zheng (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EmoVIT: Revolutionizing Emotion Insights with Visual Instruction Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongxia Xie (Jilin University) · Chu-Jun Peng (National Yang Ming Chiao Tung University) · Yu-Wen Tseng (Department of computer science and informational engineering, National Taiwan University) · Hung-Jen Chen (National Yang Ming Chiao Tung University) · Chan-Feng Hsu (National Chiao Tung University) · Hong-Han Shuai (National Yang Ming Chiao Tung University) · Wen-Huang Cheng (National Taiwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://rangwani-harsh.github.io/DeiT-LT/" target="_blank">DeiT-LT: Distillation Strikes Back for Vision Transformer Training on Long-Tailed Datasets</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Harsh Rangwani (Indian Institute of Science) · Pradipto Mondal (Indian Institute of Technology, Kharagpur) · Mayank Mishra (CMU, Carnegie Mellon University) · Ashish Asokan (Indian Institute of Science, Indian institute of science, Bangalore) · R. Venkatesh Babu (Indian Institute of Science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://craigleili.github.io/projects/genzi/" target="_blank">GenZI: Zero-Shot 3D Human-Scene Interaction Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lei Li (Technical University of Munich) · Angela Dai ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://dotrannhattuong.github.io/ECB/website/" target="_blank">Learning CNN on ViT: A Hybrid Model to Explicitly Class-specific Boundaries for Domain Adaptation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ba Hung Ngo (Chonnam National University) · Nhat-Tuong Do-Tran (National Yang Ming Chiao Tung University) · Tuan-Ngoc Nguyen (FPT Telecom) · Hae-Gon Jeon (GIST) · Tae Jong Choi (Chonnam National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peifei Zhu (LY Corporation) · Tsubasa Takahashi (LY Corporation) · Hirokatsu Kataoka (LY Corporation)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Active Domain Adaptation with False Negative Prediction for Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuzuru Nakamura (Panasonic Holdings Corporation) · Yasunori Ishii (Panasonic Holdings Corporation) · Takayoshi Yamashita (Chubu University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>View From Above: Orthogonal viewpoint aware Cross-view Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shan Wang (ANU;CSIRO) · Chuong Nguyen (None) · Jiawei Liu (Australian National University) · Yanhao Zhang (University of Technology Sydney) · Sundaram Muthu (, CSIRO) · Fahira Afzal Maken (CSIRO) · Kaihao Zhang (Australian National University) · Hongdong Li (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Self-Calibrating Vicinal Risk Minimisation for Model Calibration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Liu (Australian National University) · Changkun Ye (Australian National University) · Ruikai Cui (Australian National University) · Nick Barnes (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CAT-Seg: Cost Aggregation for Open-vocabulary Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seokju Cho (Korea University) · Heeseong Shin (Korea University) · Sunghwan Hong (Korea University) · Anurag Arnab (Google) · Paul Hongsuck Seo (Google) · Seungryong Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Intra-view and Cross-view Geometric Knowledge for Stereo Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui Gong (Nanyang Technological University) · Weide Liu (Harvard University) · ZAIWANG GU (None) · Xulei Yang (Institute for Infocomm Research (I2R), A*STAR) · Jun Cheng (Institute For Infocomm Research, A*STAR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Time-, Memory- and Parameter-Efficient Visual Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Otniel-Bogdan Mercea (University of Tübingen) · Alexey Gritsenko (Google) · Cordelia Schmid (Inria / Google) · Anurag Arnab (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/chenshuang-zhang/imagenet_d" target="_blank">ImageNet-D: Benchmarking Neural Network Robustness on Diffusion Synthetic Object</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenshuang Zhang (Korea Advanced Institute of Science and Technology) · Fei Pan (University of Michigan - Ann Arbor) · Junmo Kim (Korea Advanced Institute of Science and Technology) · In So Kweon (Korea Advanced Institute of Science and Technology) · Chengzhi Mao (Columbia University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://jusiro.github.io/projects/clap" target="_blank">A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Julio Silva-Rodríguez (ETS Montreal) · Sina Hajimiri (École de technologie supérieure, Université du Québec) · Ismail Ben Ayed (ETS Montreal) · Jose Dolz (École de technologie supérieure)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>StegoGAN: Bootstrapping Non-bijective Image-to-Image Translation with CycleGAN Steganography</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sidi Wu (ETH Zurich) · Yizi Chen (ETHZ - ETH Zurich) · Loic Landrieu (ENPC, IGN) · Nicolas Gonthier (IGN) · Samuel Mermet (Ecole Nationale des Sciences Géographiques) · Lorenz Hurni (ETHZ - ETH Zurich) · Konrad Schindler (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Lineage</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Runpeng Yu (National University of Singapore) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Weakly Supervised Video Individual Counting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyan Liu (None) · Guorong Li (University of Chinese Academy of Sciences) · Yuankai Qi (The University of Adelaide) · Ziheng Yan (University of Chinese Academy of Sciences) · Zhenjun Han (University of the Chinese Academy of Sciences) · Anton van den Hengel (University of Adelaide) · Ming-Hsuan Yang (University of California at Merced) · Qingming Huang (University of Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Curriculum Point Prompting for Weakly-Supervised Referring Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiyuan Dai (ShanghaiTech University) · Sibei Yang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Neural Visibility Field for Active Mapping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shangjie Xue (Georgia Institute of Technology) · Jesse Dill (Georgia Institute of Technology) · Pranay Mathur (Georgia Institute of Technology) · Frank Dellaert (Georgia Tech) · Panagiotis Tsiotras (Georgia Institute of Technology) · Danfei Xu (Georgia Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Predicated Diffusion: Predicate Logic-Based Attention Guidance for Text-to-Image Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kota Sueyoshi (Osaka University) · Takashi Matsubara (Hokkaido Universiry)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MICap: A Unified Model for Identity-aware Movie Descriptions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haran Raajesh (International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad) · Naveen Reddy Desanur (International Institute of Information Technology Hyderabad) · Zeeshan Khan (INRIA) · Makarand Tapaswi (IIIT Hyderabad, Wadhwani AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Image-Text Co-Decomposition for Text-Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ji-Jia Wu (National Taiwan University) · Andy Chia-Hao Chang (National Yang Ming Chiao Tung University) · Chieh-Yu Chuang (National Yang Ming Chiao Tung University) · Chun-Pei Chen (National Yang Ming Chiao Tung University) · Yu-Lun Liu (National Yang Ming Chiao Tung University) · Min-Hung Chen (NVIDIA) · Hou-Ning Hu (MediaTek Inc.) · Yung-Yu Chuang (National Taiwan University) · Yen-Yu Lin (National Yang Ming Chiao Tung University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://uark-cviu.github.io/ASPIRe/" target="_blank">HIG: Hierarchical Interlacement Graph Approach to Scene Graph Generation in Video Understanding</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Trong-Thuan Nguyen (University of Arkansas) · Pha Nguyen (University of Arkansas) · Khoa Luu (University of Arkansas)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/XuzheZ/MAPSeg" target="_blank">MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuzhe Zhang (Columbia University) · Yuhao Wu (Duke University) · Elsa Angelini (Télécom ParisTech) · Ang Li (University of Maryland, College Park) · Jia Guo (Columbia University) · Jerod Rasmussen (University of California, Irvine) · Thomas O'Connor (University of Rochester) · Pathik Wadhwa (University of California, Irvine) · Andrea Jackowski (None) · Hai Li (Duke University) · Jonathan Posner (Duke University) · Andrew Laine (Columbia University) · Yun Wang (Emory University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/aleflabo/PREGO" target="_blank">PREGO: online mistake detection in PRocedural EGOcentric videos</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alessandro Flaborea (Sapienza University of Rome / ItalAI) · Guido M. D&amp;#x27;Amely di Melendugno (University of Roma "La Sapienza") · Leonardo Plini (Sapienza University of Rome &amp; INFN) · Luca Scofano (University of Roma "La Sapienza") · Edoardo De Matteis (Sapienza University) · Antonino Furnari (University of Catania) · Giovanni Maria Farinella (University of Catania, Italy) · Fabio Galasso (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Event Stream-based Visual Object Tracking: A High-Resolution Benchmark Dataset and A Novel Baseline</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao Wang (Anhui University) · Shiao Wang (None) · Chuanming Tang (University of Chinese Academy of Sciences) · Lin Zhu (Beijing Institute of Technology) · Bo Jiang (Anhui University) · Yonghong Tian (Peking University) · Jin Tang (Anhui University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Masked and Shuffled Blind Spot Denoising for Real-World Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hamadi Chihaoui (University of Bern) · Paolo Favaro (Institute für Informatik, University of Bern)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Benchmarking Audio Visual Segmentation for Long-Untrimmed Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Liu (The University of Queensland) · Peike Li (Futureverse AI) · Qingtao Yu (Australian National University) · Hongwei Sheng (University of Queensland) · Dadong Wang (CSIRO) · Lincheng Li () · Xin Yu (University of Queensland)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ConCon-Chi: Concept-Context Chimera Benchmark for Personalized Vision-Language Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andrea Rosasco (Università degli Studi di Genova, Istituto Italiano di Tecnologia) · Stefano Berti (Università degli Studi di Genova, Istituto Italiano di Tecnologia) · Giulia Pasquale (Istituto Italiano di Tecnologia) · Damiano Malafronte (Istituto Italiano di Tecnologia) · Shogo Sato (Sony Interactive Entertainment Inc.) · Hiroyuki Segawa (Sony Interactive Entertainment) · Tetsugo Inada (Sony Interactive Entertainment) · Lorenzo Natale (Istituto Italiano di Tecnologia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Efficient Privacy-Preserving Visual Localization Using 3D Ray Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Heejoon Moon (HANYANG university) · Chunghwan Lee (Hanyang University) · Je Hyeong Hong (Hanyang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Building Bridges across Spatial and Temporal Resolutions: Reference-Based Super-Resolution via Change Priors and Conditional Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Runmin Dong (Tsinghua University) · Shuai Yuan (The University of Hong Kong) · Bin Luo (Tsinghua University) · Mengxuan Chen (Tsinghua University) · Jinxiao Zhang (Tsinghua University) · Lixian Zhang (National Supercomputing Center in Shenzhen) · Weijia Li (Sun Yat-sen University) · Juepeng Zheng (Sun Yat-Sen University) · Haohuan Fu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Smart Help: Strategic Opponent Modeling for Proactive and Adaptive Robot Assistance in Households</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhihao Cao (Tsinghua University, Tsinghua University) · ZiDong Wang (Department of Automation, Tsinghua University, Tsinghua University) · Siwen Xie (Peking University) · Anji Liu (University of California, Los Angeles) · Lifeng Fan (Beijing Institute of General Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LayoutFormer: Hierarchical Text Detection Towards Scene Text Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Min Liang (University of Science and Technology Beijing) · Jia-Wei Ma (University of Science and Technology Beijing) · Xiaobin Zhu (University of Science and Technology Beijing) · Jingyan Qin (University of Science and Technology Beijing) · Xu-Cheng Yin (University of Science and Technology Beijing)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengqi Zhang (University of California, San Diego) · Yang Fu (University of California San Diego) · Zheng Ding (University of California, San Diego) · Sifei Liu (NVIDIA) · Zhuowen Tu (University of California, San Diego) · Xiaolong Wang (UCSD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://gen2res.github.io/" target="_blank">Restoration by Generation with Constrained Priors</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zheng Ding (University of California, San Diego) · Xuaner Zhang (Adobe) · Zhuowen Tu (University of California, San Diego) · Zhihao Xia (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Low-Latency Neural Stereo Streaming</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiqi Hou (Qualcomm Inc, QualComm) · Farzad Farhadzadeh (Qualcomm Inc, QualComm) · Amir Said (Qualcomm Inc, QualComm) · Guillaume Sautiere (Qualcomm Inc, QualComm) · Hoang Le (Qualcomm AI Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SODA: Bottleneck Diffusion Models for Representation Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Drew Hudson (Google DeepMind) · Daniel Zoran (DeepMind) · Mateusz Malinowski (MoonValley AI) · Andrew Lampinen (Google DeepMind) · Andrew Jaegle (Google DeepMind) · James McClelland (Stanford University and Google DeepMind) · Loic Matthey (DeepMind) · Felix Hill (Google) · Alexander Lerchner (Google DeepMind)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://light.princeton.edu/online-stereo-recification/" target="_blank">Flow-Guided Online Stereo Rectification for Wide Baseline Stereo</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anush Kumar (Torc Robotics) · Fahim Mannan () · Omid Hosseini Jafari (Torc Robotics) · Shile Li (Torc Robotics) · Felix Heide (Department of Computer Science, Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multiscale Vision Transformers meet Bipartite Matching for efficient single-stage Action Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ioanna Ntinou (Queen Mary University of London) · Enrique Sanchez (Samsung AI Center Cambridge) · Georgios Tzimiropoulos (Queen Mary University London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://airvlab.github.io/grasp-anything/" target="_blank">Language-driven Grasp Detection</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            An Dinh Vuong (FPT Software - AI Center) · Minh Nhat VU (ACIN Institute, TU Wien/ Austrian Institute of Technology) · Baoru Huang (University College London, University of London) · Nghia Nguyen (FPT Software) · Hieu Le (FPT Software AI Center) · Thieu Vo (Ton Duc Thang University) · Anh Nguyen (University of Liverpool)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alon Zolfi (Ben-Gurion University of the Negev) · Guy AmiT (Ben-Gurion University of the Negev) · Amit Baras () · Satoru Koda (Fujitsu Limited) · Ikuya Morikawa (Fujitsu Research) · Yuval Elovici (Ben Gurion University of the Negev) · Asaf Shabtai (Ben-Gurion University of the Negev)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Random Entangled Tokens for Adversarially Robust Vision Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huihui Gong (University of Sydney) · Minjing Dong (City University of Hong Kong) · Siqi Ma (University of New South Wales) · Seyit Camtepe (CSIRO) · Surya Nepal (, CSIRO) · Chang Xu (University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/924973292/EDITOR" target="_blank">Magic Tokens: Select Diverse Tokens for Multi-modal Object Re-Identification</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pingping Zhang (Dalian University of Technology) · Yuhao Wang (Dalian University of Technology) · Yang Liu (Dalian University of Technology) · Zhengzheng Tu (Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, Anhui University) · Huchuan Lu (Dalian University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Retrieval-Augmented Open-Vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jooyeon Kim (Korea University) · Eulrang Cho (Samsung Research) · Sehyung Kim (Korea University) · Hyunwoo J. Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shanshan Zhong (SUN YAT-SEN UNIVERSITY) · Zhongzhan Huang (Sun Yat-Sen University) · Shanghua Gao (Harvard University) · Wushao Wen (SUN YAT-SEN UNIVERSITY) · Liang Lin (Sun Yat-sen University) · Marinka Zitnik (Harvard University) · Pan Zhou (Sea Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Revisiting Counterfactual Problems in Referring Expression Comprehension</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhihan Yu (Beijing University of Posts and Telecommunications) · Ruifan Li (Beijing University of Post and Telecommunication)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/NetX-lab/GoMathL2O-Official" target="_blank">Towards Robust Learning to Optimize with Theoretical Guarantees</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qingyu Song (The Chinese University of Hong Kong) · Wei Lin (The Chinese University of Hong Kong) · Juncheng Wang (Hong Kong Baptist University) · Hong Xu (CUHK)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>JRDB-PanoTrack: An Open-world Panoptic Segmentation and Tracking Robotic Dataset in Crowded Human Environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Duy Tho Le (Monash University) · Chenhui Gou (Monash University) · Stavya Datta (Monash University) · Hengcan Shi (None) · Ian Reid (University of Adelaide) · Jianfei Cai (Monash University) · Hamid Rezatofighi (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Boosting Image Restoration via Priors from Pre-trained Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaogang Xu (Zhejiang Lab) · Shu Kong (University of Macau, Texas A&amp;M University) · Tao Hu (National University of Singapore) · Zhe Liu (Zhejiang Lab) · Hujun Bao (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised 3D Structure Inference from Category-Specific Image Collections</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weikang Wang (Rheinische Friedrich-Wilhelms Universität Bonn) · Dongliang Cao (University of Bonn) · Florian Bernard (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3D Neural Edge Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lei Li (ETH Zurich) · Songyou Peng (ETH Zurich &amp; MPI Tübingen) · Zehao Yu (None) · Shaohui Liu (ETH Zurich) · Rémi Pautrat (Microsoft Mixed Reality &amp; AI lab) · Xiaochuan Yin (Utopilot) · Marc Pollefeys (ETH Zurich / Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CFPL-FAS: Class Free Prompt Learning for Generalizable Face Anti-spoofing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ajian Liu (NLPR, CASIA) · Shuai Xue (Beijing Institute of Technology) · Gan Jianwen (Macao University of Science and Techonology) · Jun Wan () · Yanyan Liang (Macau University of Science and Technology) · Jiankang Deng (Imperial College London &amp; Huawei UKRD) · Sergio Escalera (Computer Vision Center) · Zhen Lei (Institute of Automation,  Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VOODOO 3D: VOlumetric pOrtrait Disentanglement fOr Online 3D head reenactment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Phong Tran (MBZUAI) · Egor Zakharov (ETH Zurich) · Long Nhat Ho (Mohamed bin Zayed University of Artificial Intelligence) · Anh Tran (VinAI Research) · Liwen Hu (Pinscreen) · Hao Li (Mohamed bin Zayed University of Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DreamSalon: A Staged Diffusion Framework for Preserving Identity-Context in Editable Face Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haonan Lin (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/cure-lab/MMA-Diffusion" target="_blank">MMA-Diffusion: MultiModal Attack on Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yijun Yang (The Chinese University of Hong Kong) · Ruiyuan Gao (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Xiaosen Wang (Huazhong University of Science and Technology) · Tsung-Yi Ho (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Xu Nan (Institute of Automation, Chinese Academy of Sciences) · Qiang Xu (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Online Task-Free Continual Generative and Discriminative Learning via Dynamic Cluster Memory</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            飞 叶 (University of York) · Adrian Bors (MBZUAI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DifFlow3D: Toward Robust Uncertainty-Aware Scene Flow Estimation with Iterative Diffusion-Based Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiuming Liu (Shanghai Jiao Tong University) · Guangming Wang (University of Cambridge) · Weicai Ye (Zhejiang University) · Chaokang Jiang () · Jinru Han (Shanghai Jiao Tong University) · Zhe Liu (Shanghai Jiaotong University) · Guofeng Zhang (Zhejiang University) · Dalong Du (PhiGent Robotics) · Hesheng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Named Entity Driven Zero-Shot Image Manipulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhida Feng (Wuhan University of Science and Technology) · Li Chen (Wuhan University of Science and Technology) · Jing Tian (National University of Singapore) · Jiaxiang Liu (Baidu) · Shikun Feng (Baidu)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Content-Style Decoupling for Unsupervised Makeup Transfer without Generating Pseudo Ground Truth</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaoyang Sun (Wuhan University of Technology) · Shengwu Xiong (Wuhan University of Technology) · Yaxiong Chen (Wuhan University of Technology) · Yi Rong (Wuhan University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Physics-guided Shape-from-Template: Monocular Video Perception through Neural Surrogate Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            David Stotko (University of Bonn) · Nils Wandel (University of Bonn) · Reinhard Klein (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SIRA: Scalable Inter-frame Relation and Association for Radar Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ryoma Yataka (Mitsubishi Electric Research Laboratories (MERL)) · Pu (Perry) Wang (None) · Petros Boufounos (Mitsubishi Electric Research Laboratories) · Ryuhei Takahashi (Mitsubishi Electric Corporation)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhe Li (华中科技大学) · Laurence Yang (Hainan University) · Bocheng Ren (None) · Xin Nie (Huazhong University of Science and Technology) · Zhangyang Gao (Westlake University, China) · Cheng Tan (Zhejiang University &amp; Westlake University) · Stan Z. Li (Westlake University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>General Point Model Pretraining with Autoencoding and Autoregressive</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhe Li (华中科技大学) · Zhangyang Gao (Westlake University, China) · Cheng Tan (Zhejiang University &amp; Westlake University) · Bocheng Ren (None) · Laurence Yang (Hainan University) · Stan Z. Li (Westlake University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhixuan Liu (Carnegie Mellon University) · Peter Schaldenbrand (CMU, Carnegie Mellon University) · Beverley-Claire Okogwu (CMU, Carnegie Mellon University) · Wenxuan Peng (Nanyang Technological University) · Youngsik Yun (Dongguk University) · Andrew Hundt (Carnegie Mellon University) · Jihie Kim (Dongguk University) · Jean Oh (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SyncMask: Synchronized Attentional Masking for Fashion-centric Vision-Language Pretraining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chull Hwan Song (Dealicious Inc) · Taebaek Hwang (None) · Jooyoung Yoon (Dealicious Inc) · Shunghyun Choi (Dealicious Inc.) · Yeong Hyeon Gu (Sejong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>You Only Need Less Attention Each Stage in Vision Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuoxi Zhang (Huazhong University of Science and Technology) · Hanpeng Liu (Huazhong University of Science and Technology) · Stephen Lin (Microsoft Research) · Kun He (Huazhong University of Sceince and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MoST: Multi-modality Scene Tokenization for Motion Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Norman Mu (UC Berkeley) · Jingwei Ji (Waymo LLC) · Zhenpei Yang (Waymo LLC) · Nathan Harada (Waymo LLC) · Haotian Tang (Massachusetts Institute of Technology) · Kan Chen (Waymo) · Charles R. Qi (Waymo) · Runzhou Ge (Waymo) · Kratarth Goel (Waymo) · Zoey Yang (Waymo) · Scott Ettinger (Waymo LLC) · Rami Al-Rfou (Waymo) · Dragomir Anguelov (Waymo) · Yin Zhou (Waymo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Object Dynamics Modeling with Hierarchical Point Cloud-based Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chanho Kim (Oregon State University) · Li Fuxin (Oregon State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>On Scaling up a Multilingual Vision and Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xi Chen (Google) · Josip Djolonga (Google) · Piotr Padlewski (Google) · Basil Mustafa (Google) · Soravit Changpinyo (Google Research) · Jialin Wu (Google) · Carlos Riquelme Ruiz (Google) · Sebastian Goodman (Google) · Xiao Wang (Google DeepMind) · Yi Tay (Google) · Siamak Shakeri (Research, Google) · Mostafa Dehghani (Google DeepMind) · Daniel Salz (Google) · Mario Lučić (Google) · Michael Tschannen (Google DeepMind) · Arsha Nagrani (Google ) · Hexiang Hu (Google Deepmind) · Mandar Joshi (Google DeepMind) · Bo Pang (Google) · Ceslee Montgomery (Google) · Paulina Pietrzyk (Google) · Marvin Ritter (Google DeepMind) · AJ Piergiovanni (Google) · Matthias Minderer (Google) · Filip Pavetic (Google) · Austin Waters (Google) · Gang Li (Google) · Ibrahim Alabdulmohsin (Google) · Lucas Beyer (Google Brain/DM Zürich) · Julien Amelot (Research, Google) · Kenton Lee (Google Research) · Andreas Steiner (Google) · Yang Li (Google) · Daniel Keysers (Google DeepMind) · Anurag Arnab (Google) · Yuanzhong Xu (Google) · Keran Rong (Google Deepmind) · Alexander Kolesnikov (Google) · Mojtaba Seyedhosseini (Google) · Anelia Angelova (Google) · Xiaohua Zhai (Google) · Neil Houlsby (Google) · Radu Soricut (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Distilling Vision-Language Models on Millions of Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yue Zhao (UT Austin) · Long Zhao (Google Research) · Xingyi Zhou (Google) · Jialin Wu (Google) · Chun-Te Chu (Google Research) · Hui Miao (Google) · Florian Schroff (Google) · Hartwig Adam (Google Research) · Ting Liu (Google Research) · Boqing Gong (Google) · Philipp Krähenbühl (University of Texas at Austin) · Liangzhe Yuan (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Structure-from-Motion from Pixel-wise Correspondences</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Philipp Lindenberger (Department of Computer Science, ETHZ - ETH Zurich) · Paul-Edouard Sarlin (ETH Zurich) · Marc Pollefeys (ETH Zurich / Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DreamPropeller: Supercharge Text-to-3D Generation with Parallel Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linqi Zhou (Stanford University) · Andy Shih (Stanford University) · Chenlin Meng (None) · Stefano Ermon (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improved Visual Grounding through Self-Consistent Explanations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruozhen He (Rice University) · Paola Cascante-Bonilla (Rice University) · Ziyan Yang (Rice University) · Alex Berg (None) · Vicente Ordonez (Rice University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/xiaoyuanpigo/maxlin" target="_blank">Towards General Robustness Verification of MaxPool-based Convolutional Neural Networks via Tightening Linear Approximation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Xiao (Nanjing University) · Shiqing Ma (University of Massachusetts at Amherst) · Juan Zhai (University of Massachusetts at Amherst) · Chunrong Fang (Nanjing University) · Jinyuan Jia (Pennsylvania State University) · Zhenyu Chen (nanjing university)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Detection of Long Consistent Cycles and its Application to Distributed Synchronization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shaohan Li (University of Minnesota, Minneapolis) · Yunpeng Shi (University of California, Davis) · Gilad Lerman (University of Minnesota, Minneapolis)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UnionFormer: Unified-Learning Transformer with Multi-View Representation for Image Manipulation Detection and Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuaibo Li (Beijing University of Technology &amp; Institute of Automation, Chinese Academy of Sciences) · Wei Ma (Beijing University of Technology) · Jianwei Guo (Institute of Automation, Chinese Academy of Sciences) · Shibiao Xu (Beijing University of Posts and Telecommunications) · Benchong Li (Beijing University of Technology) · Xiaopeng Zhang (Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://sites.google.com/view/drl-ae" target="_blank">Learning to Control Camera Exposure via Reinforcement Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kyunghyun Lee (LG AI Research) · Ukcheol Shin (Carnegie Mellon University (CMU)) · Byeong-Uk Lee (KRAFTON, Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Behind the Veil: Enhanced Indoor 3D Scene Reconstruction with Occluded Surfaces Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Su Sun (Purdue University) · Henry Zhao (Bosch Research) · Yuliang Guo (Bosch US Research) · Ruoyu Wang (Bosch) · Xinyu Huang (Robert Bosch Research NA) · Yingjie Victor Chen (Purdue University) · Liu Ren (Bosch Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://zyqz97.github.io/Aerial_Lifting/" target="_blank">Aerial Lifting: Neural Urban Semantic and Building Instance Lifting from Aerial Imagery</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqi Zhang (The Chinese University of Hong Kong, Shenzhen) · Guanying Chen (The Chinese University of Hong Kong, Shenzhen) · Jiaxing Chen (Sun Yat-Sen University) · Shuguang Cui (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BlockGCN: Redefine Topology Awareness for Skeleton-Based Action Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxuan Zhou (University of Mannheim) · Xudong Yan (City University of Macau) · Zhi-Qi Cheng (Carnegie Mellon University) · Yan Yan (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Qi Dai (Microsoft Research Asia) · Xian-Sheng Hua (Terminus Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunsheng Ma (Purdue University) · Can Cui (Purdue University) · Xu Cao (University of Illinois Urbana-Champaign) · Wenqian Ye (University of Virginia) · Peiran Liu (Purdue University) · Juanwu Lu (Purdue University) · Amr Abdelraouf (None) · Rohit Gupta (Toyota Motor Corporation) · Kyungtae Han (Toyota Motor North America) · Aniket Bera (Purdue University) · James Rehg (None) · Ziran Wang (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MAPLM: A Real-World Large-Scale Vision-Language Dataset for Map and Traffic Scene Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Cao (University of Illinois Urbana-Champaign) · Tong Zhou (Tencent AI Lab) · Yunsheng Ma (Purdue University) · Wenqian Ye (University of Virginia) · Can Cui (Purdue University) · Kun Tang (Tencent) · Zhipeng Cao (Tencent) · Kaizhao Liang (University of Texas at Austin) · Ziran Wang (Purdue University) · James Rehg (None) · chao zheng (tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Summarize the Past to Predict the Future: Natural Language Descriptions of Context Boost Multimodal Object Interaction Anticipation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Razvan Pasca (None) · Alexey Gavryushin (ETHZ - ETH Zurich) · Muhammad Hamza (Department of Informatics, University of Zurich, University of Zurich) · Yen-Ling Kuo (University of Virginia, Charlottesville) · Kaichun Mo (NVIDIA Research) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Otmar Hilliges (None) · Xi Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Weakly-Supervised Audio-Visual Video Parsing with Prototype-based Pseudo-Labeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kranthi Kumar Rachavarapu (Indian Institute of Technology Madras) · Kalyan Ramakrishnan (University of Oxford) · A. N. Rajagopalan (Indian Institute of Technology Madras)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://oppo-us-research.github.io/SpacetimeGaussians-website/" target="_blank">Spacetime Gaussian Feature Splatting for Real-Time Dynamic View Synthesis</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhan Li (OPPO US Research Center &amp; Portland State University) · Zhang Chen (OPPO US Research Center) · Zhong Li (InnoPeak Technology) · Yi Xu (OPPO US Research Center)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Versatile Medical Image Segmentation Learned from Multi-Source Datasets via Model Self-Disambiguation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyang Chen (University of Pennsylvania, University of Pennsylvania) · Hao Zheng (University of Pennsylvania, University of Pennsylvania) · Yuemeng LI (University of Pennsylvania) · Yuncong Ma (University of Pennsylvania, University of Pennsylvania) · Liang Ma (University of Pennsylvania, University of Pennsylvania) · Hongming Li (University of Pennsylvania, University of Pennsylvania) · Yong Fan (University of Pennsylvania)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Accelerating Neural Field Training via Soft Mining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shakiba Kheradmand (University of British Columbia) · Daniel Rebain (None) · Gopal Sharma (None) · Hossam Isack (Google) · Abhishek Kar (Google) · Andrea Tagliasacchi (Simon Fraser University, Google Brain) · Kwang Moo Yi (University Of British Columbia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Quantifying Uncertainty in Motion Prediction with Variational Bayesian Mixture</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juanwu Lu (Purdue University) · Can Cui (Purdue University) · Yunsheng Ma (Purdue University) · Aniket Bera (Purdue University) · Ziran Wang (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unmixing Diffusion for Self-Supervised Hyperspectral Image Denoising</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haijin Zeng (IMEC &amp; Universiteit Gent) · Jiezhang Cao (ETH Zürich) · Yongyong Chen (Harbin Institute of Technology (Shenzhen)) · Kai Zhang (None) · Hiep Luong (Universiteit Gent - IMEC) · Wilfried Philips (Universiteit Gent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diffusion-driven GAN Inversion for Multi-Modal Face Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jihyun Kim (Yonsei University, LG Electronics) · Changjae Oh (Queen Mary University London) · Hoseok Do (LG Electronics) · Soohyun Kim (Korea University) · Kwanghoon Sohn (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>The Mirrored Influence Hypothesis: Efficient Data Influence Estimation by Harnessing Forward Passes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Myeongseob Ko (Virginia Polytechnic Institute and State University) · Feiyang Kang (Virginia Polytechnic Institute and State University) · Weiyan Shi (Stanford University) · Ming Jin (Virginia Tech) · Zhou Yu (Columbia University) · Ruoxi Jia (Virginia Tech)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deep Equilibrium Diffusion Restoration with Parallel Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiezhang Cao (ETH Zürich) · Yue Shi (Shanghai Jiaotong University) · Kai Zhang (None) · Yulun Zhang (Shanghai Jiao Tong University) · Radu Timofte (University of Würzburg) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Robust Distillation via Untargeted and Targeted Intermediate Adversarial Samples</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junhao Dong (Nanyang Technological University) · Piotr Koniusz (Australian National University) · Junxi Chen (SUN YAT-SEN UNIVERSITY) · Z. Wang (University of British Columbia) · Yew-Soon Ong (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Accurate Spatial Gene Expression Prediction by Integrating Multi-Resolution Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youngmin Chung (Sung Kyun Kwan University) · Ji Hun Ha (Sung Kyun Kwan University) · Kyeong Chan Im (Sungkyunkwan University) · Joo Sang Lee (Sungkyunkwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic Image Restoration In the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fanghua Yu (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Jinjin Gu (University of Sydney) · Zheyuan Li (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences) · Jinfan Hu (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences) · Xiangtao Kong (Hong Kong Polytechnic University) · Xintao Wang (Tencent) · Jingwen He (Shanghai ai lab) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Chao Dong (SIAT)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Descriptor and Word Soups: Overcoming the Parameter Efficiency Accuracy Tradeoff for Out-of-Distribution Few-shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Christopher Liao (None) · Theodoros Tsiligkaridis (MIT Lincoln Laboratory, Massachusetts Institute of Technology) · Brian Kulis (Boston University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Referring Image Editing: Object-level Image Editing via Referring Expressions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chang Liu (None) · Xiangtai Li (Nanyang Technological University) · Henghui Ding (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BiPer: Binary Neural Networks using a Periodic Function</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Edwin Vargas (Universidad Industrial de Santander) · Claudia Correa (Universidad Industrial de Santander) · Carlos Hinojosa (KAUST) · Henry Arguello (Universidad Industrial de Santander)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Streaming Dense Video Captioning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingyi Zhou (Google) · Anurag Arnab (Google) · Shyamal Buch (Google) · Shen Yan (Google Research) · Austin Myers (Google) · Xuehan Xiong (Google) · Arsha Nagrani (Google ) · Cordelia Schmid (Inria / Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dual Prior Unfolding for Snapshot Compressive Imaging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiancheng Zhang (Northwest Polytechnical University Xi'an) · Haijin Zeng (IMEC &amp; Universiteit Gent) · Jiezhang Cao (ETH Zürich) · Yongyong Chen (Harbin Institute of Technology (Shenzhen)) · Dengxiu Yu (Northwest Polytechnical University) · Yinping Zhao (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MoReVQA: Exploring Modular Reasoning Models for Video Question Answering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juhong Min (POSTECH) · Shyamal Buch (Google) · Arsha Nagrani (Google ) · Minsu Cho (POSTECH) · Cordelia Schmid (Inria / Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Can Language Beat Numerical Regression? Language-Based Multimodal Trajectory Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Inhwan Bae (GIST) · Junoh Lee (Gwangju Institute of Science and Technology) · Hae-Gon Jeon (GIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SNI-SLAM: Semantic Neural Implicit SLAM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siting Zhu (Shanghai Jiao Tong University) · Guangming Wang (University of Cambridge) · Hermann Blum (Computer Vision and Geometry Lab, ETH Zürich) · Jiuming Liu (Shanghai Jiao Tong University) · LiangSong (China University of Mining Technology - Xuzhou) · Marc Pollefeys (ETH Zurich / Microsoft) · Hesheng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Hu (Alibaba)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shijie Zhou (University of California, Los Angeles) · Haoran Chang (University of California, Los Angeles) · Sicheng Jiang (University of California, Los Angeles) · Zhiwen Fan (University of Texas, Austin) · Zehao Zhu (University of Texas at Austin) · Dejia Xu (University of Texas at Austin) · Pradyumna Chari (University of California, Los Angeles) · Suya You (University of Southern California) · Zhangyang Wang (University of Texas at Austin) · Achuta Kadambi (UCLA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>On Train-Test Class Overlap and Detection for Image Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chull Hwan Song (Dealicious Inc) · Jooyoung Yoon (Dealicious Inc) · Taebaek Hwang (None) · Shunghyun Choi (Dealicious Inc.) · Yeong Hyeon Gu (Sejong University) · Yannis Avrithis (IARAI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>No More Ambiguity in 360<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-2-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mo&gt;&amp;#x2218;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-6" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-7" class="mjx-mrow"><span id="MJXc-Node-8" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-9" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-10" class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.128em; padding-bottom: 0.316em;">∘</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mo>∘</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-2">^\circ</script> Room Layout via Bi-Layout Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu-Ju Tsai (University of California, Merced) · Jin-Cheng Jhang (National Tsing Hua University) · JINGJING ZHENG (None) · Wei Wang (Amazon) · Albert Chen (Amazon) · Min Sun (Amazon/NTHU) · Cheng-Hao Kuo (Amazon) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://kuai-lab.github.io/cvpr2024waterf/" target="_blank">WateRF: Robust Watermarks in Radiance Fields for Protection of Copyrights</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youngdong Jang (Korea University) · Dong In Lee (Korea University) · MinHyuk Jang (Korea University) · Jong Wook Kim (Korea University) · Feng Yang (Google Research) · Sangpil Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PolarRec: Improving Radio Interferometric Data Reconstruction Using Polar Coordinates</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruoqi Wang (The Hong Kong University of Science and Technology (Guangzhou)) · Zhuoyang Chen (The Hong Kong University of Science and Technology (Guangzhou)) · Jiayi Zhu (Hong Kong University of Science and Technology (Guangzhou)) · Qiong Luo (Hong Kong University of Science and Technology) · Feng Wang (Guangzhou University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NeLF-Pro: Neural Light Field Probes for Multi-Scale Novel View Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zinuo You (ETH Zurich) · Andreas Geiger (University of Tübingen) · Anpei Chen (Department of Computer Science, ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Disentangled Pre-training for Human-Object Interaction Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhuolong Li (South China University of Technology) · Xingao Li (South China University of Technology) · Changxing Ding (South China University of Technology) · Xiangmin Xu (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Decompose-and-Compose: A Compositional Approach to Mitigating Spurious Correlation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fahimeh Hosseini Noohdani (Sharif University of Technology) · Parsa Hosseini (Sharif University of Technology) · Aryan Yazdan Parast (Sharif University of Technology) · Hamidreza Araghi (Sharif University of Technology) · Mahdieh Baghshah (Sharif University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TI2V-Zero: Zero-Shot Image Conditioning for Text-to-Video Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haomiao Ni (The Pennsylvania State University) · Bernhard Egger (Massachusetts Institute of Technology) · Suhas Lohit (Mitsubishi Electric Research Labs) · Anoop Cherian (Mitsubishi Electric Research Labs (MERL)) · Ye Wang (Mitsubishi Electric Research Labs) · Toshiaki Koike-Akino (Mitsubishi Electric Research Labs. (MERL)) · Sharon X. Huang (Pennsylvania State University) · Tim Marks (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Can Biases in ImageNet Models Explain Generalization?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Paul Gavrikov (Offenburg University) · Janis Keuper (Institute for Machine Learning and Analytics, Offenburg University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>No Time to Train: Empowering Non-Parametric Networks for Few-shot 3D Scene Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiangyang Zhu (City University of Hong Kong) · Renrui Zhang (MMLab of CUHK &amp;amp;amp; Shanghai AI Laboratory) · Bowei He (City University of Hong Kong) · Ziyu Guo (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Jiaming Liu (Peking University) · Han Xiao (The Chinese University of Hong Kong &amp; Shanghai AI Laboratory) · Chaoyou Fu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Hao Dong (None) · Peng Gao (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Layout-Agnostic Scene Text Image Synthesis with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qilong Zhangli (Rutgers University) · Jindong Jiang (Rutgers University) · Di Liu (Rutgers University, New Brunswick) · Licheng Yu (None) · Xiaoliang Dai (Facebook) · Ankit Ramchandani (Meta Platforms, Inc.) · Guan Pang (Facebook) · Dimitris N. Metaxas (Rutgers) · Praveen Krishnan (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Spectral Snapshot Reconstruction with Spectral-Spatial Rectification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiancheng Zhang (Northwest Polytechnical University Xi'an) · Haijin Zeng (IMEC &amp; Universiteit Gent) · Yongyong Chen (Harbin Institute of Technology (Shenzhen)) · Dengxiu Yu (Northwest Polytechnical University) · Yinping Zhao (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MetaCloak: Preventing Unauthorized Subject-driven Text-to-image Diffusion-based Synthesis via Meta-learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixin Liu (Lehigh Universisty) · Chenrui Fan (Huazhong University of Science and Technology) · Yutong Dai (Lehigh University) · Xun Chen (Samsung Research America) · Pan Zhou (Huazhong University of Science and Technology) · Lichao Sun (Lehigh University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Language-conditioned Detection Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jang Hyun Cho (University of Texas, Austin) · Philipp Krähenbühl (University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Virtual Immunohistochemistry Staining for Histological Images Assisted by Weakly-supervised Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahan Li (Harbin Institute of Technology) · Jiuyang Dong (Harbin Institute of Technology) · Shenjin Huang (None) · Xi Li (Department of Gastroenterology, Shenzhen Hospital, Peking University) · Junjun Jiang (Harbin Institute of Technology) · Xiaopeng Fan (Harbin Institute of Technology) · Yongbing Zhang (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards HDR and HFR Video from Rolling-Mixed-Bit Spikings</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yakun Chang (None) · Yeliduosi Xiaokaiti (Peking University) · Yujia Liu (School of Computer Science, Peking University, Beijing, China) · Bin Fan (None) · Zhaojun Huang (Peking University) · Tiejun Huang (Peking University) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CDFormer: When Degradation Prediction Embraces Diffusion Model for Blind Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qingguo Liu (Nanjing University of Aeronautics and Astronautics) · Chenyi Zhuang (Nanjing University of Aeronautics and Astronautics) · Pan Gao (Nanjing University of Aeronautics and Astronautics, Tsinghua University) · Jie Qin (Nanjing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RoHM: Robust Human Motion Reconstruction via Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siwei Zhang (ETH Zurich) · Bharat Lal Bhatnagar (Eberhard-Karls-Universität Tübingen) · Yuanlu Xu (Meta Reality Labs Research) · Alexander Winkler (Meta) · Petr Kadlecek (Meta Reality Labs Research) · Siyu Tang (ETH Zurich) · Federica Bogo (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Why Not Use Your Textbook? Knowledge-Enhanced Procedure Planning of Instructional Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kumaranage Ravindu Nagasinghe (Mohamed bin Zayed University of Artificial Intelligence) · Honglu Zhou (Rutgers University) · Malitha Gunawardhana (University of Auckland) · Martin Renqiang Min (NEC Laboratories America) · Daniel Harari (Weizmann Institute of Science) · Muhammad Haris Khan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>iToF-flow-based High Frame Rate Depth Imaging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu Meng (Nanjing University) · Zhou Xue (Li Auto) · Xu Chang (Bytedance Inc) · Xuemei Hu (Nanjing University) · Tao Yue (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Continual Motion Prediction Learning Framework via Meta-Representation Learning and Optimal Memory Buffer Retention Strategy</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dae Jun Kang (None) · Dongsuk Kum (Korea Advanced Institute of Science and Technology) · Sanmin Kim (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leili Goli (University of Toronto) · Cody Reading (Simon Fraser University) · Silvia Sellán (University of Toronto) · Alec Jacobson (University of Toronto and Adobe Systems) · Andrea Tagliasacchi (Simon Fraser University, Google Brain)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HIT: Estimating Internal Human Implicit Tissues from the Body Surface</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Marilyn Keller (Max Planck Institute for Inteligent Systems) · Vaibhav ARORA (INRIA) · Abdelmouttaleb Dakri (None) · Shivam Chandhok (INRIA) · Jürgen Machann (Institute for Diabetes Research and Metabolic Diseases, Helmholtz Center Munich at the University of Tuebingen) · Andreas Fritsche (Eberhard-Karls-Universität Tübingen) · Michael J. Black (University of Tübingen) · Sergi Pujades (INRIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Task-Driven Exploration: Decoupling and Inter-Task Feedback for Joint Moment Retrieval and Highlight Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jin Yang (Xi'an jiao tong university) · Ping Wei (None) · Huan Li (Xi'an Jiaotong University) · Ziyang Ren (Xi&amp;#x27;an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fitting Flats to Flats</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gabriel Dogadov (Technische Universität Berlin) · Ugo Finnendahl (Technische Universität Berlin) · Marc Alexa (TU Berlin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>4D-DRESS: A 4D Dataset of Real-World Human Clothing With Semantic Annotations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenbo Wang (ETHZ - ETH Zurich) · Hsuan-I Ho (ETHZ - ETH Zurich) · Chen Guo (ETH Zurich) · Boxiang Rong (ETHZ - ETH Zurich) · Artur Grigorev () · Jie Song (ETHZ - ETH Zurich) · Juan Jose Zarate (Department of Computer Science, ETHZ - ETH Zurich) · Otmar Hilliges (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://yangxue0827.github.io/auto_mc-reward.html" target="_blank">Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Li (The Chinese University of Hong Kong) · Xue Yang (Shanghai AI Laboratory) · Zhaokai Wang (Shanghai Jiao Tong University) · Xizhou Zhu (Shanghai AI Laboratory) · Jie Zhou (None) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Xiaogang Wang (The Chinese University of Hong Kong) · Hongsheng Li (The Chinese University of Hong Kong) · Lewei Lu (SenseTime) · Jifeng Dai (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SHAP-EDITOR: Instruction-guided Latent 3D Editing in Seconds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minghao Chen (University of Oxford) · Junyu Xie (University of Oxford) · Iro Laina (University of Oxford) · Andrea Vedaldi (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://tobias-kirschstein.github.io/diffusion-avatars/" target="_blank">DiffusionAvatars: Deferred Diffusion for High-fidelity 3D Head Avatars</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tobias Kirschstein (Department of Informatics, Technische Universität München) · Simon Giebenhain (Technische Universität München) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Know Your Neighbors: Improving Single-View Reconstruction via Spatial Vision-Language Reasoning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui Li (None) · Tobias Fischer (ETH Zurich) · Mattia Segu (ETH Zurich - Swiss Federal Institute of Technology) · Marc Pollefeys (ETH Zurich / Microsoft) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Federico Tombari (Google, TUM)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Robust Self-calibration of Focal Lengths from the Fundamental Matrix</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Viktor Kocur (Comenius University in Bratislava) · Daniel Kyselica (Comenius University in Bratislava) · Zuzana Kukelova (Czech Technical University in Prague)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Spin-UP: Spin Light for Natural Light Uncalibrated Photometric Stereo</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zongrui Li (Nanyang Technological University) · Zhan Lu (Nanyang Technological University) · Haojie Yan (Zhejiang University) · Boxin Shi (Peking University) · Gang Pan (Zhejiang University) · Qian Zheng (Zhejiang University) · Xudong Jiang (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GenTron: Diffusion Transformers for Image and Video Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shoufa Chen (The University of Hong Kong) · Mengmeng Xu (Meta AI) · Jiawei Ren (Nanyang Technological University) · Yuren Cong (Institute of Information Processing, Leibniz University Hanover) · Sen He (Meta AI) · Yanping Xie (Meta) · Animesh Sinha (Meta AI) · Ping Luo (The University of Hong Kong) · Tao Xiang (University of Surrey) · Juan-Manuel Pérez-Rúa (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Task-Customized Mixture of Adapters for General Image Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pengfei Zhu (Tianjin University) · Yang Sun (Tianjin University) · Bing Cao (Tianjin University) · Qinghua Hu (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>JDEC: JPEG Decoding via Enhanced Continuous Cosine Coefficients</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Woo Kyoung Han (Korea University) · Sunghoon Im (DGIST) · Jaedeok Kim (NVIDIA) · Kyong Hwan Jin (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Alpha Invariance: On Inverse Scaling Between Distance and Volume Density in Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Joshua Ahn (University of Chicago) · Haochen Wang (Toyota Technological Institute at Chicago) · Raymond A. Yeh (Purdue University) · Greg Shakhnarovich (Toyota Technological Institute at Chicago)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MRFP: Learning Generalizable Semantic Segmentation from Sim-2-Real with Multi-Resolution Feature Perturbation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sumanth Udupa (Indian Institute of Science) · Prajwal Gurunath (Indian Institute of Science) · Aniruddh Sikdar (Indian Institute of Science) · Suresh Sundaram (Indian Institute of Science, Indian institute of science, Bangalore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Real-Time Simulated Avatar from Head-Mounted Sensors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengyi Luo (Carnegie Mellon University) · Jinkun Cao (Carnegie Mellon University) · Rawal Khirodkar (Meta) · Alexander Winkler (Meta) · Jing Huang (Facebook) · Kris Kitani (Carnegie Mellon University) · Weipeng Xu (Meta Reality Labs Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Space Alignments Towards Universal LiDAR Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youquan Liu (Hochschule Bremerhaven) · Lingdong Kong (National University of Singapore) · Xiaoyang Wu (The University of Hong Kong) · Runnan Chen (None) · Xin Li (East China Normal University) · Liang Pan (Shanghai AI Lab) · Ziwei Liu (Nanyang Technological University) · Yuexin Ma (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>BEVSpread: Spread Voxel Pooling for Bird’s-Eye-View Representation in Vision-based Roadside 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenjie Wang (Zhejiang University) · Yehao Lu (Zhejiang University) · Guangcong Zheng (None) · Shuigenzhan (Zhejiang University) · Xiaoqing Ye (Baidu Inc.) · Zichang Tan (Baidu) · Jingdong Wang (Baidu) · Gaoang Wang (Zhejiang University) · Xi Li (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GeoReF: Geometric Alignment Across Shape Variation for Category-level Object Pose Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linfang Zheng (University of Birmingham) · Tze Ho Elden Tse (University of Birmingham) · Chen Wang (Department of computer science, the University of Hong Kong) · Yinghan Sun (Southern University of Science and Technology) · Hua Chen (Southern University of Science and Technology) · Aleš Leonardis (University of Birmingham) · Wei Zhang (Southern University of Science and Technology of China) · Hyung Jin Chang ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OpenESS: Event-based Semantic Scene Understanding with Open Vocabularies</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lingdong Kong (National University of Singapore) · Youquan Liu (Hochschule Bremerhaven) · Lai Xing Ng (Institute for Infocomm Research (I2R), A*STAR) · Benoit Cottereau (CNRS) · Wei Tsang Ooi (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous and Instruction-guided Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Brian Yang (School of Computer Science, Carnegie Mellon University) · Huangyuan Su (Computer Science, School of Engineering and Applied Sciences, Harvard University) · Nikolaos Gkanatsios (Carnegie Mellon University) · Tsung-Wei Ke (CMU, Carnegie Mellon University) · Ayush Jain (Carnegie Mellon University) · Jeff Schneider (Carnegie Mellon University) · Katerina Fragkiadaki (CMU)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andong Wang (University of Hong Kong) · Bo Wu (MIT-IBM Watson AI Lab) · Sunli Chen (Tsinghua University) · Zhenfang Chen (MIT-IBM Watson AI lab) · Haotian Guan (The University of Hong Kong) · Wei-Ning Lee (University of Hong Kong) · Li Erran Li (AWS AI, Amazon) · Chuang Gan (MIT-IBM Watson AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Gear-NeRF: Free-Viewpoint Rendering and Tracking with Motion-aware Spatio-Temporal Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinhang Liu (HKUST) · Yu-Wing Tai (None) · Chi-Keung Tang (The Hong Kong University of Science and Technology) · Pedro Miraldo (None) · Suhas Lohit (Mitsubishi Electric Research Labs) · Moitreya Chatterjee (Mitsubishi Electric Research Labs)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DriveWorld: 4D Pre-trained Scene Understanding via World Models for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Min (Peking University) · Dawei Zhao (Defense Innovation Institute) · Liang Xiao (Defense Innovation Institute) · Jian Zhao () · Xinli Xu (Hong Kong University of Science and Technology) · Zheng Zhu (Tsinghua University) · Lei Jin (Beijing University of Posts and Telecommunications) · Jianshu Li (Ant Group) · Yulan Guo (SUN YAT-SEN UNIVERSITY) · Junliang Xing (Tsinghua University) · Liping Jing (Beijing Jiaotong University) · Yiming Nie (National University of Defense Technology) · Bin Dai (National University of Defense Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffusionLight: Light Probes for Free by Painting a Chrome Ball</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pakkapon Phongthawee (Vidyasirimedhi Institute of Science and Technology) · Worameth Chinchuthakun (Tokyo Institute of Technology) · Nontaphat Sinsunthithet (Vidyasirimedhi Institute of Science and Technology) · Varun Jampani (Google Research) · Amit Raj (Google ) · Pramook Khungurn (Cornell University) · Supasorn Suwajanakorn (Vidyasirimedhi Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OpenStreetView-5M: The Many Roads to Global Visual Geolocation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guillaume Astruc (ENPC/IGN/CNES) · Nicolas Dufour (Ecole Nationale des Ponts et Chausees) · Ioannis Siglidis (Ecole Nationale des Ponts et Chausees) · Constantin Aronssohn (ENPC, Ecole Nationale des Ponts et Chausées) · Nacim Bouia (Ecole Normale Superieure) · Stephanie Fu (University of California, Berkeley) · Romain Loiseau (IMAGINE - LIGM - ENPC, LASTIG - IGN) · Van Nguyen Nguyen (Ecole des Ponts ParisTech) · Charles Raude (ENPC, Ecole Nationale des Ponts et Chausees) · Elliot Vincent (Imagine (LIGM) - Willow (Inria)) · Lintao XU (Université Gustave Eiffel) · Hongyu Zhou (Ecole Nationale des Ponts et Chausees) · Loic Landrieu (ENPC, IGN)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Depth Prompting for Sensor-Agnostic Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jin-Hwi Park (GIST) · Chanhwi Jeong (Gwangju Institute of Science and Technology) · Junoh Lee (Gwangju Institute of Science and Technology) · Hae-Gon Jeon (GIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siyuan Liang (National University of Singapore) · Mingli Zhu (The Chinese University of Hong Kong(Shen Zhen)) · Aishan Liu () · Baoyuan Wu (The Chinese University of Hong Kong, Shenzhen) · Xiaochun Cao (SUN YAT-SEN UNIVERSITY) · Ee-Chien Chang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SiTH: Single-view Textured Human Reconstruction with Image-Conditioned Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hsuan-I Ho (ETHZ - ETH Zurich) · Jie Song (ETHZ - ETH Zurich) · Otmar Hilliges (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FISBe: A real-world benchmark dataset for instance segmentation of long-range thin filamentous structures</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lisa Mais (Max Delbrück Center for Molecular Medicine) · Peter Hirsch (Max Delbrück Center for Molecular Medicine) · Claire Managan (HHMI Janelia Research Campus) · Ramya Kandarpa (Environmental Resources Management (ERM)) · Josef Rumberger (Max Delbrück Center for Molecular Medicine) · Annika Reinke (German Cancer Research Center) · Lena Maier-Hein (German Cancer Research Center (DKFZ)) · Gudrun Ihrke (HHMI Janelia Research Campus) · Dagmar Kainmueller (Universität Potsdam)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://shadowfax11.github.io/cads/" target="_blank">Passive Snapshot Coded Aperture Dual-Pixel RGB-D Imaging</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bhargav Ghanekar (Rice University) · Salman Siddique Khan (Rice University) · Pranav Sharma (IIT Madras, India) · Shreyas Singh (Indian Institute of Technology, Madras) · Vivek Boominathan (Rice University) · Kaushik Mitra (Indian Institute of Technology, Madras, Dhirubhai Ambani Institute Of Information and Communication Technology) · Ashok Veeraraghavan (William Marsh Rice University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Supreeth Narasimhaswamy (Stony Brook University, New York) · Huy Anh Nguyen (Stony Brook University) · Lihan Huang (University of Science and Technology of China) · Minh Hoai (State University of New York, Stony Brook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dual-View Visual Contextualization for Web Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jihyung Kil (Ohio State University, Columbus) · Chan Hee Song (The Ohio State University) · Boyuan Zheng (Ohio State University, Columbus) · Xiang Deng (Google) · Yu Su (Ohio State University) · Wei-Lun Chao (Ohio State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SkySense: A Multi-Modal Remote Sensing Foundation Model Towards Universal Interpretation for Earth Observation Imagery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Guo (Ant Group) · Jiangwei Lao (Ant Group) · Bo Dang (Wuhan University) · Yingying Zhang (Hikvision Research Institute) · Lei Yu (antgroup) · Lixiang Ru (Ant Group) · Liheng Zhong (Ant Group) · Ziyuan Huang (National University of Singapore) · Kang Wu (Wuhan University) · Dingxiang Hu (mybank) · HUIMEI HE (Ant Group) · Jian Wang (, Institute of automation, Chinese academy of science) · Jingdong Chen (Ant Group) · Ming Yang (Ant Group) · Yongjun Zhang (None) · Yansheng Li (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://yangchanghee.github.io/Person-in-Place_page/" target="_blank">Person in Place: Generating Associative Skeleton-Guidance Maps for Human-Object Interaction Image Editing</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            ChangHee Yang (LG Electornic) · ChanHee Kang (Sogang University) · Kyeongbo Kong (Pusan National University) · Hanni Oh (Sogang University) · Suk-Ju Kang (Sogang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/qimaqi/Continuous-Pose-in-NeRF" target="_blank">Continuous Pose for Monocular Cameras in Neural Implicit Representation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qi Ma (ETH Zurich, INSAIT Sofia) · Danda Paudel (INSAIT, Sofia University) · Ajad Chhatkuli (Swiss Federal Institute of Technology) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Detector-Free Structure from Motion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingyi He (Zhejiang University) · Jiaming Sun (Image Derivative Inc.) · Yifan Wang (Zhejiang University) · Sida Peng (None) · Qixing Huang (University of Texas at Austin) · Hujun Bao (Zhejiang University) · Xiaowei Zhou (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learned Lossless Image Compression based on Bit Plane Slicing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhe Zhang (Wuhan University) · Huairui Wang (Wuhan University) · Zhenzhong Chen (Wuhan University) · Shan Liu (Tencent Media Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3DToonify: Creating Your High-Fidelity 3D Stylized Avatar Easily from 2D Portrait Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifang Men (Alibaba Group) · Hanxi Liu (Peking University) · Yuan Yao (Alibaba group) · Miaomiao Cui (Alibaba Group) · Xuansong Xie (Alibaba Group) · Zhouhui Lian (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unleashing the Potential of SAM for Medical Adaptation via Hierarchical Decoding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiheng Cheng (East China Normal University) · Qingyue Wei (Stanford University) · Hongru Zhu (None) · Yan Wang (East China Normal University) · Liangqiong Qu (The University of Hong Kong) · Wei Shao (University of Florida) · Yuyin Zhou (UC Santa Cruz)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jia-Wei Liu (National University of Singapore) · Yan-Pei Cao (Tencent ARC Lab) · Jay Zhangjie Wu (National University of Singapore) · Weijia Mao (NUS) · Yuchao Gu (None) · Rui Zhao (None) · Jussi Keppo (National University of Singapore) · Ying Shan (Tencent) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SonicVisionLM: Playing Sound with Vision Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhifeng Xie (shanghai university) · Shengye Yu (Shanghai University) · Qile He (Shanghai University) · Mengtian Li (Shanghai University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo Boundary Enrichment and Online Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Wu (University of Science and Technology of China) · Huabin Liu (Shanghai Jiao Tong University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Xiao Sun (Shanghai Artificial Intelligence Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation with Deterministic Sampling Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zike Wu (Nanyang Technological University) · Pan Zhou (Sea Group) · YI Xuanyu (National Technological University) · Xiaoding Yuan (Johns Hopkins University) · Hanwang Zhang (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diffusion Time-step Curriculum for One Image to 3D Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            YI Xuanyu (National Technological University) · Zike Wu (Nanyang Technological University) · Qingshan Xu (Nanyang Technological University) · Pan Zhou (Sea Group) · Joo Lim (I2R, A*STAR) · Hanwang Zhang (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Continual-MAE: Adaptive Distribution Masked Autoencoders for Continual Test-Time Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaming Liu (Peking University) · Ran Xu (Beijing University of Posts and Telecommunications) · Senqiao Yang (Harbin Institute of Technology) · Renrui Zhang (MMLab of CUHK &amp;amp;amp; Shanghai AI Laboratory) · Qizhe Zhang (Peking University) · Zehui Chen (University of Science and Technology of China) · Yandong Guo (OPPO Research Institute) · Shanghang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CapsFusion: Rethinking Image-Text Data at Scale</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiying Yu (Tsinghua University) · Quan Sun (BAAI) · Xiaosong Zhang (Beijing Academy of Artificial Intelligence) · Yufeng Cui (Beihang University) · Fan Zhang (Beijing Academy of Artificial Intelligence) · Yue Cao (Beijing Academy of Artificial Intelligence) · Xinlong Wang (Beijing Academy of Artificial Intelligence) · Jingjing Liu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generative Multimodal Models are In-Context Learners</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Quan Sun (BAAI) · Yufeng Cui (Beihang University) · Xiaosong Zhang (Beijing Academy of Artificial Intelligence) · Fan Zhang (Beijing Academy of Artificial Intelligence) · Qiying Yu (Tsinghua University) · Yueze Wang (Beijing Academy of Artificial Intelligence) · Yongming Rao (Tsinghua University) · Jingjing Liu (Tsinghua University, Tsinghua University) · Tiejun Huang (Peking University) · Xinlong Wang (Beijing Academy of Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LoS: Local Structure Guided Stereo Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kunhong Li (SUN YAT-SEN UNIVERSITY) · Longguang Wang (National University of Defense Technology) · Ye Zhang (SUN YAT-SEN UNIVERSITY) · Kaiwen Xue (Huawei Cloud Computing Technologies Co., Ltd.) · Shunbo Zhou (Huawei Technologies Ltd.) · Yulan Guo (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://diffusemix.github.io/" target="_blank">DiffuseMix: Label-Preserving Data Augmentation with Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Khawar Islam (FloppyDisk.AI) · Muhammad Zaigham Zaheer (Mohamed bin Zayed University of Artificial Intelligence) · Arif Mahmood (Information Technology University, Lahore) · Karthik Nandakumar (Mohamed Bin Zayed University of Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Depth-Aware Concealed Crop Detection in Dense Agricultural Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liqiong Wang (China Three Gorges University) · Jinyu Yang (University of Birmingham) · Yanfu Zhang (College of William and Mary) · Fangyi Wang (China Three Gorges University) · Feng Zheng (Southern University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yun Liu (Tsinghua University) · Haolin Yang (Beijing University of Posts and Telecommunications) · Xu Si (Tsinghua University) · Ling Liu (Beijing Institute of Technology) · Zipeng Li (Tsinghua University, Tsinghua University) · Yuxiang Zhang (Tsinghua University, Tsinghua University) · Yebin Liu (Tsinghua University) · Li Yi ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://www.liuyebin.com/HHMR/HHMR.html" target="_blank">HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengcheng Li (Tsinghua University, Tsinghua University) · Hongwen Zhang (Beijing Normal University) · Yuxiang Zhang (Tsinghua University, Tsinghua University) · Ruizhi Shao (Tsinghua University, Tsinghua University) · Tao Yu (Tsinghua University, Tsinghua University) · Yebin Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://huliangxiao.github.io/GaussianAvatar" target="_blank">GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liangxiao Hu (Harbin Institute of Technology) · Hongwen Zhang (Beijing Normal University) · Yuxiang Zhang (Tsinghua University, Tsinghua University) · Boyao ZHOU (Tsinghua University) · Boning Liu (Department of Automation, Tsinghua University) · Shengping Zhang (Harbin Institute of Technology) · Liqiang Nie (Harbin Institute of Technology (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://li-ronghui.github.io/lodge" target="_blank">Lodge: A Coarse to Fine Diffusion Network for Long Dance Generation guided by the Characteristic Dance Primitives</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ronghui Li (Tsinghua University) · Yuxiang Zhang (Tsinghua University, Tsinghua University) · Yachao Zhang (Tsinghua University) · Hongwen Zhang (Beijing Normal University) · Jie Guo (Peng Cheng Laboratory) · Yan Zhang (ETH Zurich) · Yebin Liu (Tsinghua University) · Xiu Li (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://zhangyux15.github.io/ProxyCapV2" target="_blank">ProxyCap: Real-time Monocular Full-body Capture in World Space via Human-Centric Proxy-to-Motion Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxiang Zhang (Tsinghua University, Tsinghua University) · Hongwen Zhang (Beijing Normal University) · Liangxiao Hu (Harbin Institute of Technology) · Jiajun Zhang (Beijing University of Posts and Telecommunications) · Hongwei Yi (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Shengping Zhang (Harbin Institute of Technology) · Yebin Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wen Yin (Huazhong University of Science and Technology) · Jian Lou (Zhejiang University) · Pan Zhou (Huazhong University of Science and Technology) · Yulai Xie (Huazhong University of Science and Technology) · Dan Feng (Huazhong University of Science and Technology) · Yuhua Sun (None) · Tailai Zhang (Huazhong University of Science and Technology) · Lichao Sun (Lehigh University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AiOS: All-in-One-Stage Expressive Human Pose and Shape Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qingping SUN (City University of Hong Kong) · Yanjun Wang (Shanghai Jiao Tong University) · Ailing Zeng (IDEA) · Wanqi Yin (SenseTime Research ) · Chen Wei (SenseTime International PTE. LTD.) · Wenjia Wang (University of Hong Kong) · Haiy Mei (None) · Chi LEUNG (City University of Hong Kong) · Ziwei Liu (Nanyang Technological University) · Lei Yang (The Chinese University of Hong Kong) · Zhongang Cai (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Generalized Zero-Shot Learning by Exploring the Diverse Semantics from  External Class Names</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yapeng Li (Wuhan University) · Yong Luo (Wuhan University) · Zengmao Wang (Wuhan University) · Bo Du (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bootstrapping Chest CT Image Understanding by Distilling Knowledge from X-ray Expert Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weiwei Cao (University of Science and Technology of China) · Jianpeng Zhang (None) · Yingda Xia (Alibaba Group) · Tony C. W. MOK (Alibaba DAMO Academy) · Zi Li (Alibaba DAMO Academy) · Xianghua Ye (Zhejiang University) · Le Lu (Alibaba Group) · Jian Zheng (Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences) · Yuxing Tang (Alibaba Group) · Ling Zhang (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Modality-Agnostic Structural Image Representation Learning for Deformable Multi-Modality Medical Image Registration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tony C. W. MOK (Alibaba DAMO Academy) · Zi Li (Alibaba DAMO Academy) · Yunhao Bai () · Jianpeng Zhang (None) · Wei Liu (Alibaba Group) · Yan-Jie Zhou (DAMO Academy, Alibaba Group) · Ke Yan (Alibaba DAMO Academy) · Dakai Jin (Alibaba Group) · Yu Shi (China Medical University Shenyang) · Xiaoli Yin (China Medical University Shenyang) · Le Lu (Alibaba Group) · Ling Zhang (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuzhou Huang (None) · Liangbin Xie (Macau) · Xintao Wang (Tencent) · Ziyang Yuan (Tsinghua University, Tsinghua University) · Xiaodong Cun (Tencent AI Lab) · Yixiao Ge (Tencent) · Jiantao Zhou (University of Macau) · Chao Dong (SIAT) · Rui Huang (The Chinese University of Hong Kong, Shenzhen) · Ruimao Zhang (The Chinese University of Hong Kong (Shenzhen)) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/tmtuan1307/NAYER" target="_blank">NAYER: Noisy Layer Data Generation for Efficient and Effective Data-free Knowledge Distillation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minh-Tuan Tran (Monash University) · Trung Le (Monash University) · Xuan-May Le (University of Melbourne) · Mehrtash Harandi (Monash University) · Quan Tran (servicenow) · Dinh Phung (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LASO: Language-guided Affordance Segmentation on 3D Object</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yicong Li (national university of singaore, National University of Singapore) · Na Zhao (Singapore University of Technology and Design) · Junbin Xiao (None) · Chun Feng (University of Science and Technology of China) · Xiang Wang (University of Science and Technology of China) · Tat-seng Chua (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Cinematic Behavior Transfer via NeRF-based Differentiable Filming</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuekun Jiang (Shanghai Artificial Intelligence Laboratory) · Anyi Rao (Stanford University) · Jingbo Wang (Shanghai AI LAB) · Dahua Lin (The Chinese University of Hong Kong) · Bo Dai (Shanghai AI Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://control4darxiv.github.io/" target="_blank">Control4D: Efficient 4D Portrait  Editing with Text</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruizhi Shao (Tsinghua University, Tsinghua University) · Jingxiang Sun (None) · Cheng Peng (Tsinghua University, Tsinghua University) · Zerong Zheng (Tsinghua University) · Boyao ZHOU (Tsinghua University) · Hongwen Zhang (Beijing Normal University) · Yebin Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://humannorm.github.io/" target="_blank">HumanNorm: Learning Normal Diffusion Model for High-quality and Realistic 3D Human Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Huang (Northwest Polytechnical University Xi'an) · Ruizhi Shao (Tsinghua University, Tsinghua University) · Qi Zhang (Northwest Polytechnical University Xi'an) · Hongwen Zhang (Beijing Normal University) · Ying Feng (Northwest Polytechnical University Xi'an) · Yebin Liu (Tsinghua University) · Qing Wang (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://yuelangx.github.io/gaussianheadavatar" target="_blank">Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuelang Xu (Tsinghua University, Tsinghua University) · Benwang Chen (Tsinghua University, Tsinghua University) · Zhe Li (Tsinghua University) · Hongwen Zhang (Beijing Normal University) · Lizhen Wang (Tsinghua University, Tsinghua University) · Zerong Zheng (Tsinghua University) · Yebin Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ID-like Prompt Learning for Few-Shot Out-of-Distribution Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichen Bai (None) · Zongbo Han (Tianjin University) · Bing Cao (Tianjin University) · Xiaoheng Jiang (Zhengzhou University) · Qinghua Hu (Tianjin University) · Changqing Zhang (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://qiuyu96.github.io/CoDeF/" target="_blank">CoDeF: Content Deformation Fields for Temporally Consistent Video Processing</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Ouyang (Department of Computer Science and Engineering, Hong Kong University of Science and Technology) · Qiuyu Wang (Ant Group) · Yuxi Xiao (Zhejiang University) · Qingyan Bai (Hong Kong University of Science and Technology) · Juntao Zhang (Hong Kong University of Science and Technology) · Kecheng Zheng (Ant Group) · Xiaowei Zhou (None) · Qifeng Chen (Hong Kong University of Science and Technology) · Yujun Shen (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diffusion Models Without Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jing Nathan Yan (Cornell University) · Jiatao Gu (Apple (MLR)) · Alexander Rush (Cornell Tech)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HIMap: HybrId Representation Learning for End-to-end Vectorized HD Map Construction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yi ZHOU (Samsung Research China-Beijing(SRCB)) · Hui Zhang (Samsung Rearch China-Beijing(SRCB)) · Jiaqian Yu (Samsung R&amp;D Institute China - Beijing) · yifan yang (Samsung) · Sangil Jung (samsung) · Seung-In Park (Samsung Advanced Institute of Technology) · ByungIn Yoo (Samsung Advanced Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-3-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mn&gt;360&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-11" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-12" class="mjx-mrow"><span id="MJXc-Node-13" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">360</span></span><span id="MJXc-Node-14" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span><span id="MJXc-Node-15" class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">x</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>360</mn><mo>+</mo><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">360+x</script>: A Panoptic Multi-modal Scene Understanding Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Chen (University of Birmingham) · Yuqi Hou (University of Birmingham) · Chenyuan Qu (University of Birmingham) · Irene Testini (Cardiff University) · Xiaohan Hong (University of Birmingham) · Jianbo Jiao (University of Birmingham)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Promptable Behaviors: Personalizing Multi-Objective Rewards from Human Preferences</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minyoung Hwang (Seoul National University) · Luca Weihs (Allen Institute for Artificial Intelligence) · Chanwoo Park (Massachusetts Institute of Technology) · Kimin Lee (KAIST) · Aniruddha Kembhavi (Allen Institute for Artificial Intelligence) · Kiana Ehsani (Allen Institute for Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>C3Net: Compound Conditioned ControlNet for Multimodal Content Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juntao Zhang (Hong Kong University of Science and Technology) · Yuehuai LIU (Hong Kong University of Science and Technology) · Yu-Wing Tai (None) · Chi-Keung Tang (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learnable Earth Parser: Discovering 3D Prototypes in Aerial Scans</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Romain Loiseau (IMAGINE - LIGM - ENPC, LASTIG - IGN) · Elliot Vincent (Imagine (LIGM) - Willow (Inria)) · Mathieu Aubry (ENPC) · Loic Landrieu (ENPC, IGN)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Domain-Rectifying Adapter for Cross-Domain Few-Shot Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            嘉鹏 苏 (Harbin Institute of Technology) · Qi Fan (The Hong Kong University of Science and Technology) · Wenjie Pei (Harbin Institute of Technology) · Guangming Lu (Harbin Institute of Technology, Shenzhen) · Fanglin Chen (Harbin Institute of Technology (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Test-Time Zero-Shot Temporal Action Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Benedetta Liberatori (University of Trento) · Alessandro Conti (University of Trento) · Paolo Rota (University of Trento) · Yiming Wang (Fondazione Bruno Kessler) · Elisa Ricci (University of Trento)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GAFusion: Adaptive Fusing LiDAR and Camera with Multiple Guidance for 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaotian Li (Nanjing University of Posts and Telecommunications) · Baojie Fan (Nanjing University of Posts and Telecommunications) · Jiandong Tian (The Shenyang Institute of Automation, Chinese Academy of Sciences) · Huijie Fan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Resurrecting Old Classes with New Data for Exemplar-Free Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dipam Goswami (Computer Vision Center) · Albin Soutif (Computer Vision Center, Universitat Autònoma de Barcelona) · Yuyang Liu (Shenyang Institute of Automation, Chinese Academy of Sciences/ University of Chinese Academy of Sciences) · Sandesh Kamath (Computer Vision Center, Universitat Autónoma de Barcelona) · Bartłomiej Twardowski (Computer Vision Center / IDEAS NCBR) · Joost van de Weijer (Computer Vision Center Barcelona)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FairCLIP: Harnessing Fairness in Vision-Language Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Luo (Harvard Ophthalmology AI Lab) · MIN SHI (Harvard University) · Muhammad Osama Khan (New York University) · Muhammad Muneeb Afzal (New York University) · Hao Huang (New York University) · Shuaihang Yuan (New York University) · Yu Tian (None) · Luo Song (Mass Eye and Ear) · Ava Kouhana (Harvard Ophthalmology AI lab) · Tobias Elze (Harvard University) · Yi Fang (New York University) · Mengyu Wang (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ChatScene: Knowledge-Enabled Safety-Critical Scenario Generation for Autonomous Vehicles</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Zhang (UIUC) · Chejian Xu (University of Illinois at Urbana-Champaign) · Bo Li (UIUC)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised Universal Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xudong Wang (Electrical Engineering &amp; Computer Science Department, University of California Berkeley) · Dantong Niu (University of California, Berkeley) · Xinyang Han (UC Berkeley) · Long Lian (University of California, Berkeley) · Roei Herzig (Tel Aviv University) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>The More You See in 2D, the More You Perceive in 3D</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyang Han (UC Berkeley) · Zelin Gao () · Angjoo Kanazawa (UC Berkeley) · Shubham Goel (Avataar) · Yossi Gandelsman (University of California, Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Finsler-Laplace-Beltrami Operators with Application to Shape Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Simon Weber (Technische Universität München) · Thomas Dagès (Technion - Israel Institute of Technology) · Maolin Gao (None) · Daniel Cremers (Technical University Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Probabilistic Speech-Driven 3D Facial Motion Synthesis: New Benchmarks, Methods, and Applications</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Karren Yang (Apple) · Anurag Ranjan (Apple) · Jen-Hao Rick Chang (Apple) · Raviteja Vemulapalli (None) · Oncel Tuzel (Apple)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VidLA: Video-Language Alignment at Scale</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mamshad Nayeem Rizve (Amazon) · Fan Fei (Amazon) · Jayakrishnan Unnikrishnan (Amazon) · Son Dinh Tran (Amazon) · Benjamin Yao (Amazon) · Belinda Zeng (Amazon) · Mubarak Shah (University of Central Florida) · Trishul Chilimbi (Department of Computer Science, University of Wisconsin - Madison)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>4SAVED - Four Seasons Autonomous Vehicle Environment Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daniel Kent (Michigan State University) · Mohammed Alyaqoub (Michigan State University) · Xiaohu Lu (Michigan State University) · Sayed Khatounabadi (Michigan State University) · Kookjin Sung (Michigan State University) · Cole Scheller (Michigan State University) · Alexander Dalat (University of Michigan - Ann Arbor) · Xinwei Guo (Michigan State University) · Asma Bin Thabit (Michigan State University) · Roberto Muntaner Whitley (Michigan State University) · Hayder Radha (Michigan State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>360Loc: A Dataset and Benchmark for Omnidirectional Visual Localization with Cross-device Queries</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huajian Huang (The Hong Kong University of Science and Technology) · Changkun Liu (Hong Kong University of Science and Technology) · Yipeng Zhu (Hong Kong University of Science and Technology) · Hui Cheng (SUN YAT-SEN UNIVERSITY) · Tristan Braud (Hong Kong University of Science and Technology) · Sai-Kit Yeung (The Hong Kong University of Science and Technology (HKUST))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MeshPose: Unifying DensePose and 3D Body Mesh reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Eric-Tuan Le (University College London) · Antonios Kakolyris (Snap Inc.) · Petros Koutras (Snap Inc.) · Himmy Tam (Snap Inc.) · Efstratios Skordos (Snap Inc.) · George Papandreou (Snap Inc.) · Riza Alp Guler (Snap Inc.) · Iasonas Kokkinos (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MULAN: A Multi Layer Annotated Dataset for Controllable Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Petru-Daniel Tudosiu (Huawei) · Yongxin Yang (Queen Mary University of London) · Shifeng Zhang (Huawei Technologies Ltd.) · Fei Chen (Huawei Noah's Ark Lab) · Steven McDonagh (University of Edinburgh) · Gerasimos Lampouras (Huawei Technologies Ltd.) · Ignacio Iacobacci (Huawei Noah's Ark Lab) · Sarah Parisot (Huawei)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LightIt: Illumination Modeling and Control for Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peter Kocsis (None) · Kalyan Sunkavalli (Adobe Research) · Julien Philip (Adobe Systems) · Matthias Nießner (Technical University of Munich) · Yannick Hold-Geoffroy (Adobe Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://robinbruneau.github.io/publications/rnb_neus.html" target="_blank">RNb-NeuS: Reflectance and Normal-based Multi-View 3D Reconstruction</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Baptiste Brument (IRIT, University of Toulouse, France) · Robin Bruneau (University of Copenhagen) · Yvain Queau (CNRS) · Jean Mélou (IRIT) · Francois Lauze (Department fo Computer Science, University of Copenhagen) · Jean-Denis Durou (IRIT) · Lilian Calvet (OR-X, Balgrist Hospital, University of Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CaDeT: a Causal Disentanglement Approach for Robust Trajectory Prediction in Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mozhgan Pourkeshavarz (None) · Junrui Zhang (University of Toronto) · Amir Rasouli (Huawei Technologies Canada)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://cvpr.thecvf.com/Conferences/2024/telling-left-from-right.github.io" target="_blank">Telling Left from Right: Identifying Geometry-Aware Semantic Correspondence</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyi Zhang (Shanghai Jiao Tong University) · Charles Herrmann (Google) · Junhwa Hur (Google) · Eric Chen (University of Illinois Urbana-Champaign) · Varun Jampani (Google Research) · Deqing Sun (Google) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-Guided 3D Face Synthesis - From Generation to Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunjie Wu (NetEase, Inc.) · Yapeng Meng (Tsinghua University, Tsinghua University) · Zhipeng Hu (Leihuo Game, NetEase) · Lincheng Li () · Haoqian Wu (NetEase Fuxi AI Lab) · Kun Zhou (Zhejiang University) · Weiwei Xu (Zhejiang University) · Xin Yu (University of Queensland)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EgoGen: An Egocentric Synthetic Data Generator</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gen Li (ETH Zurich) · Kaifeng Zhao (ETHZ - ETH Zurich) · Siwei Zhang (ETH Zurich) · Xiaozhong Lyu (Department of Computer Science, ETHZ - ETH Zurich) · Mihai Dusmanu (Microsoft) · Yan Zhang (ETH Zurich) · Marc Pollefeys (ETH Zurich / Microsoft) · Siyu Tang (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Single Domain-Generalized Object Detection: A Focus on Diversification and Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Muhammad Sohail Danish (Mohamed bin Zayed University of Artificial Intelligence) · Muhammad Haris Khan (None) · Muhammad Akhtar Munir (None) · M. Sarfraz (Karlsruher Institut für Technologie) · Mohsen Ali (Information Technology University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GP-NeRF: Generalized Perception NeRF for Context-Aware 3D Scene Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Li (Northwest Polytechnical University) · Dingwen Zhang (Northwestern Polytechnical University) · Yalun Dai (Nanyang Technological University) · Nian Liu (Mohamed bin Zayed University of Artificial Intelligence) · Lechao Cheng (Hefei University of Technology) · Li Jingfeng (Northwest Polytechnical University Xi'an) · Jingdong Wang (Baidu) · Junwei Han (Northwestern Polytechnical University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Validating Privacy-Preserving Face Recognition under a Minimum Assumption</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hui Zhang (Anhui University) · Xingbo Dong (Anhui University) · YenLungLai (Anhui University) · Ying Zhou (Anhui University) · Xiaoyan ZHANG (Anhui University) · Xingguo Lv (Anhui University) · Zhe Jin (Anhui University) · Xuejun Li (Anhui University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3D Geometry-aware Deformable Gaussian Splatting for Dynamic View Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhicheng Lu (Northwest Polytechnical University Xi&amp;amp;amp;#x27;an) · xiang guo (Northwest Polytechnical University Xi'an) · Le Hui (Nanjing University Of Science And Technology) · Tianrui Chen (Northwest Polytechnical University Xi'an) · Min Yang (None) · Xiao Tang (None) · feng zhu (None) · Yuchao Dai (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HUNTER: Unsupervised Human-centric 3D Detection via Transferring Knowledge from Synthetic Instances to Real Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichen Yao (ShanghaiTech University) · Zimo Jiang (ShanghaiTech University) · YUJING SUN (the University of Hong Kong, University of Hong Kong) · Zhencai Zhu (Innovation Academy for Microsatellites) · Xinge Zhu (The Chinese University of Hong Kong) · Runnan Chen (None) · Yuexin Ma (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujun Shi (national university of singaore, National University of Singapore) · Chuhui Xue (ByteDance Inc.) · Jun Hao Liew (ByteDance) · Jiachun Pan (National University of Singapore) · Hanshu Yan (ByteDance) · Wenqing Zhang (Huazhong University of Science and Technology) · Vincent Y. F. Tan (National University of Singapore) · Song Bai (ByteDance)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adversarially Robust Few-shot Learning via Parameter Co-distillation of Similarity and Class Concept Learners</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junhao Dong (Nanyang Technological University) · Piotr Koniusz (Australian National University) · Junxi Chen (SUN YAT-SEN UNIVERSITY) · Xiaohua Xie (SUN YAT-SEN UNIVERSITY) · Yew-Soon Ong (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generative Latent Coding for Ultra-Low Bitrate Image Compression</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaoyang Jia (University of Science and Technology of China) · Jiahao Li (Microsoft Research Asia) · Bin Li (Microsoft) · Houqiang Li (University of Science and Technology of China) · Yan Lu (Microsoft Research Asia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SPU-PMD: Self-Supervised Point Cloud Upsampling via Progressive Mesh Deformation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanzhe Liu (None) · Rong Chen (Dalian Maritime University) · Yushi Li (Xi'an Jiaotong-Liverpool University) · Yixi Li (Dalian Martime University) · Xuehou Tan (Tokai University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Differentiable Point-based Inverse Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hoon-Gyu Chung (POSTECH) · Seokjun Choi (Pohang University of Science and Technology) · Seung-Hwan Baek (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PromptCoT: Align Prompt Distribution via Adapted Chain-of-Thought</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyi Yao (None) · Yijiang Liu (Nanjing University) · Zhen Dong (UC Berkeley) · Mingfei Guo (Stanford University) · Helan Hu (Peking University) · Kurt Keutzer (EECS, UC Berkeley) · Li Du (Nanjing University) · Daquan Zhou (National University of Singapore) · Shanghang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GS-IR: 3D Gaussian Splatting for Inverse Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhihao Liang (South China University of Technology) · Qi Zhang (Tencent AI Lab) · Ying Feng (Tencent AI Lab) · Ying Shan (Tencent) · Kui Jia (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Uncovering What, Why and How:  A Comprehensive Benchmark for Causation Understanding of Video Anomaly</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hang Du (Beijing University of Posts and Telecommunications) · Sicheng Zhang (Beijing University of Posts and Telecommunications) · Binzhu Xie (Beijing University of Posts and Telecommunications) · Guoshun Nan (Beijing University of Posts and Telecommunications) · Jiayang Zhang (Beijing University of Posts and Telecommunications) · Junrui Xu (Beijing University of Posts and Telecommunications) · Hangyu Liu (Beijing University of Posts and Telecommunications) · Sicong Leng (Nanyang Technological University) · Jiangming Liu (Yunnan University) · Hehe Fan (None) · Dajiu Huang (South China University) · Jing Feng (Beijing University of Posts and Telecommunications) · Linli Chen (Sichuan University) · Can Zhang (Beijing University of Posts and Telecommunications) · Xuhuan Li (Beijing University of Posts and Telecommunications) · Hao Zhang (Beijing University of Posts and Telecommunications) · Jianhang Chen (Beijing University of Posts and Telecommunications) · Qimei Cui (Beijing University of Posts and Telecommunications) · Xiaofeng Tao (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Long-Tail Class Incremental Learning via Independent Sub-prototype Construction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xi Wang (Xidian University) · Xu Yang (Xi'an University of Electronic Science and Technology) · jie yin (None) · Kun Wei (Xidian University) · Cheng Deng (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Make Pixels Dance: High-Dynamic Video Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Zeng (ByteDance) · Guoqiang Wei (ByteDance) · Jiani Zheng (None) · Jiaxin Zou (ByteDance Ltd.) · Yang Wei (East China Normal University) · Yuchen Zhang ( ByteDance Research) · Hang Li (ByteDance Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://sites.google.com/view/diffusion-edfs" target="_blank">Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyunwoo Ryu (None) · Jiwoo Kim (Yonsei University) · Hyunseok An (Yonsei University) · Junwoo Chang (Yonsei University) · Joohwan Seo (University of California, Berkeley) · Taehan Kim (Samsung) · Yubin Kim (Massachusetts Institute of Technology) · Chaewon Hwang (Ewha Women's University) · Jongeun Choi (Yonsei University) · Roberto Horowitz (University of California, Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking Diffusion Model for Multi-Contrast MRI Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangyuan Li (Zhejiang University) · Chen Rao (Zhejiang University) · Juncheng Mo (Zhejiang University) · Zhanjie Zhang (Zhejiang University) · Wei Xing (Zhejiang University) · Lei Zhao (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OrCo: Towards Better Generalization via Orthogonality and Contrast for Few-Shot Class-Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Noor Ahmed (Saarland Informatics Campus, Max-Planck Institute) · Anna Kukleva (MPII) · Bernt Schiele (Max Planck Institute for Informatics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking the Spatial Inconsistency in Classifier-Free Diffusion Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dazhong Shen (Shanghai Artificial Intelligence Laboratory) · Guanglu Song (Sensetime X-Lab) · Zeyue Xue (The University of Hong Kong) · Fu-Yun Wang (The Chinese University of Hong Kong) · Yu Liu (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Equi-angular Representations for Online Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minhyuk Seo (Yonsei University) · Hyunseo Koh (Gwangju Institute of Science and Technology) · Wonje Jeung (Yonsei University) · Minjae Lee (Yonsei University) · San Kim (Yonsei University) · Hankook Lee (Sungkyunkwan University) · Sungjun Cho (LG AI Research) · Sungik Choi (LG AI Research) · Hyunwoo Kim (Zhejiang Lab) · Jonghyun Choi (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Collaborative Semantic Occupancy Prediction with Hybrid Feature Fusion in Connected Automated Vehicles</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui Song (Technical University of Munich) · Chenwei Liang (Fraunhofer) · Hu Cao (Technical University of Munich) · Zhiran Yan (Technische Hochschule Ingolstadt) · Walter Zimmer (Technical University of Munich (TUM)) · Markus Gross (Fraunhofer IVI) · Andreas Festag (Technische Hochschule Ingolstadt) · Alois Knoll (Technical University Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CosmicMan: A Text-to-Image Foundation Model for Humans</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shikai Li (Shanghai AI Lab) · Jianglin Fu (None) · Kaiyuan Liu (None) · Wentao Wang (Shanghai AI Laboratory) · Kwan-Yee Lin (The Chinese University of Hong Kong) · Wayne Wu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Bird’s Eye View Semantic Segmentation by Task Decomposition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianhao Zhao (Wuhan University) · Yongcan Chen (Wuhan University) · Yu Wu (Wuhan University) · Tianyang Liu (Wuhan University) · Bo Du (Wuhan University) · Peilun Xiao (Didi Research) · shi qiu (None) · Hongda Yang (Beijing DiDi Infinity Technology and Development Co., Ltd.) · Guozhen Li (Didi Global) · yi yang (Didi Global) · Yutian Lin (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Video Compression with Feature Modulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahao Li (Microsoft Research Asia) · Bin Li (Microsoft) · Yan Lu (Microsoft Research Asia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GenesisTex: Adapting Image Denoising Diffusion to Texture Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenjian Gao (None) · Boyan Jiang (Fudan University) · Xinghui Li (Tsinghua University, Tsinghua University) · YingPeng Zhang (South China University of Technology) · Qian Yu (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>KD-DETR: Knowledge Distillation for Detection Transformer with Consistent Distillation Points Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu Wang (Baidu) · Xin Li (Baidu) · Shengzhao Wen (Baidu) · gang zhang (Baidu Inc.) · Haixiao Yue (Baidu) · Haocheng Feng (Baidu) · Junyu Han (Baidu) · Errui Ding (Baidu Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Puff-Net: Efficient Style Transfer with Pure Content and Style Feature Fusion Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sizhe Zheng (None) · Pan Gao (Nanjing University of Aeronautics and Astronautics, Tsinghua University) · Peng Zhou (Nanjing University of Aeronautics and Astronautics) · Jie Qin (Nanjing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Efficient Test-Time Adaptation of Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Adilbek Karmanov (Mohamed bin Zayed University of Artificial Intelligence) · Dayan Guan (Nanyang Technological University) · Shijian Lu (Nanyang Technological University) · Abdulmotaleb El Saddik (Mohamed bin Zayed University of Artificial Intelligence) · Eric P. Xing (Mohamed bin Zayed Univeristy of AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Selective-Stereo: Adaptive Frequency Information Selection for Stereo Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xianqi Wang (Huazhong University of Science and Technology) · Gangwei Xu (Huazhong University of Science and Technology) · Hao Jia (Huazhong University of Science and Technology) · Xin Yang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mozhgan Pourkeshavarz (None) · Mohammad Sabokrou (Okinawa Institute of Science and Technology (OIST)) · Amir Rasouli (Huawei Technologies Canada)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuwen Tan (Huazhong University of Science and Technology) · Qinhao Zhou (Huazhong University of Science and Technology) · Xiang Xiang (None) · Ke Wang (Alibaba Group) · Yuchuan Wu (Alibaba Group) · Yongbin Li (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SpikeNeRF: Learning Neural Radiance Fields from Continuous Spike Stream</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lin Zhu (Beijing Institute of Technology) · Kangmin Jia (Beijing Institute of Technology) · Yifan Zhao (Beihang University) · Yunshan Qi (BeiHang University) · Lizhi Wang (None) · Hua Huang (Beijing Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yonglu Li (Shanghai Jiaotong University) · Xiaoqian Wu (None) · Xinpeng Liu (Shanghai Jiao Tong University) · Zehao Wang (Shanghai Jiao Tong University) · Yiming Dou (University of Michigan - Ann Arbor) · Yikun Ji (Shanghai Jiaotong University) · Junyi Zhang (Shanghai Jiao Tong University) · Yixing Li (Shanghai Jiao Tong University) · Xudong LU (The Chinese University of Hong Kong) · Jingru Tan (Central South University) · Cewu Lu (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Implicit Motion Function</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yue Gao (Microsoft Research) · Jiahao Li (Microsoft Research Asia) · Lei Chu (Microsoft Research Asia) · Yan Lu (Microsoft Research Asia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ICP-Flow: LiDAR Scene Flow Estimation with ICP</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yancong Lin (Delft University of Technology) · Zimin Xia (Motional)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hierarchical Intra-modal Correlation Learning for Label-free 3D Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Kang () · Lei Chu (Microsoft Research Asia) · Jiahao Li (Microsoft Research Asia) · Xuejin Chen (University of Science and Technology of China) · Yan Lu (Microsoft Research Asia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ExtraNeRF: Visibility-Aware View Extrapolation of Neural Radiance Fields with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Meng-Li Shih (University of Washington) · Wei-Chiu Ma (Cornell University) · Lorenzo Boyice (Google) · Aleksander Holynski (UC Berkeley &amp; Google Research) · Forrester Cole (Google) · Brian Curless (University of Washington) · Janne Kontkanen (Research, Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Perturbing Attention Gives You More Bang for the Buck: Subtle Imaging Perturbations That Efficiently Fool Customized Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingyao Xu (Beijing Jiaotong University) · Yuetong Lu (Beijing Jiaotong University) · Yandong Li (Google Research) · Siyang Lu (Beijing Jiaotong University) · Dongdong Wang (University of Central Florida) · Xiang Wei (Beijing Jiaotong university)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Intensity-Robust Autofocus for Spike Camera</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changqing Su (Peking University) · Zhiyuan Ye (Nanchang Hangkong University) · Yongsheng Xiao (Nanchang Hangkong University) · You Zhou (Nanjing University) · Zhen Cheng (Tsinghua University, Tsinghua University) · Bo Xiong (Peking University) · Zhaofei Yu (Peking University) · Tiejun Huang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Visual Prompting for Generalized Few-shot Segmentation: A Multi-scale Approach</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mir Hossain Hossain (None) · Mennatullah Siam (None) · Leonid Sigal (University Of British Columbia) · Jim Little (University of British Columbia, Canada)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>S2MAE: A Spatial-Spectral Pretraining Foundation Model for Spectral Remote Sensing Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuyang Li (None) · Danfeng Hong (Chinese Academy of Sciences, Aerospace Information Research Institute) · Jocelyn Chanussot (INRIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Object Pose Estimation via the Aggregation of Diffusion Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianfu Wang (University of Chinese Academy of Sciences) · Guosheng Hu (Oosto) · Hongguang Wang (Shenyang Institute of Automation)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FSC: Few-point Shape Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xianzu Wu (Jianghan University) · Xianfeng Wu (Jianghan University) · Tianyu Luan (State University of New York at Buffalo) · Yajing Bai (Jianghan University) · Zhongyuan Lai (Jianghan University) · Junsong Yuan (State University of New York at Buffalo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RTMO: Towards High-Performance One-Stage Real-Time Multi-Person Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Lu (SIGS, Tsinghua University) · Tao Jiang (Shanghai AI Laboratory) · Yining Li (Shanghai AI Laboratory) · Xiangtai Li (Nanyang Technological University) · Kai Chen (Shanghai AI Laboratory) · Wenming Yang (Tsinghua University,)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Resolution Limit of Single-Photon LIDAR</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Stanley H. Chan (Purdue University, USA) · Hashan K Weerasooriya (Purdue University) · Weijian Zhang (Purdue University) · Pamela Abshire (University of Maryland, College Park) · Istvan Gyongy (University of Edinburgh, University of Edinburgh) · Robert Henderson (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://fytalon.github.io/pienerf/" target="_blank">PIE-NeRF: Physics-based Interactive Elastodynamics with NeRF</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yutao Feng (Zhejiang University) · Yintong Shang (University of Utah) · Xuan Li (None) · Tianjia Shao (Zhejiang University) · Chenfanfu Jiang (University of California, Los Angeles) · Yin Yang (University of Utah)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ViLa-MIL: Dual-scale Vision-Language Multiple Instance Learning for Whole Slide Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiangbo Shi (Xi'an Jiaotong University) · Chen Li (Xi'an Jiaotong University) · Tieliang Gong (Xi'an Jiaotong University) · Yefeng Zheng (None) · Huazhu Fu (Institute of High Performance Computing, Singapore, A*STAR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MOHO: Learning Single-view Hand-held Object Reconstruction with Multi-view Occlusion-Aware Supervision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenyangguang Zhang (Tsinghua University) · Guanlong Jiao (Tsinghua University, Tsinghua University) · Yan Di (Technische Universität München) · Gu Wang (Tsinghua University) · Ziqin Huang (Tsinghua University, Tsinghua University) · Ruida Zhang (Department of Automation, Tsinghua University, Tsinghua University) · Fabian Manhardt (Google) · Bowen Fu (Technische Universität München) · Federico Tombari (Google, TUM) · Xiangyang Ji (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://open3dis.github.io/" target="_blank">Open3DIS: Open-Vocabulary 3D Instance Segmentation with 2D Mask Guidance</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Phuc Nguyen (VinAI Research) · Tuan Duc Ngo (UMass Amherst) · Evangelos Kalogerakis (UMass Amherst) · Chuang Gan (MIT-IBM Watson AI Lab) · Anh Tran (VinAI Research) · Cuong Pham (Posts &amp; Telecommunications Institute of Technology and VinAI Research) · Khoi Nguyen (VinAI Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Class Incremental Learning with Multi-Teacher Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haitao Wen (University of Electronic Science and Technology of China) · Lili Pan (University of Electronic Science and Technology of China) · Yu Dai (University of Electronic Science and Technology of China) · Heqian Qiu (University of Electronic Science and Technology of China) · Lanxiao Wang (University of Electronic Science and Technology of China) · Qingbo Wu (University of Electronic Science and Technology of China) · Hongliang Li (University of Electronic Science and Technology of China, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kanchana Ranasinghe (None) · Satya Narayan Shukla (Meta AI) · Omid Poursaeed (Meta AI) · Michael Ryoo (Stony Brook University) · Tsung-Yu Lin (Department of Computer Science, University of Massachusetts, Amherst)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fusing Personal and Environmental Cues for Identification and Segmentation of First-Person Camera Wearers in Third-Person View</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziwei Zhao (Indiana University) · Yuchen Wang (Indiana University) · Chuhua Wang (Indiana University, Bloomington)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AlignMiF: Geometry-Aligned Multimodal Implicit Field for Enhanced LiDAR-Camera Joint Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tao Tang (SYSU) · Guangrun Wang (University of Oxford) · Yixing Lao (None) · Peng Chen (Alibaba Group) · Jie Liu (North China University of Technology) · Liang Lin (SUN YAT-SEN UNIVERSITY, Tsinghua University) · Kaicheng Yu (Alibaba Group) · Xiaodan Liang (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GOAT-Bench: A Benchmark for Multi-modal Lifelong Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mukul Khanna (Georgia Institute of Technology) · Ram Ramrakhya (None) · Gunjan Chhablani (Georgia Institute of Technology) · Sriram Yenamandra (Georgia Institute of Technology) · Theo Gervet (Carnegie Mellon University) · Matthew Chang (University of Illinois, Urbana Champaign) · Zsolt Kira (Georgia Institute of Technology) · Devendra Singh Chaplot (Carnegie Mellon University) · Dhruv Batra (FAIR (Meta) and Georgia Tech) · Roozbeh Mottaghi (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Marina Neseem (Brown University) · Conor McCullough (Google) · Randy Hsin (Google) · Chas Leichner (Google) · Shan Li (Google) · In Suk Chong (Google) · Andrew Howard (Google) · Lukasz Lew (Research, Google) · Sherief Reda (Brown University) · Ville-Mikko Rautio (Google) · Daniele Moro (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Boosting Neural Representations for Videos with a Conditional Decoder</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            XINJIE ZHANG (The Hong Kong University of Science and Technology) · Ren Yang (Microsoft Research Asia) · Dailan He (The Chinese University of Hong Kong) · Xingtong Ge (Beijing Institute of Technology) · Tongda Xu (Tsinghua University) · Yan Wang (Tsinghua University, Tsinghua University) · Hongwei Qin (SenseTime Co.) · Jun Zhang (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UniVS: Unified and Universal Video Segmentation with Prompts as Queries</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minghan Li (The Hong Kong Polytechnic University ) · Shuai Li (The Hong Kong Polytechnic University) · Xindong Zhang (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) · Lei Zhang (The Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Don’t drop your samples! Coherence-aware training benefits Conditional diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nicolas Dufour (Ecole Nationale des Ponts et Chausees) · Victor Besnier (Valeo.ai) · Vicky Kalogeiton (Ecole polytechnique, IP Paris) · David Picard (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3D Building Reconstruction from Monocular Remote Sensing Images with Multi-level Supervisions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weijia Li (Sun Yat-sen University) · Haote Yang (PJLab) · Zhenghao Hu (SUN YAT-SEN UNIVERSITY) · Juepeng Zheng (Sun Yat-Sen University) · Gui-Song Xia (Wuhan University) · Conghui He (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>In Search of a Data Transformation That Accelerates Neural Field Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junwon Seo (None) · Sangyoon Lee (POSTECH) · Kwang In Kim (Pohang University of Science and Technology) · Jaeho Lee (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning the 3D Fauna of the Web</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zizhang Li (Zhejiang University) · Dor Litvak (University of Texas at Austin) · Ruining Li (University of Oxford) · Yunzhi Zhang (Stanford University) · Tomas Jakab (University of Oxford) · Christian Rupprecht (University of Oxford) · Shangzhe Wu (Stanford University) · Andrea Vedaldi (University of Oxford) · Jiajun Wu (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongjie Wang (Princeton University) · Bhishma Dedhia (Princeton University) · Niraj Jha (Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Modular Blind Video Quality Assessment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wen Wen (City University of Hong Kong) · Mu Li (The Chinese University of Hong Kong, Shenzhen) · Yabin ZHANG (Bytedance) · Yiting Liao (Bytedance) · Junlin Li (ByteDance Inc.) · Li zhang (Bytedance Inc.) · Kede Ma (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Robust 3D Object Detection with LiDAR and 4D Radar Fusion in Various Weather Conditions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujeong Chae (KAIST) · Hyeonseong Kim (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SIGNeRF: Scene Integrated Generation for Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jan-Niklas Dihlmann (Eberhard-Karls-Universität Tübingen) · Andreas Engelhardt (University of Tübingen) · Hendrik Lensch (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Synergistic Global-space Camera and Human Reconstruction from Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yizhou Zhao (Carnegie Mellon University) · Tuanfeng Y. Wang (None) · Bhiksha Raj (Carnegie Mellon University) · Min Xu (Carnegie Mellon University) · Jimei Yang (Adobe Research) · Chun-Hao P. Huang (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Honghao Chen (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xiangxiang Chu (MeiTuan) · Renyongjian (University of the Chinese Academy of Sciences) · Xin Zhao (University of Science and Technology Beijing) · Kaiqi Huang (, Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Generalizing to Unseen Domains with Few Labels</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chamuditha Jayanga Galappaththige (Queensland University of Technology) · Sanoojan Baliah (Mohamed bin Zayed University of Artificial Intelligence) · Malitha Gunawardhana (University of Auckland) · Muhammad Haris Khan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Detailed and Robust 3D Clothed Human Reconstruction with High-Frequency and Low-Frequency Information of Parametric Body Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifan Yang (South China University of Technology) · Dong Liu (South China University of Technology) · Shuhai Zhang (South China University of Technology) · Zeshuai Deng (SCUT) · Zixiong Huang (South China University of Technology) · Mingkui Tan (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Snapshot Lidar: Fourier embedding of phasors for single-image depth reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sarah Friday (Dartmouth College) · Yunzi Shi (Dartmouth College) · Yaswanth Kumar Cherivirala (Univ. of Michigan/NVIDIA) · Vishwanath Saragadam (University of California, Riverside) · Adithya Pediredla (Dartmouth College)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://dr-hair.github.io/Dr-Hair/" target="_blank">Dr.Hair: Reconstructing Scalp-Connected Hair Strands without Pre-training via Differentiable Rendering of Line Segments</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yusuke Takimoto (Huawei Technologies Japan K.K.) · Hikari Takehara (Huawei Technologies Japan K.K.) · Hiroyuki Sato (Huawei Technologies Japan K.K.) · Zihao Zhu (Keio University) · Bo Zheng (Huawei Technologies Japan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/ICTMCG/Make-Your-Anchor" target="_blank">Make-Your-Anchor: A Diffusion-based 2D Avatar Generation Framework</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyao Huang (, Chinese Academy of Sciences) · Fan Tang (Institute of Computing Technology, CAS) · Yong Zhang (Tencent AI Lab) · Xiaodong Cun (Tencent AI Lab) · Juan Cao (Institute of Computing Technology, Chinese Academy of Sciences) · Jintao Li (Institute of Computing Technology, Chinese Academy of Sciences) · Tong-yee Lee (National Cheng Kung University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GLID: Pre-training a Generalist Encoder-Decoder Vision Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jihao Liu (The Chinese University of Hong Kong) · Jinliang Zheng (Tsinghua University) · Yu Liu (The Chinese University of Hong Kong) · Hongsheng Li (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsigned Orthogonal Distance Fields: An Accurate Neural Implicit Representation for Diverse 3D Shapes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            YuJie Lu (Donghua University, Shanghai) · Long Wan (Donghua University, Shanghai) · Nayu Ding (Donghua University, Shanghai) · Yulong Wang (Donghua University, Shanghai) · Shuhan Shen (Institute of automation, Chinese academy of science) · Shen Cai (Donghua University) · Lin Gao (University of Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Edge-Aware 3D Instance Segmentation Network with Intelligent Semantic Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wonseok Roh (Korea University) · Hwanhee Jung (Korea University) · Giljoo Nam (Meta) · Jinseop Yeom (Korea University) · Hyunje Park (Korea University) · Sang Ho Yoon (KAIST) · Sangpil Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>In-distribution Public Data Synthesis with Diffusion Models for Differentially Private Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinseong Park (Seoul National University) · Yujin Choi (Seoul National University) · Jaewook Lee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifei Chen (Huawei) · Dapeng Chen (Huawei Technologies Ltd.) · Ruijin Liu (Xi'an Jiaotong University) · Sai Zhou (Huawei Technologies Ltd.) · Wenyuan Xue (Huawei Technologies Ltd.) · Wei Peng (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Qi (National University of Singapore) · Zehong Yan (National University of Singapore) · Wynne Hsu (National University of Singapore) · Mong Li Lee (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Spatial-Aware Regression for Keypoint Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongkai Wang (Peking University) · Shiliang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Binding Touch to Everything: Learning Unified Multimodal Tactile Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fengyu Yang (Yale University) · Chao Feng () · Ziyang Chen (University of Michigan) · Hyoungseob Park (Yale University) · Daniel Wang (Yale University) · Yiming Dou (University of Michigan - Ann Arbor) · Ziyao Zeng (Yale University) · xien chen (Yale University) · Suchisrit Gangopadhyay (Yale University) · Andrew Owens (University of Michigan) · Alex Wong (Yale University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nikita Drobyshev (Meta) · Antoni Bigata Casademunt (Imperial College London) · Konstantinos Vougioukas (Facebook) · Zoe Landgraf (Facebook) · Stavros Petridis (Facebook) · Maja Pantic (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Shadow-Enlightened Image Outpainting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hang Yu (Shanghai University) · Ruilin Li (None) · Shaorong Xie (Shanghai University) · Jiayan Qiu (Univerisity of Leicester)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qi Yang (School of Artificial Intelligence, University of Chinese Academy of Sciences.) · Xing Nie (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Tong Li (Meituan) · Gaopengfei (Beijing SanKuai Online Technology Co., Ltd.) · Ying Guo (Meituan) · Cheng Zhen (Meituan) · Pengfei Yan (Meituan) · Shiming Xiang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>D3still: Decoupled Differential Distillation for Asymmetric Image Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yi Xie (South China University of Technology) · Yihong Lin (South China University of Technology) · Wenjie Cai () · Xuemiao Xu (South China University of Technology) · Huaidong Zhang (South China University of Technology) · Yong Du (Ocean University of China) · Shengfeng He (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LiDAR-Net: A Real-scanned 3D Point Cloud Dataset for Indoor Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanwen Guo (Nanjing University) · Yuanqi Li (Nanjing University) · Dayong Ren (nanjing university) · Xiaohong Zhang (None) · Jiawei Li (Nanjing University) · Liang Pu (None) · Changfeng Ma (Nanjing University) · xiaoyu zhan (Nanjing University) · Jie Guo (Nanjing University) · Mingqiang Wei (Nanjing University of Aeronautics and Astronautics) · Yan Zhang (None) · Piaopiao Yu (Nanjing University) · Shuangyu Yang (Nanjing University) · Donghao Ji (nanjing university) · Huisheng Ye (Nanjing University) · Hao Sun (nanjing university) · Yansong Liu (nanjing university) · Yinuo Chen (Nanjing University) · Jiaqi Zhu (nanjing university) · Hongyu Liu (nanjing university)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Delving into the Trajectory Long-tail Distribution for Muti-object Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sijia Chen (Huazhong University of Science and Technology) · En Yu (Huazhong University of Science and Technology) · Jinyang Li (Huazhong University of Science and Technology) · Wenbing Tao (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Single-View Refractive Index Tomography with Neural Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Brandon Zhao (California Institute of Technology) · Aviad Levis (California Institute of Technology) · Liam Connor (California Institute of Technology) · Pratul P. Srinivasan (Google Research) · Katherine Bouman (California Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MTLoRA: Low-Rank Adaptation Approach for Efficient Multi-Task Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ahmed Agiza (None) · Marina Neseem (Brown University) · Sherief Reda (Brown University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zuoyue Li (ETH Zürich) · Zhenqiang Li (The University of Tokyo) · Zhaopeng Cui (None) · Marc Pollefeys (ETH Zurich / Microsoft) · Martin R. Oswald (University of Amsterdam)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adaptive Softassign via Hadamard-Equipped Sinkhorn</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Binrui Shen (Xi'an Jiaotong-Liverpool University) · Qiang Niu (Xi'an Jiaotong-Liverpool University) · Shengxin Zhu (Beijing Normal Unversity)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Analyzing and Improving the Training Dynamics of Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tero Karras (NVIDIA) · Miika Aittala (NVIDIA) · Jaakko Lehtinen (Aalto University &amp; NVIDIA) · Janne Hellsten (NVIDIA) · Timo Aila (NVIDIA) · Samuli Laine (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OneLLM: One Framework to Align All Modalities with Language</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaming Han (The Chinese University of Hong Kong) · Kaixiong Gong (None) · Yiyuan Zhang (The Chinese University of Hong Kong) · Jiaqi Wang (Shanghai AI Laboratory) · Kaipeng Zhang (Shanghai AI Laboratory) · Dahua Lin (The Chinese University of Hong Kong) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Peng Gao (The Chinese University of Hong Kong) · Xiangyu Yue (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Cross-view and Cross-pose Completion for 3D Human Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Matthieu Armando (Naver Labs Europe) · Salma Galaaoui (Naver Labs Europe) · Fabien Baradel (NAVER LABS Europe) · Thomas Lucas (Naver Labs Europe) · Vincent Leroy (Naver Labs Europe) · Romain BRÉGIER (None) · Philippe Weinzaepfel (Naver Labs Europe) · Grégory Rogez (Naver Labs Europe)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>See, Say, and Segment: Correcting False Premises with LMMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tsung-Han Wu (University of California, Berkeley) · Giscard Biamby (University of California, Berkeley) · David Chan (University of California Berkeley) · Lisa Dunlap (University of California, Berkeley) · Ritwik Gupta (Defense Innovation Unit) · Xudong Wang (Electrical Engineering &amp; Computer Science Department, University of California Berkeley) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Joseph Gonzalez (University of California - Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DetDiffusion: Synergizing Generative and Perceptive Models for Enhanced Data Generation and Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yibo Wang (Tsinghua  University) · Ruiyuan Gao (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Kai Chen (The Hong Kong University of Science and Technology) · Kaiqiang Zhou (Huawei Technologies Ltd.) · Yingjie CAI (The Chinese University of Hong Kong) · Lanqing Hong (Huawei Technologies Ltd.) · Zhenguo Li (Huawei) · Lihui Jiang (Huawei Technologies Ltd.) · Dit-Yan Yeung (Hong Kong University of Science and Technology) · Qiang Xu (The Chinese University of Hong Kong) · Kai Zhang (Shenzhen International Graduate School, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Discriminative Dynamics with Label Corruption for Noisy Label Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Suyeon Kim (Pohang University of Science and Technology) · Dongha Lee (Yonsei University) · SeongKu Kang (University of Illinois Urbana-Champaign) · Sukang Chae (Pohang University of Science and Technology) · Sanghwan Jang (POSTECH) · Hwanjo Yu (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dynamic Inertial Poser (DynaIP): Part-Based Motion Dynamics Learning for Enhanced Human Pose Estimation with Sparse Inertial Sensors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu Zhang (Shanghai Jiaotong University) · Songpengcheng Xia () · Lei Chu (University of Southern California) · Jiarui Yang (Shanghai Jiaotong University) · Qi Wu (Shanghai Jiaotong University) · Ling Pei (Shanghai Jiao Tong Univeristy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Cam4DOcc: Benchmark for Camera-Only 4D Occupancy Forecasting in Autonomous Driving Applications</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyi Ma (Shanghai Jiao Tong University) · Xieyuanli Chen (National University of Defense Technology) · Jiawei Huang (HAOMO Technology Co., Ltd) · Jingyi Xu (Beijing Institute of Technology) · Zhen Luo (Beijing Institute of Technology) · Jintao Xu (Xi'an Jiaotong University) · Weihao Gu (Tsinghua University, Tsinghua University) · Rui Ai (HAOMO.AI Technology Co.,Ltd. ) · Hesheng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Question Aware Vision Transformer for Multimodal Reasoning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Roy Ganz (Technion - Israel Institute of Technology, Technion) · Yair Kittenplon (AWS AI Labs) · Aviad Aberdam (Amazon AWS AI) · Elad Ben Avraham (Amazon) · Oren Nuriel (Amazon) · Shai Mazor (Amazon) · Ron Litman (Amazon AI Labs)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking Generalizable Face Anti-spoofing via Hierarchical Prototype-guided Distribution Refinement in Hyperbolic Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengyang Hu (Shanghai Jiao Tong University) · Ke-Yue Zhang (Tencent) · Taiping Yao (Tencent Youtu Lab) · Shouhong Ding (Tencent Youtu Lab) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Efficient Replay in Federated Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichen Li (Huazhong University of Science and Technology) · Qunwei Li (Ant Group) · Haozhao Wang (Huazhong University of Science and Technology) · Ruixuan Li (Huazhong University of Science and Technology) · Wenliang Zhong (Ant Group) · Guannan Zhang (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SceneFun3D: Fine-Grained Functionality and Affordance Understanding in 3D Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alexandros Delitzas (ETH Zurich) · Ayça Takmaz (None) · Federico Tombari (Google, TUM) · Robert Sumner (Massachusetts Institute of Technology) · Marc Pollefeys (ETH Zurich / Microsoft) · Francis Engelmann (Department of Computer Science, ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Effective Video Mirror Detection with Inconsistent Motion Cues</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alex Warren (Swansea University) · Ke Xu (City University of Hong Kong) · Jiaying Lin (City University of Hong Kong) · Gary Tam (Swansea University) · Rynson W.H. Lau (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Desigen: A Pipeline for Controllable Design Template Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haohan Weng (South China University of Technology) · Danqing Huang (Microsoft) · YU QIAO (Central South University) · Hu Zheng (Keio University, Tokyo Institute of Technology) · Chin-Yew Lin (Microsoft) · Tong Zhang (South China University of Technology) · C. L. Philip Chen (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ControlRoom3D: Room Generation using Semantic Controls</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jonas Schult (Rheinisch Westfälische Technische Hochschule Aachen) · Sam Tsai (Meta) · Lukas Hoellein (None) · Bichen Wu (Facebook) · Jialiang Wang (Facebook) · Chih-Yao Ma (Facebook) · Kunpeng Li (Meta) · Xiaofang Wang (Meta) · Felix Wimbauer (Technical University of Munich) · Zijian He (None) · Peizhao Zhang (Facebook) · Bastian Leibe (RWTH Aachen University) · Peter Vajda (Facebook) · Ji Hou (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LAN: Learning to Adapt Noise for Image Denoising</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changjin Kim (Hanyang University) · Tae Hyun Kim (Hanyang Univ.) · Sungyong Baik (Hanyang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kelvin C.K. Chan (Google) · Yang Zhao (Google) · Xuhui Jia (Google) · Ming-Hsuan Yang (University of California at Merced) · Huisheng Wang (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diff-BGM: A Diffusion Model for Video Background Music Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sizhe Li (Peking University) · Yiming Qin (Peking University) · Minghang Zheng (Peking University) · Xin Jin (Beijing Electronic Science and Technology Institute) · Yang Liu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Restricted Memory Banks Improve Video Object Segmentation: A Revisit</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junbao Zhou () · Ziqi Pang (UIUC) · Yu-Xiong Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiaLoc: An Iterative Approach to Embodied Dialog Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chao Zhang (Toshiba Europe Ltd) · Mohan Li (Toshiba Europe Ltd) · Ignas Budvytis (University of Cambridge) · Stephan Liwicki (Toshiba Europe Ltd)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Super-Resolution Reconstruction from Bayer-Pattern Spike Streams</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanchen Dong (Peking University) · Ruiqin Xiong (Peking University) · Jian Zhang (None) · Zhaofei Yu (Peking University) · Xiaopeng Fan (Harbin Institute of Technology) · Shuyuan Zhu (University of Electronic Science and Technology of China) · Tiejun Huang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Artist-Friendly Relightable and Animatable Neural Heads</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yingyan Xu (Department of Computer Science, ETHZ - ETH Zurich) · Prashanth Chandran (None) · Sebastian Weiss (DisneyResearch|Studios) · Markus Gross (Disney Research, Disney) · Gaspard Zoss (Disney Research, Disney) · Derek Bradley (DisneyResearch|Studios)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SwiftBrush: One-Step Text-to-Image Diffusion Model with Variational Score Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Thuan Nguyen (VinAI Research) · Anh Tran (VinAI Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Looking 3D: Anomaly Detection with 2D-3D Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ankan Kumar Bhunia (The University of Edinburgh) · Changjian Li (University of Edinburgh) · Hakan Bilen (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://zhang-zx.github.io/AVID/" target="_blank">AVID: Any-Length Video Inpainting with Diffusion Model</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhixing Zhang (Rutgers University) · Bichen Wu (Facebook) · Xiaoyan Wang (Massachusetts Institute of Technology) · Yaqiao Luo (Facebook) · Luxin Zhang (Meta) · Yinan Zhao (Facebook) · Peter Vajda (Facebook) · Dimitris N. Metaxas (Rutgers) · Licheng Yu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Circuit Design and Efficient Simulation of Quantum Inner Product and Empirical Studies of Its Effect on Near-Term Hybrid Quantum-Classic Machine Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Xiong (Shanghai Jiao Tong University) · Yehui Tang (Shanghai Jiaotong University) · Xinyu Ye (Shanghai Jiaotong University) · Junchi Yan (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OpenBias: Open-set Bias Detection in Text-to-Image Generative Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Moreno D'Incà (University of Trento) · Elia Peruzzo (University of Trento) · Massimiliano Mancini (University of Trento) · Dejia Xu (University of Texas at Austin) · Vidit Goel (Georgia Tech | UIUC / Oregon | PAIR) · Xingqian Xu (University of Illinois, Urbana Champaign) · Zhangyang Wang (University of Texas at Austin) · Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR) · Nicu Sebe (University of Trento)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Shadows Don’t Lie and Lines Can't Bend! Generative Models don't know Projective Geometry...for now</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ayush Sarkar (Department of Computer Science at University of Illinois Urbana-Champaign) · Hanlin Mai (University of Illinois Urbana Champaign) · Amitabh Mahapatra (University of Illinois Urbana-Champaign) · David Forsyth (University of Illinois at Urbana-Champaign) · Svetlana Lazebnik (University of Illinois at Urbana-Champaign) · Anand Bhattad (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Point Cloud Pre-training with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            xiao zheng (None) · Xiaoshui Huang (Shanghai AI Laboratory) · Guofeng Mei (Fondazione Bruno Kessler) · Zhaoyang Lyu (Shanghai AI Laboratory) · Yuenan Hou (Shanghai AI Laboratory) · Wanli Ouyang (University of Sydney) · Bo Dai (Shanghai AI Laboratory) · Yongshun Gong (Shandong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://schardong.github.io/ifmorph/" target="_blank">Neural Implicit Morphing of Face Images</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guilherme Schardong (Institute of Systems and Robotics, University of Coimbra) · Tiago Novello (IMPA) · Hallison Paz (IMPA) · Iurii Medvedev (Institute of Systems and Robotics, University of Coimbra) · Vinícius Silva (PUC-Rio) · Luiz Velho (IMPA) · Nuno Gonçalves (University of Coimbra)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GDA: Generalized Diffusion for Robust Test-time Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yun-Yun Tsai (Columbia University) · Fu-Chen Chen (Amazon Lab126) · Albert Chen (Amazon) · Junfeng Yang (Columbia University) · Che-Chun Su (Amazon) · Min Sun (Amazon/NTHU) · Cheng-Hao Kuo (Amazon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VSRD: Instance-Aware Volumetric Silhouette Rendering for Weakly Supervised 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihua Liu (Tokyo Institute of Technology) · Hiroki Sakuma (T2 Inc.) · Masatoshi Okutomi (Tokyo Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Permutation Equivariance of Transformers and Its Applications</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hengyuan Xu (None) · Liyao Xiang (Shanghai Jiao Tong University) · Hangyu Ye (Shanghai Jiaotong University) · Dixi Yao (University of Toronto) · Pengzhi Chu (Shanghai Jiaotong University) · Baochun Li (University of Toronto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain Adaptive Segmentation of 3D Point Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhimin Yuan (School of Informatics Xiamen University) · Wankang Zeng (Xiamen University) · Yanfei Su (Xiamen University) · Weiquan Liu (Xiamen University) · Ming Cheng (Xiamen University) · Yulan Guo (SUN YAT-SEN UNIVERSITY) · Cheng Wang (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SubT-MRS Datasets: Pushing SLAM Towards All-weather Environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shibo Zhao (Carnegie Mellon University) · Yuanjun Gao (Carnegie Mellon University) · Tianhao Wu (University of Virginia, Charlottesville) · Damanpreet Singh (CMU, Carnegie Mellon University) · Rushan Jiang (Oracle) · Haoxiang Sun (Carnegie Mellon University) · Mansi Sarawata (CMU, Carnegie Mellon University) · Warren Whittaker (Carnegie Mellon University) · Ian Higgins (Carnegie Mellon University) · Shaoshu Su (State University of New York at Buffalo) · Yi Du (State University of New York at Buffalo) · Can Xu (None) · John Keller (Carnegie Mellon University) · Jay Karhade (Carnegie Mellon University) · Lucas Nogueira (Carnegie Mellon University) · Sourojit Saha (CMU, Carnegie Mellon University) · Yuheng Qiu (CMU, Carnegie Mellon University) · Ji Zhang (Carnegie Mellon University) · Wenshan Wang (School of Computer Science, Carnegie Mellon University) · Chen Wang (University at Buffalo) · Sebastian Scherer (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SpecNeRF: Gaussian Directional Encoding for Specular Reflections</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Ma (None) · Vasu Agrawal (Meta Reality Labs Research) · Haithem Turki (Carnegie Mellon University) · Changil Kim (Facebook) · Chen Gao (Meta) · Pedro V. Sander (Hong Kong University of Science and Technology) · Michael Zollhoefer (Meta) · Christian Richardt (Meta Reality Labs)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generating Enhanced Negatives for Training Language-Based Object Detectors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiyu Zhao (Rutgers University, New Brunswick) · Long Zhao (Google Research) · Vijay Kumar BG (NEC Laboratories America) · Yumin Suh (NEC Labs America) · Dimitris N. Metaxas (Rutgers) · Manmohan Chandraker (UC San Diego) · Samuel Schulter (NEC Laboratories America)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Beyond Seen Primitive Concepts and Attribute-Object Compositional Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nirat Saini (University of Maryland College Park) · Khoi Pham (University of Maryland, College Park) · Abhinav Shrivastava (University of Maryland)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SLICE: Stabilized LIME for Consistent Explanations for Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Revoti Prasad Bora (Norwegian University of Science and Technology) · Kiran Raja (Norwegian University of Science and Technology) · Philipp Terhörst (Paderborn University, Germany) · Raymond Veldhuis (University of Twente) · Raghavendra Ramachandra (Norwegian University of Science and Technology (NTNU))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://democaricature.github.io/" target="_blank">DemoCaricature: Democratising Caricature Generation with a Rough Sketch</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dar-Yen Chen (SketchX) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Subhadeep Koley (University of Surrey) · Aneeshan Sain (University of Surrey) · Pinaki Nath Chowdhury (University of Surrey) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SPIDeRS: Structured Polarization for Invisible Depth and Reflectance Sensing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tomoki Ichikawa (Kyoto University) · Shohei Nobuhara (Kyoto Institute of Technology) · Ko Nishino (Kyoto University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Contrastive Learning for DeepFake Classification and Localization via Multi-Label Ranking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cheng-Yao Hong (Academia Sinica) · Yen-Chi Hsu (Department of computer science and informational engineering, National Taiwan University) · Tyng-Luh Liu (IIS/Academia Sinica)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junsheng Zhou (Tsinghua University) · Weiqi Zhang (Tsinghua University) · Baorui Ma (BAAI) · Kanle Shi (Kuaishou Technology) · Yu-Shen Liu (None) · Zhizhong Han (Wayne State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jialong Zuo (Huazhong University of Science and Technology) · Hanyu Zhou (Huazhong University of Science and Technology) · Ying Nie (Huawei Noah's Ark Lab) · Feng Zhang (Huazhong University of Science and Technology) · Tianyu Guo (Peking University) · Nong Sang (Huazhong University of Science and Technology) · Yunhe Wang (Huawei Noah's Ark Lab) · Changxin Gao (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/kcshum/pose-conditioned-NeRF-object-fusion" target="_blank">Language-driven Object Fusion into Neural Radiance Fields with Pose-Conditioned Dataset Updates</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ka Chun SHUM (The Hong Kong University of Science and Technology) · Jaeyeon Kim (Hong Kong University of Science and Technology) · Binh-Son Hua (Trinity College Dublin) · Thanh Nguyen (Deakin University) · Sai-Kit Yeung (The Hong Kong University of Science and Technology (HKUST))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VTimeLLM: Empower LLM to Grasp Video Moments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Huang (Tsinghua University) · Xin Wang (None) · Hong Chen (None) · Zihan Song (Tsinghua University, Tsinghua University) · Wenwu Zhu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning for RGB-D 6DoF Object Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yongliang Lin (Zhejiang University) · Yongzhi Su (German Research Center for AI (DFKI)) · Praveen Nathan (German Research Center for AI) · Sandeep Inuganti (German Research Center for AI) · Yan Di (Technische Universität München) · Martin Sundermeyer (None) · Fabian Manhardt (Google) · Didier Stricker (Universität Kaiserslautern) · Jason Rambach (None) · Yu Zhang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Instance-aware Exploration-Verification-Exploitation for Instance ImageGoal Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaohan Lei () · Min Wang (Institute of Artificial Intelligence, Hefei Comprehensive National Science Center) · Wengang Zhou (University of Science and Technology of China) · Li Li (University of Science and Technology of China) · Houqiang Li (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AnyScene: Customized Image Synthesis with Composited Foreground</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruidong Chen (Tianjin University) · Lanjun Wang (Tianjin University) · Weizhi Nie (Tianjin University) · Yongdong Zhang (University of Science and Technology of China) · An-An Liu (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PLACE: Adaptive Layout-Semantic Fusion for Semantic Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengyao Lv (University of Hong Kong) · Yuxiang Wei (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) · Wangmeng Zuo (Harbin Institute of Technology) · Kwan-Yee K. Wong (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TE-TAD: Towards Fully End-to-End Temporal Action Detection via Time-Aligned Coordinate Expression</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ho-Joong Kim (Korea University) · Jung-Ho Hong (Korea University) · Heejo Kong (Korea University) · Seong-Whan Lee (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanhui Wang (None) · Jianmin Bao (Microsoft) · Wenming Weng (None) · Ruoyu Feng (University of Science and Technology of China) · Dacheng Yin (University of Science and Technology of China) · Tao Yang (Xi'an JiaoTong University) · Jingxu Zhang (Research, Microsoft) · Qi Dai (Microsoft Research Asia) · Zhiyuan Zhao (Microsoft) · Chunyu Wang (Microsoft) · Kai Qiu (Microsoft) · Yuhui Yuan (Microsoft Research Asia) · Xiaoyan Sun (University of Science and Technology of China) · Chong Luo (Microsoft Research Asia) · Baining Guo (Microsoft Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Saeed Khorram (Apple) · Mingqi Jiang (Oregon State University) · Mohamad Shahbazi (ETH Zürich) · Mohamad Hosein Danesh (McGill University) · Li Fuxin (Oregon State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TRINS: Towards Multimodal Language Models That Can Read</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruiyi Zhang (Adobe Research) · Yanzhe Zhang (Georgia Institute of Technology) · Jian Chen (Mohamed bin Zayed University of Artificial Intelligence) · Yufan Zhou (State University of New York, Buffalo) · Jiuxiang Gu (Adobe Systems) · Changyou Chen (State University of New York, Buffalo) · Tong Sun (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MorpheuS: Neural Dynamic 360<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-4-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2218;&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-16" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-17" class="mjx-mrow"><span id="MJXc-Node-18" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-19" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-20" class="mjx-texatom" style=""><span id="MJXc-Node-21" class="mjx-mrow"><span id="MJXc-Node-22" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.128em; padding-bottom: 0.316em;">∘</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo>∘</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-4">^{\circ}</script> Surface Reconstruction from Monocular RGB-D Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hengyi Wang (University College London) · Jingwen Wang (University College London) · Lourdes Agapito (University College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://emotion-diffusion.github.io/" target="_blank">A Unified and Interpretable Emotion Representation and Expression Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Reni Paskaleva (First Private Mathematical High School, Sofia, Bulgaria) · Mykyta Holubakha (INSAIT) · Andela Ilic (ETHZ - ETH Zurich) · Saman Motamed (INSAIT, Sofia University) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Danda Paudel (INSAIT, Sofia University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>One More Step: A Versatile Plug-and-Play Module for Rectifying Diffusion Schedule Flaws and Enhancing Low-Frequency Controls</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minghui Hu (Nanyang Technological University) · Jianbin Zheng (South China University of Technology) · Chuanxia Zheng (University of Oxford) · Chaoyue Wang (JD Explore Academy) · Dacheng Tao (None) · Tat-Jen Cham (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiVa-360: The Dynamic Visual Dataset for Immersive Neural Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cheng-You Lu (University of Technology Sydney) · Peisen Zhou (Brown University) · Angela Xing (Brown University) · Chandradeep Pokhariya (International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad) · Arnab Dey (Université de Nice-Sophia Antipolis) · Ishaan Shah (International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad) · Rugved Mavidipalli (Brown University) · Dylan Hu (Brown University) · Andrew Comport (CNRS) · Kefan Chen (Brown University) · Srinath Sridhar (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SnAG: Scalable and Accurate Video Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fangzhou Mu (University of Wisconsin-Madison) · SICHENG MO (University of California, Los Angeles) · Yin Li (University of Wisconsin, Madison)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyu Tian (Australian National University) · Shu Zou (Australian National University) · Zhaoyuan Yang (General Electric) · Jing Zhang (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LPSNet: End-to-End Human Pose and Shape Estimation with Lensless Imaging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyang Ge (Tianjin University) · Qiao Feng (None) · Hailong Jia (Tianjin University) · Xiongzheng Li (None) · Xiangjun Yin (None) · You Zhou (Nanjing University) · Jingyu Yang (Tianjin University) · Kun Li (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>6-DoF Pose Estimation with MultiScale Residual Correlation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuelong Li (Amazon) · Yafei Mao (Amazon) · Raja Bala (Amazon) · Sunil Hadap (Amazon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jongwoo Choi (Visual Media Lab, KAIST) · Kwanggyoon Seo (KAIST) · Amirsaman Ashtari (MD Anderson Cancer Center) · Junyong Noh (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/Nicholas0228/Revelio" target="_blank">CGI-DM: Digital Copyright Authentication for Diffusion Models via Contrasting Gradient Inversion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyu Wu (Shanghai Jiaotong University) · Yang Hua (Queen's University Belfast) · Chumeng Liang (University of Southern California) · Jiaru Zhang (Shanghai Jiao Tong University) · Hao Wang (Louisiana State University) · Tao Song (Shanghai Jiao Tong University) · Haibing Guan (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Robust Depth Enhancement via Polarization Prompt Fusion Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kei IKEMURA (KTH Royal Institute of Technology) · Yiming Huang (HKUST) · Felix Heide (Department of Computer Science, Princeton University) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Qifeng Chen (Hong Kong University of Science and Technology) · Chenyang Lei (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning to Predict Activity Progress by Self-Supervised Video Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gerard Donahue (Northeastern University) · Ehsan Elhamifar (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PI3D: Efficient Text-to-3D Generation with Pseudo-Image Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ying-Tian Liu (Tsinghua University, Tsinghua University) · Yuan-Chen Guo (Tsinghua University) · Guan Luo (Tsinghua University, Tsinghua University) · Heyi Sun (Tsinghua University, Tsinghua University) · Wei Yin ( Shenzhen DJI Sciences and Technologies Ltd.) · Song-Hai Zhang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Ranking Distillation for Open-Ended Video Question Answering with Insufficient Labels</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianming Liang (Sun Yat-sen University) · Chaolei Tan (SUN YAT-SEN UNIVERSITY) · Beihao Xia (Huazhong University of Science and Technology) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY) · Jian-Fang Hu (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DrivingGaussian: Composite Gaussian Splatting for Surrounding Dynamic Autonomous Driving Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyu Zhou (Peking University) · Zhiwei Lin (Peking University) · Xiaojun Shan (Peking Univerisity) · Yongtao Wang (Peking University) · Deqing Sun (Google) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Interactive3D: Create What You Want by Interactive 3D Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shaocong Dong (Hong Kong University of Science and Technology) · Lihe Ding (The Chinese University of Hong Kong) · Zhanpeng Huang (SenseTime Research) · Zibin Wang (Sensetime Group Limited) · Tianfan Xue (The Chinese University of Hong Kong) · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Finding Lottery Tickets in Vision Models via Data-driven Spectral Foresight Pruning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leonardo Iurada (Polytechnic Institute of Turin) · Marco Ciccone (Politecnico di Torino) · Tatiana Tommasi (Politecnico di Torino)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CORE-MPI: Consistency Object Removal with Embedding MultiPlane Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Donggeun Yoon (Chungnam National University / KETI) · Donghyeon Cho (Hanyang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Amodal Ground Truth and Completion in the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guanqi Zhan (VGG, University of Oxford) · Chuanxia Zheng (University of Oxford) · Weidi Xie (Shanghai Jiaotong University) · Andrew Zisserman (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MiKASA: Multi-Key-Anchor Scene-Aware Transformer for 3D Visual Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chun-Peng Chang (DFKI) · Shaoxiang Wang (German Research Center for AI) · Alain Pagani (German Research Center for Artificial Intelligence (DFKI)) · Didier Stricker (Universität Kaiserslautern)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://haoningwu3639.github.io/StoryGen_Webpage/" target="_blank">Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chang Liu (Shanghai Jiaotong University) · Haoning Wu (Shanghai Jiao Tong University) · Yujie Zhong (Meituan Inc.) · Xiaoyun Zhang (Shanghai Jiao Tong University) · Yanfeng Wang (Shanghai Jiao Tong University) · Weidi Xie (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Seeing Unseen: Discover Novel Biomedical Concepts via Geometry-Constrained Probabilistic Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianan Fan (University of Sydney) · Dongnan Liu (University of Sydney) · Hang Chang (Lawrence Berkeley National Lab) · Heng Huang (University of Pittsburgh) · Mei Chen () · Weidong Cai (The University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Real-Time Exposure Correction via Collaborative Transformations and Adaptive Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziwen Li (Huazhong University of Science and Technology) · Feng Zhang (Huazhong University of Science and Technology) · Meng Cao (Mohamed bin Zayed University of Artificial Intelligence) · Jinpu Zhang (Huazhong University of Science and Technology) · Yuanjie Shao (Huazhong University of Science and Technology) · Yuehuan Wang (Huazhong University of Science and Technology) · Nong Sang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://rmurai.co.uk/projects/GaussianSplattingSLAM/" target="_blank">Gaussian Splatting SLAM</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hidenobu Matsuki (Imperial College London) · Riku Murai (Imperial College London) · Paul Kelly (Imperial College London) · Andrew J. Davison (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://simplehand.github.io/" target="_blank">A Simple Baseline for Efficient Hand Mesh Reconstruction</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            zhishan zhou (None) · shihao zhou (None) · Zhi Lv (None) · minqiang zou (None) · Yao Tang (None) · Jiajun Liang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EFormer: Enhanced Transformer towards Semantic-Contour Features of Foreground for Portraits Matting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zitao Wang (None) · Qiguang Miao (Xidian University) · Yue Xi (Xi'an University of Electronic Science and Technology) · Peipei Zhao (Xi'an University of Electronic Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://carloshinojosa.me/project/privacy-face-deid/" target="_blank">Privacy-preserving Optics for Enhancing Protection in Face De-identification</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jhon Lopez (Universidad Industrial de Santander) · Carlos Hinojosa (KAUST) · Henry Arguello (Universidad Industrial de Santander) · Bernard Ghanem (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BilevelPruning: Unified Dynamic and Static Channel Pruning for Convolutional Neural Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shangqian Gao (University of Pittsburgh) · Yanfu Zhang (College of William and Mary) · Feihu Huang (Nanjing University of Aeronautics and Astronautics) · Heng Huang (University of Pittsburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiayi Guo (Tsinghua University, Tsinghua University) · Xingqian Xu (University of Illinois, Urbana Champaign) · Yifan Pu (Tsinghua University, Tsinghua University) · Zanlin Ni (Tsinghua University) · Chaofei Wang (Tsinghua University, Tsinghua University) · Manushree Vasu (Georgia Institute of Technology) · Shiji Song (Tsinghua University, Tsinghua University) · Gao Huang (Tsinghua University, Tsinghua University) · Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dual Memory Networks: A Versatile Adaptation Approach for Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yabin Zhang (The Hong Kong Polytechnic University) · Wenjie Zhu (None) · Hui Tang (Hong Kong University of Science and Technology) · Zhiyuan Ma (None) · Kaiyang Zhou (Hong Kong Baptist University) · Lei Zhang (The Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Simple and Effective Point-based Network for Event Camera 6-DOFs Pose Relocalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongwei Ren (Hong Kong University of Science and Technology) · Jiadong Zhu (The Hong Kong University of Science and Technology (Guangzhou)) · Yue Zhou (Hong Kong University of Science and Technology) · Haotian FU (Hong Kong University of Science and Technology) · Yulong Huang (Central South University) · Bojun Cheng (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Transductive Zero-Shot <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-5-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x0026;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-23" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-24" class="mjx-mrow"><span id="MJXc-Node-25" class="mjx-mi"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.441em; padding-bottom: 0.378em;">&amp;</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">&amp;</mi></math></span></span><script type="math/tex" id="MathJax-Element-5">\&</script> Few-Shot CLIP</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ségolène Martin (TU Berlin) · Yunshi HUANG (École de technologie supérieure, Université du Québec) · Fereshteh Shakeri (École de technologie supérieure) · Jean-Christophe Pesquet (CentraleSupelec) · Ismail Ben Ayed (ETS Montreal)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LP++: A Surprisingly Strong Linear Probe for Few-Shot CLIP</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunshi HUANG (École de technologie supérieure, Université du Québec) · Fereshteh Shakeri (École de technologie supérieure) · Jose Dolz (École de technologie supérieure) · Malik Boudiaf (École de technologie supérieure) · Houda Bahig (University of Montreal) · Ismail Ben Ayed (ETS Montreal)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Estimating Extreme 3D Image Rotations using Cascaded Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shay Dekel (Bar Ilan University) · Yosi Keller (Bar Ilan University) · Martin Čadík (Brno University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ReCoRe: Regularized Contrastive Representation Learning of World Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rudra P,K. Poudel (Toshiba Europe Ltd) · Harit Pandya (Toshiba Europe) · Stephan Liwicki (Toshiba Europe Ltd) · Roberto Cipolla (University of Cambridge)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://mlpc-ucsd.github.io/TokenCompose/" target="_blank">TokenCompose: Text-to-Image Diffusion with Token-level Supervision</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zirui Wang (Princeton University) · Zhizhou Sha (Tsinghua University, Tsinghua University) · Zheng Ding (University of California, San Diego) · Yilin Wang (Tsinghua University, Tsinghua University) · Zhuowen Tu (University of California, San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/mlvlab/vid-TLDR" target="_blank">vid-TLDR: Training Free Token merging for Light-weight Video Transformer</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Joonmyung Choi (Korea University) · Sanghyeok Lee (Korea University) · Jaewon Chu (Korea University) · Minhyuk Choi (Korea University) · Hyunwoo J. Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/mlvlab/MCTF" target="_blank">Multi-criteria Token Fusion with One-step-ahead Attention for Efficient Vision Transformers</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sanghyeok Lee (Korea University) · Joonmyung Choi (Korea University) · Hyunwoo J. Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unmixing before Fusion: A Generalized Paradigm for Multi-Source-based Hyperspectral Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Yu (None) · Erting Pan (Wuhan University) · Xinya Wang (Wuhan University) · Yuheng Wu (Wuhan University) · Xiaoguang Mei (Wuhan University) · Jiayi Ma (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Referring Expression Counting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siyang Dai (Singapore University of Technology and Design) · Jun Liu () · Ngai-Man Cheung (Singapore University of Technology and Design)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shraman Pramanick (None) · Guangxing Han (Columbia University) · Rui Hou (Meta Inc. ) · Sayan Nag (University of Toronto) · Ser-Nam Lim (Meta AI) · Nicolas Ballas (Facebook) · Qifan Wang (Meta AI) · Rama Chellappa (Johns Hopkins University) · Amjad Almahairi (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Few-Shot Object Detection with Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangxing Han (Columbia University) · Ser-Nam Lim (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NARUTO: Neural Active Reconstruction from Uncertain Target Observations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyue Feng (Clemson University) · Huangying Zhan (OPPO US Research Center) · Zheng Chen (Indiana University, Bloomington) · Qingan Yan (OPPO US Research Center) · Xiangyu Xu (None) · Changjiang Cai (None) · Bing Li (Clemson University) · Qilun Zhu (Clemson University) · Yi Xu (OPPO US Research Center)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hide in Thicket: Generating Imperceptible and Rational Adversarial Perturbations on 3D Point Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianrui Lou (None) · Xiaojun Jia (, Chinese Academy of Sciences) · Jindong Gu (University of Oxford &amp; Google Research) · Li Liu (University of Oulu) · Siyuan Liang (National University of Singapore) · Bangyan He (Institute of Information Engineering, CAS) · Xiaochun Cao (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CAPE: CAM as a Probabilistic Ensemble for Enhanced DNN Interpretation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Townim Chowdhury (None) · Kewen Liao (Australian Catholic University) · Vu Minh Hieu Phan (University of Adelaide) · Minh-Son To (Flinders University of South Australia) · Yutong Xie (University of Adelaide) · Kevin Hung (Royal Adelaide Hospital) · David Ross (University of South Australia) · Anton van den Hengel (University of Adelaide) · Johan Verjans (University of Adelaide) · Zhibin Liao (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Decomposing Disease Descriptions for Enhanced Pathology Detection: A Multi-Aspect Vision-Language Pre-training Framework</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vu Minh Hieu Phan (University of Adelaide) · Yutong Xie (University of Adelaide) · Yuankai Qi (The University of Adelaide) · Lingqiao Liu (None) · Liyang Liu (University of Adelaide) · Bowen Zhang (The University of Adelaide) · Zhibin Liao (University of Adelaide) · Qi Wu (University of Adelaide) · Minh-Son To (Flinders University of South Australia) · Johan Verjans (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PairAug: What Can Augmented Image-Text Pairs Do for Radiology?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yutong Xie (University of Adelaide) · Qi Chen (The University of Adelaide) · Sinuo Wang (University of Adelaide) · Minh-Son To (Flinders University of South Australia) · Iris Lee (South Australia medical imaging) · Ee Win Khoo (The Queen Elizabeth Hospital) · Kerolos Hendy (Flinders University of South Australia) · Daniel Koh (Monash University, Malaysia Campus) · Yong Xia (Northwestern Polytechnical University) · Qi Wu (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Task-Aware Encoder Control for Deep Video Compression</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingtong Ge (Beijing Institute of Technology) · Jixiang Luo (sensetime) · XINJIE ZHANG (The Hong Kong University of Science and Technology) · Tongda Xu (Tsinghua University) · Guo Lu (Shanghai Jiaotong University) · Dailan He (The Chinese University of Hong Kong) · Jing Geng (Beijing Institute of Technology) · Yan Wang (Tsinghua University, Tsinghua University) · Jun Zhang (The Hong Kong University of Science and Technology) · Hongwei Qin (SenseTime Co.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MarkovGen: Structured Prediction for Efficient Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sadeep Jayasumana (Google) · Daniel Glasner (Google) · Srikumar Ramalingam (Google) · Andreas Veit (Google) · Ayan Chakrabarti (Google) · Sanjiv Kumar (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Open-World Human-Object Interaction Detection via Multi-modal Prompts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jie Yang (The Chinese University of Hong Kong, Shenzhen) · Bingliang Li (The Chinese University of Hong Kong (Shenzhen)) · Ailing Zeng (IDEA) · Lei Zhang (International Digital Economy Academy (IDEA)) · Ruimao Zhang (The Chinese University of Hong Kong (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EGTR: Extracting Graph from Transformer for Scene Graph Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinbae Im (NAVER Cloud) · JeongYeon Nam (Naver Cloud) · Nokyung Park (NAVER) · Hyungmin Lee (NAVER) · Seunghyun Park (NAVER Cloud)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Spectrum AUC Difference (SAUCD): Human Aligned 3D Shape Evaluation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianyu Luan (State University of New York at Buffalo) · Zhong Li (InnoPeak Technology) · Lele Chen (Sony America) · Xuan Gong (Harvard University) · Lichang Chen (Department of Computer Science, University of Maryland, College Park) · Yi Xu (OPPO US Research Center) · Junsong Yuan (State University of New York at Buffalo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://wangjiongw.github.io/freeman/" target="_blank">FreeMan: Towards benchmarking 3D human pose estimation under Real-World Conditions</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiong WANG (Fudan University) · Fengyu Yang (Chinese University of Hong Kong(Shenzhen)) · Bingliang Li (The Chinese University of Hong Kong (Shenzhen)) · Wenbo Gou (Carnegie Mellon University) · Danqi Yan (The Chinese University of Hong Kong Shenzhen) · Ailing Zeng (IDEA) · Yijun Gao (Tencent Turing Lab) · Junle Wang (Tencent) · Yanqing Jing (Tencent) · Ruimao Zhang (The Chinese University of Hong Kong (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DIMAT: Decentralized Iterative Merging-And-Training for Deep Learning Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nastaran Saadati (Iowa State University) · Minh Pham (New York University) · Nasla Saleem (Iowa State University) · Joshua R. Waite (Iowa State University) · Aditya Balu (Iowa State University) · Zhanhong Jiang (Iowa State University) · Chinmay Hegde (New York University) · Soumik Sarkar (Iowa State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>BodyMAP - Jointly Predicting Body Mesh and 3D Applied Pressure Map for People in Bed</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Abhishek Tandon (Carnegie Mellon University) · Anujraaj Goyal (Carnegie Mellon University) · Henry M. Clever (NVIDIA) · Zackory Erickson (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AssistGUI: Task-Oriented PC Graphical User Interface Automation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Difei Gao (None) · Lei Ji (Research, Microsoft) · Zechen Bai (Show Lab, National University of Singapore) · Mingyu Ouyang (National University of Singaore) · Peiran Li (national university of singaore, National University of Singapore) · Dongxing Mao (SUTD) · Qin WU (National University of Singapore) · Weichen Zhang (National University of Singapore) · Peiyi Wang (national university of singaore, National University of Singapore) · Xiangwu Guo (South China University of Technology) · Hengxu Wang (national university of singaore, National University of Singapore) · Luowei Zhou (Google) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Physics-Augmented Continuum Neural Radiance Field-Based Geometry-Agnostic System Identification with Lagrangian Particle Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Takuhiro Kaneko (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Low-Resource Vision Challenges for Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhua Zhang (University of Amsterdam) · Hazel Doughty (Leiden University) · Cees G. M. Snoek (University of Amsterdam)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PAIR Diffusion: A Comprehensive Multimodal Object-Level Image Editor</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vidit Goel (Georgia Tech | UIUC / Oregon | PAIR) · Elia Peruzzo (University of Trento) · Yifan Jiang (University of Texas at Austin) · Dejia Xu (University of Texas at Austin) · Xingqian Xu (University of Illinois, Urbana Champaign) · Nicu Sebe (University of Trento) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Zhangyang Wang (University of Texas at Austin) · Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Utility-Fairness Trade-Offs and How to Find Them</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sepehr Dehdashtian (Michigan State University) · Bashir Sadeghi (Michigan State University) · Vishnu Naresh Boddeti (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Continuous 3D Words for Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ta-Ying Cheng (Department of Computer Science, University of Oxford) · Matheus Gadelha (Adobe Systems) · Thibault Groueix (Adobe Systems) · Matthew Fisher (Adobe Research) · Radomir Mech (University of Calgary) · Andrew Markham (University of Oxford) · Niki Trigoni (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Region-Based Representations Revisited</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Michal Shlapentokh-Rothman (University of Illinois at Urbana Champaign) · Ansel Blume (University of Illinois Urbana Champaign) · Yao Xiao (University of Illinois at Urbana-Champaign) · Yuqun Wu (Department of Computer Science) · Sethuraman T V (Department of Computer Science) · Heyi Tao (University of Illinois at Urbana-Champaign) · Jae Yong Lee (University of Illinois at Urbana-Champaign) · Wilfredo Torres-Calderon (Reconstruct) · Yu-Xiong Wang (None) · Derek Hoiem (University of Illinois at Urbana-Champaign)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiang Yue (Ohio State University) · Yuansheng Ni (University of Waterloo) · Kai Zhang (Ohio State University, Columbus) · Tianyu Zheng (Beijing University of Posts and Telecommunications) · Ruoqi Liu (Ohio State University) · Ge Zhang (University of Waterloo) · Samuel Stevens (Ohio State University, Columbus) · Dongfu Jiang (University of Waterloo) · Weiming Ren (University of Waterloo) · Yuxuan Sun (Westlake University) · Cong Wei (University of Waterloo) · Botao Yu (The Ohio State University) · Ruibin Yuan (Hong Kong University of Science and Technology) · Renliang Sun (International Digital Economy Academy) · Ming Yin (Princeton University) · Boyuan Zheng (Ohio State University, Columbus) · Zhenzhu Yang (China University of Geoscience Beijing) · Yibo Liu (University of Victoria) · Wenhao Huang (BAAI) · Huan Sun (Ohio State University, Columbus) · Yu Su (Ohio State University) · Wenhu Chen (University of Waterloo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Conditional Denoising Diffusion Probabilistic Model for Point Cloud Upsampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qu Wentao (Nanjing University of Science and Technology) · Yuantian Shao (Nanjing University of Science and Technology) · Lingwu Meng (Nanjing University of Science and Technology) · Xiaoshui Huang (Shanghai AI Laboratory) · Liang Xiao (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Solution of Point-Line Absolute Pose</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Petr Hruby (Department of Computer Science, ETHZ - ETH Zurich) · Timothy Duff (University of Washington) · Marc Pollefeys (ETH Zurich / Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CAMixerSR: Only Details Need More "Attention"</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Wang (Nankai University) · Yi Liu (ByteDance Inc.) · Shijie Zhao (ByteDance Inc.) · Junlin Li (ByteDance Inc.) · Li zhang (Bytedance Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weizhen He () · Yiheng Deng (Zhejiang University) · SHIXIANG TANG (The Chinese University of Hong Kong) · Qihao CHEN (Liaoning Technical University) · Qingsong Xie (OPPO) · Yizhou Wang (None) · Lei Bai (Shanghai AI Laboratory) · Feng Zhu (SenseTime Group LTD) · Rui Zhao (Qing Yuan Research Institute, Shanghai Jiao Tong University) · Wanli Ouyang (University of Sydney) · Donglian Qi (Zhejiang University) · Yunfeng Yan (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CoDe: An Explicit Content Decoupling Framework for Image Restoration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Enxuan Gu (Dalian University of Technology) · Hongwei Ge (Dalian University of Technology) · Yong Guo (Max-Planck Institute for Informatics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SPOT: Self-Training with Patch-Order Permutation for Object-Centric Learning with Autoregressive Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ioannis Kakogeorgiou (National Technical University of Athens) · Spyros Gidaris (Valeo.ai) · Konstantinos Karantzalos (IMIS - "Athena" Research Center) · Nikos Komodakis (University of Crete)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ToNNO: Tomographic Reconstruction of a Neural Network’s Output for Weakly Supervised Segmentation of 3D Medical Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Marius Schmidt-Mengin (None) · Alexis Benichoux (INRIA) · Shibeshih Belachew (Therapanacea) · Nikos Komodakis (University of Crete) · Nikos Paragios (Ecole Centrale de Paris)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Physics-aware Hand-object Interaction Denoising</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haowen Luo (Tsinghua University, Tsinghua University) · Yunze Liu (None) · Li Yi ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Inter-X: Towards Versatile Human-Human Interaction Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liang Xu (Shanghai Jiao Tong University) · Xintao Lv (Shanghai Jiaotong University) · Yichao Yan (Shanghai Jiao Tong University) · Xin Jin (Eastern Institute of Technology, Ningbo) · Wu Shuwen (Shanghai Jiaotong University) · Congsheng Xu (Shanghai Jiaotong University) · Yifan Liu (Shanghai Jiao Tong University) · Yizhou Zhou (WeChat AI) · Fengyun Rao (WeChat, Tencent Inc.) · Xingdong Sheng (Shanghai Jiaotong University) · Yunhui LIU (Lenovo Research) · Wenjun Zeng (None) · Xiaokang Yang (Shanghai Jiao Tong University, China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yijun Yang (University of Technology Sydney) · Tianyi Zhou (University of Maryland, College Park) · kanxue Li (Yunnan University) · Dapeng Tao (Yunnan University) · Lusong Li (JDT) · Li Shen (JD Explore Academy) · Xiaodong He (JD AI Research) · Jing Jiang (University of Technology Sydney) · Yuhui Shi (Southern University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>One-Shot Open Affordance Learning with Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gen Li (University of Edinburgh) · Deqing Sun (Google) · Laura Sevilla-Lara (University of Edinburgh) · Varun Jampani (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Self-Supervised Dual Contouring</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ramana Sundararaman (École Polytechnique) · Roman Klokov (École Polytechnique) · Maks Ovsjanikov (Ecole Polytechnique, France)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Eric Slyman (Oregon State University) · Stefan Lee (Oregon State University) · Scott Cohen (Adobe Systems) · Kushal Kafle (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Accurate and Robust Architectures via Neural Architecture Search</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuwei Ou (Sichuan University) · Yuqi Feng (Sichuan University) · Yanan Sun (Sichuan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FedMef: Towards Memory-efficient Federated Dynamic Pruning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hong Huang (City University of Hong Kong) · Weiming Zhuang (Sony Research) · Chen Chen (Sony AI) · Lingjuan Lyu (Sony AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>C<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-6-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mtext&gt;2&lt;/mtext&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-26" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-27" class="mjx-mrow"><span id="MJXc-Node-28" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-29" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-30" class="mjx-mtext" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">2</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mtext>2</mtext></msup></math></span></span><script type="math/tex" id="MathJax-Element-6">^\text{2}</script>RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiqun Lin (The Hong Kong University of Science and Technology) · Jiewen Yang (Hong Kong University of Science and Technology) · hualiang wang (HKUST) · Xinpeng Ding (The Hong Kong University of Science and Technology) · Wei Zhao (Beijing University of Aeronautics and Astronautics) · Xiaomeng Li (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Tactile-Augmented Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiming Dou (University of Michigan - Ann Arbor) · Fengyu Yang (Yale University) · Yi Liu (University of Michigan - Ann Arbor) · Antonio Loquercio (University of California, Berkeley) · Andrew Owens (University of Michigan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Consistent Prompting for Rehearsal-Free Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhanxin Gao (Sun Yat-sen University) · Jun Cen (None) · Xiaobin Chang (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MedBN: Robust Test-Time Adaptation against Malicious Test Samples</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyejin Park (Pohang University of Science and Technology (POSTECH)) · Jeongyeon Hwang (Pohang University of Science and Technology) · Sunung Mun (Pohang University of Science and Technology) · Sangdon Park (POSTECH) · Jungseul Ok (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Open-Vocabulary Video Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Wu (Northwest Polytechnical University Xi'an) · Xuerong Zhou (Northwest Polytechnical University Xi'an) · Guansong Pang (Singapore Management University) · Yujia Sun (Xi'an University of Electronic Science and Technology) · Jing Liu (Guangzhou Institute of Technology, Xidian University) · Peng Wang (Northwestern Polytechnical University) · Yanning Zhang (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Beyond First-Order Tweedie: Solving Inverse Problems using Latent Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Litu Rout (University of Texas at Austin) · Yujia Chen (Google) · Abhishek Kumar (Google DeepMind) · Constantine Caramanis (University of Texas, Austin) · Sanjay Shakkottai (University of Texas, Austin) · Wen-Sheng Chu (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AUEditNet: Dual-Branch Facial Action Unit Intensity Manipulation with Implicit Disentanglement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiwei Jin (None) · Zhen Wang (Qualcomm Technologies, Inc.) · Lei Wang (Qualcomm) · Peng Liu (Qualcomm Inc, QualComm) · Ning Bi (QualComm) · Truong Nguyen (University of California, San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Language Model Guided Interpretable Video Action Reasoning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ning Wang (xidian university) · Guangming Zhu (Xidian University) · Hongsheng Li (Xi'an University of Electronic Science and Technology) · Liang Zhang (Xidian University) · Syed Afaq Ali Shah (Edith Cowan University) · Mohammed Bennamoun (University of Western Australia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Purified and Unified Steganographic Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            GuoBiao Li (Fudan University) · Sheng Li (Fudan University) · Zicong Luo (Fudan University) · Zhenxing Qian (Fudan University) · Xinpeng Zhang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deformable One-shot Face Stylization via DINO Semantic Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Zhou (Shenzhen University) · Zichong Chen (Shenzhen University) · Hui Huang (Shenzhen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Density-Guided Semi-Supervised 3D Semantic Segmentation with Dual-Space Hardness Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianan Li (University of Chinese Academy of Sciences) · Qiulei Dong (Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ardian Umam (National Yang Ming Chiao Tung University) · Cheng-Kun Yang (MediaTek) · Min-Hung Chen (NVIDIA) · Jen-Hui Chuang (None) · Yen-Yu Lin (National Yang Ming Chiao Tung University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Training Efficiency of Diffusion Models via Multi-Stage Framework and Tailored Multi-Decoder Architectures</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huijie Zhang (University of Michigan - Ann Arbor) · Yifu Lu (University of Michigan - Ann Arbor) · Ismail Alkhouri (Michigan State University; University of Michigan) · Saiprasad Ravishankar (Michigan State University) · Dogyoon Song (University of Michigan - Ann Arbor) · Qing Qu (University of Michigan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient Motion Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Zhou (SenseTime Research) · Hao Shao (None) · Letian Wang (University of Toronto) · Steven L. Waslander (University of Toronto) · Hongsheng Li (The Chinese University of Hong Kong) · Yu Liu (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VecFusion: Vector Font Generation with Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vikas Thamizharasan (University of Massachusetts Amherst) · Difan Liu (Adobe Research) · Shantanu Agarwal (Balbix) · Matthew Fisher (Adobe Research) · Michaël Gharbi (Massachusetts Institute of Technology) · Oliver Wang (Adobe Research) · Alec Jacobson (University of Toronto and Adobe Systems) · Evangelos Kalogerakis (UMass Amherst)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Noise-free Out-of-Sight Trajectory Prediction With Vision-Positioning Denoising</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haichao Zhang (Northeastern University) · Yi Xu (Northeastern University) · Hongsheng Lu (Toyota Motor North America) · Takayuki Shimizu (Toyota Motor North America, Inc.) · Yun Fu (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learned representation-guided diffusion models for large-image generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alexandros Graikos (Stony Brook University) · Srikar Yellapragada (Stony Brook University) · Minh-Quan Le (State University of New York at Stony Brook) · Saarthak Kapse (State University of New York at Stony Brook) · Prateek Prasanna (State University of New York, Stony Brook) · Joel Saltz (State University of New York at Stony Brook) · Dimitris Samaras (Stony Brook University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Building Optimal Neural Architectures using Interpretable Knowledge</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Keith Mills (University of Alberta) · Fred Han (Huawei Technologies Ltd.) · Mohammad Salameh (Huawei Technologies Canada Ltd.) · Shengyao Lu (University of Alberta) · CHUNHUA ZHOU (Huawei Technologies Ltd.) · Jiao He (huawei) · Fengyu Sun (Tongji University) · Di Niu (University of Alberta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bridging Remote Sensors with Multisensor Geospatial Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boran Han (Amazon/AWS) · Shuai Zhang (Amazon) · Xingjian Shi (Boson AI) · Markus Reichstein (Max-Planck Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IBD-SLAM: Learning Image-Based Depth Fusion for Generalizable SLAM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minghao Yin (The University of Hong Kong) · Shangzhe Wu (Stanford University) · Kai Han (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multimodal Industrial Anomaly Detection by Crossmodal Feature Mapping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alex Costanzino (University of Bologna) · Pierluigi Zama Ramirez (University of Bologna) · Giuseppe Lisanti (University of Bologna) · Luigi Di Stefano (University of Bologna)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Bilateral Event Mining and Complementary for Event Stream Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhilin Huang (Tsinghua University) · Quanmin Liang (Sun Yat-sen University) · Yijie Yu (Tsinghua University) · Chujun Qin (China Southern Power Grid ) · Xiawu Zheng (Xiamen University) · Kai Huang (SUN YAT-SEN UNIVERSITY,) · Zikun Zhou (Peng Cheng Laboratory) · Wenming Yang (Tsinghua University,)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CyberDemo: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jun Wang (University of California, San Diego) · Yuzhe Qin (University of California, San Diego, University of California, San Diego) · Kaiming Kuang (University of California, San Diego) · Yigit Korkmaz (University of Southern California) · Akhilan Gurumoorthy (University of California, San Diego) · Hao Su (UCSD) · Xiaolong Wang (UCSD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Plasticity in Online Continual Learning via Collaborative Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Maorong Wang (The University of Tokyo) · Nicolas Michel (None) · Ling Xiao (None) · Toshihiko Yamasaki (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Video Harmonization with Triplet Spatio-Temporal Variation Patterns</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zonghui Guo () · XinYu Han (Ocean University of China) · Jie Zhang (Institute of Computing Technology, Chinese Academy of Sciences) · Shiguang Shan (Institute of Computing Technology, Chinese Academy of Sciences) · Haiyong Zheng (Ocean University of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Semantic Line Combination Detector</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            JINWON KO (Korea University, Seoul) · Dongkwon Jin (Korea University) · Chang-Su Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GeneAvatar: Generic Expression-Aware Volumetric Head Avatar Editing from a Single Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chong Bao (Zhejiang University) · Yinda Zhang (Google) · Yuan Li (Zhejiang University) · Xiyu Zhang (Zhejiang University) · Bangbang Yang (ByteDance Inc) · Hujun Bao (Zhejiang University) · Marc Pollefeys (ETH Zurich / Microsoft) · Guofeng Zhang (Zhejiang University) · Zhaopeng Cui (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Segment Any Event Streams via Weighted Adaptation of Pivotal Tokens</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiwen Chen (Xidian University) · Zhiyu Zhu (City University of Hong Kong) · Yifan Zhang (City University of Hong Kong) · Junhui Hou (City University of Hong Kong) · Guangming Shi (Xidian University) · Jinjian Wu (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RichDreamer: A Generalizable Normal-Depth Diffusion Model for Detail Richness in Text-to-3D</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lingteng Qiu (None) · Guanying Chen (The Chinese University of Hong Kong, Shenzhen) · Xiaodong Gu (Alibaba Group) · Qi Zuo (Alibaba Group) · Mutian Xu (None) · Yushuang Wu (The Chinese University of Hong Kong (Shenzhen)) · Weihao Yuan (Alibaba Group) · Zilong Dong (Alibaba Group) · Liefeng Bo (None) · Xiaoguang Han (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Modal Hallucination Control by Visual Information Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alessandro Favero (EPFL - EPF Lausanne) · Luca Zancato (AWS AI Labs) · Matthew Trager (Amazon) · Siddharth Choudhary (Amazon AGI) · Pramuditha Perera (Amazon) · Alessandro Achille (California Institute of Technology) · Ashwin Swaminathan (University of Maryland, College Park) · Stefano Soatto (AWS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Laplacian-guided Entropy Model in Neural Codec with Blur-dissipated Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Atefeh Khoshkhahtinat (None) · Ali Zafari (West Virginia University) · Piyush Mehta (West Virginia University) · Nasser Nasrabadi (West Virginia University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VideoCutLER: Surprisingly Simple Unsupervised Video Instance Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xudong Wang (Electrical Engineering &amp; Computer Science Department, University of California Berkeley) · Ishan Misra (Facebook) · Ziyun Zeng (UCB) · Rohit Girdhar (Meta) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengjie Wang (Tencent Youtu Lab; Shanghai Jiao Tong University) · wenbing zhu (Fudan University) · Bin-Bin Gao (None) · Zhenye Gan (Tencent Youtu Lab) · Jiangning Zhang (Tencent Youtu Lab) · Zhihao Gu (Shanghai Jiao Tong University) · Bruce Qian (None) · Mingang Chen (Shanghai Development Center of Computer Software Technology) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ModaVerse: Efficiently Transforming Modalities with LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyu Wang (University of Adelaide) · Bohan Zhuang (Monash University) · Qi Wu (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3D Face Tracking from 2D Video through Iterative Dense UV to Image Flow</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Felix Taubner (LG Electronics) · Prashant Raina (LG Electronics) · Mathieu Tuli (LG Electronics Canada Incorporated, TAIL) · Eu Wern Teh (LG Corporation) · Chul Lee (LG Electronics) · Jinmiao Huang (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://pico-ai-team.github.io/hmd-poser" target="_blank">HMD-Poser: On-Device Real-time Human Motion Tracking from Scalable Sparse Observations</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Dai (Bytedance Inc.) · Yang Zhang (None) · Tao Liu (ByteDance Inc.) · ZhenFan (Bytedance) · Tianyuan Du (Bytedance) · Zhuo Su (ByteDance) · Xiaozheng Zheng (ByteDance) · Zeming Li (BYTEDANCE)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SD-DiT: Unleashing the Power of Self-supervised Discrimination in Diffusion Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui Zhu (Chinese University of Hong Kong (Shenzhen)) · Yingwei Pan (HiDream.ai) · Yehao Li (HiDream.ai) · Ting Yao (JD AI Research) · Zhenglong Sun (The Chinese University of Hong Kong, Shenzhen) · Tao Mei (JD Explore Academy) · Chang-Wen Chen (The Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Traffic Scene Parsing through the TSP6K Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng-Tao Jiang (vivo Mobile Communication (Hangzhou) Co., Ltd.) · Yuqi Yang (Nankai University) · Yang Cao (Hong Kong University of Science and Technology) · Qibin Hou (Nankai University) · Ming-Ming Cheng (Nankai University, Tsinghua University) · Chunhua Shen (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/SubmissionsIn/MVCAN" target="_blank">Investigating and Mitigating the Side Effects of Noisy Views for Self-Supervised Clustering Algorithms in Practical Multi-View Scenarios</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jie Xu (University of Electronic Science and Technology of China) · Yazhou Ren (University of Electronic Science and Technology of China) · Xiaolong Wang (University of Electronic Science and Technology of China) · Lei Feng (Nanyang Technological University) · Zheng Zhang (Harbin Institute of Technology) · Gang Niu (RIKEN) · Xiaofeng Zhu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Garment Recovery with Shape and Deformation Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ren Li (EPFL) · Corentin Dumery (EPFL) · Benoît Guillard (Swiss Federal Institute of Technology Lausanne) · Pascal Fua (Swiss Federal Institute of Technology Lausanne)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Boosting Spike Camera Image Reconstruction from a Perspective of Dealing with Spike Fluctuations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui Zhao (None) · Ruiqin Xiong (Peking University) · Jing Zhao (cncert) · Jian Zhang (None) · Xiaopeng Fan (Harbin Institute of Technology) · Zhaofei Yu (Peking University) · Tiejun Huang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Frequency Decoupling for Motion Magnification via Multi-Level Isomorphic Architecture</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fei Wang (Hefei University of Technology) · Dan Guo (Hefei University of Technology) · Kun Li (Hefei University of Technology) · Zhun Zhong (University of Nottingham) · Meng Wang (Hefei University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>KPConvX: Modernizing Kernel Point Convolution with Kernel Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hugues Thomas (Apple Inc.) · Yao-Hung Hubert Tsai (Apple) · Timothy Barfoot (University of Toronto) · Jian Zhang (Apple)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Panacea: Panoramic and Controllable Video Generation for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqing Wen (Megvii Technology Inc.) · Yucheng Zhao (University of Science and Technology of China) · Yingfei Liu (Megvii Technology Inc.) · Fan Jia (Megvii Technology Inc.) · Yanhui Wang (None) · Chong Luo (Microsoft Research Asia) · Chi Zhang (Columbia University) · Tiancai Wang (Megvii Technology Inc.) · Xiaoyan Sun (University of Science and Technology of China) · Xiangyu Zhang (MEGVII Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Muyang Li (None) · Tianle Cai (Princeton University) · Jiaxin Cao (Lepton AI) · Qinsheng Zhang (Georgia Institute of Technology) · Han Cai (Massachusetts Institute of Technology) · Junjie Bai (Lepton AI Inc.) · Yangqing Jia (Lepton AI) · Kai Li (Princeton University) · Song Han (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>What Moves Together Belongs Together</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jenny Seidenschwarz (Department of Informatics, Technische Universität München) · Aljoša Ošep (Carnegie Mellon University) · Francesco Ferroni () · Simon Lucey (University of Adelaide) · Laura Leal-Taixe (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking FID: Towards a Better Evaluation Metric for Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sadeep Jayasumana (Google) · Srikumar Ramalingam (Google) · Andreas Veit (Google) · Daniel Glasner (Google) · Ayan Chakrabarti (Google) · Sanjiv Kumar (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pinelopi Papalampidi (Google) · Skanda Koppula (Google Deepmind) · Shreya Pathak (Google) · Justin Chiu (Google) · Joseph Heyward (Google) · Viorica Patraucean (DeepMind) · Jiajun Shen (DeepMind) · Antoine Miech (DeepMind) · Andrew Zisserman (University of Oxford) · Aida Nematzadeh (Google Deepmind)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NeRSP: Neural 3D Reconstruction for Reflective Objects with Sparse Polarized Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufei Han (None) · Heng Guo (Beijing University of Posts and Telecommunications) · Koki Fukai (Osaka University) · Hiroaki Santo (Osaka University) · Boxin Shi (Peking University) · Fumio Okura (Osaka University) · Zhanyu Ma (Beijing University of Post and Telecommunication) · Yunpeng Jia (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PRDP: Proximal Reward Difference Prediction for Large-Scale Reward Finetuning of Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fei Deng (Google) · Qifei Wang (Google) · Wei Wei (Google) · Tingbo Hou (Google Research) · Matthias Grundmann (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-Driven Image Editing via Learnable Regions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanze Lin (University of Oxford) · Yi-Wen Chen (University of California, Merced) · Yi-Hsuan Tsai (Google) · Lu Jiang (Carnegie Mellon University) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CricaVPR: Cross-image Correlation-aware Representation Learning for Visual Place Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feng Lu (Tsinghua University) · Xiangyuan Lan (Peng Cheng Laboratory) · Lijun Zhang (University of Chinese Academy of Sciences) · Dongmei Jiang (Peng Cheng Laboratory) · Yaowei Wang (Pengcheng Laboratory) · Chun Yuan (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            David Charatan (Massachusetts Institute of Technology) · Sizhe Lester Li (Massachusetts Institute of Technology) · Andrea Tagliasacchi (Simon Fraser University, Google Brain) · Vincent Sitzmann (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking Few-shot 3D Point Cloud Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaochong An (University of Copenhagen) · Guolei Sun (None) · Yun Liu (Institute for Infocomm Research, A*STAR) · Fayao Liu (Institute for Infocomm Research, A*STAR) · Zongwei Wu (Bayerische Julius-Maximilians-Universität Würzburg) · Dan Wang (University of Copenhagen) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Serge Belongie (University of Copenhagen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Frozen CLIP: A Strong Backbone for Weakly Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bingfeng Zhang (China University of Petroleum (East China)) · Siyue Yu (Xi'an Jiaotong-Liverpool University) · Yunchao Wei (Beijing Jiaotong University) · Yao Zhao (Beijing Jiaotong University) · Jimin Xiao (Xi'an Jiaotong-Liverpool University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LEDITS++: Limitless Image Editing using Text-to-Image Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Manuel Brack (Technische Universität Darmstadt) · Felix Friedrich (TU Darmstadt, Hessian.AI) · Katharina Kornmeier (Align Technology) · Linoy Tsaban (Hugging Face) · Patrick Schramowski (TU Darmstadt) · Kristian Kersting (TU Darmstadt) · Apolinário Passos (Universidade de Brasília)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Beyond Text: Frozen Large Language Models in Visual Signal Comprehension</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lei Zhu (Peking University) · Fangyun Wei (None) · Yanye Lu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Revamping Federated Learning Security from a Defender's Perspective: A Unified Defense with Homomorphic Encrypted Data Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Naveen Kumar Kummari (Indian Institute of Technology Hyderabad, India) · Reshmi Mitra (Southeast Missouri State University) · Krishna Mohan Chalavadi (Indian Institute of Technology Hyderabad)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PeerAiD: Improving Adversarial Distillation from a Specialized Peer Tutor</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaewon Jung (Seoul National University) · Hongsun Jang (Seoul National University) · Jaeyong Song (Seoul National University) · Jinho Lee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adversarial Distillation Based on Slack Matching and Attribution Region Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shenglin Yin (Peking University) · Zhen Xiao (Peking University) · Mingxuan Song (Peking University) · Jieyi Long (Theta Labs, Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Universal Robustness via Median Random Smoothing for Real-World Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zakariya Chaouai (Paris-Saclay University, CEA, List) · Mohamed Tamaazousti (CEA/LIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RCBEVDet: Radar-camera Fusion in Bird’s Eye View for 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiwei Lin (Peking University) · Zhe Liu (University of Electronic Science and Technology of China) · Zhongyu Xia (Peking University) · Xinhao Wang (Peking University) · Yongtao Wang (Peking University) · Shengxiang Qi (Chongqing Changan Automobile Co., Ltd) · Yang Dong (Chongqing Changan Automobile Co., Ltd.) · Nan Dong (changan) · Le Zhang (University of Electronic Science and Technology of China) · Ce Zhu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning by Correction: Efficient Tuning Task for Zero-Shot Generative Vision-Language Reasoning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rongjie Li (SIST ,ShanghaiTech University) · Yu Wu (ShanghaiTech University) · Xuming He (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FineParser: A Fine-grained Spatio-temporal Action Parser for Human-centric Action Quality Assessment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinglin Xu (University of Science and Technology Beijing) · Sibo Yin (Peking University) · Guohao Zhao (Peking University) · Zishuo Wang (None) · Yuxin Peng (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://vchitect.github.io/VBench-project/" target="_blank">VBench: Comprehensive Benchmark Suite for Video Generative Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziqi Huang (Nanyang Technological University) · Yinan He (Shanghai AI Laboratory) · Jiashuo Yu (Shanghai AI Laboratory) · Fan Zhang (None) · Chenyang Si (Nanyang Technological University  Singapore) · Yuming Jiang (Nanyang Technological University) · Yuanhan Zhang (Nanyang Technological University) · Tianxing Wu (Nanyang Technological University) · Jin Qingyang (Nanyang Technological University) · Nattapol Chanpaisit (Nanyang Technological University) · Yaohui Wang (Shanghai AI Laboratory) · Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) · Limin Wang (Nanjing University) · Dahua Lin (The Chinese University of Hong Kong) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>The Neglected Tails in Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shubham Parashar (Texas A&amp;M University - College Station) · Tian Liu (Texas A&amp;M University - College Station) · Zhiqiu Lin (Carnegie Mellon University) · Xiangjue Dong (Texas A&amp;amp;M University - College Station) · Yanan Li (Zhejiang Lab) · James Caverlee (Texas A&amp;M University) · Deva Ramanan (Carnegie Mellon University) · Shu Kong (University of Macau, Texas A&amp;M University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multi-View Attentive Contextualization for Multi-View 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xianpeng Liu (North Carolina State University) · Ce Zheng (University of Central Florida) · Ming Qian (None) · Nan Xue (Ant Group) · Chen Chen () · Zhebin Zhang (OPPO) · Chen Li (Innopeak Technology Inc.) · Tianfu Wu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SeNM-VAE: Semi-Supervised Noise Modeling with Hierarchical Variational Autoencoder</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dihan Zheng (Tsinghua University) · Yihang Zou (Tsinghua University) · Xiaowen Zhang (Hisilicon) · Chenglong Bao (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SIFU: Side-view Conditioned Implicit Function for Real-world Usable Clothed Human Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zechuan Zhang (Zhejiang University) · Zongxin Yang (Zhejiang University) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/OpenDriveLab/DriveAGI" target="_blank">Generalized Predictive Model for Autonomous Driving</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiazhi Yang (Shanghai AI Laboratory) · Shenyuan Gao (HKUST) · Yihang Qiu (Shanghai Jiao Tong University) · Li Chen (The University of Hong Kong) · Tianyu Li (Fudan University) · Bo Dai (Shanghai AI Laboratory) · Kashyap Chitta () · Penghao Wu (University of California, San Diego) · Jia Zeng (Shanghai Jiaotong University) · Ping Luo (The University of Hong Kong) · Jun Zhang (The Hong Kong University of Science and Technology) · Andreas Geiger (University of Tübingen) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Hongyang Li (Shanghai AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Scaling Up Dynamic 3D Human-Scene Interaction Modelling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nan Jiang (Peking University) · Zhiyuan Zhang (Department of Automation, Tsinghua University) · Hongjie Li (Peking University) · Xiaoxuan Ma (Peking University) · Zan Wang (Beijing Institute of Technology) · Yixin Chen (BIGAI) · Tengyu Liu (None) · Yixin Zhu (Peking University) · Siyuan Huang (Beijing Institute of General Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Calibrating Multi-modal Representations: A Pursuit of Group Robustness without Annotations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenyu You (Yale University) · Yifei Min (Yale University) · Weicheng Dai (Yale University) · Jasjeet Sekhon (Yale University) · Lawrence Staib (Yale University) · James Duncan (Yale University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Theoretically Achieving Continuous Representation of Oriented Bounding Boxes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zikai Xiao (None) · Guo-Ye Yang (None) · Xue Yang (Shanghai AI Laboratory) · Tai-Jiang Mu (Tsinghua University, Tsinghua University) · Junchi Yan (Shanghai Jiao Tong University) · Shi-Min Hu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Source-Free Domain Adaptation with Frozen Multimodal Foundation Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Song Tang (University of Shanghai for Science and Technology) · Wenxin Su (University of Shanghai for Science and Technology) · Mao Ye (University of Electronic Science and Technology of China) · Xiatian Zhu (University of Surrey)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Grounding and Enhancing Grid-based Models for Neural Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zelin Zhao (SJTU) · FENGLEI FAN (The Chinese University of Hong Kong) · Wenlong Liao (Shanghai Jiaotong University) · Junchi Yan (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Data-Free Quantization via Pseudo-label Filtering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chunxiao Fan (Hefei University of Technology) · Ziqi Wang (Hefei University of Technology) · Dan Guo (Hefei University of Technology) · Meng Wang (Hefei University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Neural Sign Actors: A diffusion model for 3D sign language production from text</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vasileios Baltatzis (None) · Rolandos Alexandros Potamias (Imperial College London) · Evangelos Ververas (Huawei Technologies Ltd.) · Guanxiong Sun (Huawei Technologies Ltd.) · Jiankang Deng (Imperial College London &amp; Huawei UKRD) · Stefanos Zafeiriou (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/Liu-haoyue/NER-Net" target="_blank">Seeing Motion at Nighttime with an Event Camera</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyue Liu (Huazhong University of Science and Technology) · Shihan Peng (Huazhong University of Science and Technology) · Lin Zhu (Beijing Institute of Technology) · Yi Chang (Huazhong University of Science and Technology) · Hanyu Zhou (Huazhong University of Science and Technology) · Luxin Yan (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UnScene3D: Unsupervised 3D Instance Segmentation for Indoor Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            David Rozenberszki (None) · Or Litany (NVIDIA / Technion) · Angela Dai ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HumMUSS: Human Motion Understanding using State Space Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Arnab Mondal (McGill University) · Stefano Alletto (Apple) · Denis Tome (Apple)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Modeling Multimodal Social Interactions: New Challenges and Baselines with Densely Aligned Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sangmin Lee (University of Illinois Urbana-Champaign) · Bolin Lai (Georgia Institute of Technology) · Fiona Ryan (Georgia Institute of Technology) · Bikram Boote (University of Illinois, Urbana Champaign) · James Rehg (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>APISR: Anime Production Inspired Real-World Anime Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boyang Wang (University of Michigan - Ann Arbor) · Fengyu Yang (Yale University) · Xihang Yu (University of Michigan - Ann Arbor) · Chao Zhang (Zhejiang University) · Hanbin Zhao (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FreeControl: Training-Free Spatial Control of Any Text-to-Image Diffusion Model with Any Condition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            SICHENG MO (University of California, Los Angeles) · Fangzhou Mu (University of Wisconsin-Madison) · Kuan Heng Lin (University of California, Los Angeles) · Yanli Liu (Shein Technology LLC) · Bochen Guan (OPPO US Research Center) · Yin Li (University of Wisconsin, Madison) · Bolei Zhou (University of California, Los Angeles)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Discriminative Sample-Guided and Parameter-Efficient Feature Space Adaptation for Cross-Domain Few-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rashindrie Perera (University of Melbourne) · Saman Halgamuge (University of Melbourne)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://shapewalk.github.io/" target="_blank">ShapeWalk: Compositional Shape Editing through Language-Guided Chains</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Habib Slim (KAUST) · Mohamed Elhoseiny (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LAKE-RED: Camouflaged Images Generation by Latent Background Knowledge Retrieval-Augmented Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pancheng Zhao (Nankai University) · Peng Xu (Tsinghua University, Tsinghua University) · Pengda Qin (Alibaba Group) · Deng-Ping Fan (ETH Zurich) · Zhicheng Zhang (Nankai University) · Guoli Jia (None) · Bowen Zhou (Tsinghua University) · Jufeng Yang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WaveFace: Authentic Face Restoration with Efficient Frequency Recovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunqi Miao (The university of Warwick) · Jiankang Deng (Imperial College London &amp; Huawei UKRD) · Jungong Han (Aberystwyth University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hierarchical Histogram Threshold Segmentation – Auto-terminating High-detail Oversegmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Thomas Chang (Nuremberg Institute of Technology) · Simon Seibt (Georg-Simon-Ohm-Fachhochschule Nürnberg) · Bartosz von Rymon Lipinski (Technical University oAS Nuremberg)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>G-FARS: Gradient-Field-based Auto-Regressive Sampling for 3D Part Grouping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junfeng Cheng (Imperial College London) · Tania Stathaki (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dual-Enhanced Coreset Selection with Class-wise Collaboration for Online Blurry Class Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yutian Luo (Renmin University of China) · Shiqi Zhao (China Unicom Research Institute) · Haoran Wu (China Unicom Research Institute ) · Zhiwu Lu (Renmin University of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generative Multi-modal Models are Good Class Incremental Learners</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xusheng Cao (Nankai University) · Haori Lu (Nankai University) · Linlan Huang (Nankai University) · Xialei Liu (Nankai University) · Ming-Ming Cheng (Nankai University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TULIP: Multi-camera 3D Precision Assessment of Parkinson's Disease</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kyungdo Kim (Duke University) · Sihan Lyu (Duke University) · Sneha Mantri (Duke University) · Timothy DUNN (Duke University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WaveMo: Learning Wavefront Modulations to See Through Scattering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingyang Xie (University of Maryland, College Park) · Haiyun Guo (Rice University) · Brandon Y. Feng (Massachusetts Institute of Technology) · Lingbo Jin (Rice University) · Ashok Veeraraghavan (William Marsh Rice University) · Christopher Metzler (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BANF: Band-limited Neural Fields for Levels of Detail Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ahan Shabanov (Simon Fraser University) · Shrisudhan Govindarajan (Simon Fraser University) · Cody Reading (Simon Fraser University) · Leili Goli (University of Toronto) · Daniel Rebain (None) · Kwang Moo Yi (University Of British Columbia) · Andrea Tagliasacchi (Simon Fraser University, Google Brain)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Weakly Supervised Point Cloud Semantic Segmentation via Artificial Oracle</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyeokjun Kweon (KAIST) · Jihun Kim (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>StraightPCF: Straight Point Cloud Filtering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dasith de Silva Edirimuni (Deakin University) · Xuequan Lu (La Trobe University) · Gang Li (Deakin University) · Lei Wei (Deakin University) · Antonio Robles-Kelly (Defence Science and Technology Group (DST), Deakin University) · Hongdong Li (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SynFog: A Photo-realistic Synthetic Fog Dataset based on End-to-end Imaging Simulation for Advancing Real-World Defogging in Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiming Xie (Shenzhen International Graduate School, Tsinghua University) · Henglu Wei (Tsinghua University, Tsinghua University) · Zhenyi Liu (Stanford University) · Xiaoyu Wang (Department of Automation, Tsinghua University) · Xiangyang Ji (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SwitchLight: Co-design of Physics-driven Architecture and Pre-training Framework for Human Portrait Relighting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hoon Kim (Beeble Inc.) · Minje Jang (Beeble Inc.) · Wonjun Yoon (Beeble Inc.) · Jisoo Lee (Beeble Inc.) · Donghyun Na (Beeble Inc.) · Sanghyun Woo (New York University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>An Empirical Study of Scaling Law for Scene Text Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Miao Rang (Huawei Noah&amp;amp;amp;#x27;s Ark Lab) · Zhenni Bi (Huawei Noah Ark Lab) · Chuanjian Liu (Huawei Technologies Ltd.) · Yunhe Wang (Huawei Noah's Ark Lab) · Kai Han (Huawei Noah's Ark Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PeVL: Pose-Enhanced Vision-Language Model for Fine-Grained Human Action Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haosong Zhang (School of Computer Science and  Engineering, Nanyang Technological University) · Mei Leong (, A*STAR) · Liyuan Li (I2R, A*STAR) · Weisi Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Sparse Semi-Detr: Sparse Learnable Queries for Semi-Supervised Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tahira Shehzadi () · Khurram Azeem Hashmi (DFKI - German Research Center for AI) · Didier Stricker (Universität Kaiserslautern) · Muhammad Zeshan Afzal (German Research Center for AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bo Zou (Computer Science, Tsinghua University, Tsinghua University) · Chao Yang (Shanghai AI Laboratory) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Chengbin Quan (Tsinghua University, Tsinghua University) · Youjian Zhao (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://adacheng.github.io/EgoThink/" target="_blank">EgoThink: Evaluating First-Person Perspective Thinking Capability of Vision-Language Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sijie Cheng (None) · Zhicheng Guo (Tsinghua University, Tsinghua University) · Jingwen Wu (University of Toronto) · Kechen Fang (Tsinghua University) · Peng Li (Tsinghua University) · Huaping Liu (Tsinghua University, Tsinghua University) · Yang Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/csxmli2016/w-plus-adapter" target="_blank">When StyleGAN Meets Stable Diffusion: a <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-7-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;W&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-31" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-32" class="mjx-mrow"><span id="MJXc-Node-33" class="mjx-texatom"><span id="MJXc-Node-34" class="mjx-mrow"><span id="MJXc-Node-35" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.046em;"><span id="MJXc-Node-36" class="mjx-texatom"><span id="MJXc-Node-37" class="mjx-mrow"><span id="MJXc-Node-38" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.378em; padding-right: 0.046em;">W</span></span></span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-39" class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">+</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><msub><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">W</mi></mrow><mo>+</mo></msub></mrow></math></span></span><script type="math/tex" id="MathJax-Element-7">{\mathcal{W}_+}</script> Adapter for Personalized Image Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoming Li (MMLab@NTU) · Xinyu Hou (Nanyang Technological University) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiazhou Zhou (Hong Kong University of Science and Technology) · Xu Zheng (HKUST) · Yuanhuiyi Lyu (Hong Kong University of Science and Technology) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UnSAMFlow: Unsupervised Optical Flow Guided by Segment Anything Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuai Yuan (Duke University, Meta, TikTok) · Lei Luo (Meta) · Zhuo Hui (Facebook) · Can Pu (Facebook) · Xiaoyu Xiang (Meta) · Rakesh Ranjan () · Denis Demandolx (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Low-power, Continuous Remote Behavioral Localization with Event Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Friedhelm Hamann (TU Berlin) · Suman Ghosh (TU Berlin) · Ignacio Juarez Martinez (University of Oxford) · Tom Hart (Oxford Brookes University) · Alex Kacelnik (University of Oxford) · Guillermo Gallego (TU Berlin-ECDF-SCIoI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Attention-Propagation Network for Egocentric Heatmap to 3D Pose Lifting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taeho Kang (Seoul National University) · Youngki Lee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GPLD3D: Latent Diffusion of 3D Shape Generative Models by Enforcing Geometric and Physical Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Dong (Alibaba Group) · Qi Zuo (Alibaba Group) · Xiaodong Gu (Alibaba Group) · Weihao Yuan (Alibaba Group) · zhengyi zhao (Alibaba Group) · Zilong Dong (Alibaba Group) · Liefeng Bo (None) · Qixing Huang (University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Progress-Aware Online Action Segmentation for Egocentric Procedural Task Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhan Shen (Northeastern University) · Ehsan Elhamifar (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Your Image is My Video: Reshaping the Receptive Field via Image-To-Video Differentiable AutoAugmentation and Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sofia Casarin (Free University of Bozen-Bolzano) · Cynthia Ugwu (Free University of Bozen) · Sergio Escalera (Computer Vision Center) · Oswald Lanz (Free University of Bozen-Bolzano)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NRDF: Neural Riemannian Distance Fields for Learning Articulated Pose Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yannan He (University of Tübingen) · Garvita Tiwari (University of Tuebingen and MPI-Saarbrucken) · Tolga Birdal (Imperial College London) · Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) · Gerard Pons-Moll (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ConsistNet: Enforcing 3D Consistency for Multi-view Images Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiayu Yang (Australian National University) · Ziang Cheng (Australian National University) · Yunfei Duan (Tencent Game) · Pan Ji (Tencent XR Vision Labs) · Hongdong Li (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adaptive Random Feature Regularization on Fine-tuning Deep Neural Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shin'ya Yamaguchi (Kyoto University) · Sekitoshi Kanai (NTT) · Kazuki Adachi (NTT) · Daiki Chijiwa (NTT, The University of Tokyo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LOTUS: Evasive and Resilient Backdoor Attacks through Sub-Partitioning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siyuan Cheng (Purdue University) · Guanhong Tao (Purdue University) · Yingqi Liu (Microsoft) · Guangyu Shen (Purdue University) · Shengwei An (Purdue University) · Shiwei Feng (Purdue University, West Lafayette) · Xiangzhe Xu (Purdue University) · Kaiyuan Zhang (Computer Science, Purdue University) · Shiqing Ma (University of Massachusetts at Amherst) · Xiangyu Zhang (, Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhancing 3D Object Detection with 2D Detection-Guided Query Anchors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoxuanye Ji (Xi'an Jiaotong University) · Pengpeng Liang (Zhengzhou University) · Erkang Cheng (Nullmax)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Progressive Semantic-Guided Vision Transformer for Zero-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiming Chen (Carnegie Mellon University) · Wenjin Hou (Huazhong University of Science and Technology) · Salman Khan (Mohamed bin Zayed University of Artificial Intelligence) · Fahad Shahbaz Khan (MBZUAI; Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PracticalDG: Perturbation Distillation on Vision-Language Models for Hybrid Domain Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zining Chen (Beijing University of Posts and Telecommunications) · Weiqiu Wang (Beijing University of Posts and Telecommunications) · Zhicheng Zhao (Beijing University of Posts and Telecommunications) · Fei Su (Beijing University of Posts and Telecommunications) · Aidong Men (Beijing University of Posts and Telecommunications) · Hongying Meng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/RyanZhaoIc/PLM.git" target="_blank">Estimating Noisy Class Posterior with Part-level Labels for Noisy Label Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui Zhao (Xi'an Jiaotong University) · Bin Shi (Xi'an Jiaotong University) · Jianfei Ruan (Xi'an Jiaotong University) · Tianze Pan (Xi'an Jiaotong University) · Bo Dong (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ManiFPT: Defining and Analyzing Fingerprints of Generative Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hae Jin Song (University of Southern California) · Mahyar Khayatkhoei (USC/ISI) · Wael AbdAlmageed (Clemson University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EFHQ: Multi-purpose ExtremePose-Face-HQ dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Trung Dao (VinAI) · Duc H Vu (VinAI Research) · Cuong Pham (Posts &amp; Telecommunications Institute of Technology and VinAI Research) · Anh Tran (VinAI Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/JingyangXiang/MaxQ" target="_blank">MaxQ: Multi-Axis Query for N:M Sparsity Network</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingyang Xiang (Zhejiang University) · Siqi Li (Zhejiang University) · Junhao Chen (Zhejiang University) · Zhuangzhi Chen (Zhejiang University of Technology) · Tianxin Huang (Tencent youtu lab) · Linpeng Peng (Zhejiang University) · Yong Liu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/MingyuLee82/TGI_AD_v1" target="_blank">Text-Guided Variational Image Generation for Industrial Anomaly Detection and Segmentation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingyu Lee (Chung-Ang University, LGCNS) · Jongwon Choi (Chung-Ang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://pangzecheung.github.io/SingDiffusion/" target="_blank">Tackling the Singularities at the Endpoints of Time Intervals in Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pengze Zhang (Sun Yat-sen University) · Hubery Yin (Tencent) · Chen Li (Tencent) · Xiaohua Xie (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://q-future.github.io/Q-Instruct" target="_blank">Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoning Wu (Nanyang Technological University) · Zicheng Zhang (Shanghai Jiaotong University) · Erli Zhang (Nanyang Technological University) · Chaofeng Chen (Nanyang Technological University) · Liang Liao (Nanyang Technological University) · Annan Wang (Nanyang Technological University) · Kaixin Xu (I2R, A*STAR) · Chunyi Li (None) · Jingwen Hou (Nanyang Technological University) · Guangtao Zhai (Shanghai Jiao Tong University) · Xue Geng (Institute for Infocomm Research, A*STAR) · Wenxiu Sun (SenseTime Research and Tetras.AI) · Qiong Yan (SenseTime Research) · Weisi Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>One-Class Face Anti-spoofing via Spoof Cue Map-Guided Feature Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pei-Kai Huang (Department of Computer Science, National Tsing Hua University) · Cheng-Hsuan Chiang (National Tsinghua University) · Tzu-Hsien Chen (National Tsinghua University) · Jun-Xiong Chong (National Tsing Hua University) · Tyng-Luh Liu (IIS/Academia Sinica) · Chiou-Ting Hsu (National Tsing Hua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Enhancing the Power of OOD Detection via Sample-Aware Model Selection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feng Xue (Shanghai Jiaotong University) · Zi He (HuNan University) · Yuan Zhang (Beijing Normal University) · Chuanlong Xie (Beijing Normal University) · Zhenguo Li (Huawei) · Falong Tan (Hunan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>REACTO: Reconstructing Articulated Objects from a Single Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaoyue Song (Nanyang Technological University) · Jiacheng Wei (Nanyang Technological University) · Chuan-Sheng Foo (Centre for Frontier AI Research, A*STAR) · Guosheng Lin (Nanyang Technological University) · Fayao Liu (Institute for Infocomm Research, A*STAR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Soften to Defend: Towards Adversarial Robustness via Self-Guided Label Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daiwei Yu (Hangzhou City University) · Zhuorong Li (HangZhou City University) · Lina Wei (Hangzhou City University ) · Canghong Jin (Hangzhou City University) · Yun Zhang (Hangzhou City University) · Sixian Chan (the College of Computer Science and Technology at Zhejiang University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Structured Gradient-based Interpretations via Norm-Regularized Adversarial Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shizhan Gong (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Qi Dou (The Chinese University of Hong Kong) · Farzan Farnia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MuseChat: A Conversational Music Recommendation System for Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhikang Dong (State University of New York at Stony Brook) · Bin Chen (Bytedance Inc.) · Xiulong Liu (University of Washington) · Pawel Polak (State University of New York at Stony Brook) · Peng Zhang (Bytedance)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://udonda.github.io/RALF/" target="_blank">Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daichi Horita (The University of Tokyo) · Naoto Inoue (CyberAgent) · Kotaro Kikuchi (None) · Kota Yamaguchi (CyberAgent) · Kiyoharu Aizawa (The University of Tokyo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/kaiyuyue/nxtp" target="_blank">Object Recognition as Next Token Prediction</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kaiyu Yue (University of Maryland, College Park) · Bor-Chun Chen (Facebook) · Jonas Geiping (University of Maryland, College Park) · Hengduo Li (Meta AI) · Tom Goldstein (University of Maryland, College Park) · Ser-Nam Lim (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siddharth Srivastava (TensorTour Inc) · Gaurav Sharma (TensorTour Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Transfer CLIP for Generalizable Image Denoising</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jun Cheng (Huazhong University of Science and Technology) · Dong Liang (Huazhong University of Science and Technology) · Shan Tan (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LUWA Dataset: Learning Lithic Use-Wear Analysis on Microscopic Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jing Zhang (New York University) · Irving Fang (New York University) · Hao Wu (New York University) · Akshat Kaushik (New York University) · Alice Rodriguez (New York University) · Hanwen Zhao (New York University) · Juexiao Zhang (New York University) · Zhuo Zheng (Stanford University) · Radu Iovita (New York University) · Chen Feng (New York University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Correlation-aware Coarse-to-fine MLPs for Deformable Medical Image Registration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingyuan Meng (The University of Sydney) · Dagan Feng (University of Sydney) · Lei Bi (the University of Sydney) · Jinman Kim (University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment and Spatially-variant Deformation Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Shi (Northwest Polytechnical University Xi&amp;amp;#x27;an) · Hui Deng (Northwest Polytechnical University Xi'an) · Yuchao Dai (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multiway Point Cloud Mosaicking with Diffusion and Global Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shengze Jin (Department of Computer Science, ETHZ - ETH Zurich) · Iro Armeni (Stanford University) · Marc Pollefeys (ETH Zurich / Microsoft) · Daniel Barath (ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IDGuard: Robust, General, Identity-centric POI Proactive Defense Against Face Editing Abuse</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunshu Dai (SUN YAT-SEN UNIVERSITY) · Jianwei Fei (Nanjing University of Information Science and Technology) · Fangjun Huang (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PixelLM: Pixel Reasoning with Large Multimodal Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhongwei Ren (Beijing Jiaotong University) · Zhicheng Huang (University of Science and Technology Beijing) · Yunchao Wei (Beijing Jiaotong University) · Yao Zhao (Beijing Jiaotong University) · Dongmei Fu (University of Science and Technology Beijing) · Jiashi Feng (ByteDance) · Xiaojie Jin (ByteDance Inc./TikTok)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Cross Initialization for Face Personalization of Text-to-Image Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lianyu Pang (None) · Jian Yin () · Haoran Xie (Lingnan University) · Qiping Wang (East China Normal University) · Qing Li (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) · Xudong Mao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/Zongwei97/UnTrack" target="_blank">Single-Model and Any-Modality for Video Object Tracking</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zongwei Wu (Bayerische Julius-Maximilians-Universität Würzburg) · Jilai Zheng (Shanghai Jiaotong University) · Xiangxuan Ren (Shanghai Jiao Tong University) · Florin-Alexandru Vasluianu (Bayerische Julius-Maximilians-Universität Würzburg) · Chao Ma (Shanghai Jiao Tong University) · Danda Paudel (INSAIT, Sofia University) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Radu Timofte (University of Würzburg)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>iKUN: Speak to Trackers without Retraining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhao Du (Beijing University of Posts and Telecommunications) · Cheng Lei (Beijing University of Posts and Telecommunications) · Zhicheng Zhao (Beijing University of Posts and Telecommunications) · Fei Su (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Neural Fields as Distributions: Signal Processing Beyond Euclidean Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daniel Rebain (None) · Soroosh Yazdani (Google) · Kwang Moo Yi (University Of British Columbia) · Andrea Tagliasacchi (Simon Fraser University, Google Brain)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Weakly-Supervised Emotion Transition Learning for Diverse 3D Co-speech Gesture Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingqun Qi (The Hong Kong University of Science and Technology) · Jiahao Pan (Hong Kong University of Science and Technology) · Peng Li (Tsinghua University) · Ruibin Yuan (Hong Kong University of Science and Technology) · Xiaowei Chi (Hong Kong University of Science and Technology) · Mengfei Li (Hong Kong University of Science and Technology) · Wenhan Luo (SUN YAT-SEN UNIVERSITY) · Wei Xue (Hong Kong University of Science and Technology) · Shanghang Zhang (Peking University) · Qifeng Liu (The Hong Kong University of Science and Technology) · Yike Guo (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OmniMotionGPT: Animal Motion Generation with Limited Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhangsihao Yang (None) · Mingyuan Zhou (Innopeak Technology) · Mengyi Shan (University of Washington) · Bingbing Wen (University of Washington) · Ziwei Xuan (Innopeak Technology) · Mitch Hill (None) · Junjie Bai (CuraCloud Corporation) · Guo-Jun Qi (University of Central Florida) · Yalin Wang (Arizona State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Feature Re-Embedding: Towards Foundation Model-Level Performance in Computational Pathology</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenhao Tang (Chongqing University) · Fengtao ZHOU (Department of Computer Science and Engineering, Hong Kong University of Science and Technology) · Sheng Huang (Chongqing University) · Xiang Zhu (Chongqing University) · Yi Zhang (Chongqing University) · Bo Liu (Rutgers University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Stratified Avatar Generation from Sparse Observations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Han Feng (Wuhan University) · Wenchao Ma (Pennsylvania State University) · Quankai Gao (University of Southern California) · Xianwei Zheng (Wuhan University) · Nan Xue (Ant Group) · Huijuan Xu (Pennsylvania State University--University Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HDRFlow: Real-Time HDR Video Reconstruction with Large Motions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gangwei Xu (Huazhong University of Science and Technology) · Yujin Wang (Shanghai Artificial Intelligence  Laboratory) · Jinwei Gu (The Chinese University of Hong Kong) · Tianfan Xue (The Chinese University of Hong Kong) · Xin Yang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SocialCircle: Learning the Angle-based Social Interaction Representation for Pedestrian Trajectory Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Conghao Wong (Huazhong University of Science and Technology) · Beihao Xia (Huazhong University of Science and Technology) · Ziqian Zou (Huazhong University of Science and Technology) · Yulong Wang (Huazhong Agricultural University) · Xinge You (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihan Wang (Institute of Computing Technology, Chinese Academy of Sciences) · Xiangyang Li (Institue of Computing Technology, Chinese Academy of Sciences) · Jiahao Yang (Institute of Computing Technology, Chinese Academy of Sciences) · Yeqi Liu (Institute of Computing Technology, Chinese Academy of Sciences) · Junjie Hu (University of Wisconsin, Madison) · Ming Jiang (Indiana University) · Shuqiang Jiang (Institute of Computing Technology, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Self-Supervised Class-Agnostic Motion Prediction with Spatial and Temporal Consistency Regularizations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kewei Wang (Huazhong University of Science and Technology) · Yizheng Wu (Nanyang Technological University) · Jun Cen (None) · Zhiyu Pan (None) · Xingyi Li (Huazhong University of Science and Technology) · Zhe Wang (Sensetime Group Limited) · Zhiguo Cao () · Guosheng Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Do Vision and Language Encoders Represent the World Similarly?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mayug Maniparambil (ML Labs, Dublin City University) · Raiymbek Akshulakov (University of California, Berkeley) · YASSER ABDELAZIZ DAHOU DJILALI (Technology Innovation Institute) · Mohamed El Amine Seddik (Technology Innovation Institute) · Sanath Narayan (Technology Innovation Institute) · Karttikeya Mangalam (University of California Berkeley) · Noel O'Connor (Dublin City University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VideoMAC: Video Masked Autoencoders Meet ConvNets</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gensheng Pei (Nanjing University of Science and Technology) · Tao Chen (None) · Xiruo Jiang (None) · 刘华峰 Liu (Nanjing University of Science and Technology) · Zeren Sun (Nanjing University of Science and Technology) · Yazhou Yao (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TRIP: Temporal Residual Learning with Image Noise Prior for Image-to-Video Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhongwei Zhang (University of Science and Technology of China) · Fuchen Long (JD.com) · Yingwei Pan (HiDream.ai) · Zhaofan Qiu (University of Science and Technology of China) · Ting Yao (JD AI Research) · Yang Cao (University of Science and Technology of China) · Tao Mei (JD Explore Academy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Pose-Transformed Equivariant Network for  3D Point Trajectory Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruixuan Yu (Shandong University) · Jian Sun (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning SO(3)-Invariant Semantic Correspondence via Local Shape Transform</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chunghyun Park (POSTECH) · Seungwook Kim (POSTECH) · Jaesik Park (Seoul National University) · Minsu Cho (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FedSelect: Personalized Federated Learning with Customized Selection of Parameters for Fine-Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rishub Tamirisa (Lapis Labs) · Chulin Xie (University of Illinois, Urbana Champaign) · Wenxuan Bao (University of Illinois Urbana Champaign) · Andy Zhou (Lapis Labs) · Ron Arel (Lapis Lapis, UIUC) · Aviv Shamsian (Bar-Ilan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Do You Remember? Dense Video Captioning with Cross-Modal Memory Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minkuk Kim (Kyung Hee University) · Hyeon Kim (Kyunghee University) · Jinyoung Moon (ETRI) · Jinwoo Choi (Kyung Hee University) · Seong Tae Kim (Kyung Hee University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Test-Time Adaptation for Depth Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyoungseob Park (Yale University) · Anjali W Gupta (Yale) · Alex Wong (Yale University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2" target="_blank">MVBench: A Comprehensive Multi-modal Video Understanding Benchmark</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kunchang Li (SIAT, UCAS) · Yali Wang (SIAT, Chinese Academy of Sciences) · Yinan He (Shanghai AI Laboratory) · Yizhuo Li (The University of Hong Kong) · Yi Wang (Shanghai AI Laboratory) · Yi Liu (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences) · Zun Wang (Australian National University) · Jilan Xu (None) · Guo Chen (Nanjing University) · Ping Luo (The University of Hong Kong) · Limin Wang (Nanjing University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Hyperparameter Optimization with Adaptive Fidelity Identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiantong Jiang (The University of Western Australia) · Zeyi Wen (Hong Kong University of Science and Technology (Guangzhou)) · Atif Mansoor (University of Western Australia) · Ajmal Mian (University of Western Australia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MESA: Matching Everything by Segmenting Anything</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yesheng Zhang (Shanghai Jiaotong University) · Xu Zhao (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dispel Darkness for Better Fusion: A Controllable Visual Enhancer based on Cross-modal Conditional Adversarial Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Zhang (Wuhan University) · Linfeng Tang (Wuhan University) · Xinyu Xiang (Wuhan University) · Xuhui Zuo (Wuhan University) · Jiayi Ma (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://www.fujii.nuee.nagoya-u.ac.jp/Research/EventLF/" target="_blank">Time-Efficient Light-Field Acquisition Using Coded Aperture and Events</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuji Habuchi (Nagoya University) · Keita Takahashi (Nagoya University) · Chihiro Tsutake (Nagoya University) · Toshiaki Fujii (Nagoya University) · Hajime Nagahara (Osaka University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Video-P2P: Video Editing with Cross-attention Control</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shaoteng Liu (The Chinese University of Hong Kong) · Yuechen Zhang (The Chinese University of Hong Kong) · Wenbo Li (Huawei Technologies Ltd.) · Zhe Lin (Adobe Research) · Jiaya Jia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GenN2N: Generative NeRF2NeRF Translation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiangyue Liu () · Han Xue (Tsinghua University, Tsinghua University) · Kunming Luo (Hong Kong University of Science and Technology) · Ping Tan (Hong Kong University of Science and Technology) · Li Yi ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Deformable ConvNets: Rethinking Dynamic and Sparse Operator for Vision Applications</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuwen Xiong (University of Toronto) · Zhiqi Li (Nanjing University) · Yuntao Chen (CAIR, HKISI, CAS) · Feng Wang (Tsinghua University, Tsinghua University) · Xizhou Zhu (Shanghai AI Laboratory) · Jiapeng Luo (SenseTime Research) · Wenhai Wang (Shanghai AI Laboratory) · Tong Lu (Nanjing University) · Hongsheng Li (The Chinese University of Hong Kong) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Lewei Lu (SenseTime) · Jie Zhou (None) · Jifeng Dai (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dual-scale Transformer for Large-scale Single-Pixel Imaging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gang Qu (Westlake University) · Ping Wang (Zhejiang University) · Xin Yuan (Westlake University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>What, when, and where? -- Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Brian Chen (Samsung Research America) · Nina Shvetsova (None) · Andrew Rouditchenko (Massachusetts Institute of Technology) · Daniel Kondermann (Quality Match GmbH) · Samuel Thomas (IBM Research) · Shih-Fu Chang (Columbia University) · Rogerio Feris (International Business Machines) · James Glass (Massachusetts Institute of Technology) · Hilde Kuehne (University of Bonn             MIT-IBM Watson AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>2S-UDF: A Novel Two-stage UDF Learning Method for Robust Non-watertight Model Reconstruction from Multi-view Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junkai Deng (Institute of Software, Chinese Academy of Sciences) · Fei Hou (Institute of Software, Chinese Academy of Sciences) · Xuhui Chen (Institute of  Software, Chinese Academy of Sciences) · Wencheng Wang (Institute of Software, Chinese Academy of Sciences) · Ying He (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Dynamic Kernel Prior Model for Unsupervised Blind Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhixiong Yang (National University of Defense Technology) · Jingyuan Xia (National University of Defense Technology) · Shengxi Li (Beihang University) · Xinghua Huang (National University of Defense Technology) · Shuanghui Zhang (National University of Defense Technology) · Zhen Liu (National University of Defense Technology) · Yaowen Fu (National University of Defense Technology) · Yongxiang Liu (National University of Defense Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Continuous Optical Zooming: A Benchmark for Arbitrary-Scale Image Super-Resolution in Real World</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huiyuan Fu (Beijing University of Posts and Telecommunications) · Fei Peng (Beijing University of Posts and Telecommunications) · Xianwei Li (Beijing University of Posts and Telecommunications) · Yejun Li (Beijing University of Posts and Telecommunications) · Xin Wang (State University of New York at Stony Brook) · Huadong Ma (Beijing University of Post and Telecommunication, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Parameter Efficient Self-Supervised Geospatial Domain Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linus Scheibenreif (University of St.Gallen) · Michael Mommert (Stuttgart University of Applied Sciences) · Damian Borth (University of St.Gallen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multimodal Representation Learning by Alternating Unimodal Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaohui Zhang (Beijing Jiaotong University) · Jaehong Yoon (University of North Carolina at Chapel Hill) · Mohit Bansal (University of North Carolina at Chapel Hill) · Huaxiu Yao (Department of Computer Science, University of North Carolina at Chapel Hill)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Semantic Correspondence with Viewpoint-Guided Spherical Maps</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Octave Mariotti (University of Edinburgh) · Oisin Mac Aodha (University of Edinburgh) · Hakan Bilen (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SNED: Superposition Network Architecture Search for Efficient Video Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengang Li (Northeastern University) · Yan Kang (None) · Yuchen Liu (None) · Difan Liu (Adobe Research) · Tobias Hinz (Adobe Systems) · Feng Liu (Adobe Systems) · Yanzhi Wang (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OmniParser: A Unified Framework for Text Spotting, Key Information Extraction and Table Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianqiang Wan (Alibaba Group) · Sibo Song (Alibaba Group) · Wenwen Yu (Huazhong University of Science and Technology) · Yuliang Liu (Huazhong University of Science and Technology) · Wenqing Cheng (Huazhong University of Science and Technology) · Fei Huang (Alibaba Group) · Xiang Bai (Huazhong University of Science and Technology) · Cong Yao (Alibaba DAMO Academy) · Zhibo Yang (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Compositional Video Understanding with Spatiotemporal Structure-based Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hoyeoung Yun (Hanyang University) · Jinwoo Ahn (Hanyang University) · Minseo Kim (Hanyang University) · Eun-Sol Kim (Hanyang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VidToMe: Video Token Merging for Zero-Shot Video Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xirui Li (Shanghai Jiaotong University) · Chao Ma (Shanghai Jiao Tong University) · Xiaokang Yang (Shanghai Jiao Tong University, China) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CoDi-2: Interleaved and In-Context Any-to-Any Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zineng Tang (University of North Carolina, Chapel Hill) · Ziyi Yang (Microsoft) · MAHMOUD KHADEMI (Microsoft) · Yang Liu (Microsoft) · Chenguang Zhu (Zoom) · Mohit Bansal (University of North Carolina at Chapel Hill)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Selectively Informative Description can Reduce Undesired Embedding Entanglements in Text-to-Image Personalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jimyeong Kim (Seoul National University) · Jungwon Park (Seoul National University) · Wonjong Rhee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Transferable and Principled Efficiency for Open-Vocabulary Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingxuan Xu (Beijing Jiaotong University) · Wuyang Chen (University of Texas at Austin) · Yao Zhao (Beijing Jiaotong University) · Yunchao Wei (Beijing Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SecondPose: SE(3)-Consistent Dual-Stream Feature Fusion for Category-Level Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yamei Chen (Technische Universität München) · Yan Di (Technische Universität München) · Guangyao Zhai (Technical University of Munich) · Fabian Manhardt (Google) · Chenyangguang Zhang (Tsinghua University) · Ruida Zhang (Department of Automation, Tsinghua University, Tsinghua University) · Federico Tombari (Google, TUM) · Nassir Navab (TU Munich) · Benjamin Busam (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Robust Synthetic-to-Real Transfer for Stereo Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Zhang (Beijing University of Aeronautics and Astronautics) · Jiahe Li (Beijing University of Aeronautics and Astronautics) · Lei Huang (Beihang University) · Xiaohan Yu (Macquarie University) · Lin Gu (RIKEN / the University of Tokyo) · Jin Zheng (Beijing University of Aeronautics and Astronautics) · Xiao Bai (Beijing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generating Handwritten Mathematical Expressions From Symbol Graphs: An End-to-End Pipeline</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu chen (Beijing Waiyan Online Digital Technology Co., Ltd) · Fei Gao (Hangzhou Institute of Technology, Xidian University) · YanguangZhang (Hangzhou Dianzi University) · Maoying Qiao (University of Technology Sydney) · Nannan Wang (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://cardinalblue.github.io/artadapter.github.io/" target="_blank">ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dar-Yen Chen (SketchX) · Hamish Tennent (PicCollage) · Ching-Wen Hsu (PicCollage)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>On the Robustness of Language Guidance for Low-Level Vision Tasks: Findings from Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Agneet Chatterjee (Arizona State University) · Tejas Gokhale (University of Maryland, Baltimore County) · Chitta Baral (Arizona State University) · 'YZ' Yezhou Yang (Arizona State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Inhwan Bae (GIST) · Young-Jae Park (GIST) · Hae-Gon Jeon (GIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SG-PGM: Partial Graph Matching Network with Semantic Geometric Fusion for 3D Scene Graph Alignment and Its Downstream Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yaxu Xie (German Research Center for Artificial Intelligence) · Alain Pagani (German Research Center for Artificial Intelligence (DFKI)) · Didier Stricker (Universität Kaiserslautern)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TexVocab: Texture Vocabulary-conditioned Human Avatars</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxiao Liu (None) · Zhe Li (Tsinghua University) · Yebin Liu (Tsinghua University) · Haoqian Wang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>When Visual Grounding Meets Gigapixel-level Large-scale Scenes: Benchmark and Approach</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            TAO MA (Peking University) · Bing Bai (Qiyuan Lab) · Haozhe Lin (None) · Heyuan Wang (Peking University) · Yu Wang (Qiyuan Lab) · Lin Luo (Peking University) · Lu Fang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mitigating Motion Blur in Neural Radiance Fields with Events and Frames</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Marco Cannici (Robotics and Perception Group, Department of Informatics, University of Zurich) · Davide Scaramuzza (University of Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://wimmerth.github.io/back-to-3d.html" target="_blank">Back to 3D: Few-Shot 3D Keypoint Detection with Back-Projected 2D Features</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Thomas Wimmer (École Polytechnique &amp; Technical University of Munich) · Peter Wonka (KAUST) · Maks Ovsjanikov (Ecole Polytechnique, France)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeyi Sun (Shanghai Jiao Tong University) · Ye Fang (None) · Tong Wu (None) · Pan Zhang (Shanghai Artificial Intelligence Laboratory) · Yuhang Zang (Nanyang Technological University) · Shu Kong (University of Macau, Texas A&amp;M University) · Yuanjun Xiong (Mthreads) · Dahua Lin (The Chinese University of Hong Kong) · Jiaqi Wang (Shanghai AI Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Discriminability-Driven Channel Selection for Out-of-Distribution Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yue Yuan (Shandong University) · Rundong He (Shandong University) · Yicong Dong (Shandong University) · Zhongyi Han (Shandong University) · Yilong Yin (Shandong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://ruoyidu.github.io/demofusion/demofusion.html" target="_blank">DemoFusion: Democratising High-Resolution Image Generation With No <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-8-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; /&gt;" role="presentation"><span id="MJXc-Node-40" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-41" class="mjx-mrow"></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"></math></span></span><script type="math/tex" id="MathJax-Element-8"></script>$</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruoyi DU (Beijing University of Posts and Telecommunications) · Dongliang Chang (Tsinghua University) · Timothy Hospedales (None) · Yi-Zhe Song (None) · Zhanyu Ma (Beijing University of Post and Telecommunication)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SHViT: Single-Head Vision Transformer with Memory Efficient Macro Design</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seokju Yun (University of Seoul) · Youngmin Ro (University of Seoul)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SketchINR: A First Look into Sketches as Implicit Neural Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hmrishav Bandyopadhyay (University of Surrey) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Pinaki Nath Chowdhury (University of Surrey) · Aneeshan Sain (University of Surrey) · Tao Xiang (University of Surrey) · Timothy Hospedales (None) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Makeup Prior Models for 3D Facial Makeup Estimation and Applications</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingchao Yang (Cyberagent) · Takafumi Taketomi (CyberAgent) · Yuki Endo (University of Tsukuba) · Yoshihiro Kanamori (University of Tsukuba)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Masked Spatial Propagation Network for Sparsity-Adaptive Depth Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinyoung Jun (Korea University) · Jae-Han Lee (Gauss Labs) · Chang-Su Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Holoported Characters: Real-time Free-viewpoint Rendering of Humans from Sparse RGB Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ashwath Shetty (Saarland Informatics Campus, Max-Planck Institute) · Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) · Guoxing Sun (Max Planck Institute for Informatics) · Diogo Luvizon (Saarland Informatics Campus, Max-Planck Institute) · Vladislav Golyanik (MPI for Informatics) · Christian Theobalt (MPI Informatik)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neighbor Relations Matter in Video Scene Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Tan (Chongqing University) · Hongxing Wang (Chongqing University) · Jiaxin Li (Chongqing University) · Zhilong Ou (Chongqing University) · Zhangbin Qian (Chongqing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NOPE: Novel Object Pose Estimation from a Single Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Van Nguyen Nguyen (Ecole des Ponts ParisTech) · Thibault Groueix (Adobe Systems) · Georgy Ponimatkin (CIIRC, Czech Technical University, Czech Technical University of Prague) · Yinlin Hu (Magic Leap) · Renaud Marlet (INRIA) · Mathieu Salzmann (EPFL) · Vincent Lepetit (Ecole des Ponts ParisTech)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://drive-wm.github.io/" target="_blank">Driving into the Future: Multiview Visual Forecasting and Planning with World Model for Autonomous Driving</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqi Wang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Jiawei He (Institute of automation, Chinese Academy of Sciences) · Lue Fan (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Hongxin Li (Institute of Automation, Chinese Academy of Sciences) · Yuntao Chen (CAIR, HKISI, CAS) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youtian Lin (Nanjing university) · Zuozhuo Dai (Alibaba Group) · Siyu Zhu (Fudan University) · Yao Yao (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/dalision/ModalBiasAVSR" target="_blank">A Study of Dropout-Induced Modality Bias on Robustness to Missing Video Frames for Audio-Visual Speech Recognition</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yusheng Dai (University of Science and Technology of China) · HangChen (University of Science and Technology of China) · Jun Du (University of Science and Technology of China) · Ruoyu Wang (University of Science and Technology of China) · shihao chen (University of Science and Technology of China) · Haotian Wang (University of Science and Technology of China) · Chin-Hui Lee (Georgia Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changhoon Kim (Arizona State University) · Kyle Min (Intel Labs) · Maitreya Patel (Arizona State University) · Sheng Cheng (Arizona State University) · 'YZ' Yezhou Yang (Arizona State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hyper-MD: Mesh Denoising with Customized Parameters Aware of Noise Intensity and Geometric Characteristics</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingtao Wang (Harbin Institute of Technology) · Hongliang Wei (Harbin Institute of Technology) · Xiaopeng Fan (Harbin Institute of Technology) · Debin Zhao (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MULDE: Multiscale Log-Density Estimation via Denoising Score Matching for Video Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jakub Micorek (Technische Universität Graz) · Horst Possegger (Graz University of Technology) · Dominik Narnhofer (Technische Universität Graz) · Horst Bischof (Graz University of Technology) · Mateusz Kozinski (Technische Universität Graz)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MuRF: Multi-Baseline Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haofei Xu (ETH Zurich) · Anpei Chen (Department of Computer Science, ETHZ - ETH Zurich) · Yuedong Chen (Monash University) · Christos Sakaridis (ETH Zurich) · Yulun Zhang (Shanghai Jiao Tong University) · Marc Pollefeys (ETH Zurich / Microsoft) · Andreas Geiger (University of Tübingen) · Fisher Yu (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Link-Context Learning for Multimodal LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Tai (Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China) · Weichen Fan (HyperGAI) · Zhao Zhang (Sensetime Research) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>InNeRF360: Text-Guided 3D-Consistent Object Inpainting on 360-degree Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongqing Wang (EPFL) · Tong Zhang (EPFL) · Alaa Abboud (EPFL - EPF Lausanne) · Sabine Süsstrunk (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Cloud-Device Collaborative Learning for Multimodal Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guanqun Wang (Peking University) · Jiaming Liu (Peking University) · Chenxuan Li (Peking university) · Yuan Zhang (Peking University) · Ma Junpeng (Peking University) · Xinyu Wei (Peking University) · Kevin Zhang (Peking University) · Maurice Chong (Peking University) · Renrui Zhang (MMLab of CUHK &amp;amp;amp; Shanghai AI Laboratory) · Yijiang Liu (Nanjing University) · Shanghang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawang Bai (None) · Kuofeng Gao (Tsinghua University, Tsinghua University) · Shaobo Min (University of Science and Technology of China) · Shu-Tao Xia (Shenzhen International Graduate School, Tsinghua University) · Zhifeng Li (Tencent) · Wei Liu (Tencent AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/ragavsachdeva/magi" target="_blank">The Manga Whisperer: Automatically Generating Transcriptions for Comics</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ragav Sachdeva (University of Oxford) · Andrew Zisserman (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TeMO: Towards Text-Driven 3D Stylization for Multi-Object Meshes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuying Zhang (Nankai University) · Bo-Wen Yin (Nankai University) · yuming chen (None) · Zheng Lin (Nankai University) · Yunheng Li (Nankai University) · Qibin Hou (Nankai University) · Ming-Ming Cheng (Nankai University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unsupervised Template-assisted  Point Cloud Shape Correspondence Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiacheng Deng (University of Science and Technology of China) · Jiahao Lu (University of Science and Technology of China) · Tianzhu Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>X-3D: Explicit 3D Structure Modeling for Point Cloud Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuofeng Sun (Beijing University of Posts and Telecommunications) · Yongming Rao (Tsinghua University) · Jiwen Lu (Tsinghua University) · Haibin Yan (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Model Stealing Defense with Noise Transition Matrix</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dong-Dong Wu (Southeast University) · Chilin Fu (Ant Group) · Weichang Wu (Alibaba Group) · Wenwen Xia (Shanghai Jiaotong University) · Xiaolu Zhang (None) · JUN ZHOU (Ant Group) · Min-Ling Zhang (Southeast University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Zero-Painter: Training-Free Layout Control for Text-to-Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Marianna Ohanyan (Picsart) · Hayk Manukyan (Picsart AI Research) · Zhangyang Wang (University of Texas at Austin) · Shant Navasardyan (Picsart AI Research) · Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HOIAnimator: Text-Prompt Human-Object Animations Generation with Perceptive Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenfeng Song (Beijing Information Science and Technology University) · Xinyu Zhang (Beijing Information Science and Technology University) · Shuai Li (Beijing University of Aeronautics and Astronautics) · Yang Gao (Beijing University of Aeronautics and Astronautics) · Aimin Hao (None) · Xia HOU (Beijing Information Science &amp; Technology University) · Chenglizhao Chen (China University of Petroleum) · Ning Li (Beijing Information Science and Technology University) · Hong Qin (Stony Brook University (State University of New York at Stony Brook))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>InstructVideo: Instructing Video Diffusion Models with Human Feedback</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hangjie Yuan (Zhejiang University) · Shiwei Zhang (Alibaba Group) · Xiang Wang (Huazhong University of Science and Technology) · Yujie Wei (Fudan University) · Tao Feng (Tsinghua University) · Yining Pan (Singapore University of Technology and Design) · Yingya Zhang (Alibaba Group) · Ziwei Liu (Nanyang Technological University) · Samuel Albanie (University of Cambridge) · Dong Ni (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VideoCon: Robust Video-Language Alignment via Contrast Captions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hritik Bansal (University of California, Los Angeles) · Yonatan Bitton (Google) · Idan Szpektor (Google) · Kai-Wei Chang (University of California, Los Angeles) · Aditya Grover (University of California, Los Angeles)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UniBind: LLM-Augmented Unified and Balanced Representation Space to Bind Them All</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanhuiyi Lyu (Hong Kong University of Science and Technology) · Xu Zheng (HKUST) · Jiazhou Zhou (Hong Kong University of Science and Technology) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HEAL-SWIN: A Vision Transformer On The Sphere</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Oscar Carlsson (Division of Algebra and Geometry, Department of Mathematical Sciences, Chalmers University of Technology and University of Gothenburg) · Jan E. Gerken (Chalmers University of Technology) · Hampus Linander (Chalmers University of Technology) · Heiner Spiess (Technische Universität Berlin) · Fredrik Ohlsson (Umea University) · Christoffer Petersson (Zenseact) · Daniel Persson (Chalmers University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>How Far Can We Compress Instant NGP-Based NeRF?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yihang Chen (Shanghai Jiao Tong University) · Qianyi Wu (Monash University) · Mehrtash Harandi (Monash University) · Jianfei Cai (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Putting the Object Back into Video Object Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ho Kei Cheng (University of Illinois Urbana-Champaign) · Seoung Wug Oh (Adobe Systems) · Brian Price (Adobe Research) · Joon-Young Lee (Adobe Research) · Alexander G. Schwing (UIUC)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards 3D Vision with Low-Cost Single-Photon Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fangzhou Mu (University of Wisconsin-Madison) · Carter Sifferman (University of Wisconsin - Madison) · Sacha Jungerman (University of Wisconsin - Madison) · Yiquan Li (University of Wisconsin - Madison) · Zhiyue Han (None) · Michael Gleicher (Department of Computer Sciences, University of Wisconsin - Madison) · Mohit Gupta (Department of Computer Sciences, University of Wisconsin - Madison) · Yin Li (University of Wisconsin, Madison)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://zkyseu.github.io/lane2seq.github.io/" target="_blank">Lane2Seq: Towards Unified Lane Detection via Sequence Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kunyang Zhou (Southeast University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://kaist-viclab.github.io/fmanet-site" target="_blank">FMA-Net: Flow Guided Dynamic Filtering and Iterative Feature Refinement with Multi-Attention for Joint Video Super-Resolution and Deblurring</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Geunhyuk Youk (Korea Advanced Institute of Science and Technology) · Jihyong Oh (Chung-Ang University) · Munchurl Kim (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CuVLER: Enhanced Unsupervised Object Discoveries through Exhaustive Self-Supervised Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shahaf Arica (Technion - Israel Institute of Technology) · Or Rubin (Technion - Israel Institute of Technology) · Sapir Gershov (Technion - Israel Institute of Technology) · Shlomi Laufer (Technion)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Color Shift Estimation-and-Correction for Image Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiyu Li (City University of Hong Kong) · Ke Xu (City University of Hong Kong) · Gerhard Hancke Hancke (None) · Rynson W.H. Lau (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UniDepth: Universal Monocular Metric Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Luigi Piccinelli (ETH Zurich) · Yung-Hsu Yang (None) · Christos Sakaridis (ETH Zurich) · Mattia Segu (ETH Zurich - Swiss Federal Institute of Technology) · Siyuan Li (ETH Zurich) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Fisher Yu (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dexterous Grasp Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guo-Hao Xu (Sun Yat-sen University) · Yi-Lin Wei (SUN YAT-SEN UNIVERSITY) · Dian Zheng (None) · Xiao-Ming Wu (SUN YAT-SEN UNIVERSITY) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://praeclarumjj3.github.io/vcoder/" target="_blank">VCoder: Versatile Vision Encoders for Multimodal Large Language Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jitesh Jain (Georgia Tech) · Jianwei Yang (Microsoft Research) · Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>QN-Mixer: A Quasi-Newton MLP-Mixer Model for Sparse-View CT Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ishak Ayad (ETIS &amp; AGM, CY Cergy Paris University, ENSEA, CNRS) · Nicolas Larue (ETIS , CY Cergy Paris University, ENSEA, CNRS, University of Ljubljana) · Mai K. Nguyen (ETIS , CY Cergy Paris University, ENSEA, CNRS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Cross-dimension Affinity Distillation for 3D EM Neuron Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyu Liu (University of Science and Technology of China) · Miaomiao Cai (University of Science and Technology of China) · Yinda Chen (University of Science and Technology of China) · Yueyi Zhang (University of Science and Technology of China) · Te Shi (Institute of Artificial Intelligence, Hefei Comprehensive National Science Center) · Ruobing Zhang (Suzhou Institute of Biomedical Engineering and Technology) · Xuejin Chen (University of Science and Technology of China) · Zhiwei Xiong (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Producing and Leveraging Online Map Uncertainty in Trajectory Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xunjiang Gu (University of Toronto) · Guanyu Song (University of Toronto) · Igor Gilitschenski (University of Toronto) · Marco Pavone (NVIDIA) · Boris Ivanovic (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Boosting Self-Supervision for Single-View Scene Completion via Knowledge Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Keonhee Han (Technical University of Munich) · Dominik Muhle (Technical University of Munich) · Felix Wimbauer (Technical University of Munich) · Daniel Cremers (Technical University Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CrossKD: Cross-Head Knowledge Distillation for Dense Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            JiaBao Wang (Nankai University) · yuming chen (None) · Zhaohui Zheng (Nankai University) · Xiang Li (Nankai University) · Ming-Ming Cheng (Nankai University, Tsinghua University) · Qibin Hou (Nankai University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TiNO-Edit: Timestep and Noise Optimization for Robust Diffusion-Based Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sherry X. Chen (University of California, Santa Barbara) · Yaron Vaxman (cloudinary) · Elad Ben Baruch (Cloudinary) · David Asulin (Cloudinary Ltd.) · Aviad Moreshet (Cloudinary) · Kuo-Chin Lien (Layer AI) · Misha Sra (University of California, Santa Barbara) · Pradeep Sen (UC Santa Barbara)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Leveraging Camera Triplets for Efficient and Accurate Structure-from-Motion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lalit Manam (Indian Institute of Science) · Venu Madhav Govindu (Indian Institute of Science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Seeing the World through Your Eyes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hadi Alzayer (University of Maryland) · Kevin Zhang (University of Maryland, College Park) · Brandon Y. Feng (Massachusetts Institute of Technology) · Christopher Metzler (University of Maryland, College Park) · Jia-Bin Huang (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Equivariant Multi-Modality Image Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixiang Zhao (Xi'an Jiaotong University) · Haowen Bai (Xi'an Jiaotong University) · Jiangshe Zhang (Xi'an Jiaotong University) · Yulun Zhang (Shanghai Jiao Tong University) · Kai Zhang (None) · Shuang Xu (Northwest Polytechnical University Xi'an) · Dongdong Chen (Heriot-Watt University) · Radu Timofte (University of Würzburg) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PDF: A Probability-Driven Framework for Open World 3D Point Cloud Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinfeng Xu (Huazhong University of Science and Technology) · Siyuan Yang (HUST) · Xianzhi Li (Huazhong University of Science and Technology) · Yuan Tang (Huazhong University of Science and Technology) · yixue Hao (Huazhong University of Science and Technology) · Long Hu (Huazhong University of Science and Technology) · Min Chen (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Residual Denoising Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Liu (Shenyang Institute of Automation, Chinese Academy of Sciences) · Qiang Wang (Shenyang University) · Huijie Fan (None) · Yinong Wang (University of Hong Kong) · Yandong Tang (Shenyang Institue of Automation) · Liangqiong Qu (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PromptKD: Unsupervised Prompt Distillation for Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zheng Li (Nankai University) · Xiang Li (Nankai University) · xinyi fu (Ant group) · Xin Zhang (Nankai University) · Weiqiang Wang (University of Southern California) · Shuo Chen (RIKEN) · Jian Yang (Nankai University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CCEdit: Creative and Controllable Video Editing via Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruoyu Feng (University of Science and Technology of China) · Wenming Weng (None) · Yanhui Wang (None) · Yuhui Yuan (Microsoft Research Asia) · Jianmin Bao (Microsoft) · Chong Luo (Microsoft Research Asia) · Zhibo Chen (University of Science and Technology of China) · Baining Guo (Microsoft Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CORES: Convolutional Response-based Score for Out-of-distribution Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Keke Tang (Guangzhou University) · Chao Hou (Guangzhou University) · Weilong Peng (None) · Runnan Chen (None) · Peican Zhu (Northwest Polytechnical University Xi'an) · Wenping Wang (Texas A&amp;M University - College Station) · Zhihong Tian (Guangzhou University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Discover and Mitigate Multiple Biased Subgroups in Image Classifiers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeliang Zhang (University of Rochester) · Mingqian Feng (University of Rochester) · Zhiheng Li (Amazon AGI) · Chenliang Xu (University of Rochester)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Groupwise Query Specialization and Quality-Aware Multi-Assignment for Transformer-based Visual Relationship Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jongha Kim (Korea University) · Jihwan Park (Korea University) · Jinyoung Park (Korea University) · Jinyoung Kim (Korea University) · Sehyung Kim (Korea University) · Hyunwoo J. Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MoDE: CLIP Data Experts via Clustering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Ma (Columbia University) · Po-Yao Huang (Facebook) · Saining Xie (Facebook) · Shang-Wen Li (Facebook) · Luke Zettlemoyer (University of Washington) · Shih-Fu Chang (Columbia University) · Wen-tau Yih (Meta Platforms, Inc.) · Hu Xu (FAIR, Multimodal Foundation)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>S-DyRF: Reference-Based Stylized Radiance Fields for Dynamic Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingyi Li (Huazhong University of Science and Technology) · Zhiguo Cao () · Yizheng Wu (Nanyang Technological University) · Kewei Wang (Huazhong University of Science and Technology) · Ke Xian (Nanyang Technological University) · Zhe Wang (Sensetime Group Limited) · Guosheng Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SatSynth: Augmenting Image-Mask Pairs through Diffusion Models for Aerial Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aysim Toker (Technical University Munich) · Marvin Eisenberger (Technical University Munich) · Daniel Cremers (Technical University Munich) · Laura Leal-Taixe (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dual-consistency Model Inversion for Non-exemplar Class Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihuan Qiu (University of Electronic Science and Technology of China) · Yi Xu (Dalian University of Technology) · Fanman Meng (University of Electronic Science and Technology of China) · Hongliang Li (University of Electronic Science and Technology of China, Tsinghua University) · Linfeng Xu (University of Electronic Science and Technology of China) · Qingbo Wu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Class Tokens Infusion for Weakly Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sung-Hoon Yoon (KAIST) · Hoyong Kwon (KAIST) · Hyeonseong Kim (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PointOBB: Learning Oriented Object Detection via Single Point Supervision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junwei Luo (Wuhan University) · Xue Yang (Shanghai AI Laboratory) · Yi Yu (Southeast University) · Qingyun Li (Harbin Institute of Technology) · Junchi Yan (Shanghai Jiao Tong University) · Yansheng Li (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EventEgo3D: 3D Human Motion Capture from Egocentric Event Streams</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Christen Millerdurai (Max Planck Institute for Informatics) · Hiroyasu Akada (Max Planck Institute for Informatics) · Jian Wang (Max Planck Institute for Informatics) · Diogo Luvizon (Saarland Informatics Campus, Max-Planck Institute) · Christian Theobalt (MPI Informatik) · Vladislav Golyanik (MPI for Informatics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LCD: Towards Hierarchical Embeddings with Localizability, Composability, and Decomposability Learned from Anatomy</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammad Reza Hosseinzadeh Taher (Arizona State University) · Michael Gotway (Mayo Clinic) · Jianming Liang (Arizona State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SeD: Semantic-Aware Discriminator for Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bingchen Li (University of Science and Technology of China) · Xin Li (None) · Hanxin Zhu (University of Science and Technology of China) · YEYING JIN (National University of Singapore) · Ruoyu Feng (University of Science and Technology of China) · Zhizheng Zhang (Microsoft Research) · Zhibo Chen (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Category-Level Multi-Part Multi-Joint 3D Shape Assembly</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichen Li (Massachusetts Institute of Technology) · Kaichun Mo (NVIDIA Research) · Yueqi Duan (None) · He Wang (None) · Jiequan Zhang (None) · Lin Shao (National University of Singapore) · Wojciech Matusik (Massachusetts Institute of Technology) · Leonidas Guibas (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TeTriRF: Temporal Tri-Plane Radiance Fields for Efficient Free-Viewpoint Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minye Wu (KU Leuven) · Zehao Wang (KU Leuven) · Georgios Kouros (Department of Electrical Engineering, KU Leuven, Belgium, KU Leuven) · Tinne Tuytelaars (KU Leuven)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>JoAPR: Cleaning the Lens of Prompt Learning for Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            YUNCHENG GUO (None) · Xiaodong Gu (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/NKI-AI/kandinsky-calibration" target="_blank">Kandinsky Conformal Prediction: Efficient Calibration of Image Segmentation Algorithms</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Joren Brunekreef (Netherlands Cancer Institute) · Eric Marcus (Netherlands Cancer Institute) · Ray Sheombarsing (None) · Jan-Jakob Sonke (Netherlands Cancer Institute) · Jonas Teuwen (Netherlands Cancer Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Category Agnostic Model for Visual Rearrangement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuyi Liu (Institute of Computing Technology,University of the Chinese Academy of Sciences) · Xinhang Song (None) · Weijie Li (Alibaba Group) · XIAOHAN Wang (Xi'an Jiaotong University) · Shuqiang Jiang (Institute of Computing Technology, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>WorDepth: Variational Language Prior for Monocular Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyao Zeng (Yale University) · Hyoungseob Park (Yale University) · Fengyu Yang (Yale University) · Daniel Wang (Yale University) · Stefano Soatto (University of California, Los Angeles) · Dong Lao (University of California, Los Angeles) · Alex Wong (Yale University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tai Wang (Shanghai AI Laboratory) · Xiaohan Mao (Shanghai Jiaotong University) · Chenming Zhu (The Chinese University Of Hong Kong, Shenzhen) · Runsen Xu (The Chinese University of Hong Kong) · Ruiyuan Lyu (Shanghai AI Laboratory) · Peisen Li (Tsinghua University, Tsinghua University) · Xiao Chen (The Chinese University of Hong Kong) · Wenwei Zhang (None) · Kai Chen (Shanghai AI Laboratory) · Tianfan Xue (The Chinese University of Hong Kong) · Xihui Liu (The University of Hong Kong) · Cewu Lu (Shanghai Jiao Tong University) · Dahua Lin (The Chinese University of Hong Kong) · Jiangmiao Pang (Shanghai AI Laboratory )
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Interpretable Measures of Conceptual Similarity by  Complexity-Constrained Descriptive Auto-Encoding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alessandro Achille (California Institute of Technology) · Greg Ver Steeg (University of California, Riverside) · Tian Yu Liu (University of California, Los Angeles) · Matthew Trager (Amazon) · Carson Klingenberg (Amazon Web Services) · Stefano Soatto (AWS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DETRs Beat YOLOs on Real-time Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yian Zhao (Peking University) · Wenyu Lv (Baidu) · Shangliang Xu (Baidu) · Jinman Wei (Tianjin University) · Guanzhong Wang (Baidu) · Qingqing Dang (Baidu) · Yi Liu (None) · Jie Chen (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DIOD: Self-Distillation Meets Object Discovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sandra Kara (CEA) · Hejer AMMAR (CEA) · Julien Denize (CEA) · Florian Chabot (CEA) · Quoc Cuong PHAM (CEA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guillaume Jaume (Harvard University) · Anurag Vaidya (Massachusetts Institute of Technology) · Richard J. Chen (Harvard University) · Drew F. K. Williamson (Massachusetts General Hospital, Harvard University) · Paul Pu Liang (Carnegie Mellon University) · Faisal Mahmood (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FlashEval: Towards Fast and Accurate Evaluation of Text-to-image Diffusion Generative Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            LIn Zhao (Infinigence) · Tianchen Zhao (Tsinghua University, Tsinghua University) · Zinan Lin (Microsoft Research) · Xuefei Ning (Tsinghua University, Tsinghua University) · Guohao Dai (Shanghai Jiaotong University) · Huazhong Yang (Tsinghua University, Tsinghua University) · Yu Wang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://k8xu.github.io/amodal" target="_blank">Amodal Completion via Progressive Mixed Context Diffusion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Katherine Xu (University of Pennsylvania) · Lingzhi Zhang (School of Engineering and Applied Science, University of Pennsylvania) · Jianbo Shi (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Attention-Driven Training-Free Efficiency Enhancement of Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongjie Wang (Princeton University) · Difan Liu (Adobe Research) · Yan Kang (None) · Yijun Li (Adobe Research) · Zhe Lin (Adobe Research) · Niraj Jha (Princeton University) · Yuchen Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Deep Generative Model based Rate-Distortion for Image Downscaling Assessment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            yuanbang liang (Cardiff Univeristy) · Bhavesh Garg (IIT Bombay) · Paul L. Rosin (Cardiff University) · Yipeng Qin (Cardiff University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Naturally Supervised 3D Visual Grounding with Language-Regularized Concept Learners</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chun Feng (University of Science and Technology of China) · Joy Hsu (Stanford University) · Weiyu Liu (Stanford University) · Jiajun Wu (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Forecasting of 3D Whole-body Human Poses with Grasping Objects</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            yan haitao (None) · Qiongjie Cui (Nanjing University of Science and Technology) · Jiexin Xie (Fudan University) · Shijie Guo (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VA3: Virtually Assured Amplification Attack on Probabilistic Copyright Protection for Text-to-Image Generative Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiang Li (National University of Singapore) · Qianli Shen (National University of Singapore) · Kenji Kawaguchi (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Revisiting the Domain Shift and Sample Uncertainty in Multi-source Active Domain Transfer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenqiao Zhang (National University of Singapore) · Zheqi Lv (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Co-Evaluation of Cameras, HDR, and Algorithms for Industrial-Grade 6DoF Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Agastya Kalra (Google) · Guy Stoppi (Intrinsic) · Dmitrii Marin (Intrinsic) · Vage Taamazyan (Intrinsic) · Aarrushi Shandilya (Intrinsic AI) · Rishav Agarwal (Intrinsic) · Anton Boykov (University of Waterloo) · Aaron Chong (Google) · Michael Stark (Intrinsic)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Correcting Diffusion Generation through Resampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujian Liu (University of California, Santa Barbara) · Yang Zhang (International Business Machines) · Tommi Jaakkola (Massachusetts Institute of Technology) · Shiyu Chang (UC Santa Barbara)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianqing Zhang (Shanghai Jiao Tong University &amp; Tsinghua University) · Yang Liu (Tsinghua University, Tsinghua University) · Yang Hua (Queen's University Belfast) · Jian Cao (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Partial-to-Partial Shape Matching with Geometric Consistency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Viktoria Ehm (Technische Universität München) · Maolin Gao (None) · Paul Roetzer (University of Bonn) · Marvin Eisenberger (Technical University Munich) · Daniel Cremers (Technical University Munich) · Florian Bernard (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deep Imbalanced Regression via Hierarchical Classification Adjustment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haipeng Xiong (National University of Singapore) · Angela Yao (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HoloVIC: Large-scale Dataset and Benchmark for Multi-Sensor Holographic Intersection and Vehicle-Infrastructure Cooperative</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            CONG MA (Senseauto Research) · Qiao Lei (SenseAuto Research) · Chengkai Zhu (SenseAuto Research) · Kai Liu (SenseAuto Research) · Zelong Kong (SenseAuto Research) · Liqing (SenseAuto) · Xueqi Zhou (Beijing Sensetime Technology Development Co., Ltd.) · Yuheng KAN (Zhejiang University) · Wei Wu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            song yiran (None) · Qianyu Zhou (Shanghai Jiao Tong University) · Xiangtai Li (Nanyang Technological University) · Deng-Ping Fan (ETH Zurich) · Xuequan Lu (La Trobe University) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-guided Explorable Image Super-resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kanchana Vaishnavi Gandikota (None) · Paramanand Chandramouli (Universität Siegen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffMorpher: Unleashing the Capability of Diffusion Models for Image Morphing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kaiwen Zhang (Tsinghua University) · Yifan Zhou (Nanyang Technological University) · Xudong XU (Shanghai AI Laboratory) · Bo Dai (Shanghai AI Laboratory) · Xingang Pan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NC-SDF: Enhancing Indoor Scene Reconstruction Using Neural SDFs with View-Dependent Normal Compensation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyi Chen (Zhejiang University) · Xiaolong Wu (Georgia Institute of Technology) · Yu Zhang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Correspondence-Free Non-Rigid Point Set Registration Using Unsupervised Clustering Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingyang Zhao (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Jiang Jingen (Shandong University) · Lei Ma (Peking University) · Shiqing Xin (Shandong University) · Gaofeng Meng (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Dong-Ming Yan (Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FedSOL: Stabilized Orthogonal Learning with Proximal Restrictions in Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gihun Lee (KAIST AI) · Minchan Jeong (Korea Advanced Institute of Science and Technology) · SangMook Kim (KAIST) · Jaehoon Oh (Samsung Advanced Institute of Technology) · Se-Young Yun (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LAENeRF: Local Appearance Editing for Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lukas Radl (Graz University of Technology) · Michael Steiner (Technische Universität Graz) · Andreas Kurz (Technische Universität Graz) · Markus Steinberger (Technische Universität Graz)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Don't Look into the Dark: Latent Codes for Pluralistic Image Inpainting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haiwei Chen (USC-ICT, Vision and Graphics Lab) · Yajie Zhao (University of Southern California)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Point Cloud Diffusion for Disentangled 3D Shape and Appearance Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Philipp Schröppel (University of Freiburg, Germany) · Christopher Wewer (Max Planck Institute for Informatics, Saarland Informatics Campus) · Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) · Eddy Ilg (None) · Thomas Brox (University of Freiburg)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Multi-dimensional Human Preference for Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sixian Zhang (None) · Bohan Wang (Kuaishou) · Junqiang Wu (Kuaishou) · Yan Li (kuaishou) · Tingting Gao (China Agricultural University) · Di ZHANG (Kuaishou Technology) · Zhongyuan Wang (Kuaishou Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MeLFusion: Synthesizing Music from Image and Language Cues using Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sanjoy Chowdhury (None) · Sayan Nag (University of Toronto) · Joseph J (Adobe Systems) · Balaji Vasan Srinivasan (Adobe Research) · Dinesh Manocha (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DeCoTR: Enhancing Depth Completion with 2D and 3D Attentions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunxiao Shi (Qualcomm AI Research Qualcomm) · Manish Singh (Qualcomm AI Research) · Hong Cai (Qualcomm AI Research) · Fatih Porikli (QualComm)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Exploiting Style Latent Flows for Generalizing Video Deepfake Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jongwook Choi (Chung-Ang University) · Taehoon Kim (Chung-Ang University) · Yonghyun Jeong (NAVER) · Seungryul Baek (UNIST) · Jongwon Choi (Chung-Ang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bayesian Differentiable Physics for Cloth Digitalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Deshan Gong (University of Leeds) · Ningtao Mao (University of Leeds) · He Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MemSAM: Taming Segment Anything Model for Echocardiography Video Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaolong Deng (Shenzhen University) · Huisi Wu (Shenzhen University) · Runhao Zeng (Shenzhen MSU-BIT University) · Jing Qin (Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeyinzi Jiang (Alibaba Group) · Chaojie Mao (Alibaba Group) · Yulin Pan (Alibaba Group, China) · Zhen Han (Alibaba Group) · Jingfeng Zhang (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tsai-Shien Chen (University of California, Merced) · Aliaksandr Siarohin (Snap Inc.) · Willi Menapace (University of Trento) · Ekaterina Deyneka (Snap Inc.) · Hsiang-wei Chao (Snap Inc.) · Byung Jeon (Snap Inc.) · Yuwei Fang (Snap Inc.) · Hsin-Ying Lee (Snap Inc.) · Jian Ren (Snap Inc.) · Ming-Hsuan Yang (University of California at Merced) · Sergey Tulyakov (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DREAM: Diffusion Rectification and Estimation-Adaptive Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinxin Zhou (Ohio State University, Columbus) · Tianyu Ding (Microsoft) · Tianyi Chen (Microsoft) · Jiachen Jiang (Ohio State University, Columbus) · Ilya Zharkov (Microsoft) · Zhihui Zhu (Ohio State University, Columbus) · Luming Liang (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised Semantic Segmentation Through Depth-Guided Feature Correlation and Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leon Sick (Ulm University) · Dominik Engel (Ulm University) · Pedro Hermosilla (Technische Universität Wien) · Timo Ropinski (Ulm University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Willi Menapace (University of Trento) · Aliaksandr Siarohin (Snap Inc.) · Ivan Skorokhodov (KAUST) · Ekaterina Deyneka (Snap Inc.) · Tsai-Shien Chen (University of California, Merced) · Anil Kag (Snap Inc.) · Yuwei Fang (Snap Inc.) · Aleksei Stoliar (None) · Elisa Ricci (University of Trento) · Jian Ren (Snap Inc.) · Sergey Tulyakov (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://frozenburning.github.io/projects/urhand/" target="_blank">URHand: Universal Relightable Hands</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaoxi Chen (Nanyang Technological University) · Gyeongsik Moon (None) · Kaiwen Guo (Google) · Chen Cao (Facebook) · Stanislav Pidhorskyi (Meta) · Tomas Simon (Meta) · Rohan Joshi (Facebook) · Yuan Dong (Facebook) · Yichen Xu (Meta platforms inc) · Bernardo Pires (Meta Platforms Inc.) · He Wen (Meta Platformts, Inc.) · Lucas Evans (Meta) · Bo Peng (Meta Platforms Inc.) · Julia Buffalini (Meta) · Autumn Trimble (Meta) · Kevyn McPhail (Meta) · Melissa Schoeller (Meta Platforms Inc) · Shoou-I Yu (Reality Labs Research, Meta) · Javier Romero (None) · Michael Zollhoefer (Meta) · Yaser Sheikh (Meta) · Ziwei Liu (Nanyang Technological University) · Shunsuke Saito (Reality Labs Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffPortrait3D: Controllable Diffusion for Zero-Shot Portrait View Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuming Gu (USC Institute for Creative Technologies, University of Southern California) · Hongyi Xu (Bytedance) · You Xie (Bytedance) · Guoxian Song (Bytedance Inc) · Yichun Shi (ByteDance) · Di Chang (University of Southern California) · Jing Yang (USC Institute for Creative Technologies) · Linjie Luo (ByteDance Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhancing Visual Continual Learning with Language-Guided Supervision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bolin Ni (Institute of Automation, Chinese Academy of Sciences) · Hongbo Zhao (Institute of Automation, Chinese Academy of Sciences) · Chenghao Zhang (Alibaba Group) · Ke Hu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Gaofeng Meng (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Shiming Xiang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Video Prediction by Modeling Videos as Continuous Multi-Dimensional Processes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gaurav Shrivastava (Department of Computer Science, University of Maryland, College Park) · Abhinav Shrivastava (University of Maryland)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised Keypoints from Pretrained Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Eric Hedlin (University of British Columbia) · Gopal Sharma (None) · Shweta Mahajan (University of British Columbia) · Xingzhe He (None) · Hossam Isack (Google) · Abhishek Kar (Google) · Helge Rhodin (UBC) · Andrea Tagliasacchi (Simon Fraser University, Google Brain) · Kwang Moo Yi (University Of British Columbia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generating Human Motion in 3D Scenes from Text Descriptions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhi Cen (Zhejiang University) · Huaijin Pi (Zhejiang University) · Sida Peng (None) · Zehong Shen (Zhejiang University) · Minghui Yang (Ant Group) · Shuai Zhu (Ant Group) · Hujun Bao (Zhejiang University) · Xiaowei Zhou (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Light the Night: A Multi-Condition Diffusion Framework for Unpaired Low-Light Enhancement in Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            JINLONG LI (Cleveland State University) · Baolu Li (Cleveland State University) · Zhengzhong Tu (University of Texas at Austin) · XINYU LIU (Cleveland State University) · Qing Guo (Institute of High Performance Computing, Singapore, A*STAR) · Felix Juefei Xu () · Runsheng Xu (University of California, Los Angeles) · Hongkai Yu (Cleveland State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with Wavelet Augmentation Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuang Ai (Institute of Automation, Chinese Academy of Sciences) · Xiaoqiang Zhou (University of Science and Technology of China) · Huaibo Huang (Institute of Automation, Chinese Academy of Sciences) · Lei Zhang (The Hong Kong Polytechnic University) · Ran He (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generalizing 6-DoF Grasp Detection via Domain Prior Knowledge</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoxiang Ma (Beihang University) · Modi Shi (Beijing University of Aeronautics and Astronautics) · Boyang GAO (Geometry Robotics ltd. &amp; Harbin Institute of Technology) · Di Huang (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bridging the Synthetic-to-Authentic Gap: Distortion-Guided Unsupervised Domain Adaptation for Blind Image Quality Assessment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aobo Li (Xidian University) · Jinjian Wu (Xidian University) · Yongxu Liu (Xidian University) · Leida Li (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://taegyeong-lee.github.io/text2video" target="_blank">Grid Diffusion Models for Text-to-Video Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taegyeong Lee (Ulsan National Institute of Science and Technology) · Soyeong Kwon (Ulsan National Institute of Science and Technology) · Taehwan Kim (UNIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Neural Parametric Gaussians for Monocular Non-Rigid Object Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Devikalyan Das (Max Planck Institute for Informatics) · Christopher Wewer (Max Planck Institute for Informatics, Saarland Informatics Campus) · Raza Yunus (Saarland Informatics Campus, Max-Planck Institute) · Eddy Ilg (None) · Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Personalized Residuals for Concept-Driven Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cusuh Ham (None) · Matthew Fisher (Adobe Research) · James Hays (Georgia Institute of Technology) · Nicholas Kolkin (Adobe Systems) · Yuchen Liu (None) · Richard Zhang (Adobe Systems) · Tobias Hinz (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Making Vision Transformers Truly Shift-Equivariant</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Renan A. Rojas-Gomez (University of Illinois at Urbana Champaign) · Teck-Yian Lim (DSO National Laboratories) · Minh Do (University of Illinois at Urbana-Champaign) · Raymond A. Yeh (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://huanngzh.github.io/EpiDiff/" target="_blank">EpiDiff: Enhancing Multi-View Synthesis via Localized Epipolar-Constrained Diffusion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zehuan Huang (Beijing University of Aeronautics and Astronautics) · Hao Wen (Beijing University of Aeronautics and Astronautics) · Junting Dong (None) · Yaohui Wang (Shanghai AI Laboratory) · Yangguang Li (Shanghai AI Laboratory) · Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) · Yan-Pei Cao (Tencent ARC Lab) · Ding Liang (Tsinghua University, Tsinghua University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Bo Dai (Shanghai AI Laboratory) · Lu Sheng (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RankED: Addressing Imbalance and Uncertainty in Edge Detection Using Ranking-based Losses</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            bedrettin cetinkaya (Middle East Technical University) · Sinan Kalkan (Middle East Technical University) · Emre Akbas (METU)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MovieChat: From Dense Token to Sparse Memory for Long Video Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Enxin Song () · Wenhao Chai (University of Washington) · Guanhong Wang (Zhejiang University) · Haoyang Zhou (Zhejiang University) · Feiyang Wu (Zhejiang University) · Yucheng Zhang (Zhejiang University) · Tian Ye (Hong Kong University of Science and Technology, Guangzhou Campus) · Haozhe Chi (Zhejiang University) · Xun Guo (Microsoft Research Asia) · Yanting Zhang (Donghua University, Shanghai) · Yan Lu (Microsoft Research Asia) · Jenq-Neng Hwang (None) · Gaoang Wang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TransNeXt: Robust Foveal Visual Perception for Vision Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dai Shi (Independent Researcher)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Triangular Distribution in Visual World</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ping Chen (Great Wall Motor Company Limited) · Xingpeng Zhang (Southwest Petroleum University) · Chengtao Zhou (Microbt) · dichao Fan (MicroBT) · Peng Tu (RuqiMobility Inc.) · Le Zhang (shenzhen MicroBT Electronics Technology Corporation ) · Yanlin Qian (Tampere University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Free3D: Consistent Novel View Synthesis without 3D Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chuanxia Zheng (University of Oxford) · Andrea Vedaldi (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fine-grained Bipartite Concept Factorization for Clustering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chong Peng (None) · Pengfei Zhang (Qingdao University) · Yongyong Chen (Harbin Institute of Technology (Shenzhen)) · zhao kang (University of Electronic Science and Technology of China) · Chenglizhao Chen (China University of Petroleum) · Qiang Cheng (University of Kentucky)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GLiDR: Topologically Regularized Graph Generative Network for Sparse LiDAR Point Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Prashant Kumar (Indian Institute of Technology Delhi) · Kshitij Madhav Bhat (Indian Institute of Technology Indore) · Vedang Bhupesh Shenvi Nadkarni (Birla Institute of Technology and Science Pilani (BITS Pilani)) · Prem Kalra (Indian Institute of Technology, Delhi)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generalized Event Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Varun Sundar (University of Wisconsin, Madison) · Matthew Dutson (University of Wisconsin, Madison) · Andrei Ardelean (NovoViz) · Claudio Bruschini (EPFL - EPF Lausanne) · Edoardo Charbon (EPFL - EPF Lausanne) · Mohit Gupta (Department of Computer Sciences, University of Wisconsin - Madison)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuang Ai (Institute of Automation, Chinese Academy of Sciences) · Huaibo Huang (Institute of Automation, Chinese Academy of Sciences) · Xiaoqiang Zhou (University of Science and Technology of China) · Jiexiang Wang (University of Science and Technology of China) · Ran He (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DIEM: Decomposition-Integration Enhancing Multimodal Insights</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyi Jiang (None) · Guoming Wang (Zhejiang University) · Junhao Guo (Zhejiang University) · Juncheng Li (Zhejiang University) · Wenqiao Zhang (National University of Singapore) · Rongxing Lu (University of New Brunswick) · Siliang Tang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ESCAPE: Encoding Super-keypoints for Category-Agnostic Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Khoi D Nguyen (University of Wisconsin - Madison) · Chen Li (National University of Singapore) · Gim Hee Lee (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Balancing Act: Distribution-Guided Debiasing in Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rishubh Parihar (Indian Institute of Science, Bangalore) · Abhijnya Bhat (Indian Institute of Science, Indian institute of science, Bangalore) · Abhipsa Basu (Indian Institute of Science) · Saswat Mallick (Indian Institute of Science, Indian institute of science, Bangalore) · Jogendra Kundu Kundu (None) · R. Venkatesh Babu (Indian Institute of Science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multi-modal In-Context Learning Makes an Ego-evolving Scene Text Recognizer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Zhao (East China Normal University) · Jingqun Tang (Bytedance) · Chunhui Lin (Bytedance) · Binghong Wu (Bytedance) · Can Huang (Bytedance) · Hao Liu (Bytedance Inc.) · Xin Tan (East China Normal University) · Zhizhong Zhang (East China Normal University) · Yuan Xie (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NIVeL: Neural Implicit Vector Layers for Text-to-Vector Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vikas Thamizharasan (University of Massachusetts Amherst) · Difan Liu (Adobe Research) · Matthew Fisher (Adobe Research) · Nanxuan Zhao (Adobe Research) · Evangelos Kalogerakis (UMass Amherst) · Michal Lukáč (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Enhancing Quality of Compressed Images by Mitigating Enhancement Bias Towards Compression Domain</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qunliang Xing (Beihang University) · Mai Xu (Beihang University, Tsinghua University) · Shengxi Li (Beihang University) · Xin Deng (Beijing University of Aeronautics and Astronautics) · Meisong Zheng (Alibaba Group) · huaida liu (Alibaba Group) · Ying Chen (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Accurate Training Data for Occupancy Map Prediction in Automated Driving using Evidence Theory</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jonas Kälble (Bosch Center for Artificial Intelligence) · Sascha Wirges (Robert Bosch GmbH, Bosch) · Maxim Tatarchenko (Bosch) · Eddy Ilg (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chao Xu (Zhejiang University) · Yang Liu (Alibaba Group) · Jiazheng Xing (Zhejiang University) · Weida Wang (Xingji Meizu Group) · Mingze Sun (None) · Jun Dan (Zhejiang University) · Tianxin Huang (Tencent youtu lab) · Siyuan Li (Westlake University, Zhejiang University) · Zhi-Qi Cheng (Carnegie Mellon University) · Ying Tai (Nanjing University) · Baigui Sun (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuai Yang (Nanyang Technological University) · Yifan Zhou (Nanyang Technological University) · Ziwei Liu (Nanyang Technological University) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Backdoor Defense via Test-Time Detecting and Repairing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiyang Guan (Institute of Automation,  Chinese Academy of Sciences) · Jian Liang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Ran He (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhixuan Liang (The University of Hong Kong) · Yao Mu (The University of Hong Kong) · Hengbo Ma (None) · Masayoshi Tomizuka (University of California, Berkeley) · Mingyu Ding (UC Berkeley) · Ping Luo (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ViT-CoMer: Vision Transformer with Convolutional Multi-scale Feature Interaction for Dense Predictions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chunlong Xia (Baidu) · Xinliang Wang (Baidu) · Feng Lv (Baidu) · Xin Hao (Beijing Institute of Technology) · Yifeng Shi (Baidu)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ZONE: Zero-Shot Instruction-Guided Local Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shanglin Li (Beijing University of Aeronautics and Astronautics) · Bohan Zeng (Beijing University of Aeronautics and Astronautics) · Yutang Feng (Beijing University of Aeronautics and Astronautics) · Sicheng Gao (Bayerische Julius-Maximilians-Universität Würzburg) · Xuhui Liu (Beihang University) · Jiaming Liu (Xiaohongshu) · Li Lin (Xiamen University) · Xu Tang (Shanghaitech University) · Yao Hu (Zhejiang University, Tsinghua University) · Jianzhuang Liu (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Baochang Zhang (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Count without Annotations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lukas Knobel (University of Amsterdam &amp; TNO) · Tengda Han (University of Oxford, University of Oxford) · Yuki Asano (University of Amsterdam)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HIVE: Harnessing Human Feedback for Instructional Visual Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shu Zhang (SalesForce.com) · Xinyi Yang (Salesforce Research) · Yihao Feng (Salesforce Research) · Can Qin (Northeastern University) · Chia-Chih Chen (Salesforce) · Ning Yu (Salesforce Research) · Zeyuan Chen (SalesForce.com) · Huan Wang (SalesForce.com) · Silvio Savarese (Salesforce) · Stefano Ermon (Stanford University) · Caiming Xiong (Salesforce Research) · Ran Xu (SalesForce.com)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Backward-Compatible Continual Learning of Image Compression</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhihao Duan (Purdue University) · Ming Lu (Nanjing University) · Justin Yang (Purdue University) · Jiangpeng He (Purdue University) · Zhan Ma (Nanjing University) · Fengqing Zhu (Purdue University, Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Clustering for Protein Representation Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruijie Quan (Zhejiang University) · Wenguan Wang (Zhejiang University) · Fan Ma (None) · Hehe Fan (None) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Segment Referred Objects from Narrated Egocentric Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhan Shen (Northeastern University) · Huiyu Wang (Facebook) · Xitong Yang (Meta) · Matt Feiszli (Meta AI) · Ehsan Elhamifar (None) · Lorenzo Torresani (Facebook) · Effrosyni Mavroudi ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>What Sketch Explainability Really Means for Downstream Tasks ?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hmrishav Bandyopadhyay (University of Surrey) · Pinaki Nath Chowdhury (University of Surrey) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Aneeshan Sain (University of Surrey) · Tao Xiang (University of Surrey) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lingmin Ran (National University of Singapore) · Xiaodong Cun (Tencent AI Lab) · Jia-Wei Liu (National University of Singapore) · Rui Zhao (None) · Song Zijie (Fudan University) · Xintao Wang (Tencent) · Jussi Keppo (National University of Singapore) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SURE: SUrvey REcipes for building reliable and robust deep networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuting Li (China Three Gorges University) · Yingyi Chen (Department of Electrical Engineering, KU Leuven, Belgium, KU Leuven) · Xuanlong Yu (Université Paris-Saclay) · Dexiong Chen (Max Planck Institute of Biochemistry) · Xi Shen (Tencent AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text Grouping Adapter: Adapting Pre-trained Text Detector for Layout Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianci Bi (Xi'an Jiaotong University) · Xiaoyi Zhang (Research, Microsoft) · Zhizheng Zhang (Microsoft Research) · Wenxuan Xie (Microsoft Research Asia) · Cuiling Lan (Microsoft) · Yan Lu (Microsoft Research Asia) · Nanning Zheng (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Label Propagation for Zero-shot Classification with Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vladan Stojnić (Czech Technical University in Prague) · Yannis Kalantidis (NAVER LABS Europe) · Giorgos Tolias (CTU in Prague)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3D-SceneDreamer: Text-Driven 3D-Consistent Scene Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Songchun Zhang (Zhejiang University) · Yibo Zhang (Jilin University) · Quan Zheng (Institute of Software, Chinese Academy of Sciences) · Rui Ma (Jilin University) · Wei Hua (Zhejiang Lab) · Hujun Bao (Zhejiang University) · Weiwei Xu (Zhejiang University) · Changqing Zou (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Visual-Augmented Dynamic Semantic Prototype for Generative Zero-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenjin Hou (Huazhong University of Science and Technology) · Shiming Chen (Carnegie Mellon University) · Shuhuang Chen (Huazhong University of Science and Technology) · Ziming Hong (The University of Sydney) · Yan Wang (Alibaba Group) · Xuetao Feng (Alibaba Group) · Salman Khan (Mohamed bin Zayed University of Artificial Intelligence) · Fahad Shahbaz Khan (MBZUAI; Linköping University) · Xinge You (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/ZPDu/Boosting-Object-Detection-with-Zero-Shot-Day-Night-Domain-Adaptation" target="_blank">Boosting Object Detection with Zero-Shot Day-Night Domain Adaptation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhipeng Du (University of Edinburgh &amp; King's College London) · Miaojing Shi (King's College London) · Jiankang Deng (Imperial College London &amp; Huawei UKRD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CoDi: Conditional Diffusion Distillation for Higher-Fidelity and Faster Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kangfu Mei (Johns Hopkins University) · Mauricio Delbracio (None) · Hossein Talebi (Google Research) · Zhengzhong Tu (University of Texas at Austin) · Vishal M. Patel (Johns Hopkins University) · Peyman Milanfar (Peyman Milanfar)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyu Ma (University of California, Irvine) · Shahin Mahdizadehaghdam (Meta) · Bichen Wu (Facebook) · Zhipeng Fan (Facebook) · Yuchao Gu (None) · Wenliang Zhao (Meta Inc) · Lior Shapira (Meta) · Xiaohui Xie (University of California, Irvine)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsegment Anything by Simulating Deformation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahao Lu (National University of Singapore) · Xingyi Yang (National University of Singapore) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-9-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi class=&quot;MJX-tex-caligraphic&quot; mathvariant=&quot;script&quot;&gt;Z&lt;/mi&gt;&lt;/mrow&gt;&lt;mo&gt;&amp;#x2217;&lt;/mo&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-42" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-43" class="mjx-mrow"><span id="MJXc-Node-44" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.042em;"><span id="MJXc-Node-45" class="mjx-texatom"><span id="MJXc-Node-46" class="mjx-mrow"><span id="MJXc-Node-47" class="mjx-mi"><span class="mjx-char MJXc-TeX-cal-R" style="padding-top: 0.441em; padding-bottom: 0.316em; padding-right: 0.042em;">Z</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.615em; padding-left: 0.148em; padding-right: 0.071em;"><span id="MJXc-Node-48" class="mjx-mo" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.191em; padding-bottom: 0.316em;">∗</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow class="MJX-TeXAtom-ORD"><mi class="MJX-tex-caligraphic" mathvariant="script">Z</mi></mrow><mo>∗</mo></msup></math></span></span><script type="math/tex" id="MathJax-Element-9">\mathcal{Z}^*</script>: Zero-shot <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-10-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-49" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-50" class="mjx-mrow"><span id="MJXc-Node-51" class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.112em;"><span id="MJXc-Node-52" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em; padding-right: 0.032em;">S</span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span id="MJXc-Node-53" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">−</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mi>S</mi><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-10">\underline{S}</script>tyle <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-11-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-54" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-55" class="mjx-mrow"><span id="MJXc-Node-56" class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.14em;"><span id="MJXc-Node-57" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.12em;">T</span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span id="MJXc-Node-58" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">−</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mi>T</mi><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-11">\underline{T}</script>ransfer via <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-12-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-59" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-60" class="mjx-mrow"><span id="MJXc-Node-61" class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.014em;"><span id="MJXc-Node-62" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.253em;">A</span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span id="MJXc-Node-63" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">−</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mi>A</mi><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-12">\underline{A}</script>ttention <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-13-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;munder&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;mo&gt;&amp;#x005F;&lt;/mo&gt;&lt;/munder&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-64" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-65" class="mjx-mrow"><span id="MJXc-Node-66" class="mjx-munderover"><span class="mjx-itable"><span class="mjx-row"><span class="mjx-cell"><span class="mjx-op" style="padding-left: 0.01em;"><span id="MJXc-Node-67" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.316em;">R</span></span></span></span></span><span class="mjx-row"><span class="mjx-under" style="padding-top: 0.12em;"><span id="MJXc-Node-68" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.316em; padding-bottom: 0.441em;">−</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><munder><mi>R</mi><mo>_</mo></munder></math></span></span><script type="math/tex" id="MathJax-Element-13">\underline{R}</script>eweighting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yingying Deng (None) · Xiangyu He (Meituan) · Fan Tang (Institute of Computing Technology, CAS) · Weiming Dong (Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Cross-spectral Gated-RGB Stereo Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Samuel Brucker (Torc Robotics) · Stefanie Walz (Mercedes-Benz AG) · Mario Bijelic (Princeton University) · Felix Heide (Department of Computer Science, Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Self-Adaptive Reality-Guided Diffusion for Artifact-Free Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qingping Zheng (Northwestern Polytechnical University) · Ling Zheng (Tsinghua-Fuzhou Institute for Data Technology) · Yuanfan Guo (Huawei Technologies Ltd.) · Ying Li (Northwestern Polytechnical University) · Songcen Xu (Huawei Noah's Ark Lab) · Jiankang Deng (Imperial College London &amp; Huawei UKRD) · Hang Xu (Huawei Noah‘s Ark Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/tianyi-lab/HallusionBench" target="_blank">HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianrui Guan (University of Maryland, College Park) · Fuxiao Liu (University of Maryland) · Xiyang Wu (University of Maryland, College Park) · Ruiqi Xian (University of Maryland, College Park) · Zongxia Li (University of Maryland, College Park) · Xiaoyu Liu (University of Maryland, College Park) · Xijun Wang (University of Maryland, College Park) · Lichang Chen (Department of Computer Science, University of Maryland, College Park) · Furong Huang (Department of Computer Science, University of Maryland) · Yaser Yacoob (University of Maryland, College Park) · Dinesh Manocha (University of Maryland, College Park) · Tianyi Zhou (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Spike-guided Motion Deblurring with Unknown Modal Spatiotemporal Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiyuan Zhang (Peking University) · Shiyan Chen (Peking University) · Yajing Zheng (Peking University) · Zhaofei Yu (Peking University) · Tiejun Huang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Task Dense Prediction via Mixture of Low-Rank Experts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqi Yang (Nankai University) · Peng-Tao Jiang (vivo Mobile Communication (Hangzhou) Co., Ltd.) · Qibin Hou (Nankai University) · Hao Zhang (vivo Mobile Communication （Hangzhou）Co., Ltd) · Jinwei Chen (vivo Mobile Communication Co., Ltd.) · Bo Li (vivo Mobile Communication Co.,Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LAA-Net: Localized Artifact Attention Network for Quality-Agnostic and Generalizable Deepfake Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dat NGUYEN (University of Luxembourg) · Nesryne Mejri (SnT, University of Luxembourg) · Inder Pal Singh (University of Luxemburg) · Polina Kuleshova (University of Luxemburg) · Marcella Astrid (University of Luxemburg) · Anis Kacem (University of Luxemburg) · Enjie Ghorbel (CRISTAL laboratory, ENSI, University of Manouba) · Djamila Aouada (SnT, University of Luxembourg)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EASE-DETR: Easing the Competition among Object Queries</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yulu Gao (Beijing University of Aeronautics and Astronautics) · Yifan Sun (Baidu Research) · Xudong Ding (Beijing University of Aeronautics and Astronautics) · Chuyang Zhao (Baidu) · Si Liu (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CAT-DM: Controllable Accelerated Virtual Try-on with Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianhao Zeng (Tianjin University) · Dan Song (Tianjin University) · Weizhi Nie (Tianjin University) · Hongshuo Tian (Tianjin University) · Tongtong Wang (Tencent LightSpeed Studio) · An-An Liu (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiyuan Zhang (The Chinese University of Hong Kong) · Xiaohan Ding (Tencent AI Lab) · Kaixiong Gong (None) · Yixiao Ge (Tencent) · Ying Shan (Tencent) · Xiangyu Yue (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Readout Guidance: Learning Control from Diffusion Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Grace Luo (University of California, Berkeley) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Oliver Wang (Adobe Research) · Dan Goldman (Google) · Aleksander Holynski (UC Berkeley &amp; Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Transcriptomics-guided Slide Representation Learning in Computational Pathology</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guillaume Jaume (Harvard University) · Lukas Oldenburg (Harvard University) · Anurag Vaidya (Massachusetts Institute of Technology) · Richard J. Chen (Harvard University) · Drew F. K. Williamson (Massachusetts General Hospital, Harvard University) · Thomas Peeters (Harvard University) · Andrew Song (Brigham and Women's hospital) · Faisal Mahmood (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ConTex-Human: Free-View Rendering of Human from a Single Image with Texture-Consistent Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiangjun Gao ((HKUST) The Hong Kong University of Science and Technology) · Xiaoyu Li (Tencent AI Lab) · Chaopeng Zhang (Tencent AI Lab) · Qi Zhang (Tencent AI Lab) · Yan-Pei Cao (Tencent ARC Lab) · Ying Shan (Tencent) · Long Quan (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>InstructDiffusion: A Generalist Modeling Interface for Vision Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zigang Geng (University of Science and Technology of China) · Binxin Yang (University of Science and Technology of China) · Tiankai Hang (Southeast University) · Chen Li (Xi'an Jiaotong University) · Shuyang Gu (Research, Microsoft) · Ting Zhang (Beijing Normal University) · Jianmin Bao (Microsoft) · Zheng Zhang (Microsoft) · Houqiang Li (University of Science and Technology of China) · Han Hu (Microsft Research Asia) · Dong Chen (Microsoft) · Baining Guo (Microsoft Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Making Visual Sense of Oracle Bones for You and Me</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Runqi Qiao (Beijing University of Posts and Telecommunications) · LAN YANG (Beijing University of Posts and Telecommunications) · Kaiyue Pang (SketchX AI) · Honggang Zhang (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ProTeCt: Prompt Tuning for Taxonomic Open Set Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tz-Ying Wu (University of California, San Diego) · Chih-Hui Ho (University of California San Diego) · Nuno Vasconcelos (University of California San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Mosaic-SDF for 3D Generative Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lior Yariv (Weizmann Institute of Science) · Omri Puny (Weizmann Institute of Science) · Oran Gafni (Meta AI) · Yaron Lipman (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Characteristics Matching Based Hash Codes Generation for Efficient Fine-grained Image Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen-Duo Chen (Shandong University) · Li-Jun Zhao (Shandong University) · Zi-Chao Zhang (Shandong University) · Xin Luo (Shandong University) · Xin-Shun Xu (Shandong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Understanding Cross and Self-Attention in Stable Diffusion for Text-Guided Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bingyan Liu (South China University of Technology) · Chengyu Wang (Alibaba Group) · Tingfeng Cao (South China University of Technology) · Kui Jia (South China University of Technology) · Jun Huang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DyBluRF: Dynamic Neural Radiance Fields from Blurry Monocular Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huiqiang Sun (None) · Xingyi Li (Huazhong University of Science and Technology) · Liao Shen (Huazhong University of Science and Technology) · Xinyi Ye (School of Artificial Intelligence and Automation, Huazhong University of Science and Technology) · Ke Xian (Nanyang Technological University) · Zhiguo Cao ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ViewDiff: 3D-Consistent Image Generation with Text-To-Image Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lukas Hoellein (None) · Aljaž Božič (Facebook) · Norman Müller (Meta) · David Novotny (Facebook) · Hung-Yu Tseng (Meta) · Christian Richardt (Meta Reality Labs) · Michael Zollhoefer (Meta) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Binarized Low-light Raw Video Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gengchen Zhang (Beijing Institute of Technology) · Yulun Zhang (Shanghai Jiao Tong University) · Xin Yuan (Westlake University) · Ying Fu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianyu Yu (Tsinghua University, Tsinghua University) · Yuan Yao (Tsinghua University) · Haoye Zhang (Tsinghua University, Tsinghua University) · Taiwen He (Tsinghua University, Tsinghua University) · Yifeng Han (Zhejiang University) · Ganqu Cui (Tsinghua University, Tsinghua University) · Jinyi Hu (Tsinghua University, Tsinghua University) · Zhiyuan Liu (Tsinghua University) · Hai-Tao Zheng (Tsinghua University, Tsinghua University) · Maosong Sun (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jack Urbanek (Facebook) · Florian Bordes (Meta AI) · Pietro Astolfi (Meta AI) · Mary Williamson (Meta AI (FAIR)) · Vasu Sharma (Meta AI/ CMU) · Adriana Romero-Soriano (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GaussianAvatar: Efficient Animatable Human Modeling from Monocular Video Using Gaussians-on-Mesh</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jing Wen (University of Illinois Urbana-Champaign) · Xiaoming Zhao (UIUC) · Jason Ren (Apple) · Alexander G. Schwing (UIUC) · Shenlong Wang (University of Illinois, Urbana Champaign)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Coherent Temporal Synthesis for Incremental Action Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            GUODONG DING (NATIONAL UNIVERSITY OF SINGAPORE) · Hans Golong (National University of Singapore) · Angela Yao (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Practical Measurements of Translucent Materials with Inter-Pixel Translucency Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenyu Chen (Nanjing University) · Jie Guo (Nanjing University) · Shuichang Lai (Nanjing University) · Ruoyu Fu (nanjing university) · mengxun kong (None) · Chen Wang (Nanjing University) · Hongyu Sun (Guangdong Oppo Mobile Telecommunications Corp., Ltd) · Zhebin Zhang (OPPO) · Chen Li (Innopeak Technology Inc.) · Yanwen Guo (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Omni-Q: Omni-Directional Scene Understanding for Unsupervised Visual Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sai Wang (Wuhan University) · Yutian Lin (Wuhan University) · Yu Wu (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhangyang Xiong () · Chenghong Li (The Chinese University of Hong Kong, Shenzhen) · Kenkun Liu (The Chinese University of Hong Kong (Shenzhen）) · Hongjie Liao (Chinese University of Hong Kong, Shenzhen) · Jianqiao HU (The Chinese University of Hong Kong, Shenzhen) · Junyi Zhu (The Chinese University of Hongkong, Shenzhen) · Shuliang Ning (The Chinese University of HongKong, ShenZhen) · Lingteng Qiu (None) · Chongjie Wang (The Chinese University of Hong Kong ，Shenzhen) · Shijie Wang (The Chinese University of Hong Kong, Shenzhen) · Shuguang Cui (The Chinese University of Hong Kong, Shenzhen) · Xiaoguang Han (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ES<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-14-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-69" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-70" class="mjx-mrow"><span id="MJXc-Node-71" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-72" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-73" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">3</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-14">^3</script>: Evolving Self-Supervised Learning of Robust Audio-Visual Speech Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanhang Zhang (Institute of Computing Technology, Chinese Academy of Sciences) · Shuang Yang (Institute of Computing Technology, Chinese Academy of Sciences) · Shiguang Shan (Institute of Computing Technology, Chinese Academy of Sciences) · Xilin Chen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Depth-aware Test-Time Training for Zero-shot Video Object Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weihuang Liu (University of Macau) · Xi Shen (Tencent AI Lab) · Haolun Li (University of Macau) · Xiuli Bi (Chongqing University of Posts and Telecommunications) · Bo Liu (Chongqing University of Posts and Telecommunications) · Chi-Man Pun (University of Macau) · Xiaodong Cun (Tencent AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>KP-RED: Exploiting Semantic Keypoints for Joint 3D Shape Retrieval and Deformation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruida Zhang (Department of Automation, Tsinghua University, Tsinghua University) · Chenyangguang Zhang (Tsinghua University) · Yan Di (Technische Universität München) · Fabian Manhardt (Google) · Xingyu Liu (Tsinghua University, Tsinghua University) · Federico Tombari (Google, TUM) · Xiangyang Ji (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Communication-Efficient Federated Learning with Accelerated Client Gradient</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Geeho Kim (Seoul National University) · Jinkyu Kim (Seoul National University) · Bohyung Han (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Taming Stable Diffusion for Text to 360<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-15-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2218;&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-74" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-75" class="mjx-mrow"><span id="MJXc-Node-76" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-77" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-78" class="mjx-texatom" style=""><span id="MJXc-Node-79" class="mjx-mrow"><span id="MJXc-Node-80" class="mjx-mo"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.128em; padding-bottom: 0.316em;">∘</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mrow class="MJX-TeXAtom-ORD"><mo>∘</mo></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-15">^{\circ}</script> Panorama Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cheng Zhang (None) · Qianyi Wu (Monash University) · Camilo Cruz Gambardella (Monash University) · Xiaoshui Huang (Shanghai AI Laboratory) · Dinh Phung (Monash University) · Wanli Ouyang (University of Sydney) · Jianfei Cai (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Real-time 3D-aware Portrait Video Relighting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziqi Cai (Chinese Academy of Sciences &amp; Beijing Jiaotong University) · Kaiwen Jiang (None) · Shu-Yu Chen (Chinese Academy of Sciences) · Yu-Kun Lai (Cardiff University) · Hongbo Fu (City University of Hong Kong) · Boxin Shi (Peking University) · Lin Gao (University of Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huajian Huang (The Hong Kong University of Science and Technology) · Longwei Li (SUN YAT-SEN UNIVERSITY) · Hui Cheng (SUN YAT-SEN UNIVERSITY) · Sai-Kit Yeung (The Hong Kong University of Science and Technology (HKUST))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Attention Calibration for Disentangled Text-to-Image Personalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanbing Zhang (East China University of Science and Technology) · Mengping Yang (East China University of Science and Technology) · Qin Zhou (East China University of Science and Technology) · Zhe Wang (East China University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Progressive Multi-Frequency Representation for Image Warping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jun Xiao (The Hong Kong Polytechnic University) · Zihang Lyu (The Hong Kong Polytechnic University) · Cong Zhang (Hong Kong Polytechnic University) · Yakun Ju (Nanyang Technological University) · Changjian Shui (Vector Institute) · Kin-man Lam (The Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HandDiff: 3D Hand Pose Estimation with Diffusion on Image-Point Cloud</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            WENCAN CHENG (None) · Hao Tang (ETH Zurich and CMU) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Jong Hwan Ko (Sungkyunkwan University (SKKU))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NeISF: Neural Incident Stokes Field for Geometry and Material Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenhao Li (Osaka University) · Taishi Ono (Sony Semiconductor Solutions Europe) · Takeshi Uemori (Sony Semiconductor Solutions Corporation) · Hajime Mihara (Sony Semiconductor Solutions Corporation) · Alexander Gatto (Sony Semiconductor Solutions Europe) · Hajime Nagahara (Osaka University) · Yusuke Moriuchi (Sony Semiconductor Solutions Corporation)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MaskPLAN: Masked Generative Layout Planning from Partial Input</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hang Zhang (ETHZ - ETH Zurich) · Anton Savov (ETHZ - ETH Zurich) · Benjamin Dillenburger (ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Dual-Augmentor Framework for Domain Generalization in 3D Human Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qucheng Peng (University of Central Florida) · Ce Zheng (University of Central Florida) · Chen Chen ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rapid 3D Model Generation with Intuitive 3D Input</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianrun Chen (Zhejiang University) · Chaotao Ding (Huzhou university) · Shangzhan Zhang (Zhejiang University) · Chunan Yu (Huzhou University) · Ying Zang (Huzhou University) · Zejian Li (Zhejiang University) · Sida Peng (None) · Lingyun Sun (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MCD: Diverse Large-Scale Multi-Campus Dataset for Robot Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Thien-Minh Nguyen (Nanyang Technological University) · Shenghai Yuan (National Technological University) · Thien Nguyen (Nanyang Technological University) · Pengyu Yin (Nanyang Technological University) · Haozhi Cao (Nanyang Technological University) · Lihua Xie (Nanyang Technological University) · Maciej Wozniak (KTH Royal Institute of Technology) · Patric Jensfelt (KTH Royal Institute of Technology, Stockholm, Sweden) · Marko Thiel (Hamburg University of Technology) · Justin Ziegenbein (Hamburg University of Technology) · Noel Blunder (Institute for Technical Logistics - Hamburg University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Open-vocabulary object 6D pose estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaime Corsetti (Fondazione Bruno Kessler &amp; University of Trento) · Davide Boscaini (Fondazione Bruno Kessler) · Changjae Oh (Queen Mary University London) · Andrea Cavallaro (EPFL - EPF Lausanne) · Fabio Poiesi (Fondazione Bruno Kessler)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Splatter Image: Ultra-Fast Single-View 3D Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Stanislaw Szymanowicz (University of Oxford, University of Oxford) · Christian Rupprecht (University of Oxford) · Andrea Vedaldi (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Diff-Plugin: Revitalizing Details for Diffusion-based Low-level Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhao Liu (City University of Hong Kong) · Zhanghan Ke (City University of Hong Kong) · Fang Liu (City University of Hong Kong) · Nanxuan Zhao (Adobe Research) · Rynson W.H. Lau (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tong Wu (None) · Guandao Yang (None) · Zhibing Li (The Chinese University of Hong Kong) · Kai Zhang (Adobe Systems) · Ziwei Liu (Nanyang Technological University) · Leonidas Guibas (Stanford University) · Dahua Lin (The Chinese University of Hong Kong) · Gordon Wetzstein (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-16-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;C&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-81" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-82" class="mjx-mrow"><span id="MJXc-Node-83" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em; padding-right: 0.045em;">C</span></span><span id="MJXc-Node-84" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">r</span></span><span id="MJXc-Node-85" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">o</span></span><span id="MJXc-Node-86" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">w</span></span><span id="MJXc-Node-87" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em; padding-right: 0.003em;">d</span></span><span id="MJXc-Node-88" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em;">D</span></span><span id="MJXc-Node-89" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.316em;">i</span></span><span id="MJXc-Node-90" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.503em; padding-right: 0.06em;">f</span></span><span id="MJXc-Node-91" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.503em; padding-right: 0.06em;">f</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>C</mi><mi>r</mi><mi>o</mi><mi>w</mi><mi>d</mi><mi>D</mi><mi>i</mi><mi>f</mi><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-16">CrowdDiff</script>: Multi-hypothesis Crowd Density Estimation using Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yasiru Ranasinghe (Johns Hopkins University) · Nithin Gopalakrishnan Nair (Johns Hopkins University) · Wele Gedara Chaminda Bandara (Johns Hopkins University) · Vishal M. Patel (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Boosting Diffusion Models with Moving Average Sampling in Frequency Domain</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yurui Qian (University of Science and Technology of China) · Qi Cai (JD) · Yingwei Pan (HiDream.ai) · Yehao Li (HiDream.ai) · Ting Yao (JD AI Research) · Qibin Sun (University of Science and Technology of China) · Tao Mei (JD Explore Academy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Exploring Vision Transformers for 3D Human Motion-Language Models with Motion Patches</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qing Yu (LY Corporation) · Mikihiro Tanaka (LY Corporation) · Kent Fujiwara (LY Corporation)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/tmtuan1307/LANDER" target="_blank">Text-Enhanced Data-free Approach for Federated Class-Incremental Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minh-Tuan Tran (Monash University) · Trung Le (Monash University) · Xuan-May Le (University of Melbourne) · Mehrtash Harandi (Monash University) · Dinh Phung (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>G-NeRF: Geometry-enhanced Novel View Synthesis from Single-View Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixiong Huang (South China University of Technology) · Qi Chen (The University of Adelaide) · Libo Sun (University of Adelaide) · Yifan Yang (South China University of Technology) · Naizhou Wang (CVTE research) · Qi Wu (University of Adelaide) · Mingkui Tan (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised Salient Instance Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Tian (Huawei Technologies Ltd.) · Ke Xu (City University of Hong Kong) · Rynson W.H. Lau (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Paint-it: Text-to-Texture Synthesis via Deep Convolutional Texture Map Optimization and Physically-Based Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kim Youwang (Pohang University of Science and Technology) · Tae-Hyun Oh (None) · Gerard Pons-Moll (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ViewFusion: Towards Multi-View Consistency via Interpolated Denoising</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xianghui Yang (University of Sydney) · Gil Avraham (Amazon) · Yan Zuo (Amazon) · Sameera Ramasinghe (Amazon) · Loris Bazzani (Amazon) · Anton van den Hengel (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>L-MAGIC: Language Model Assisted Generation of Images with Consistency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            zhipeng cai (Intel Labs) · Matthias Mueller (None) · Reiner Birkl (Intel Corporation) · Diana Wofk (Intel) · Shao-Yen Tseng (Intel) · JunDa Cheng (Huazhong University of Science and Technology) · Gabriela Ben Melech Stan (Intel) · Vasudev Lal (None) · Michael Paulitsch (Intel)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/ZYangChen/MoCha-Stereo" target="_blank">MoCha-Stereo: Motif Channel Attention Network for Stereo Matching</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyang Chen (Guizhou University) · Wei Long (Guizhou University) · He Yao (Guizhou University) · Yongjun Zhang (Guizhou University) · Bingshu Wang (Northwest Polytechnical University Xi'an) · Yongbin Qin (Guizhou University) · Jia Wu (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Discovering Syntactic Interaction Clues for Human-Object Interaction Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinguo Luo () · Weihong Ren (Harbin Institute of Technology, Shenzhen) · Weibo Jiang (Harbin Institute of Technology) · Xi'ai Chen (Shenyang Institute of Automation, Chinese Academy of Sciences) · Qiang Wang (Shenyang University) · Zhi Han (Shenyang Institute of Automation, Chinese Academy of Sciences) · Honghai LIU (Harbin Institute of Technology, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GLACE: Global Local Accelerated Coordinate Encoding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fangjinhua Wang (None) · Xudong Jiang (ETHZ - ETH Zurich) · Silvano Galliani (Microsoft) · Christoph Vogel (Microsoft) · Marc Pollefeys (ETH Zurich / Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Active Prompt Learning in Vision Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jihwan Bang (KAIST) · Sumyeong Ahn (Michigan State University) · Jae-Gil Lee (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HouseCat6D - A Large-Scale Multi-Modal Category Level 6D Object Perception Dataset with Household Objects in Realistic Scenarios</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            HyunJun Jung (Technische Universität München) · Shun-Cheng Wu (Technical University Munich) · Patrick Ruhkamp (Technical University Munich) · Guangyao Zhai (Technical University of Munich) · Hannah Schieber (Technische Universität München/Friedrich-Alexander Universität Erlangen-Nürnberg) · Giulia Rizzoli (University of Padua) · Pengyuan Wang (Technische Universität München) · Hongcheng Zhao (Technische Universität München) · Lorenzo Garattoni (Toyota Motor Europe) · Sven Meier (Toyota Motor Europe NV/SA) · Daniel Roth (Technische Universität München) · Nassir Navab (TU Munich) · Benjamin Busam (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FINER: Flexible spectral-bias tuning in Implicit NEural Representation by Variable-periodic Activation Functions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Liu (Nanjing University) · Hao Zhu (Nanjing University) · Qi Zhang (Tencent AI Lab) · Jingde Fu (Nanjing University) · Weibing Deng (nanjing university) · Zhan Ma (Nanjing University) · Yanwen Guo (Nanjing University) · Xun Cao (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CARZero: Cross-Attention Alignment for Radiology Zero-Shot Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoran Lai (University of Science and Technology of China) · Qingsong Yao (University of the Chinese Academy of Sciences) · Zihang Jiang (University of Science and Technology of China) · Rongsheng Wang (University of Science and Technology of China) · Zhiyang He (Xunfei Healthcare Technology Co., Ltd.) · Xiaodong Tao (Xunfei Healthcare Co. Ltd) · S Kevin Zhou (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Customization Assistant for Text-to-image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufan Zhou (State University of New York, Buffalo) · Ruiyi Zhang (Adobe Research) · Jiuxiang Gu (Adobe Systems) · Tong Sun (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://usrc-sea.github.io/UltrAvatar/" target="_blank">UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingyuan Zhou (Innopeak Technology) · Rakib Hyder (Oppo, Seattle, USA) · Ziwei Xuan (Innopeak Technology) · Guo-Jun Qi (University of Central Florida)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-17-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mtext mathvariant=&quot;bold&quot;&gt;LaRE&lt;/mtext&gt;&lt;/mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-92" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-93" class="mjx-mrow"><span id="MJXc-Node-94" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-95" class="mjx-texatom"><span id="MJXc-Node-96" class="mjx-mrow"><span id="MJXc-Node-97" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-B" style="padding-top: 0.378em; padding-bottom: 0.378em;">LaRE</span></span></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.619em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-98" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">2</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mrow class="MJX-TeXAtom-ORD"><mtext mathvariant="bold">LaRE</mtext></mrow><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-17">\textbf{LaRE}^2</script>: Latent Reconstruction Error Based Method for Diffusion-Generated Image Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunpeng Luo (Tencent Youtu Lab) · Junlong Du (Tencent YouTu Lab) · Ke Yan () · Shouhong Ding (Tencent Youtu Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Event-based Structure-from-Orbit</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ethan Elms (University of Adelaide) · Yasir Latif (The University of Adelaide) · Tae Ha Park (Stanford University) · Tat-Jun Chin (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://kaist-viclab.github.io/From_Ground_To_Objects_site/" target="_blank">From-Ground-To-Objects: Coarse-to-Fine Self-supervised Monocular Depth Estimation of Dynamic Objects with Ground Contact Prior</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaeho Moon (KAIST) · Juan Luis Gonzalez Bello (KAIST) · Byeongjun Kwon (KAIST) · Munchurl Kim (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deep-TROJ: An Inference Stage Trojan Insertion Algorithm through Efficient Weight Replacement Attack</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sabbir Ahmed (State University of New York at Binghamton) · RANYANG ZHOU (New Jersey Institute of Technology) · Shaahin Angizi (New Jersey Institute of Technology) · Adnan Rakin Rakin (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dynamic LiDAR Re-simulation using Compositional Neural Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanfeng Wu (None) · Xingxing Zuo (Caltech) · Stefan Leutenegger (Department of Informatics, Technische Universität München) · Or Litany (NVIDIA / Technion) · Konrad Schindler (ETH Zurich) · Shengyu Huang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised Blind Image Deblurring Based on Self-Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lufei Chen (Sichuan University) · Xiangpeng Tian (SiChuan University) · Shuhua Xiong (Sichuan University) · Yinjie Lei (Sichuan University) · Chao Ren (Sichuan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ProMotion: Prototypes As Motion Learners</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yawen Lu (Purdue University) · Dongfang Liu (Rochester Institute of Technology) · Qifan Wang (Meta AI) · Cheng Han (Rochester Institute of Technology) · Yiming Cui (University of Florida) · Zhiwen Cao (Purdue University) · Xueling Zhang (Rochester Institute of Technology) · Yingjie Victor Chen (Purdue University) · Heng Fan (University of North Texas)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Zero-Shot Structure-Preserving Diffusion Model for  High Dynamic Range Tone Mapping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruoxi Zhu (Fudan University) · Shusong Xu (Alibaba Group) · Peiye Liu (Alibaba Group) · Sicheng Li (Alibaba Group) · Yanheng Lu (Alibaba Group) · Dimin Niu (Alibaba Group) · Zihao Liu (Alibaba Group) · Zihao Meng (Alibaba Group) · Li Zhiyong (Alibaba Group) · Xinhua Chen (Fudan University) · Yibo Fan (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Mask Grounding for Referring Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yong Xien Chng (None) · Henry Zheng (Tsinghua University) · Yizeng Han (Tsinghua University, Tsinghua University) · Xuchong QIU (Bosch) · Gao Huang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SOAC: Spatio-Temporal Overlap-Aware Multi-Sensor Calibration using Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Quentin HERAU (Huawei/University of Burgundy) · Nathan Piasco (Huawei Technologies Ltd.) · Moussab Bennehar (Huawei Noah's Ark Lab) · Luis Guiller,o Roldao Jimenez (Huawei Technologies Ltd.) · Dzmitry Tsishkou (Huawei Technologies Ltd.) · MigniotCyrille (University of Burgundy) · Modélisation Information Systèmes (Université de Picardie Jules-Verne) · Cedric Demonceaux (Université de Bourgogne)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SemCity: Semantic Scene Generation with Triplane Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jumin Lee (Korea Advanced Institute of Science and Technology) · Sebin Lee (Korea Advanced Institute of Science and Technology (KAIST)) · Changho Jo (Neosapience) · Woobin Im (Korea Advanced Institute of Science and Technology) · Ju-hyeong Seon (Korea Advanced Institute of Science &amp; Technology) · Sung-Eui Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-18-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/msub&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mo&gt;:&lt;/mo&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-99" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-100" class="mjx-mrow"><span id="MJXc-Node-101" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.186em;"><span id="MJXc-Node-102" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.316em; padding-right: 0.186em;">V</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.219em; padding-right: 0.071em;"><span id="MJXc-Node-103" class="mjx-mi" style=""><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.316em;">k</span></span></span></span><span id="MJXc-Node-104" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em;">D</span></span><span id="MJXc-Node-105" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.128em; padding-bottom: 0.316em;">:</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>V</mi><mi>k</mi></msub><mi>D</mi><mo>:</mo></math></span></span><script type="math/tex" id="MathJax-Element-18">V_kD:</script> Improving knowledge distillation using orthogonal projections</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Roy Miles (Imperial College London) · Ismail Elezi (Huawei Noah's Ark) · Jiankang Deng (Imperial College London &amp; Huawei UKRD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Label-Efficient Group Robustness via Out-of-Distribution Concept Curation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiwei Yang (University of Washington) · Anthony Liu (University of Michigan) · Robert Wolfe (University of Washington) · Aylin Caliskan (University of Washington) · Bill Howe (University of Washington)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>StyLitGAN: Image-based Relighting via Latent Control</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anand Bhattad (None) · James Soole (University of Illinois Urbana-Champaign) · David Forsyth (University of Illinois at Urbana-Champaign)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Anomaly Score: Evaluating Generative Models and Individual Generated Images based on Complexity and Vulnerability</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaehui Hwang (Yonsei University) · Junghyuk Lee (Yonsei University) · Jong-Seok Lee (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weiyao Wang (Facebook) · Pierre Gleize (Polytech Nice Sophia) · Hao Tang (Meta Platforms) · Xingyu Chen (Facebook) · Kevin Liang (FAIR at Meta) · Matt Feiszli (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FreeDrag: Feature Dragging for Reliable Point-based Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pengyang Ling (University of Science and Technology of China) · Lin Chen (University of Science and Technology of China) · Pan Zhang (Shanghai Artificial Intelligence Laboratory) · Huaian Chen (University of Science and Technology of China) · Yi Jin (University of Science and Technology of China) · Jinjin Zheng (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://cvlab.yonsei.ac.kr/projects/IGQ-ViT/" target="_blank">Instance-Aware Group Quantization for Vision Transformers</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaehyeon Moon (Yonsei University) · Dohyung Kim (Yonsei University) · Jun Yong Cheon (Yonsei University) · Bumsub Ham (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NIFTY: Neural Object Interaction Fields for Guided Human Motion Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nilesh Kulkarni (None) · Davis Rempe (NVIDIA) · Kyle Genova (Google) · Abhijit Kundu (Google) · Justin Johnson (University of Michigan) · David Fouhey (New York University) · Leonidas Guibas (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianyi Xie (University of California, Los Angeles) · Zeshun Zong (University of California, Los Angeles) · Yuxing Qiu (UCLA &amp; LightSpeed Studios) · Xuan Li (None) · Yutao Feng (Zhejiang University) · Yin Yang (University of Utah) · Chenfanfu Jiang (University of California, Los Angeles)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Viewpoint-Aware Visual Grounding in 3D Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiangxi Shi (Oregon State University) · Zhonghua Wu (SenseTime) · Stefan Lee (Oregon State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hunting Attributes:  Context Prototype-Aware Learning for Weakly Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feilong Tang (Monash University) · Zhongxing Xu (Weill Cornell Medicine, Cornell University) · Zhaojun QU (Xi'an Jiaotong-Liverpool University) · Wei Feng (Monash University) · xingjian jiang (University of Michigan - Ann Arbor) · Zongyuan Ge (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VTQA: Visual Text Question Answering via Entity Alignment and Cross-Media Reasoning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kang chenkang (Huaihai Institute of Technology) · Xiangqian Wu (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Space-time Diffusion Features for Zero-shot Text-driven Motion Transfer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rafail Fridman (Weizmann Institute of Science) · Danah Yatim (Weizmann Institute of Science) · Omer Bar-Tal (Weizmann Institute of Science) · Yoni Kasten (NVIDIA Research) · Tali Dekel (Weizmann Institute of Science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PTM-VQA: Efficient Video Quality Assessment Leveraging Diverse PreTrained Models from the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kun Yuan (Kuaishou Technology) · Hongbo Liu (Tsinghua University) · Mading Li (Kuaishou Technology) · Muyi Sun (Institute of automation,  Chinese Academy of Sciences) · Ming Sun (Kuaishou Tech) · Jiachao Gong (Beijing Kuaishou ) · Jinhua Hao (Kuaishou Tech) · Chao Zhou (Peking University) · Yansong Tang (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Batch Normalization Alleviates the Spectral Bias in Coordinate Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhicheng Cai (Nanjing University) · Hao Zhu (Nanjing University) · Qiu Shen (Nanjing University) · Xinran Wang (Nanjing University) · Xun Cao (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GaussianAvatars: Photorealistic Head Avatars with Rigged 3D Gaussians</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shenhan Qian (Technische Universität München) · Tobias Kirschstein (Department of Informatics, Technische Universität München) · Liam Schoneveld (Woven by Toyota) · Davide Davoli (Toyota Motor Europe NV/SA associated partner by contracted services) · Simon Giebenhain (Technische Universität München) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CodedEvents: Optimal Point-Spread-Function Engineering for 3D-Tracking with Event Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sachin Shah (University of Maryland, College Park) · Matthew Chan (Department of Computer Science, University of Maryland, College Park) · Haoming Cai (University of Maryland, College Park) · Jingxi Chen (University of Maryland College Park) · Sakshum Kulshrestha (University of Maryland, College Park) · Chahat Deep Singh (University of Maryland, College Park) · Yiannis Aloimonos (University of Maryland, College Park) · Christopher Metzler (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://animatable-gaussians.github.io/" target="_blank">Animatable Gaussians: Learning Pose-dependent Gaussian Maps for High-fidelity Human Avatar Modeling</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhe Li (Tsinghua University) · Zerong Zheng (Tsinghua University) · Lizhen Wang (Tsinghua University, Tsinghua University) · Yebin Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SceneTex: High-Quality Texture Synthesis for Indoor Scenes via Diffusion Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dave Chen (Technische Universität München) · Haoxuan Li (Technische Universität München) · Hsin-Ying Lee (Snap Inc.) · Sergey Tulyakov (Snap Inc.) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OMG-Seg: Is One Model Good Enough For All Segmentation?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiangtai Li (Nanyang Technological University) · Haobo Yuan (Nanyang Technological University) · Wei Li (Nanyang Technological University) · Henghui Ding (Fudan University) · Size Wu (Nanyang Technological University) · Wenwei Zhang (None) · Yining Li (Shanghai AI Laboratory) · Kai Chen (Shanghai AI Laboratory) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Pose Adapted Shape Learning for Large-Pose Face Reenactment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gee-Sern Hsu (None) · Jie-Ying Zhang (National Taiwan University of Science and Technology) · Yu-Hsiang Huang (National Taiwan University of Science and Technology) · Wei-Jie Hong (National Taiwan University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Make Me a BNN: A Simple Strategy for Estimating Bayesian Uncertainty from Pre-trained Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gianni Franchi (ENSTA Paris) · Olivier Laurent (Université Paris-Saclay) · Maxence Leguéry (ENSTA Paris) · Andrei Bursuc (valeo.ai) · Andrea Pilzer (NVIDIA) · Angela Yao (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CADTalk: An Algorithm and Benchmark for Semantic Commenting of CAD Programs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haocheng Yuan (University of Edinburgh) · Jing Xu (University of Edinburgh, University of Edinburgh) · Hao Pan (Microsoft Research) · Adrien Bousseau (INRIA) · Niloy J. Mitra (University College London) · Changjian Li (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://aimagelab.github.io/freeda/" target="_blank">Training-Free Open-Vocabulary Segmentation with Offline Diffusion-Augmented Prototype Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Luca Barsellotti (University of Modena and Reggio Emilia) · Roberto Amoroso (University of Modena and Reggio Emilia) · Marcella Cornia (University of Modena and Reggio Emilia) · Lorenzo Baraldi (Università degli Studi di Modena e Reggio Emilia) · Rita Cucchiara (Università di Modena e Reggio Emilia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EMAGE: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haiyang Liu (the university of tokyo) · Zihao Zhu (Keio University) · Giorgio Becherini (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · YICHEN PENG (Japan Advanced Institute of Science and Technology, Tokyo Institute of Technology) · Mingyang Su (Tsinghua University, Tsinghua University) · YOU ZHOU (Huawei Technologies Ltd.) · Xuefei Zhe (City University of Hong Kong) · Naoya Iwamoto (Huawei Technologies Japan K.K.) · Bo Zheng (Huawei Technologies Japan) · Michael J. Black (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Explaining CLIP's performance disparities on data from blind/low vision users</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daniela Massiceti (Microsoft Research) · Camilla Longden (Microsoft Research, Cambridge) · Agnieszka Słowik (Microsoft) · Samuel Wills (World Bank) · Martin Grayson (Research, Microsoft) · Cecily Morrison (Microsoft Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MMCert: Provable Defense against Adversarial Attacks to Multi-modal Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanting Wang (Pennsylvania State University) · Hongye Fu (Zhejiang University) · Wei Zou (Pennsylvania State University) · Jinyuan Jia (Pennsylvania State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NB-GTR: Narrow-Band Guided Turbulence Removal</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifei Xia (Peking University) · Chu Zhou (Peking University) · Chengxuan Zhu (Peking University) · Minggui Teng (Peking University) · Chao Xu (Peking University) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mudslide: A Universal Nuclear Instance Segmentation Method</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jun Wang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LaneCPP: Continuous 3D Lane Detection using Physical Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Maximilian Pittner (Bosch) · Joel Janai (Robert Bosch GmbH, Bosch) · Alexandru Paul Condurache (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Large Language Models are Good Prompt Learners for Low-Shot Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaoheng Zheng (University of Southern California) · Jingmin Wei (University of Southern California) · Xuefeng Hu (University of Southern California) · Haidong Zhu (University of Southern California) · Ram Nevatia (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufei Zhang (None) · Jeffrey Kephart (IBM, International Business Machines) · Zijun Cui (University of Southern California) · Qiang Ji (Rensselaer Polytechnic Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LiveHPS: LiDAR-based Scene-level Human Pose and Shape Estimation in Free Environment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            yiming ren (None) · xiao han (ShanghaiTech University) · Chengfeng Zhao (ShanghaiTech University) · Jingya Wang (ShanghaiTech University) · Lan Xu (ShanghaiTech University) · Jingyi Yu (Shanghai Tech University) · Yuexin Ma (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>G3DR: Generative 3D Reconstruction in ImageNet</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pradyumna Reddy () · Ismail Elezi (Huawei Noah's Ark) · Jiankang Deng (Imperial College London &amp; Huawei UKRD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yushi Hu (University of Washington) · Otilia Stretcu (Google Research) · Chun-Ta Lu (Google Research) · Krishnamurthy Viswanathan (Google) · Kenji Hata (Google) · Enming Luo (Google) · Ranjay Krishna (University of Washington) · Ariel Fuxman (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Evidential Active Recognition: Intelligent and Prudent Open-World Embodied Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lei Fan (Northwestern University) · Mingfu Liang (Northwestern University) · Yunxuan Li (Northwestern University) · Gang Hua (Wormpex AI Research) · Ying Wu (Northwestern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Modeling Collaborator: Enabling Subjective Vision Classification With Minimal Human Effort via LLM Tool-Use</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Imad Eddine Toubal (University of Missouri) · Aditya Avinash (Google) · Neil Alldrin (Google) · Jan Dlabal (Research, Google) · Wenlei Zhou (Google) · Enming Luo (Google) · Otilia Stretcu (Google Research) · Hao Xiong (Google) · Chun-Ta Lu (Google Research) · Howard Zhou (Google Research) · Ranjay Krishna (University of Washington) · Ariel Fuxman (Google) · Tom Duerig (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OneTracker: Unifying Visual Object Tracking with Foundation Models and Efficient Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lingyi Hong (Fudan University) · Shilin Yan (Fudan University) · Renrui Zhang (MMLab of CUHK &amp;amp;amp; Shanghai AI Laboratory) · Wanyun Li (Fudan University) · Xinyu Zhou (None) · Pinxue Guo (Fudan University) · Kaixun Jiang (Fudan University) · Yiting Cheng (None) · Jinglun Li (None) · Zhaoyu Chen (Fudan University) · Wenqiang Zhang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FineSports: A Multi-person Hierarchical Sports Video Dataset for Fine-grained Action Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinglin Xu (University of Science and Technology Beijing) · Guohao Zhao (Peking University) · Sibo Yin (Peking University) · Wenhao Zhou (University of Science and Technology Beijing) · Yuxin Peng (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>An Aggregation-Free Federated Learning for Tackling Data Heterogeneity</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Wang (Institute of High Performance Computing, Singapore, A*STAR) · Huazhu Fu (Institute of High Performance Computing, Singapore, A*STAR) · Renuga Kanagavelu (Institute of High Performance Computing, Singapore, A*STAR) · Qingsong Wei (Agency for Science, Technology and Research (A*STAR)) · Yong Liu (Institute of High Performance Computing, Singapore, A*STAR) · Rick Goh (Institute of High Performance Computing, Singapore, A*STAR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Infrared Adversarial Car Stickers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaopei Zhu (Tsinghua University) · Yuqiu Liu (Beijing Forestry University) · Zhanhao Hu (UC Berkeley) · Jianmin Li (Department of computer science and technology, Tsinghua University) · Xiaolin Hu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MULTIFLOW: Shifting Towards Task-Agnostic Vision-Language Pruning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Matteo Farina (University of Trento) · Massimiliano Mancini (University of Trento) · Elia Cunegatti (University of Trento) · Gaowen Liu (None) · Giovanni Iacca (University of Trento) · Elisa Ricci (University of Trento)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CA-Jaccard: Camera-aware Jaccard Distance for Person Re-identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiyu Chen (Beijing Institute of Technology) · Zheyi Fan (Beijing Institute of Technology) · Zhaoru Chen (Beijing Institute of Technology) · Yixuan Zhu (Beijing Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Harnessing Meta-Learning for Improving Full-Frame Video Stabilization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Muhammad Kashif Ali (Hanyang University) · Eun Woo Im (Hanyang University) · Dongjin Kim (Hanyang University) · Tae Hyun Kim (Hanyang Univ.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CURSOR: Scalable Mixed-Order Hypergraph Matching with CUR Decomposition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qixuan Zheng (City University of Hong Kong) · Ming Zhang (Hong Kong Applied Science and Technology Research Institute (ASTRI)) · Hong Yan (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SI-MIL: Taming Deep MIL for Self-Interpretability in Gigapixel Histopathology</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Saarthak Kapse (State University of New York at Stony Brook) · Pushpak Pati (International Business Machines) · Srijan Das (University of North Carolina at Charlotte) · Jingwei Zhang (None) · Chao Chen (State University of New York, Stony Brook) · Maria Vakalopoulou (CentraleSupelec) · Joel Saltz (State University of New York at Stony Brook) · Dimitris Samaras (Stony Brook University) · Rajarsi Gupta (Academic medical center at State University of New York at Stony Brook) · Prateek Prasanna (State University of New York, Stony Brook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Seg2Reg: Differentiable 2D Segmentation to 1D Regression Rendering for 360 Room Layout Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cheng Sun (NVIDIA) · Wei-En Tai (National Tsinghua University) · Yu-Lin Shih (National Tsinghua University) · Kuan-Wei Chen (National Tsinghua University) · Yong-Jing Syu (National Tsinghua University) · Kent Selwyn The (National Tsinghua University) · Yu-Chiang Frank Wang (NVIDIA) · Hwann-Tzong Chen (National Tsing Hua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Boosting Adversarial Transferability by Block Shuffle and Rotation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kunyu Wang (The Chinese University of Hong Kong) · he xuanran (TikTok) · Wenxuan Wang (The Chinese University of Hong Kong) · Xiaosen Wang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Advancing Saliency Ranking with Human Fixations: Dataset, Models and Benchmarks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bowen Deng (Computer Vision Laboratory      University of Nottingham) · Siyang Song (University of Leicester) · Andrew French (University of Nottingham) · Denis Schluppeck (University of Nottingham) · Michael Pound (University of Nottingham)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GaussianEditor: Swift and Controllable 3D Editing with Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiwen Chen (Nanyang Technological University) · Zilong Chen (Tsinghua University) · Chi Zhang (Tencent ) · Feng Wang (Tsinghua University, Tsinghua University) · Xiaofeng Yang (Nanyang Technological University) · Yikai Wang (Tsinghua University) · Zhongang Cai (Nanyang Technological University) · Lei Yang (The Chinese University of Hong Kong) · Huaping Liu (Tsinghua University, Tsinghua University) · Guosheng Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SeaBird: Segmentation in Bird’s View with Dice Loss Improves Monocular 3D Detection of Large Objects</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Abhinav Kumar (Michigan State University) · Yuliang Guo (Bosch US Research) · Xinyu Huang (Robert Bosch Research NA) · Liu Ren (Bosch Research) · Xiaoming Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GALA: Generating Animatable Layered Assets from a Single Scan</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taeksoo Kim (Seoul National University) · Byungjun Kim (Seoul National University) · Shunsuke Saito (Reality Labs Research) · Hanbyul Joo (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Single Mesh Diffusion Models with Field Latents for Texture Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Thomas W. Mitchel (PlayStation) · Carlos Esteves (Google Research) · Ameesh Makadia (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unveiling the Power of Audio-Visual Early Fusion Transformers with Dense Interactions through Masked Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shentong Mo (CMU, Carnegie Mellon University) · Pedro Morgado (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hearing Anything Anywhere</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mason Wang (Stanford University) · Ryosuke Sawata (Sony Research) · Samuel Clarke (Stanford University) · Ruohan Gao (Stanford University) · Shangzhe Wu (Stanford University) · Jiajun Wu (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Move Anything with Layered Scene Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Ren (Nanyang Technological University) · Mengmeng Xu (Meta AI) · Jui-Chieh Wu (Meta) · Ziwei Liu (Nanyang Technological University) · Tao Xiang (University of Surrey) · Antoine Toisoul (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Diffusion Texture Priors for Image Restoration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tian Ye (Hong Kong University of Science and Technology, Guangzhou Campus) · Sixiang Chen (Hong Kong University of Science and Technology (GZ)) · Wenhao Chai (University of Washington) · Zhaohu Xing (Hong Kong University of Science and Technology) · Jing Qin (Hong Kong Polytechnic University) · Ge lin (Hong Kong University of Science and Technology (Guangzhou)) · Lei Zhu (Hong Kong University of Science and Technology (Guangzhou) &amp; HKUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DriveTrack: A Benchmark for Long-Range Point Tracking in Real-World Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Arjun Balasingam (Massachusetts Institute of Technology) · Joseph Chandler (Massachusetts Institute of Technology) · Chenning Li (None) · Zhoutong Zhang (Adobe Systems) · Hari Balakrishnan (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Benchmarking the Robustness of Temporal Action Detection Models Against Temporal Corruptions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Runhao Zeng (Shenzhen MSU-BIT University) · Xiaoyong Chen (Shenzhen University) · Jiaming Liang (Shenzhen University) · Huisi Wu (Shenzhen University) · Guang-Zhong Cao (Shenzhen University) · Yong Guo (Max-Planck Institute for Informatics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://delinqu.github.io/EN-SLAM" target="_blank">Implicit Event-RGBD Neural SLAM</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Delin Qu (Fudan University) · Chi Yan (Shanghai AI Laboratory) · Dong Wang (Shanghai AI Laboratory) · Jie Yin (Shanghai Jiaotong University) · Qizhi Chen (None) · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology) · Yiting Zhang (Zhejiang University) · Bin Zhao (Northwest Polytechnical University Xi'an) · Xuelong Li (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DreamControl: Control-Based Text-to-3D Generation with 3D Self-Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianyu Huang (Harbin Institute of Technology &amp; City University of Hong Kong) · Yihan Zeng (Huawei Technologies Ltd.) · Zhilu Zhang (Harbin Institute of Technology) · Wan Xu (Harbin Institute of Technology) · Hang Xu (Huawei Noah‘s Ark Lab) · Songcen Xu (Huawei Noah's Ark Lab) · Rynson W.H. Lau (City University of Hong Kong) · Wangmeng Zuo (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Scene-adaptive and Region-aware Multi-modal Prompt for Open Vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaowei Zhao () · Xianglong Liu (BUAA) · Duorui Wang (Beijing University of Aeronautics and Astronautics) · Yajun Gao (Beihang University) · Zhide Liu (Beijing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RepKPU: Point Cloud Upsampling with Kernel Point Representation and Deformation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yi Rong (Nanjing University) · Haoran Zhou (Nanjing University) · Kang Xia (nanjing university) · Cheng Mei (nanjing university) · Jiahao Wang () · Tong Lu (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://jiuntian.github.io/interactdiffusion/" target="_blank">InteractDiffusion: Interaction Control in Text-to-Image Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiun Tian Hoe (Nanyang Technological University) · Xudong Jiang (Nanyang Technological University) · Chee Seng Chan (Universiti Malaya) · Yap-peng Tan (Nanyang Technological University) · Weipeng Hu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/ispc-lab/MAP" target="_blank">MAP: MAsk-Pruning for Source-Free Model Intellectual Property Protection</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boyang Peng (Tongji University) · Sanqing Qu (Tongji University) · Yong Wu (Tongji University) · Tianpei Zou (Tongji University) · Lianghua He (Tongji University) · Alois Knoll (Technical University Munich) · Guang Chen (Tongji University) · Changjun Jiang (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Exploring Efficient Asymmetric Blind-Spots for Self-Supervised Denoising in Real-World Scenarios</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiyan Chen (Peking University) · Jiyuan Zhang (Peking University) · Zhaofei Yu (Peking University) · Tiejun Huang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic Treatment based on Anthropic Prior Knowledge</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bo Zou (Computer Science, Tsinghua University, Tsinghua University) · Shaofeng Wang (Capital Medical Universty) · Hao Liu (, Tsinghua University) · Gaoyue Sun (Imperial College London) · Yajie Wang (Tsinghua University, Tsinghua University) · Zuo FeiFei (LargeV .Inc) · Chengbin Quan (Tsinghua University, Tsinghua University) · Youjian Zhao (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Exploiting Inter-sample and Inter-feature Relations in Dataset Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenxiao Deng (Nanjing University) · Wenbin Li (Nanjing University) · Tianyu Ding (Microsoft) · Lei Wang (University of Wollonong) · Hongguang Zhang (Systems Engineering Institute, AMS) · Kuihua Huang (National University of Defense Technology) · Jing Huo (Nanjing University) · Yang Gao (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Level Neural Scene Graphs for Dynamic Urban Environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tobias Fischer (ETH Zurich) · Lorenzo Porzi (Facebook) · Samuel Rota Bulò (Meta) · Marc Pollefeys (ETH Zurich / Microsoft) · Peter Kontschieder (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://jeremycjm.github.io/proj/DiffSHEG/" target="_blank">DiffSHEG: A Diffusion-Based Approach for Real-Time Speech-driven Holistic 3D Expression and Gesture Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junming Chen (Hong Kong University of Science and Technology) · Yunfei Liu (International Digital Economy Academy (IDEA)) · Jianan Wang (None) · Ailing Zeng (IDEA) · Yu Li (International Digital Economy Academy) · Qifeng Chen (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>One-step Diffusion with Distribution Matching Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianwei Yin (Massachusetts Institute of Technology) · Michaël Gharbi (Massachusetts Institute of Technology) · Richard Zhang (Adobe Systems) · Eli Shechtman (Adobe) · Fredo Durand (Massachusetts Institute of Technology) · William Freeman (MIT and Google) · Taesung Park (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Differentiable Display Photometric Stereo</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seokjun Choi (Pohang University of Science and Technology) · Seungwoo Yoon (POSTECH) · Giljoo Nam (Meta) · Seungyong Lee (POSTECH) · Seung-Hwan Baek (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://smhongok.github.io/inv-dpm.html" target="_blank">On Exact Inversion of DPM-Solvers</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seongmin Hong (Seoul National University) · Kyeonghyun Lee (Seoul National University) · Suh Yoon Jeon (Seoul National University) · Hyewon Bae (Seoul National University) · Se Young Chun (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Re-thinking Data Availability Attacks Against Deep Neural Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Fang (Shanghai Jiao Tong University) · Bo Li (vivo Mobile Communication Co.,Ltd.) · Shuang Wu (Tencent YouTu Lab) · Shouhong Ding (Tencent Youtu Lab) · Ran Yi (Shanghai Jiao Tong University) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/TencentARC/PhotoMaker" target="_blank">PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Li (Nankai University &amp; Tencent) · Mingdeng Cao (The University of Tokyo) · Xintao Wang (Tencent) · Zhongang Qi (Tencent PCG ARC Lab) · Ming-Ming Cheng (Nankai University, Tsinghua University) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Privacy-Preserving Face Recognition Using Trainable Feature Subtraction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxi Mi (Fudan University) · Zhizhou Zhong (Fudan University) · Yuge Huang (Tencent Youtu Lab) · Jiazhen Ji (Tencent Youtu Lab) · Jianqing Xu (HIT) · Jun Wang (None) · ShaoMing Wang (WeChat Pay Lab33) · Shouhong Ding (Tencent Youtu Lab) · Shuigeng Zhou (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GROUNDHOG: Grounding Large Language Models to Holistic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichi Zhang (University of Michigan) · Ziqiao Ma (University of Michigan) · Xiaofeng Gao (Amazon AGI) · Suhaila Shakiah (Amazon) · Qiaozi Gao (Amazon) · Joyce Chai (University of Michigan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DAVE -- A Detect-and-Verify Paradigm for Low-Shot Counting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jer Pelhan (Universtiy of Ljubljana) · Alan Lukezic (University of Ljubljana) · Vitjan Zavrtanik (University of Ljubljana) · Matej Kristan (University of Ljubljana)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>D3T: Distinctive Dual-Domain Teacher Zigzagging Across RGB-Thermal Gap for Domain-Adaptive Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dinh Phat Do (Ajou University) · Taehoon Kim (Ajou University) · JAEMIN NA (Tech. Innovation Group, KT) · Jiwon Kim (Robotics Lab, Hyundai Motor Company) · Keonho LEE (Hyundai Motor Company) · Kyunghwan Cho (Hyundai Motor Company) · Wonjun Hwang (Ajou University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MAGICK: A Large-scale Captioned Dataset from Matting Generated Images using Chroma Keying</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ryan Burgert (Stony Brook University) · Brian Price (Adobe Research) · Jason Kuen (Adobe Research) · Yijun Li (Adobe Research) · Michael Ryoo (Stony Brook University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CONFORM: Contrast is All You Need for High-Fidelity Text-to-Image Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tuna Han Salih Meral (Virginia Tech) · Enis Simsar (ETH Zurich) · Federico Tombari (Google, TUM) · Pinar Yanardag (Virginia Polytechnic Institute and State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/dvlab-research/Prompt-Highlighter" target="_blank">Prompt Highlighter: Interactive Control for Multi-Modal LLMs</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuechen Zhang (The Chinese University of Hong Kong) · Shengju Qian (The Chinese University of Hong Kong) · Bohao Peng (The Chinese University of Hong Kong) · Shu Liu (The Chinese University of Hong Kong) · Jiaya Jia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Real-World Efficient Blind Motion Deblurring via Blur Pixel Discretization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Insoo Kim (Korea Advanced Institute of Science and Technology) · Jae Seok Choi (Samsung Advanced Institute of Technology (SAIT)) · Geonseok Seo (Samsung) · Kinam Kwon (Samsung) · Jinwoo Shin (Korea Advanced Institute of Science and Technology) · Hyong-Euk Lee (Samsung Advanced Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Self-Supervised Multi-Object Tracking with Path Consistency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zijia Lu (Northeastern University) · Bing Shuai (Amazon Web Service) · Yanbei Chen (Amazon) · Zhenlin Xu (Amazon) · Davide Modolo (Amazon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lin Li (King's College London) · Haoyan Guan (King's College London, University of London) · Jianing Qiu (Imperial College London) · Michael Spratling (King's College London and University of Luxembourg)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Arbitrary Motion Style Transfer with Multi-condition Motion Latent Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenfeng Song (Beijing Information Science and Technology University) · Xingliang Jin (Beijing information science and information university ) · Shuai Li (Beijing University of Aeronautics and Astronautics) · Chenglizhao Chen (China University of Petroleum) · Aimin Hao (None) · Xia HOU (Beijing Information Science &amp; Technology University) · Ning Li (Beijing Information Science and Technology University) · Hong Qin (Stony Brook University (State University of New York at Stony Brook))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3DSFLabelling: Boosting 3D Scene Flow Estimation by Pseudo Auto-labelling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaokang Jiang () · Guangming Wang (University of Cambridge) · Jiuming Liu (Shanghai Jiao Tong University) · Hesheng Wang (Shanghai Jiao Tong University) · Zhuang Ma (PhiGent) · Zhenqiang Liu (None) · LIANG (None) · Yi Shan (PhiGent Robotics) · Dalong Du (PhiGent Robotics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SPECAT: SPatial-spEctral Cumulative-Attention Transformer for High-Resolution Hyperspectral Image Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyang Yao (Department of Electronic Engineering, Tsinghua University) · Shuyang Liu (Tsinghua university) · Xiaoyun Yuan (Tsinghua University) · Lu Fang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Video-Based Human Pose Regression via Decoupled Space-Time Aggregation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jijie He (Zhejiang Gongshang University) · Wenwu Yang (Zhejiang Gongshang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Scaling Up Video Summarization Pretraining with Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dawit Argaw Argaw (None) · Seunghyun Yoon (Adobe Research) · Fabian Caba Heilbron (Adobe Research) · Hanieh Deilamsalehy (None) · Trung Bui (Adobe Research) · Zhaowen Wang (Adobe Research) · Franck Dernoncourt (Adobe Systems) · Joon Chung (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Refinement for Absolute Pose Regression with Feature Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuai Chen (University of Oxford) · Yash Bhalgat (Visual Geometry Group, University of Oxford) · Xinghui Li (University of Oxford) · Jia-Wang Bian (University of Oxford) · Kejie Li (University of Oxford) · Zirui Wang (University of Oxford) · Victor Adrian Prisacariu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Single-View Scene Point Cloud Human Grasp Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan-Kang Wang (SUN YAT-SEN UNIVERSITY)) · Chengyi Xing (Stanford University) · Yi-Lin Wei (SUN YAT-SEN UNIVERSITY) · Xiao-Ming Wu (SUN YAT-SEN UNIVERSITY) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/yzbouc/UVEB" target="_blank">UVEB: A Large-scale Benchmark and Baseline Towards Real-World Underwater Video Enhancement</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            yaofeng xie (Ocean University of China) · Lingwei Kong (Sanya Oceanographic Institution, Ocean University of China) · Kai Chen (Sanya Oceanographic Institution, Ocean University of China) · Zheng Ziqiang (Hong Kong University of Science and Technology) · Xiao Yu (Sanya Oceanographic Institution, Ocean University of China) · Zhibin Yu (Sanya Oceanographic Institution, Ocean university of China) · Bing Zheng (Sanya Oceanographic Institution, Ocean University of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GenFlow: Generalizable Recurrent Flow for 6D Pose Refinement of Novel Objects</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sungphill Moon (Naver Labs) · Hyeontae Son (Naver Labs) · Dongcheol Hur (NAVER LABS) · Sangwook Kim (Naver Labs)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyun Zheng (Peking University Shenzhen Graduate School) · Liwei Liao (Peking University) · Xufeng Li (Cityu) · Jianbo Jiao (University of Birmingham) · Rongjie Wang (PengCheng Laboratory) · Feng Gao (Peking University) · Shiqi Wang (City University of Hong Kong) · Ronggang Wang (Peking University Shenzhen Graduate School)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Depth Completion via Depth Feature Upsampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufei Wang (Northwest Polytechnical University Xi&amp;amp;#x27;an) · Ge Zhang (Northwest Polytechnical University Xi'an) · Shaoqian Wang (Northwest Polytechnical University Xi'an) · Bo Li (None) · Qi Liu (Northwest Polytechnical University Xi'an) · Le Hui (Nanjing University Of Science And Technology) · Yuchao Dai (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under Manhattan World Assumption</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nobuhiko Wakai (Panasonic Holdings Corporation) · Satoshi Sato (Panasonic Holdings Corporation) · Yasunori Ishii (Panasonic Holdings Corporation) · Takayoshi Yamashita (Chubu University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UniPAD: A Universal Pre-training Paradigm for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Honghui Yang (Zhejiang University) · Sha Zhang (None) · Di Huang (University of Sydney) · Xiaoyang Wu (The University of Hong Kong) · Haoyi Zhu (University of Science and Technology of China) · Tong He (Shanghai AI Lab) · SHIXIANG TANG (The Chinese University of Hong Kong) · Hengshuang Zhao (The University of Hong Kong) · Qibo Qiu (Zhejiang Lab) · Binbin Lin (Zhejiang University) · Xiaofei He (Zhejiang University) · Wanli Ouyang (University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Maitreya Patel (Arizona State University) · Changhoon Kim (Arizona State University) · Sheng Cheng (Arizona State University) · Chitta Baral (Arizona State University) · 'YZ' Yezhou Yang (Arizona State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Real Acoustic Fields: An Audio-Visual Room Acoustics Dataset and Benchmark</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyang Chen (University of Michigan) · Israel D. Gebru (Facebook) · Christian Richardt (Meta Reality Labs) · Anurag Kumar (Facebook) · William Laney (Meta) · Andrew Owens (University of Michigan) · Alexander Richard (Reality Labs Research, Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/chuangchuangtan/NPR-DeepfakeDetection" target="_blank">Rethinking the Up-Sampling Operations in CNN-based Generative Network for Generalizable Deepfake Detection</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chuangchuang Tan (Beijing Jiaotong University) · Huan Liu (Beijing Jiaotong University) · Yao Zhao (Beijing Jiaotong University) · Shikui Wei (Beijing jiaotong university) · Guanghua Gu (Yan Shan University) · Ping Liu (Institute of High Performance Computing, Singapore, A*STAR) · Yunchao Wei (Beijing Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Nearest Is Not Dearest: Towards Practical Defense against Quantization-conditioned Backdoor Attacks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boheng Li (Wuhan University) · Yishuo Cai (Central South University) · Haowei Li (Wuhan University) · Feng Xue (ZJU-Hangzhou Global Scientific and Technological Innovation Center) · Zhifeng Li (Tencent) · Yiming Li (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Building a Strong Pre-Training Baseline for Universal 3D Large-Scale Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoming Chen (East China Normal Univeristy) · Zhizhong Zhang (East China Normal University) · Yanyun Qu (Xiamen University) · Ruixin Zhang (Tencent Youtu Lab) · Xin Tan (East China Normal University) · Yuan Xie (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Loose Inertial Poser: Motion Capture with IMU-attached Loose-Wear Jacket</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengxu Zuo (Xiamen University) · Yiming Wang (Xiamen University) · Lishuang Zhan (Xiamen University) · Shihui Guo (Xiamen University) · Xinyu Yi (Tsinghua University) · Feng Xu (Tsinghua University, Tsinghua University) · Yipeng Qin (Cardiff University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Exposure Fusion for High-Dynamic Range Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Emmanuel Onzon (Torc Robotics) · Maximilian Bömer (Torc Robotics) · Fahim Mannan () · Felix Heide (Department of Computer Science, Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://dpt-t2i.github.io/" target="_blank">Discriminative Probing and Tuning for Text-to-Image Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leigang Qu (National University of Singapore) · Wenjie Wang (National University of Singapore) · Yongqi Li (Hong Kong Polytechnic University) · Hanwang Zhang (Nanyang Technological University) · Liqiang Nie (Harbin Institute of Technology (Shenzhen)) · Tat-seng Chua (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TEA: Test-time Energy Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yige Yuan (None) · Bingbing Xu (Institute of Computing Technology, Chinese Academy of Sciences) · Liang Hou (Kuaishou Technology) · Fei Sun (Institute of Computing Technology, Chinese Academy of Sciences) · Huawei Shen (Institute of Computing Technology, Chinese Academy of Sciences) · Xueqi Cheng (, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Model Adaptation for Time Constrained Embodied Control</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaehyun Song (Sungkyunkwan University) · Minjong Yoo (Sungkyunkwan University) · Honguk Woo (Sungkyunkwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GigaTraj: Predicting Long-term Trajectories of Hundreds of Pedestrians in Gigapixel Complex Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haozhe Lin (None) · Chunyu Wei (Tsinghua University, Tsinghua University) · Li He (None) · Yuchen Guo (Tsinghua University, Tsinghua University) · Yuchy Zhao (Tsinghua University, Tsinghua University) · Shanglong Li (Tsinghua University) · Lu Fang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Outdoor Scene Extrapolation with Hierarchical Generative Cellular Automata</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongsu Zhang (Seoul National University) · Francis Williams (NVIDIA) · Žan Gojčič (NVIDIA) · Karsten Kreis (NVIDIA) · Sanja Fidler (Department of Computer Science, University of Toronto) · Young Min Kim (Seoul National University) · Amlan Kar (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Comparing the Decision-Making Mechanisms by Transformers and CNNs via Explanation Methods</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingqi Jiang (Oregon State University) · Saeed Khorram (Apple) · Li Fuxin (Oregon State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CPLIP: Zero-Shot Learning for Histopathology with Comprehensive Vision-Language Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sajid Javed (Khalifa University of Science and Technology) · Arif Mahmood (Information Technology University, Lahore) · IYYAKUTTI IYAPPAN GANAPATHI (Khalifa University of Science, Technology and Research) · Fayaz Ali (Khalifa University of Science, Technology and Research) · Naoufel Werghi (Khalifa University) · Mohammed Bennamoun (University of Western Australia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TTA-EVF: Test-Time Adaptation for Event-based Video Frame Interpolation via Reliable Pixel and Sample Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hoonhee Cho (KAIST) · Taewoo Kim (KAIST) · Yuhwan Jeong (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>One-Prompt to Segment All Medical Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wu (None) · Min Xu (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Quantifying Task Priority for Multi-Task Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wooseong Jeong (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HiFi4G: High-Fidelity Human Performance Rendering via Compact Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuheng Jiang (ShanghaiTech University) · Zhehao Shen (ShanghaiTech University) · Penghao Wang (None) · Zhuo Su (ByteDance) · Yu Hong (ShanghaiTech University) · Yingliang Zhang (DGene Inc.) · Jingyi Yu (ShanghaiTech University) · Lan Xu (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Image Sculpting: Precise Object Editing with 3D Geometry Control</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiraphon Yenphraphai (New York University) · Xichen Pan (New York University) · Sainan Liu (Intel) · Daniele Panozzo (New York University) · Saining Xie (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UniGarmentManip: A Unified Framework for Category-Level Garment Manipulation via Dense Visual Correspondence</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruihai Wu (Peking University) · Haoran Lu (Peking University) · Yiyan Wang (Beijing Institute of Technology) · Yubo Wang (Peking University) · Hao Dong (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IPoD: Implicit Field Learning with Point Diffusion for Generalizable 3D Object Reconstruction from Single RGB-D Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yushuang Wu (The Chinese University of Hong Kong (Shenzhen)) · Luyue Shi (The Chinese University of Hong Kong, Shenzhen) · Junhao Cai (Hong Kong University of Science and Technology) · Weihao Yuan (Alibaba Group) · Lingteng Qiu (None) · Zilong Dong (Alibaba Group) · Liefeng Bo (None) · Shuguang Cui (The Chinese University of Hong Kong, Shenzhen) · Xiaoguang Han (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Audio-Visual Segmentation via Unlabeled Frame Exploitation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinxiang Liu (Shanghai Jiao Tong University) · Yikun Liu (Shanghai Jiaotong University) · Ferenas (None) · Chen Ju () · Ya Zhang (Shanghai Jiao Tong University) · Yanfeng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Doubly Abductive Counterfactual Inference for Text-based Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xue Song (Fudan University) · Jiequan Cui (Nanyang Technological University) · Hanwang Zhang (Nanyang Technological University) · Jingjing Chen (Fudan University) · Richang Hong (Hefei University of Technology) · Yu-Gang Jiang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Distilling Semantic Priors from SAM to Efficient Image Restoration Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Quan Zhang (Tsinghua University, Tsinghua University) · Xiaoyu Liu (University of Science and Technology of China) · Wei Li (Huawei Noah's Ark Lab) · Hanting Chen (Huawei Technologies Ltd.) · Junchao Liu (Huawei Noah's Ark Lab) · Jie Hu (Huawei Technologies Ltd.) · Zhiwei Xiong (None) · Chun Yuan (Tsinghua University, Tsinghua University) · Yunhe Wang (Huawei Noah's Ark Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haiwen Diao (Dalian University of Technology) · Bo Wan (KU Leuven) · Ying Zhang (Tencent) · Xu Jia (Dalian University of Technology) · Huchuan Lu (Dalian University of Technology) · Long Chen (HKUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multimodal autoregressive learning for time-aligned and contextual modalities</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            AJ Piergiovanni (Google) · Isaac Noble (Google) · Dahun Kim (Google) · Michael Ryoo (Stony Brook University) · Victor Gomes (Google) · Anelia Angelova (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Solving Masked Jigsaw Puzzles with Diffusion Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinyang Liu (Northeastern University) · Wondmgezahu Teshome (Northeastern University) · Sandesh Ghimire (QualComm) · Mario Sznaier (Northeastern University) · Octavia Camps (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SD2Event: Self-supervised Learning of Dynamic Detectors and Contextual Descriptors for Event Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Gao (University of Science and Technology of China) · Yuqing Zhu (University of Science and Technology of China) · Xinjun Li (University of Science and Technology of China) · Yimin Du (University of Science and Technology of China) · Tianzhu Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Semantics, Distortion, and Style Matter: Towards Source-free UDA for Panoramic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Zheng (HKUST) · Pengyuan Zhou (Aarhus University) · ATHANASIOS (ICT) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Evaluating Transferability in Retrieval Tasks: An Approach Using MMD and Kernel Methods</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengyu Dai (Florida State University) · Amir Hossein Raffiee (SalesForce.com) · Aashish Jain (Salesforce) · Joshua Correa (SalesForce.com)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AV-RIR: Audio-Visual Room Impulse Response Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anton Ratnarajah (University of Maryland, College Park) · Sreyan Ghosh (University of Maryland, College Park) · Sonal Kumar (University of Maryland, College Park) · Purva Chiniya (University of Maryland, College Park) · Dinesh Manocha (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CAD-SIGNet: CAD Language Inference from Point Clouds using Layer-wise Sketch Instance Guided Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammad Sadil Khan (University of Luxembourg) · Elona Dupont (SnT, University of Luxemburg) · Sk Aziz Ali (DFKI GmbH) · Kseniya Cherenkova (University of Luxemburg) · Anis Kacem (University of Luxemburg) · Djamila Aouada (SnT, University of Luxembourg)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://zxz267.github.io/OHTA/" target="_blank">OHTA: One-shot Hand Avatar via Data-driven Implicit Priors</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaozheng Zheng (ByteDance) · Chao Wen (ByteDance) · Zhuo Su (ByteDance) · Zeran Xu (Bytedance) · Zhaohu Li (ByteDance) · Yang Zhao (ByteDance) · Zhou Xue (Li Auto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>E-GPS: Explainable Geometry Problem Solving via Top-Down Solver and Bottom-Up Generator</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenjun Wu (None) · Lingling Zhang (Xi'an Jiaotong University) · Jun Liu (Xi'an Jiaotong University) · Xi Tang (Xi'an Jiaotong University) · Yaxian Wang (Xi'an Jiaotong University) · Shaowei Wang (Xi'an Jiaotong University) · QianYing Wang (lenovo group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Instance Tracking in 3D Scenes from Egocentric Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhan Zhao (University of California, Irvine) · Haoyu Ma (University of California, Irvine) · Shu Kong (University of Macau, Texas A&amp;M University) · Charless Fowlkes (University of California, Irvine)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ID-Blau: Image Deblurring by Implicit Diffusion-based reBLurring AUgmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jia-Hao Wu (National Yang Ming Chiao Tung University) · Fu-Jen Tsai (National Tsing Hua University) · Yan-Tsung Peng (National Chengchi University) · Charles Tsai (Qualcomm Inc, QualComm) · Chia-Wen Lin (National Tsing Hua University) · Yen-Yu Lin (National Yang Ming Chiao Tung University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CAGE: Controllable Articulation GEneration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiayi Liu (None) · Hou In Ivan Tam (Simon Fraser University) · Ali Mahdavi Amiri (Simon Fraser University) · Manolis Savva (Simon Fraser University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GauHuman: Articulated Gaussian Splatting from Monocular Human Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shoukang Hu (Nanyang Technological University) · Tao Hu (Nanyang Technological University) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jieming Cui (None) · Tengyu Liu (None) · Nian Liu (Beijing University of Posts and Telecommunications) · Yaodong Yang (Peking University) · Yixin Zhu (Peking University) · Siyuan Huang (Beijing Institute of General Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-19-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-106" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-107" class="mjx-mrow"><span id="MJXc-Node-108" class="mjx-msubsup"><span class="mjx-base" style="margin-right: -0.081em;"><span id="MJXc-Node-109" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.081em;">M</span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0.22em; padding-right: 0.071em;"><span id="MJXc-Node-110" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">3</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>M</mi><mn>3</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-19">M^3</script>-UDA: A New Benchmark for Unsupervised Domain Adaptive Fetal Cardiac Structure Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Pu (Hong Kong University of Science and Technology) · Liwen Wang (Anhui University) · Jiewen Yang (Hong Kong University of Science and Technology) · He Guannan (Sichuan University) · Xingbo Dong (Anhui University) · Shengli Li (Shenzhen Maternity and Child Healthcare Hospital) · Ying Tan (Shenzhen Maternity and Child Healthcare Hospital) · Ming Chen (Harbin Red Cross Central Hospital ) · Zhe Jin (Anhui University) · Kenli Li (Hunan University) · Xiaomeng Li (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Cyclic Learning for Binaural Audio Generation and Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaojian Li (Northwest Polytechnical University) · Bin Zhao (Northwest Polytechnical University Xi'an) · Yuan Yuan (Northwest Polytechnical University Xi'an)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3D Feature Tracking via Event Camera</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siqi Li (Tsinghua University) · Zhou Zhikuan (None) · Zhou Xue (Li Auto) · Yipeng Li (Tsinghua University, Tsinghua University) · Shaoyi Du (Xi'an Jiaotong University) · Yue Gao (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Frequency-aware Event-based Video Deblurring for Real-World Motion Blur</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taewoo Kim (KAIST) · Hoonhee Cho (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tsu-Ching Hsiao (Woven by Toyota) · Hao-Wei Chen (National Tsing Hua University) · Hsuan-Kung Yang (National Tsinghua University) · Chun-Yi Lee (National Tsing Hua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>QUADify: Extracting Meshes with Pixel-level Details and Materials from Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Maximilian Frühauf (ETH Zurich &amp; Disney Research | Studios) · Hayko Riemenschneider (Disney Research|Studios) · Markus Gross (Disney Research, Disney) · Christopher Schroers (Disney Research|Studios, Disney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/innovator-zero/FedHCA2" target="_blank">FedHCA<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-20-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-111" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-112" class="mjx-mrow"><span id="MJXc-Node-113" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-114" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-115" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">2</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-20">^2</script>: Towards Hetero-Client Federated Multi-Task Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxiang Lu (Shanghai Jiao Tong University) · Suizhi Huang (Shanghai Jiao Tong University) · Yuwen Yang (Shanghai Jiao Tong University) · Shalayiding Sirejiding () · Yue Ding (Shanghai Jiao Tong University) · Hongtao Lu (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Unsupervised Hierarchical Representation with Reinforcement Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruyi An (Nanyang Technological University) · Yewen Li (Nanyang Technological University) · Xu He (Huawei Technologies Ltd.) · Pengjie Gu (Nanyang Technological University) · Mengchen Zhao (South China University of Technology) · Dong Li (Huawei Technologies Ltd.) · Jianye Hao (Tianjin University) · Bo An (Nanyang Technological University) · Chaojie Wang (Skywork AI) · Mingyuan Zhou (The University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multiagent Multitraversal Multimodal Self-Driving: Open MARS Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiming Li (New York University) · Zhiheng Li (New York University) · Nuo Chen (New York University) · Moonjun Gong (New York University) · Zonglin Lyu (New York University) · Zehong Wang (New York University) · Peili Jiang (New York University) · Chen Feng (New York University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>All Rivers Run to the Sea: Private Learning with Asymmetric Flows</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yue Niu (USC) · Ramy E. Ali (Samsung) · Saurav Prakash (University of Illinois at Urbana-Champaign) · Salman Avestimehr (University of Southern California)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiverGen: Improving Instance Segmentation by Learning Wider Data Distribution with More Diverse Generative Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengxiang Fan (Zhejiang University) · Muzhi Zhu (Zhejiang University) · Hao Chen (Zhejiang University) · Yang Liu (Zhejiang University) · Weijia Wu (None) · Huaqi Zhang (Hangzhou VIVO Information Technology Co., Ltd) · Chunhua Shen (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SurMo: Surface-based 4D Motion Modeling for Dynamic Human Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tao Hu (Nanyang Technological University) · Fangzhou Hong (Nanyang Technological University) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Data Poisoning based Backdoor Attacks to Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinghuai Zhang (University of California, Los Angeles (UCLA)) · Hongbin Liu (Duke University) · Jinyuan Jia (Pennsylvania State University) · Neil Zhenqiang Gong (Duke University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://wiselabcmu.github.io/dart/" target="_blank">DART: Implicit Doppler Tomography for Radar Novel View Synthesis</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianshu Huang (Carnegie Mellon University) · John Miller (Carnegie Mellon University) · Akarsh Prabhakara (Carnegie Mellon University) · Tao Jin (CMU, Carnegie Mellon University) · Tarana Laroia (CMU, Carnegie Mellon University) · Zico Kolter (Carnegie Mellon University) · Anthony Rowe (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Video Interpolation with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siddhant Jain (Google Research) · Daniel Watson (Google DeepMind) · Aleksander Holynski (UC Berkeley &amp; Google Research) · Eric Tabellion (Google) · Ben Poole (Google) · Janne Kontkanen (Research, Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dispersed Structured Light for Hyperspectral 3D Imaging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Suhyun Shin (Pohang University of Science and Technology) · Seokjun Choi (Pohang University of Science and Technology) · Felix Heide (Department of Computer Science, Princeton University) · Seung-Hwan Baek (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DualAD: Disentangling the Dynamic and Static World for End-to-End Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Simon Doll (Eberhard-Karls-Universität Tübingen) · Niklas Hanselmann (Mercedes Benz Research &amp; Development) · Lukas Schneider (Mercedes Benz Research &amp; Development) · Richard Schulz (Mercedes Benz AG) · Marius Cordts (Mercedes-Benz) · Markus Enzweiler (Esslingen University of Applied Sciences) · Hendrik Lensch (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DeMatch: Deep Decomposition of Motion Field for Two-View Correspondence Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shihua Zhang (Wuhan University) · Zizhuo Li (Wuhan University) · Yuan Gao (Wuhan University) · Jiayi Ma (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SportsHHI: A Dataset for Human-Human Interaction Detection in Sports Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tao Wu (None) · Runyu He (Nanjing University) · Gangshan Wu (Nanjing University) · Limin Wang (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Intriguing Properties of Diffusion Models: An Empirical Study of the Natural Attack Capability in Text-to-Image Generative Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Takami Sato (None) · Justin Yue (University of California, Irvine) · Nanze Chen (University of Cambridge) · Ningfei Wang (University of California, Irvine) · Alfred Chen (University of California, Irvine)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VSCode: General Visual Salient and Camouflaged Object Detection with 2D Prompt Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyang Luo (None) · Nian Liu (Mohamed bin Zayed University of Artificial Intelligence) · Wangbo Zhao (National University of Singapore) · Xuguang Yang (Northwestern Polytechnical University Xi'an) · Dingwen Zhang (Northwestern Polytechnical University) · Deng-Ping Fan (ETH Zurich) · Fahad Shahbaz Khan (MBZUAI; Linköping University) · Junwei Han (Northwestern Polytechnical University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FAR: Flexible, Accurate and Robust 6DoF Relative Camera Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chris Rockwell (University of Michigan) · Nilesh Kulkarni (None) · Linyi Jin (None) · Jeong Joon Park (Stanford University) · Justin Johnson (University of Michigan) · David Fouhey (New York University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hierarchical Spatio-temporal Decoupling for Text-to-Video Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiwu Qing (Huazhong University of Science and Technology, Tsinghua University) · Shiwei Zhang (Alibaba Group) · Jiayu Wang (None) · Xiang Wang (Huazhong University of Science and Technology) · Yujie Wei (Fudan University) · Yingya Zhang (Alibaba Group) · Changxin Gao (Huazhong University of Science and Technology) · Nong Sang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Self-supervised debiasing using low rank regularization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Geon Yeong Park (Korea Advanced Institute of Science and Technology) · Chanyong Jung (Korea Advanced Institute of Science and Technology) · Sangmin Lee (Korea Advanced Institute of Science &amp; Technology) · Jong Chul Ye (Korea Advanced Institute of Science and Technology) · Sang Wan Lee (Korea Advanced Institute of Science &amp; Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/aeolusguan/NMRF" target="_blank">Neural Markov Random Field for Stereo Matching</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tongfan Guan (The Chinese University of Hong Kong) · Chen Wang (University at Buffalo) · Yun-Hui Liu (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Ungeneralizable Examples</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingwen Ye (National University of Singapore) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning with Unreliability: Fast Few-shot Voxel Radiance Fields with Relative Geometric Consistency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Yingjie (None) · Bangzhen Liu (South China University of Technology) · Hao Tang (School of Computer Science and Engineering, Nanjing University of Science and Technology) · Bailin Deng (Cardiff University) · Shengfeng He (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Language-only Training of Zero-shot Composed Image Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Geonmo Gu (NAVER) · Sanghyuk Chun (NAVER AI Lab) · Wonjae Kim (NAVER) · Yoohoon Kang (NAVER) · Sangdoo Yun (NAVER)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://ku-cvlab.github.io/CoPoNeRF/" target="_blank">Unifying Correspondence, Pose and NeRF for Pose-Free Novel View Synthesis from Stereo Pairs</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sunghwan Hong (Korea University) · Jaewoo Jung (Korea University) · Heeseong Shin (Korea University) · Jiaolong Yang (Microsoft Research) · Chong Luo (Microsoft Research Asia) · Seungryong Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ERMVP: Communication-Efficient and Collaboration-Robust Multi-Vehicle Perception in Challenging Environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingyu Zhang (Fudan University) · Kun Yang (Fudan University) · Yilei Wang (Fudan University) · Hanqi Wang (Fudan University) · Peng Sun (Duke Kunshan University) · Liang Song (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SPAD: Spatially Aware Multiview Diffusers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yash Kant (University of Toronto / Snap Research) · Aliaksandr Siarohin (Snap Inc.) · Ziyi Wu (University of Toronto) · Michael Vasilkovsky (Snap Inc.) · Guocheng Qian (KAUST) · Jian Ren (Snap Inc.) · Riza Alp Guler (Snap Inc.) · Bernard Ghanem (KAUST) · Sergey Tulyakov (Snap Inc.) · Igor Gilitschenski (University of Toronto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Tri-Perspective View Decomposition for Geometry-Aware Depth Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiqiang Yan (Nanjing University of Science and Technology) · Yuankai Lin (Huazhong University of Science and Technology) · Kun Wang (Nanjing University of Science and Technology) · Yupeng Zheng (Institute of Automation，Chinese Academy of Sciences) · Yufei Wang (Northwest Polytechnical University Xi&amp;amp;#x27;an) · Zhenyu Zhang (None) · Jun Li (Nanjing University of Science and Technology) · Jian Yang (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-to-3D Generation with Bidirectional Diffusion using both 3D and 2D priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lihe Ding (The Chinese University of Hong Kong) · Shaocong Dong (Hong Kong University of Science and Technology) · Zhanpeng Huang (SenseTime Research) · Zibin Wang (Sensetime Group Limited) · Yiyuan Zhang (The Chinese University of Hong Kong) · Kaixiong Gong (None) · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology) · Tianfan Xue (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Any-Shift Prompting for Generalization over Distributions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zehao Xiao (University of Amsterdam) · Jiayi Shen (University of Amsterdam) · Mohammad Mahdi Derakhshani (University of Amsterdam) · Shengcai Liao (Inception Institute of Artificial Intelligence) · Cees G. M. Snoek (University of Amsterdam)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Instruct-Imagen: Image Generation with Multi-modal Instruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hexiang Hu (Google Deepmind) · Kelvin C.K. Chan (Google) · Yu-Chuan Su (Google) · Wenhu Chen (University of Waterloo) · Yandong Li (Google Research) · Kihyuk Sohn (Google) · Yang Zhao (Google) · Xue Ben (Google) · William Cohen (Google DeepMind) · Ming-Wei Chang (Google) · Xuhui Jia (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Beyond Average: Individualized Visual Scanpath Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xianyu Chen (University of Minnesota) · Ming Jiang (University of Minnesota, Minneapolis) · Qi Zhao (University of Minnesota, Minneapolis)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/root0yang/BlindNet" target="_blank">Style Blind Domain Generalized Semantic Segmentation via Covariance Alignment and Semantic Consistence Contrastive Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Woo-Jin Ahn (Korea University) · Geun-Yeong Yang (Korea University) · Hyunduck Choi (Chonnam National University) · Myo-Taeg Lim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffusionRegPose: Enhancing Multi-Person Pose Estimation using a Diffusion-Based End-to-End Regression Approach</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dayi Tan (Tongji university) · Hansheng Chen (Stanford University) · Wei Tian (Tongji University) · Lu Xiong (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Entity-NeRF: Detecting and Removing Moving Entities in Urban Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Takashi Otonari (The University of Tokyo) · Satoshi Ikehata (NII, Tokyo Institute of Technology) · Kiyoharu Aizawa (The University of Tokyo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Test-Time Domain Generalization for Face Anti-Spoofing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qianyu Zhou (Shanghai Jiao Tong University) · Ke-Yue Zhang (Tencent) · Taiping Yao (Tencent Youtu Lab) · Xuequan Lu (La Trobe University) · Shouhong Ding (Tencent Youtu Lab) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking Prior Information Generation with CLIP for Few-Shot Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jin Wang (China University of Petroleum) · Bingfeng Zhang (China University of Petroleum (East China)) · Jian Pang (China University of Petroleum (East China)) · Honglong Chen (China University of Petroleum) · Weifeng Liu (China University of Petroleum (East China))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adaptive Slot Attention: Object Discovery with Dynamic Slot Number</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ke Fan (Fudan University) · Zechen Bai (Show Lab, National University of Singapore) · Tianjun Xiao (Amazon) · Tong He (Amazon Web Services) · Max Horn (GSK plc) · Yanwei Fu (Fudan University) · Francesco Locatello (ISTA) · Zheng Zhang (New York University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unsupervised Learning of Category-Level 3D Pose from Object-Centric Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leonhard Sommer (University of Freiburg, Albert-Ludwigs-Universität Freiburg) · Artur Jesslen (University of Freiburg) · Eddy Ilg (None) · Adam Kortylewski (University of Freiburg &amp; MPI-INF)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fooling Polarization-based Vision using Locally Controllable Polarizing Projection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhuoxiao Li (The Univerisity of Tokyo) · Zhihang Zhong (Shanghai AI Lab) · Shohei Nobuhara (Kyoto Institute of Technology) · Ko Nishino (Kyoto University) · Yinqiang Zheng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Exact Fusion via Feature Distribution Matching for Few-shot Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yingbo Zhou (East China Normal University) · Yutong Ye (None) · Pengyu Zhang (East China Normal University) · Xian Wei (Chinese Academy of Sciences) · Mingsong Chen (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Affine Equivariant Networks Based on Differential Invariants</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yikang Li (Peking University) · Yeqing Qiu (The Chinese Univeristy of Hong Kong, Shenzhen) · Yuxuan Chen (Peking University) · Lingshen He (Peking University) · Zhouchen Lin (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>C3: High-performance and low-complexity neural compression from a single image or video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyunjik Kim (DeepMind) · Matthias Bauer (Google DeepMind) · Lucas Theis (Google) · Jonathan Richard Schwarz (Harvard University) · Emilien Dupont (Google DeepMind)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AttriHuman-3D: Editable 3D Human Avatar Generation with Attribute Decomposition and Indexing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fan Yang (None) · Tianyi Chen (Nanyang Technological University) · XIAOSHENG HE (Nanyang Technological University) · Zhongang Cai (Nanyang Technological University) · Lei Yang (The Chinese University of Hong Kong) · Si Wu (South China University of Technology) · Guosheng Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ZeroRF: Fast Sparse View 360° Reconstruction with Zero Pretraining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruoxi Shi (University of California, San Diego) · Xinyue Wei (University of California, San Diego) · Cheng Wang (University of California, San Diego) · Hao Su (UCSD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Monocular Identity-Conditioned Facial Reflectance Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingyu Ren (Shanghai Jiao Tong University) · Jiankang Deng (Imperial College London &amp; Huawei UKRD) · Yuhao Cheng (Shanghai Jiaotong University) · Jia Guo (InsightFace.AI) · Chao Ma (Shanghai Jiao Tong University) · Yichao Yan (Shanghai Jiao Tong University) · Wenhan Zhu (None) · Xiaokang Yang (Shanghai Jiao Tong University, China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3DGS-Avatar: Animatable Avatars via Deformable 3D Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyin Qian (Department of Computer Science, ETHZ - ETH Zurich) · Shaofei Wang (None) · Marko Mihajlovic (Swiss Federal Institute of Technology) · Andreas Geiger (University of Tübingen) · Siyu Tang (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic Propagation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haofeng Liu (South China Normal University) · Chenshu Xu (Singapore Management University) · Yifei Yang (Singapore Management University) · Lihua Zeng (South China Normal University) · Shengfeng He (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiVAS: Video and Audio Synchronization with Dynamic Frame Rates</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Clara Maria Fernandez Labrador (Disney Research) · Mertcan Akcay (Disney Research) · Eitan Abecassis (Walt Disney Company) · Joan Massich (Disney Research) · Christopher Schroers (Disney Research|Studios, Disney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SynSP: Synergy of Smoothness and Precision in Pose Sequences Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tao Wang (Beijing University of Posts and Telecommunications) · Lei Jin (Beijing University of Posts and Telecommunications) · Zheng Wang (Wuhan University) · Jianshu Li (Ant Group) · Liang Li (None) · Fang Zhao (Tencent AI Lab) · Yu Cheng (National University of Singapore) · Li Yuan (Peking University) · Li ZHOU (Wuhan University) · Junliang Xing (Tsinghua University) · Jian Zhao ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffAssemble: A Unified Graph-Diffusion Model for 2D and 3D Reassembly</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gianluca Scarpellini (Università degli Studi di Genova, Istituto Italiano di Tecnologia) · Stefano Fiorini (Istituto Italiano di Tecnologia) · Francesco Giuliari (Istituto Italiano di Tecnologia) · Pietro Morerio (Istituto Italiano di Tecnologia) · Alessio Del Bue (Istituto Italiano di Tecnologia (IIT))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MS-DETR: Efficient DETR Training  with Mixed Supervision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chuyang Zhao (Baidu) · Yifan Sun (Baidu Research) · Wenhao Wang (University of Technology Sydney) · Qiang Chen (Baidu) · Errui Ding (Baidu Inc.) · Yi Yang (Zhejiang University) · Jingdong Wang (Baidu)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://astra-vision.github.io/MaterialPalette/" target="_blank">Material Palette: Extraction of Materials from a Single Image</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ivan Lopes (Inria) · Fabio Pizzati (University of Oxford) · Raoul de Charette (Inria)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Peng (Xiamen University) · Junwei Zhu (Tencent Youtu Lab) · Boyuan Jiang (Tencent Youtu Lab) · Ying Tai (Nanjing University) · Donghao Luo (Tencent YouTu Lab) · Jiangning Zhang (Tencent Youtu Lab) · Wei Lin (Xiamen University) · Taisong Jin (Xiamen University) · Chengjie Wang (Tencent Youtu Lab; Shanghai Jiao Tong University) · Rongrong Ji (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://tum-traffic-dataset.github.io/tumtraf-v2x" target="_blank">TUMTraf V2X Cooperative Perception Dataset</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Walter Zimmer (Technical University of Munich (TUM)) · Gerhard Arya Wardana (Department of Informatics, Technische Universität München) · Suren Sritharan (Technische Universität München) · Xingcheng Zhou (Technical University of Munich) · Rui Song (Technical University of Munich) · Alois Knoll (Technical University Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linglin Jing (Loughborough University) · Yiming Ding (Fudan University) · Yunpeng Gao (Northwest Polytechnical University Xi'an) · Zhigang Wang (Shanghai AI  Lab) · Xu Yan (None) · Dong Wang (Shanghai AI Laboratory) · Gerald Schaefer (Loughborough University) · Hui Fang (Loughborough University) · Bin Zhao (Northwest Polytechnical University Xi'an) · Xuelong Li (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Causal Mode Multiplexer: A Novel Framework for Unbiased Multispectral Pedestrian Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taeheon Kim (Korea Advanced Institute of Science &amp;amp; Technology) · Sebin Shin (KAIST) · Youngjoon Yu (Korea Advanced Institute of Science and Technology (KAIST)) · Hak Gu Kim (Chung-Ang University) · Yong Man Ro (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Human Gaussian Splatting : Real-time Rendering of Animatable Avatars</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Arthur Moreau (Huawei Noah's Ark Lab) · Jifei Song (Huawei Technologies Ltd.) · Helisa Dhamo (None) · Richard Shaw (Huawei Technologies Ltd.) · Yiren Zhou (Huawei Technologies Ltd.) · Eduardo Pérez-Pellitero (Huawei Noah's Ark Lab (UK))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Habitat Synthetic Scenes Dataset (HSSD-200): An Analysis of 3D Scene Scale and Realism Tradeoffs for ObjectGoal Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mukul Khanna (Georgia Institute of Technology) · Yongsen Mao (Simon Fraser University) · Hanxiao Jiang (University of Illinois Urbana-Champaign) · Sanjay Haresh (Qualcomm Inc, QualComm) · Brennan Shacklett (Stanford University) · Dhruv Batra (FAIR (Meta) and Georgia Tech) · Alexander William Clegg (Meta AI) · Eric Undersander (Meta) · Angel Xuan Chang (Simon Fraser University) · Manolis Savva (Simon Fraser University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning to Remove Wrinkled Transparent Film with Polarized Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaqi Tang (Hong Kong University of Science and Technology (Guangzhou)) · RUIZHENG WU (Smartmore Technology) · Xiaogang Xu (Zhejiang Lab) · Sixing Hu (Smartmore Corporation) · Ying-Cong Chen (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Sculpting Holistic 3D Representation in Contrastive Language-Image-3D Pre-training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yipeng Gao (SUN YAT-SEN UNIVERSITY) · Zeyu Wang (University of California, Santa Cruz) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY) · Cihang Xie (University of California, Santa Cruz) · Yuyin Zhou (UC Santa Cruz)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>KeyPoint Relative Position Encoding for Face Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minchul Kim (Michigan State University) · Feng Liu (Michigan State University) · Yiyang Su (None) · Anil Jain (, Michigan State University) · Xiaoming Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Training Vision Transformers for Semi-Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinting Hu (Nanyang Technological University) · Li Jiang (Max Planck Institute for Informatics) · Bernt Schiele (Max Planck Institute for Informatics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiang Li (Carnegie Mellon University) · Jinglu Wang (Microsoft Research Asia) · Xiaohao Xu (University of Michigan - Ann Arbor) · Xiulian Peng (Microsoft Research Asia) · Rita Singh (School of Computer Science, Carnegie Mellon University) · Yan Lu (Microsoft Research Asia) · Bhiksha Raj (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NECA: Neural Customizable Human Avatar</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junjin Xiao (School of Computer Science and Engineering, Sun Yat-sen University) · Qing Zhang (SUN YAT-SEN UNIVERSITY) · Zhan Xu (None) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adversarial Text to Continuous Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kilichbek Haydarov (King Abdullah University of Science and Technology) · Aashiq Muhamed (CMU, Carnegie Mellon University) · Xiaoqian Shen (King Abdullah University of Science and Technology) · Jovana Lazarevic (University of Novi Sad) · Ivan Skorokhodov (KAUST) · Chamuditha Jayanga Galappaththige (Queensland University of Technology) · Mohamed Elhoseiny (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Robust Overfitting Does Matter: Test-Time Adversarial Purification With FGSM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linyu Tang (Chongqing University) · Lei Zhang (Chongqing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Scale Video Anomaly Detection by Multi-Grained Spatio-Temporal Representation Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Menghao Zhang (Beijing University of Posts and Telecommunications) · Jingyu Wang (Beijing University of Post and Telecommunication, Tsinghua University) · Qi Qi (Beijing University of Posts and Telecommunications) · Haifeng Sun (Beijing University of Posts and Telecommunications) · Zirui Zhuang (Beijing University of Posts and Telecommunications) · Pengfei Ren (Beijing University of Posts and Telecommunications) · Ruilong Ma (Beijing University of Posts and Telecommunications) · Jianxin Liao (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Uncertainty-aware Action Decoupling Transformer for Action Anticipation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongji Guo (None) · Nakul Agarwal (None) · Shao-Yuan Lo (Johns Hopkins University) · Kwonjoon Lee (Honda Research Institute USA) · Qiang Ji (Rensselaer Polytechnic Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Boosting Adversarial Training via Fisher-Rao Norm-based Regularization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiangyu Yin (University of Liverpool) · Wenjie Ruan (University of Exeter)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>InitNO: Boosting Text-to-Image Diffusion Models via Initial Noise Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiefan Guo (Beihang University) · Jinlin Liu (Alibaba Group) · Miaomiao Cui (Alibaba Group) · Jiankai Li (Beihang University) · Hongyu Yang (Beihang University) · Di Huang (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SDPose: Tokenized Pose Estimation via Circulation-Guide Self-Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Sichen (Shanghai Jiao Tong University) · Yingyi Zhang (Tencent Youtu Lab) · Siming Huang (Duke University) · Ran Yi (Shanghai Jiao Tong University) · Ke Fan (Shanghai Jiaotong University) · Ruixin Zhang (Tencent Youtu Lab) · Peixian Chen (Xiamen University) · Jun Wang (None) · Shouhong Ding (Tencent Youtu Lab) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffLoc: Diffusion Model for Outdoor LiDAR Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wen Li (schoold of informatics xiamen university) · Yuyang Yang (Xiamen University) · Shangshu Yu (Xiamen University) · Guosheng Hu (Oosto) · Chenglu Wen (Xiamen University) · Ming Cheng (Xiamen University) · Cheng Wang (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fairy: Fast Parallellized Instruction-Guided Video-to-Video Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bichen Wu (Facebook) · Ching-Yao Chuang (Meta) · Xiaoyan Wang (Massachusetts Institute of Technology) · Yichen Jia (Facebook) · Kapil Krishnakumar (Meta, Inc.) · Tong Xiao (None) · Feng Liang (The University of Texas at Austin) · Licheng Yu (None) · Peter Vajda (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Stronger, Fewer, &amp; Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            ZHIXIANG WEI (University of science and technology of china) · Lin Chen (University of Science and Technology of China) · Xiaoxiao Ma (University of Science and Technology of China) · Huaian Chen (University of Science and Technology of China) · Tianle Liu (University of Science and Technology of China) · Pengyang Ling (University of Science and Technology of China) · Jinjin Zheng (University of Science and Technology of China) · Ben Wang (University of Science and Technology of China) · Yi Jin (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>An Asymmetric Augmented Self-Supervised Learning Method for Unsupervised Fine-Grained Image Hashing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feiran Hu (Nanjing University of Science and Technology) · Chenlin Zhang (Moonshot AI, Ltd) · Jiangliang GUO (www.ainnovation.com) · Xiu-Shen Wei (Nanjing University of Science and Technology) · Lin Zhao (Nanjing University of Science and Technology) · Anqi Xu (University of Toronto) · Lingyan Gao (AInnovation Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MimicDiffusion: Purifying Adversarial Perturbation via Mimicking Clean Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kaiyu Song (SUN YAT-SEN UNIVERSITY) · Hanjiang Lai (SUN YAT-SEN UNIVERSITY) · Yan Pan (SUN YAT-SEN UNIVERSITY) · Jian Yin ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Action Scene Graphs for Long-Form Understanding of Egocentric Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ivan Rodin (University of Catania) · Antonino Furnari (University of Catania) · Kyle Min (Intel Labs) · Subarna Tripathi (Intel Corporation) · Giovanni Maria Farinella (University of Catania, Italy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao Ma (SEA AI Lab) · Sumit Patidar (Dyson) · Iain Haughton (Dyson Ltd) · Stephen James (Dyson)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingyuan Yang (Shenzhen University) · Jiawei Feng (Shenzhen University) · Hui Huang (Shenzhen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://f-ilic.github.io/SelectivePrivacyPreservation" target="_blank">Selective, Interpretable and Motion Consistent Privacy Attribute Obfuscation for Action Recognition</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Filip Ilic (Graz University of Technology) · He Zhao (York University) · Thomas Pock (Graz University of Technology) · Richard P. Wildes (York University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://yyvhang.github.io/LEMON/" target="_blank">LEMON: Learning 3D Human-Object Interaction Relation from 2D Images</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhang Yang (University of Science and Technology of China) · Wei Zhai (University of Science and Technology of China) · Hongchen Luo (University of Science and Technology of China) · Yang Cao (University of Science and Technology of China) · Zheng-Jun Zha (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Pseudo Label Refinery for Unsupervised Domain Adaptation on Cross-dataset 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhanwei Zhang (None) · Minghao Chen (Zhejiang University) · Shuai Xiao (Alibaba Group) · Liang Peng (FABU Inc) · Hengjia Li (FABU Inc) · Binbin Lin (Zhejiang University) · Ping Li (Hangzhou Dianzi University) · Wenxiao Wang (Zhejiang University) · Boxi Wu (Zhejiang University) · Deng Cai (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Prompt-enhanced Multiple Instance Learning for Weakly Supervised Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junxi Chen (None) · Liang Li (None) · Li Su (University of Chinese Academy of Sciences) · Zheng-Jun Zha (University of Science and Technology of China) · Qingming Huang (University of Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Brush2Prompt: Contextual Prompt Generator for Object Inpainting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mang Tik Chiu (University of Illinois, Urbana Champaign) · Yuqian Zhou (University of Illinois, Urbana-Champaign) · Lingzhi Zhang (School of Engineering and Applied Science, University of Pennsylvania) · Zhe Lin (Adobe Research) · Connelly Barnes (Adobe Systems) · Sohrab Amirghodsi (Adobe) · Eli Shechtman (Adobe) · Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OST: Refining Text Knowledge with Optimal Spatio-Temporal Descriptor for General Video Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tongjia Chen (Hunan University) · Hongshan Yu (Hunan University) · Zhengeng Yang (Hunan University) · Zechuan Li (Hunan University) · Wei Sun (Hunan University) · Chen Chen ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Condition-Aware Neural Network for Controlled Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Han Cai (Massachusetts Institute of Technology) · Muyang Li (None) · Qinsheng Zhang (Georgia Institute of Technology) · Ming-Yu Liu (NVIDIA) · Song Han (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EvDiG: Event-guided Direct and Global Components Separation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            xinyu zhou (Peking University) · Peiqi Duan (None) · Boyu Li (Peking University) · Chu Zhou (Peking University) · Chao Xu (Peking University) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Sparse views, Near light: A practical paradigm for uncalibrated point-light photometric stereo</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammed Brahimi (Technische Universität München) · Bjoern Haefner (Technical University Munich) · Zhenzhang Ye (Technische Universität München) · Bastian Goldluecke (University of Konstanz) · Daniel Cremers (Technical University Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PanoContext-Former: Panoramic Total Scene Understanding with a Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Dong (Alibaba Group) · Chuan Fang (Hong Kong University of Science and Technology) · Liefeng Bo (None) · Zilong Dong (Alibaba Group) · Ping Tan (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chuwei Luo (DAMO Academy, Alibaba Group) · Yufan Shen (Zhejiang University) · Zhaoqing Zhu (Alibaba Group) · Qi Zheng (Alibaba Group) · Zhi Yu (Zhejiang University) · Cong Yao (Alibaba DAMO Academy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PrPSeg: Universal Proposition Learning for Panoramic Renal Pathology Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruining Deng (Vanderbilt University) · Quan Liu (Vanderbilt University) · Can Cui (Vanderbilt University) · Tianyuan Yao (Vanderbilt University) · Jialin Yue (Vanderbilt University) · Juming Xiong (Vanderbilt University) · Lining yu (Vanderbilt University) · Yifei Wu (Vanderbilt University) · Mengmeng Yin (Vanderbilt University) · Yu Wang (Vanderbilt University Medical Center) · Shilin Zhao (Vanderbilt University) · Yucheng Tang (NVIDIA) · Haichun Yang (Vanderbilt Unversity Medical School) · Yuankai Huo (Vanderbilt University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Modality-Collaborative Test-Time Adaptation for Action Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Baochen Xiong (Institute of Automation, Chinese Academy of Sciences; Peng Cheng Lab) · Xiaoshan Yang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Yaguang Song (Peng Cheng Laboratory) · Yaowei Wang (Pengcheng Laboratory) · Changsheng Xu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yangyi Chen (School of Computer Science, University of Illinois at Urbana-Champaign) · Karan Sikka (SRI International) · Michael Cogswell (SRI International) · Heng Ji (University of Illinois, Urbana-Champaign) · Ajay Divakaran (SRI International)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuchao Gu (None) · Xintao Wang (Tencent) · Yixiao Ge (Tencent) · Ying Shan (Tencent) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SHINOBI: SHape and Illumination using Neural Object decomposition via BRDF optimization and Inverse rendering from unconstrained Image collections</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andreas Engelhardt (University of Tübingen) · Amit Raj (Google ) · Mark Boss (Stability AI) · Yunzhi Zhang (Stanford University) · Abhishek Kar (Google) · Yuanzhen Li (Massachusetts Institute of Technology) · Ricardo Martin-Brualla (Google) · Jonathan T. Barron (Google) · Deqing Sun (Google) · Hendrik Lensch (University of Tübingen) · Varun Jampani (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LORS: Low-rank Residual Structure for Parameter-Efficient Network Stacking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jialin Li (Tencent) · Qiang Nie (Tencent Youtu Lab) · Weifu Fu (Tencent Youtu Lab) · Yuhuan Lin (Tencent Youtu Lab) · Guangpin Tao (Tencent YoutuLab) · Yong Liu (Tencent Youtu Lab) · Chengjie Wang (Tencent Youtu Lab; Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Group Activity Features Through Person Attribute Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chihiro Nakatani (TTI-J) · Hiroaki Kawashima (University of Hyogo) · Norimichi Ukita (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FairRAG: Fair Human Generation via Fair Retrieval Augmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Robik Shrestha (Rochester Institute of Technology) · Yang Zou (Amazon) · Qiuyu Chen (Amazon) · Zhiheng Li (Amazon AGI) · Yusheng Xie (Amazon) · Siqi Deng (Amazon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MicroDiffusion: Implicit Representation-Guided Diffusion for 3D Reconstruction from Limited 2D Microscopy Projections</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            mude hui (University of California, Santa Cruz) · Zihao Wei (University of Michigan - Ann Arbor) · Hongru Zhu (None) · Fei Xia (Ecole Normale Supérieure de Paris) · Yuyin Zhou (UC Santa Cruz)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Visual Layout Composer: Image-Vector Dual Diffusion Model for Design Layout Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammad Amin Shabani (Simon Fraser University) · Zhaowen Wang (Adobe Research) · Difan Liu (Adobe Research) · Nanxuan Zhao (Adobe Research) · Jimei Yang (Adobe Research) · Yasutaka Furukawa (Simon Fraser University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://diffmot.github.io/" target="_blank">DiffMOT: A Real-time Diffusion-based Multiple Object Tracker with Non-linear Prediction</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weiyi Lv (Shanghai University) · Yuhang Huang (National University of Defense Technology) · NING Zhang (PAII Inc.) · Ruei-Sung Lin (PAII Inc) · Mei Han (PAII Inc.) · Dan Zeng (Shanghai University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sicong Leng (Nanyang Technological University) · Hang Zhang (Sichuan University) · Guanzheng Chen (SUN YAT-SEN UNIVERSITY) · Xin Li (Alibaba Group) · Shijian Lu (Nanyang Technological University) · Chunyan Miao (School of Computer Science and  Engineering, Nanyang Technological University) · Lidong Bing (Alibaba DAMO Academy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixun Liang (Hong Kong University of Science and Technology) · Xin Yang (The Hong Kong University of Science and Technology) · Jiantao Lin (Hong Kong University of Science and Technology) · Haodong LI (Hong Kong University of Science and Technology) · Xiaogang Xu (Zhejiang Lab) · Ying-Cong Chen (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Preserving Fairness Generalization in Deepfake Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Lin () · Xinan He (Nanchang University) · Yan Ju (State University of New York at Buffalo) · Xin Wang (State University of New York at Albany) · Feng Ding (Nanchang University) · Shu Hu (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taeckyung Lee (KAIST) · Sorn Chottananurak (KAIST) · Taesik Gong (Bell Labs) · Sung-Ju Lee (Korea Advanced Institute of Science &amp; Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Gradient Alignment for Cross-domain Face Anti-Spoofing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Binh M. Le (Sungkyunkwan University) · Simon Woo (Sungkyunkwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Object Tracking in the Dark</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinzhe Wang (Beijing Institute of Technology) · Kang Ma (Beijing Institute of Technology) · Qiankun Liu (Beijing Institute of Technology) · Yunhao Zou (None) · Ying Fu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sikai Bai (The Hong Kong University of Science and Technology) · Jie ZHANG (The Hong Kong Polytechnic University) · Song Guo (Department of Computer Science and Engineering, Hong Kong University of Science and Technology) · Shuaicheng Li (Sensetime Group Limited) · Jingcai Guo (The Hong Kong Polytechnic University) · Jun Hou (Sensetime) · Tao Han (Northwestern Polytechnical University) · Xiaocheng Lu (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Prompt-Free Diffusion: Taking “Text” out of Text-to-Image Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingqian Xu (University of Illinois, Urbana Champaign) · Jiayi Guo (Tsinghua University, Tsinghua University) · Zhangyang Wang (University of Texas at Austin) · Gao Huang (Tsinghua University, Tsinghua University) · Irfan Essa (Georgia Institute of Technology) · Humphrey Shi (Georgia Tech | UIUC / Oregon | PAIR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Holistic Features are almost Sufficient for Text-to-Video Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kaibin Tian (None) · Ruixiang Zhao (None) · Zijie Xin (Sichuan University) · Bangxiang Lan (Renmin University of China) · Xirong Li (Renmin University of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HDQMF: Holographic Feature Decomposition Using Quantum Algorithms</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Prathyush Poduval (University of California, Irvine) · Zhuowen Zou (University of California, Irvine) · Mohsen Imani (University of California, Irvine)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/hangxu-cv/cvpr24acm" target="_blank">Rethinking Boundary Discontinuity Problem for Oriented Object Detection</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hang Xu (Hangzhou Dianzi University) · Xinyuan Liu (Institute of Computing Technology, Chinese Academy of Sciences) · Haonan Xu (ICT, Chinese Academy of Sciences) · Yike Ma (, Chinese Academy of Sciences) · Zunjie Zhu (Hangzhou Dianzi University) · Chenggang Yan (Hangzhou Dianzi University, Tsinghua University) · Feng Dai (ICT, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fair-VPT: Fair Visual Prompt Tuning for Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sungho Park (Yonsei university) · Hyeran Byun (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Visually Localize Sound Sources from Mixtures without Prior Source Knowledge</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongjin Kim (Kyung Hee University) · Sung Jin Um (Kyung Hee University) · Sangmin Lee (University of Illinois Urbana-Champaign) · Jung Uk Kim (Kyung Hee University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Task-conditioned adaptation of visual features in multi-task policy learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pierre Marza (Institut National des Sciences Appliquées de Lyon) · Laetitia Matignon (LIRIS, CNRS) · Olivier Simonin (INSA de Lyon) · Christian Wolf (Naver Labs Europe)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Autoregressive Queries for Adaptive Tracking with Spatio-Temporal Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinxia Xie (Guangxi Normal University) · Bineng Zhong (Guangxi Normal University) · Zhiyi Mo (Wuzhou university) · Shengping Zhang (Harbin Institute of Technology) · Liangtao Shi (Guangxi Normal University) · Shuxiang Song (Guangxi Normal University) · Rongrong Ji (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-agent Long-term 3D Human Pose Forecasting via Interaction-aware Trajectory Conditioning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaewoo Jeong (KAIST) · Daehee Park (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Revisiting Single Image Reflection Removal In the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yurui Zhu (University of Science and Technology of China) · Bo Li (vivo Mobile Communication Co.,Ltd.) · Xueyang Fu (University of Science and Technology of China) · Peng-Tao Jiang (vivo Mobile Communication (Hangzhou) Co., Ltd.) · Hao Zhang (vivo Mobile Communication （Hangzhou）Co., Ltd) · Qibin Sun (University of Science and Technology of China) · Zheng-Jun Zha (University of Science and Technology of China) · Jinwei Chen (vivo Mobile Communication Co., Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Augmented Identity Distraction for Face Anonymization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenzhong Kuang (Hangzhou Dianzi University) · Xiaochen Yang (Hangzhou Dianzi University) · Yingjie Shen (Hangzhou Dianzi University) · Chao Hu (Hangzhou Dianzi University) · Jun Yu (Hangzhou Dianzi University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://research.zenseact.com/publications/neurad/" target="_blank">NeuRAD: Neural Rendering for Autonomous Driving</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Adam Tonderski (Lund University) · Carl Lindström (Chalmers University of Technology) · Georg Hess (Chalmers University of Technology) · William Ljungbergh (Linköping University  Zenseact) · Lennart Svensson (Chalmers University of Technology) · Christoffer Petersson (Zenseact)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yi Yu (Southeast University) · Xue Yang (Shanghai AI Laboratory) · Qingyun Li (Harbin Institute of Technology) · Feipeng Da (Southeast University) · Jifeng Dai (Tsinghua University, Tsinghua University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Junchi Yan (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TutteNet: Injective 3D Deformations by Composition of 2D Mesh Deformations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bo Sun (University of Texas, Austin) · Thibault Groueix (Adobe Systems) · Chen Song (University of Texas at Austin) · Qixing Huang (University of Texas at Austin) · Noam Aigerman (Université de Montréal)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Poly Kernel Inception Network for Remote Sensing Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinhao Cai (Nanjing University of Science and Technology) · Qiuxia Lai (Communication University of China) · Yuwei Wang (Nanjing University of Science and Technology) · Wenguan Wang (Zhejiang University) · Zeren Sun (Nanjing University of Science and Technology) · Yazhou Yao (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://hmrishavbandy.github.io/doodle23d/" target="_blank">Doodle Your 3D: From Abstract Freehand Sketches to Precise 3D Shapes</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hmrishav Bandyopadhyay (University of Surrey) · Subhadeep Koley (University of Surrey) · Ayan Das (University of Surrey) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Aneeshan Sain (University of Surrey) · Pinaki Nath Chowdhury (University of Surrey) · Tao Xiang (University of Surrey) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://lyumengyao.github.io/projects/spm" target="_blank">One-dimensional Adapter to Rule Them All: Concepts, Diffusion Models and Erasing Applications</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengyao Lyu (Tsinghua University) · Yuhong Yang () · Haiwen Hong (Alibaba Group) · Hui Chen (Tsinghua University, Tsinghua University) · Xuan Jin (University of Science and Technology of China) · Yuan He (Alibaba Group) · Hui Xue (Zhejiang University, Tsinghua University) · Jungong Han (Aberystwyth University) · Guiguang Ding (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Turb-Seg-Res: A Segment-then-Restore Pipeline for Dynamic Videos with Atmospheric Turbulence</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ripon Saha (Arizona State University) · Dehao Qin (Clemson University) · Nianyi Li (None) · Jinwei Ye (None) · Suren Jayasuriya (Arizona State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongwei Zheng (Meituan) · Linyuan Zhou (meituan) · Han Li (Shanghai Jiaotong University) · Jinming Su (Meituan) · Xiaoming Wei (Meituan) · Xu Xiaoming (meituan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haithem Turki (Carnegie Mellon University) · Vasu Agrawal (Meta Reality Labs Research) · Samuel Rota Bulò (Meta) · Lorenzo Porzi (Facebook) · Peter Kontschieder (Meta) · Deva Ramanan (Carnegie Mellon University) · Michael Zollhoefer (Meta) · Christian Richardt (Meta Reality Labs)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IIRP-Net: Iterative Inference Residual Pyramid Network for Enhanced Image Registration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tai Ma (East China Normal University) · zhangsuwei (East China Normal University) · Jiafeng Li (East China Normal University) · Ying Wen (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dancing with Still Images: Video Distillation via Static-Dynamic Disentanglement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyu Wang (Shanghai Jiao Tong University) · Yue Xu (Shanghai Jiao Tong University) · Cewu Lu (Shanghai Jiao Tong University) · Yonglu Li (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Frequency-Adaptive Dilated Convolution for Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linwei Chen (Beijing Institute of Technology) · Lin Gu (RIKEN / the University of Tokyo) · Dezhi Zheng (None) · Ying Fu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SC-Tune: Unleashing Self-Consistent Referential Comprehension  in Large Vision Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tongtian Yue (, Institute of automation, Chinese academy of science) · Jie Cheng (State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA) · Longteng Guo (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xingyuan Dai (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zijia Zhao (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xingjian He (, Institute of automation, Chinese academy of science) · Gang Xiong (Institute of Automation, Chinese Academy of Science) · Yisheng Lv (Institute of Automation, Chinese Academy of Science) · Jing Liu (Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Style Aligned Image Generation via Shared Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Amir Hertz (Tel Aviv University) · Andrey Voynov (Google Research) · Shlomi Fruchter (Research, Google) · Daniel Cohen-Or (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NeRFDeformer: NeRF Transformation from a Single View via 3D Scene Flows</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenggang Tang (UIUC) · Jason Ren (Apple) · Xiaoming Zhao (UIUC) · Bowen Wen (NVIDIA) · Jonathan Tremblay (NVIDIA) · Stan Birchfield (NVIDIA) · Alexander G. Schwing (UIUC)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuchao Gu (None) · Yipin Zhou (Facebook) · Bichen Wu (Facebook) · Licheng Yu (None) · Jia-Wei Liu (National University of Singapore) · Rui Zhao (None) · Jay Zhangjie Wu (National University of Singapore) · David Junhao Zhang (National University of Singapore) · Mike Zheng Shou (National University of Singapore) · Kevin Tang (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Steganographic Passport: An Owner and User Verifiable Credential for Deep Model IP Protection Without Retraining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qi Cui (Nanyang Technological University) · Ruohan Meng (Nanyang Technological University) · Chaohui Xu (Nanyang Technological University) · Chip Hong Chang (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiehong Lin (South China University of Technology) · lihua liu (South China University of Technology) · Dekun Lu (South China University of Technology) · Kui Jia (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://gvecchio.com/matsynth/" target="_blank">MatSynth: A Modern PBR Materials Dataset</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Giuseppe Vecchio (University of Catania) · Valentin Deschaintre (Adobe Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-21-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;D&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-116" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-117" class="mjx-mrow"><span id="MJXc-Node-118" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em; padding-right: 0.081em;">M</span></span><span id="MJXc-Node-119" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">o</span></span><span id="MJXc-Node-120" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">n</span></span><span id="MJXc-Node-121" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em;">o</span></span><span id="MJXc-Node-122" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em;">D</span></span><span id="MJXc-Node-123" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.316em;">i</span></span><span id="MJXc-Node-124" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.503em; padding-right: 0.06em;">f</span></span><span id="MJXc-Node-125" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.503em; padding-bottom: 0.503em; padding-right: 0.06em;">f</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mi>o</mi><mi>n</mi><mi>o</mi><mi>D</mi><mi>i</mi><mi>f</mi><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-21">MonoDiff</script>: Monocular 3D Object Detection and Pose Estimation with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yasiru Ranasinghe (Johns Hopkins University) · Deepti Hegde (Johns Hopkins University) · Vishal M. Patel (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Defense Against Adversarial Attacks on No-Reference Image Quality Models with Gradient Norm Regularization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujia Liu (School of Computer Science, Peking University, Beijing, China) · Chenxi Yang (Peking University) · Dingquan Li (Peng Cheng Laboratory) · Jianhao Ding (Peking University) · Tingting Jiang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BIVDiff: A Training-free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fengyuan Shi (Nanjing University) · Jiaxi Gu (Huawei Noah‘s Ark Lab) · Hang Xu (Huawei Noah‘s Ark Lab) · Songcen Xu (Huawei Noah's Ark Lab) · Wei Zhang (Huawei Technologies Ltd.) · Limin Wang (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Suraj Patni (Indian Institute of Technology, Delhi) · Aradhye Agarwal (Indian Institute of Technology Delhi) · Chetan Arora (Indian Institute of Technology Delhi)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://vilab.hit.edu.cn/projects/bsstnet" target="_blank">Blur-aware Spatio-temporal Sparse Transformer for Video Deblurring</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huicong Zhang (Harbin Institute of Technology) · Haozhe Xie (Nanyang Technological University) · Hongxun Yao (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bi-Causal: Group Activity Recognition via Bidirectional Causality</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youliang Zhang (Wuhan University) · Wenxuan Liu (Wuhan University of Technology) · danni xu (National University of Singapore) · Zhuo Zhou (Wuhan University) · Zheng Wang (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PerAda: Parameter-Efficient Federated Learning Personalization with Generalization Guarantees</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chulin Xie (University of Illinois, Urbana Champaign) · De-An Huang (NVIDIA) · Wenda Chu (California Institute of Technology) · Daguang Xu (NVIDIA) · Chaowei Xiao (Arizona State University) · Bo Li (UIUC) · Anima Anandkumar (California Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>How to Train Neural Field Representations: A Comprehensive Study and Benchmark</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Samuele Papa (University of Amsterdam) · Riccardo Valperga (University of Amsterdam) · David Knigge (University of Amsterdam) · Miltiadis Kofinas (University of Amsterdam) · Phillip Lippe (University of Amsterdam) · Jan-Jakob Sonke (Netherlands Cancer Institute) · Efstratios Gavves ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unraveling Instance Associations: A Closer Look for Audio-Visual Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanhong Chen (University of Adelaide) · Yuyuan Liu (University of Adelaide) · Hu Wang (The University of Adelaide) · Fengbei Liu (Cornell University) · Chong Wang (University of Adelaide) · Helen Frazer (BreastScreen Victoria) · Gustavo Carneiro (University of Surrey)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Digital Life Project: Autonomous 3D Characters with Social Intelligence</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhongang Cai (Nanyang Technological University) · Jianping Jiang (Peking University) · Zhongfei Qing (SenseTime Research) · Xinying Guo (Nanyang Technological University) · Mingyuan Zhang (Nanyang Technological University) · Zhengyu Lin (Sensetime) · Haiy Mei (None) · Chen Wei (SenseTime International PTE. LTD.) · Wang Ruisi (Nanyang Technological University) · Wanqi Yin (SenseTime Research ) · Liang Pan (Shanghai AI Lab) · Xiangyu Fan (Chinese University of Hong Kong) · Han Du (Universität des Saarlandes) · Peng Gao (SenseTime LTD.) · Zhitao Yang (SenseTime Co Ltd.) · Yang Gao (SenseTime) · Jiaqi Li (SenseTime) · Tianxiang Ren (Xiamen University) · YuKun Wei (Sensetime Research) · Xiaogang Wang (The Chinese University of Hong Kong) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY) · Lei Yang (The Chinese University of Hong Kong) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Universal Semi-Supervised Domain Adaptation by Mitigating Common-Class Bias</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenyu Zhang (Institute for Infocomm Research, A*STAR) · Qingmu Liu (National University of Singapore) · Felix Ong (National University of Singapore) · Mohamed Ragab (Institute for Infocomm Research , A*STAR) · Chuan-Sheng Foo (Centre for Frontier AI Research, A*STAR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Semantic-Aware Multi-Label Adversarial Attacks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hassan Mahmood (Northeastern University) · Ehsan Elhamifar (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MonoHair: High-Fidelity Hair Modeling from a Monocular Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Keyu Wu (Zhejiang University) · LINGCHEN YANG (ETHZ - ETH Zurich) · Zhiyi Kuang (Zhejiang University) · Yao Feng (None) · Xutao Han (Zhejiang University) · Yuefan Shen (Zhejiang University) · Hongbo Fu (City University of Hong Kong) · Kun Zhou (Zhejiang University) · Youyi Zheng (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhihao Yuan (The Chinese University of Hong Kong, Shenzhen) · Jinke Ren (The Chinese University of Hong Kong, Shenzhen) · Chun-Mei Feng (None) · Hengshuang Zhao (The University of Hong Kong) · Shuguang Cui (The Chinese University of Hong Kong, Shenzhen) · Zhen Li (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Continual Self-supervised Learning: Towards Universal Multi-modal Medical Data Representation Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiwen Ye (Northwestern Polytechnical University) · Yutong Xie (University of Adelaide) · Jianpeng Zhang (None) · Ziyang Chen (Northwestern Polytechnical University) · Qi Wu (University of Adelaide) · Yong Xia (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Motion-adaptive Separable Collaborative Filters for Blind Motion Deblurring</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengxu Liu (Xi'an Jiaotong University) · Xuan Wang (Megvii Technology Inc.) · Xiangyu Xu (Xi'an Jiaotong University) · Ruhao Tian (Xi'an Jiaotong University) · Shuai Li (Megvii Technology Inc.) · Xueming Qian (Xi'an Jiaotong University, Tsinghua University) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>The Unreasonable Effectiveness of Pre-Trained Features for Camera Pose Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gabriele Trivigno (None) · Carlo Masone (Politecnico di Torino) · Barbara Caputo (Politecnico di Torino) · Torsten Sattler (Czech Technical University in Prague)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/chenlewis/Chromaticity-Map-Adapter-for-DPAD" target="_blank">CMA: A Chromaticity Map Adapter for Robust Detection of Screen-Recapture Document Images</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changsheng Chen (Shenzhen University) · Liangwei Lin (Shenzhen University) · Yongqi Chen (Shenzhen University) · Bin Li (Shenzhen University) · Jishen Zeng (Alibaba Group) · Jiwu Huang (Shenzhen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PointInfinity: Resolution-Invariant Point Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixuan Huang (University of Illinois Urbana-Champaign) · Justin Johnson (University of Michigan) · Shoubhik Debnath (FAIR, Meta) · James Rehg (None) · Chao-Yuan Wu (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CoralSCOP: Segment any COral Image on this Planet</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zheng Ziqiang (Hong Kong University of Science and Technology) · Liang Haixin (None) · Binh-Son Hua (Trinity College Dublin) · Tim, Yue Him Wong (Shenzhen University) · Put ANG (The Chinese University of Hong Kong) · Apple CHUI (Chinese University of Hong Kong) · Sai-Kit Yeung (The Hong Kong University of Science and Technology (HKUST))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SplaTAM: Splat, Track &amp; Map 3D Gaussians for Dense RGB-D SLAM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nikhil Keetha (Carnegie Mellon University) · Jay Karhade (Carnegie Mellon University) · Krishna Murthy Jatavallabhula (Massachusetts Institute of Technology) · Gengshan Yang (Reality Labs Research, Meta) · Sebastian Scherer (None) · Deva Ramanan (Carnegie Mellon University) · Jonathon Luiten (RWTH Aachen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OpticalDR: A Deep Optical Imaging Model for Privacy-Protective Depression Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuchen Pan (Harbin Institute of Technology) · Junjun Jiang (Harbin Institute of Technology) · Kui Jiang (Harbin Institute of Technology) · Zhihao Wu (Harbin Institute of Technology, Shenzhen) · Keyuan Yu (Harbin Institute of Technology) · Xianming Liu (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>F<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-22-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-126" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-127" class="mjx-mrow"><span id="MJXc-Node-128" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-129" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-130" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">3</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-22">^3</script>Loc: Fusion and Filtering for Floorplan Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changan Chen (None) · Rui Wang (Microsoft) · Christoph Vogel (Microsoft) · Marc Pollefeys (ETH Zurich / Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ADA-Track: End-to-End Multi-Camera 3D Multi-Object Tracking with Alternating Detection and Association</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuxiao Ding (Mercedes-Benz AG &amp; University of Bonn) · Lukas Schneider (Mercedes Benz Research &amp; Development) · Marius Cordts (Mercedes-Benz) · Jürgen Gall (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Self-Distilled Masked Auto-Encoders are Efficient  Video Anomaly Detectors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nicolae Ristea (University Politehnica of Bucharest) · Florinel Croitoru (University of Bucharest) · Radu Tudor Ionescu (None) · Marius Popescu (University of Bucharest) · Fahad Shahbaz Khan (MBZUAI; Linköping University) · Mubarak Shah (University of Central Florida)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Construct to Associate: Cooperative Context Learning for Domain Adaptive Point Cloud Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangrui Li (University of Technology Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EarthLoc: Astronaut Photography Localization by Indexing Earth from Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gabriele Berton (None) · Alex Stoken (University of Texas at Austin) · Barbara Caputo (Politecnico di Torino) · Carlo Masone (Politecnico di Torino)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>BioCLIP: A Vision Foundation Model for the Tree of Life</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Samuel Stevens (Ohio State University, Columbus) · Jiaman Wu (Ohio State University, Columbus) · Matthew Thompson (Ohio State University, Columbus) · Elizabeth Campolongo (The Ohio State University) · Chan Hee Song (The Ohio State University) · David Carlyn (Ohio State University) · Li Dong (Microsoft Research) · Wasila Dahdul (University of California, Irvine) · Charles Stewart (Rensselaer Polytechnic Institute) · Tanya Berger-Wolf (None) · Wei-Lun Chao (Ohio State University) · Yu Su (Ohio State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Self-Supervised Facial Representation Learning with Facial Region Awareness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zheng Gao (Queen Mary, University of London) · Ioannis Patras (Queen Mary University of London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://wuyinwei-hah.github.io/rrnet.github.io/" target="_blank">Relation Rectification in Diffusion Model</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yinwei Wu (National University of Singapore) · Xingyi Yang (National University of Singapore) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Close Imitation of Expert Retouching for Black-and-White Photography</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seunghyun Shin (GIST) · Jisu Shin (None) · Jihwan Bae (CHA University, School of Medicine) · Inwook Shim (Inha University) · Hae-Gon Jeon (GIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OmniLocalRF: Omnidirectional Local Radiance Fields from Dynamic Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongyoung Choi (Korea Advanced Institute of Science and Technology) · Hyeonjoong Jang (None) · Min H. Kim (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Split to Merge: Unifying Separated Modalities for Unsupervised Domain Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyao Li (University of Electronic Science and Technology of China) · Yuke Li (Wuhan University) · Zhekai Du (University of Electronic Science and Technology of China) · Fengling Li (University of Technology Sydney) · Ke Lu (University of Electronic Science and Technology of China) · Jingjing Li (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AZ-NAS: Assembling Zero-Cost Proxies for Network Architecture Search</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junghyup Lee (Yonsei University) · Bumsub Ham (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alexander Raistrick (Princeton University) · Lingjie Mei (Princeton University) · Karhan Kayan (Princeton University) · David Yan (Princeton University) · Yiming Zuo (Princeton University) · Beining Han (Department of Computer Science, Princeton University) · Hongyu Wen (Princeton University) · Meenal Parakh (Princeton University) · Stamatis Alexandropoulos (Princeton University) · Lahav Lipson (Princeton University) · Zeyu Ma (Princeton university) · Jia Deng (Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Transferable Structural Sparse Adversarial Attack Via Exact Group Sparsity Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Di Ming (Chongqing University of Technology) · Peng Ren (Chongqing University of Technology) · Yunlong Wang (IQVIA) · Xin Feng (Chongqing University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhuowan Li (Johns Hopkins University) · Bhavan Jasani (Amazon) · Peng Tang (Amazon) · Shabnam Ghadar (Amazon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Reconstructing Hands in 3D with Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Georgios Pavlakos (University of Texas at Austin) · Dandan Shan (None) · Ilija Radosavovic () · Angjoo Kanazawa (UC Berkeley) · David Fouhey (New York University) · Jitendra Malik (University of California at Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Accelerating Diffusion Sampling with Optimized Time Steps</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuchen Xue (Academy of Mathematics and Systems Science, Chinese Academy of Sciences) · Zhaoqiang Liu (University of Electronic Science and Technology of China) · Fei Chen (Huawei Noah's Ark Lab) · Shifeng Zhang (Huawei Technologies Ltd.) · Tianyang Hu (Huawei Noah's Ark Lab) · Enze Xie (Huawei Noah's Ark Lab) · Zhenguo Li (Huawei)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hancheng Ye (Fudan University) · Chong Yu (Fudan University    NVIDIA Corporation) · Peng Ye (Fudan University) · Renqiu Xia (Shanghai Jiao Tong University) · Bo Zhang (Shanghai AI Laboratory) · Yansong Tang (Tsinghua University) · Jiwen Lu (Tsinghua University) · Tao Chen (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OneFormer3D: One Transformer for Unified Point Cloud Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Maksim Kolodiazhnyi (Samsung) · Anna Vorontsova (Samsung) · Anton Konushin (Samsung) · Danila Rukhovich (Samsung Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hierarchical Correlation Clustering and Tree Preserving Embedding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Morteza Haghir Chehreghani (Chalmers University of technology) · Mostafa Haghir Chehreghani (Amirkabir University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IS-Fusion: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junbo Yin (Beijing Institute of Technology) · Wenguan Wang (Zhejiang University) · Runnan Chen (None) · Wei Li (Inceptio) · Ruigang Yang (Inceptio ) · Pascal Frossard (EPFL) · Jianbing Shen (University of Macau)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NC-TTT: A Noise Constrastive Approach for Test-Time Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            David OSOWIECHI (École de Technologie Supérieure, ETS Montreal) · Gustavo Vargas Hakim (École de technologie supérieure, Université du Québec) · Mehrdad Noori (École de technologie supérieure, Université du Québec) · Milad Cheraghalikhani (École de technologie supérieure, Université du Québec) · Ali Bahri (École de technologie supérieure, Université du Québec) · Moslem Yazdanpanah (École de technologie supérieure, Université du Québec) · Ismail Ben Ayed (ETS Montreal) · Christian Desrosiers (École de technologie supérieure)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://sudo-ai-3d.github.io/One2345plus_page/" target="_blank">One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minghua Liu (University of California, San Diego) · Ruoxi Shi (University of California, San Diego) · Linghao Chen (None) · Zhuoyang Zhang (IIIS, Tsinghua University) · Chao Xu (University of California, Los Angeles) · Xinyue Wei (University of California, San Diego) · Hansheng Chen (Stanford University) · Chong Zeng (Zhejiang University) · Jiayuan Gu (University of California, San Diego) · Hao Su (UCSD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>C<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-23-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-131" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-132" class="mjx-mrow"><span id="MJXc-Node-133" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-134" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-135" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">2</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-23">^2</script>KD: Bridging the Modality Gap for Cross-Modal Knowledge Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fushuo Huo (Hong Kong Polytechnic University) · Wenchao Xu (The Hong Kong Polytechnic University) · Jingcai Guo (The Hong Kong Polytechnic University) · Haozhao Wang (Huazhong University of Science and Technology) · Song Guo (Department of Computer Science and Engineering, Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>One-Shot Structure-Aware Stylized Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hansam Cho (Korea University) · Jonghyun Lee (Korea University) · Seunggyu Chang (NAVER Cloud) · Yonghyun Jeong (NAVER)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://vcai.mpi-inf.mpg.de/projects/ash/" target="_blank">ASH: Animatable Gaussian Splats for Efficient and Photoreal Human Rendering</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haokai Pang (ETH Zurich) · Heming Zhu (Max Planck Institute for Informatics, Saarland Informatics Campus) · Adam Kortylewski (University of Freiburg &amp; MPI-INF) · Christian Theobalt (MPI Informatik) · Marc Habermann (Saarland Informatics Campus, Max-Planck Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhancing Video Super-Resolution via Implicit Resampling-based Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kai Xu (National University of Singapore) · Ziwei Yu (None) · Xin Wang (Huawei Technologies Ltd.) · Michael Bi Mi (Huawei Technologies Ltd.) · Angela Yao (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jeongho Kim (KAIST) · Gyojung Gu (Korea Advanced Institute of Science and Technology) · Minho Park (KAIST) · Sunghyun Park (KAIST) · Jaegul Choo (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Looking Similar, Sounding Different: Leveraging Counterfactual Cross-Modal Pairs for Audiovisual Representation Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nikhil Singh (Massachusetts Institute of Technology) · Chih-Wei Wu (Netflix) · Iroro Orife (Netflix) · Kalayeh (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Grounded Text-to-Image Synthesis with Attention Refocusing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Quynh Phung (University of Maryland, College Park) · Songwei Ge (University of Maryland, College Park) · Jia-Bin Huang (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pavlo Melnyk (Computer Vision Laboratory, Linköping University) · Andreas Robinson (Linköping University) · Michael Felsberg (Linköping University) · Mårten Wadenbäck (Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RTracker: Recoverable Tracking via PN Tree Structured Memory</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqing Huang (Harbin Institute of Technology) · Xin Li (Peng Cheng Laboratory) · Zikun Zhou (Peng Cheng Laboratory) · Yaowei Wang (Pengcheng Laboratory) · Zhenyu He (Harbin Institute of Technology, Shenzhen) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuhuai Ren (None) · Linli Yao (Peking University) · Shicheng Li (Peking University) · Xu Sun (Peking University) · Lu Hou (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhe Chen (Nanjing University) · Jiannan Wu (University of Hong Kong) · Wenhai Wang (Shanghai AI Laboratory) · Weijie Su (University of Science and Technology of China) · Guo Chen (Nanjing University) · Sen Xing (Tsinghua University, Tsinghua University) · Zhong Muyan (Tsinghua University, Tsinghua University) · Qing-Long Zhang (Shanghai Artificial Intelligence Laboratory) · Xizhou Zhu (Shanghai AI Laboratory) · Lewei Lu (SenseTime) · Bin Li (University of Science and Technology of China) · Ping Luo (The University of Hong Kong) · Tong Lu (Nanjing University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Jifeng Dai (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Tyche: Stochastic in Context Learning for Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Marianne Rakic (Massachusetts Institute of Technology) · Hallee Wong (MIT) · Jose Javier Gonzalez Ortiz (DataBricks) · Beth Cimini (Broad Institute) · John Guttag (Massachusetts Institute of Technology) · Adrian V. Dalca (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unexplored Faces of Robustness and Out-of-Distribution: Covariate Shifts in Environment and Sensor Domains</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Eunsu Baek (Seoul National University) · Keondo Park (Seoul National University) · Ji-yoon Kim (Seoul National University) · Hyung-Sin Kim (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CLOAF: CoLlisiOn-Aware Human Flow</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andrey Davydov (EPFL) · Martin Engilberge (EPFL - EPF Lausanne) · Mathieu Salzmann (EPFL) · Pascal Fua (Swiss Federal Institute of Technology Lausanne)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Taming Self-Training for Open-Vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiyu Zhao (Rutgers University, New Brunswick) · Samuel Schulter (NEC Laboratories America) · Long Zhao (Google Research) · Zhixing Zhang (Rutgers University) · Vijay Kumar BG (NEC Laboratories America) · Yumin Suh (NEC Labs America) · Manmohan Chandraker (UC San Diego) · Dimitris N. Metaxas (Rutgers)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>What, How, and When Should Object Detectors Update in Continually Changing Test Domains?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jayeon Yoo (Seoul National  University) · Dongkwan Lee (Seoul National University) · Inseop Chung (Seoul National University) · Donghyun Kim (Korea University) · Nojun Kwak (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Correlation Structures for Vision Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Manjin Kim (POSTECH) · Paul Hongsuck Seo (Google) · Cordelia Schmid (Inria / Google) · Minsu Cho (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CLIP-Driven Open-Vocabulary 3D Scene Graph Generation via Cross-Modality Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lianggangxu Chen (East China Normal University) · Xuejiao Wang (East China Normal University) · Jiale Lu (East China Normal University) · Shaohui Lin (East China Normal University) · Changbo Wang (East China Normal University) · Gaoqi He (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Vanishing-Point-Guided Video Semantic Segmentation of Driving Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Diandian Guo (Universität Stuttgart) · Deng-Ping Fan (ETH Zurich) · Tongyu Lu (ETHZ - ETH Zurich) · Christos Sakaridis (ETH Zurich) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Equivariant plug-and-play image reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Matthieu Terris (INRIA) · Thomas Moreau (INRIA) · Nelly Pustelnik (CNRS) · Julián Tachella (CNRS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Visual Objectification in Films: Towards a New AI Task for Video Interpretation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Julie Tores (Université Côte d'Azur) · Lucile Sassatelli (Universite Cote d'Azur) · Hui-Yin Wu (Inria at Université Côte d'Azur) · Clement Bergman (INRIA) · Léa Andolfi (CELSA-Sorbonne) · Victor Ecrement (Sorbonne Université) · Frederic Precioso (Universite Cote d'Azur) · Thierry Devars (Université Paris-Sorbonne (Paris IV)) · Magali GUARESI (CNRS) · Virginie Julliard (Université Paris-Sorbonne (Paris IV)) · Sarah Lécossais (Sorbonne Paris Nord)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HRVDA: High-Resolution Visual Document Assistant</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaohu Liu (University of Science and Technology of China) · Kun Yin (Tencent YouTu Lab) · Haoyu Cao (Tencent Youtu Lab) · Xinghua Jiang (None) · Xin Li (Tencent Youtu Lab) · Yinsong Liu (Tencent Youtu Lab) · Deqiang Jiang (Tencent YouTu Lab) · Xing Sun (Tencent YouTu Lab) · Linli Xu (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Ink Dot-Oriented Differentiable Optimization for Neural Image Halftoning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Jiang (Peking University) · Bingfeng Zhou (Peking University) · Yadong Mu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://hyzhouboy.github.io/" target="_blank">Bring Event into RGB and LiDAR: Hierarchical Visual-Motion Fusion for Scene Flow</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanyu Zhou (Huazhong University of Science and Technology) · Yi Chang (Huazhong University of Science and Technology) · Zhiwei Shi (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Separate and Conquer: Decoupling Co-occurrence via Decomposition and Representation for Weakly Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiwei Yang (Fudan university) · Kexue Fu (Qilu University of Technology (Shandong Academy of Sciences)) · Minghong Duan (Fudan University) · Linhao Qu (Fudan University) · Shuo Wang (Fudan University) · Zhijian Song (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyang Chen (Northwestern Polytechnical University) · Yongsheng Pan (ShanghaiTech University) · Yiwen Ye (Northwestern Polytechnical University) · Mengkang Lu (nwpu) · Yong Xia (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FACT: Frame-Action Cross-Attention Temporal Modeling for Efficient Fully-Supervised Action Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zijia Lu (Northeastern University) · Ehsan Elhamifar (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SplattingAvatar: Realistic Real-Time Human Avatars with Mesh-Embedded Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhijing Shao (The Hong Kong University of Science and Technology (Guangzhou)) · Wang Zhaolong (Tsinghua University) · Zhuang Li (Prometheus Vision Technology Co., Ltd.) · Duotun Wang (The Hong Kong University of Science and Technology (Guangzhou)) · Xiangru Lin () · Yu Zhang (Prometheus Vision Technology Co., Ltd.) · Mingming Fan (Hong Kong University of Science and Technology) · Zeyu Wang (The Hong Kong University of Science and Technology (Guangzhou))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LocLLM: Exploiting Generalizable Human Keypoint Localization via Large Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongkai Wang (Peking University) · shiyu xuan (Peking University) · Shiliang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Instance-based Max-margin for Practical Few-shot Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minghao Fu (None) · Ke Zhu (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking Multi-view Representation Learning via Distilled Disentangling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guanzhou Ke (Beijing Jiaotong University) · Bo Wang (Peking University) · Xiao-Li Wang (Nanjing University of Science and Technology) · Shengfeng He (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Grounding Everything: Emerging Localization Properties in Vision-Language Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Walid Bousselham (Johann Wolfgang Goethe Universität Frankfurt am Main) · Felix Petersen (Stanford University) · Vittorio Ferrari (Synthesia) · Hilde Kuehne (University of Bonn             MIT-IBM Watson AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ShapeMatcher: Self-Supervised Joint Shape Canonicalization, Segmentation, Retrieval and Deformation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Di (Technische Universität München) · Chenyangguang Zhang (Tsinghua University) · Chaowei Wang (Northwestern Polytechnical University, Northwest Polytechnical University Xi'an) · Ruida Zhang (Department of Automation, Tsinghua University, Tsinghua University) · Guangyao Zhai (Technical University of Munich) · Yanyan Li (Technical University Munich) · Bowen Fu (Technische Universität München) · Xiangyang Ji (Tsinghua University) · Shan Gao (Northwest Polytechnical University Xi'an)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SVDTree: Semantic Voxel Diffusion for Single Image Tree Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Li (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhihao Liu (The University of Tokyo) · Bedrich Benes (Purdue University) · Xiaopeng Zhang (Institute of Automation, Chinese Academy of Sciences) · Jianwei Guo (Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Han Liang (ShanghaiTech University) · Jiacheng Bao (Shanghai Tech University) · Ruichi Zhang (ShanghaiTech University) · Sihan Ren (ShanghaiTech University) · Yuecheng Xu (ShanghaiTech University) · Sibei Yang (None) · Xin Chen (University of Chinese Academy of Sciences, ShanghaiTech University) · Jingyi Yu (ShanghaiTech University) · Lan Xu (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhancing Intrinsic Features for Debiasing via Investigating Class-Discerning Common Attributes in Bias-Contrastive Pair</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jeonghoon Park (Korea Advanced Institute of Science and Technology) · Chaeyeon Chung (Korea Advanced Institute of Science and Technology) · Jaegul Choo (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://cvpr.thecvf.com/Conferences/2024/www.lotvsmmau.net" target="_blank">Abductive Ego-View Accident Video Understanding for Safe Driving Perception</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianwu Fang (Xi'an Jiaotong University) · Lei-lei Li (Chang'an university) · Junfei Zhou (Chang'an university) · Junbin Xiao (None) · Hongkai Yu (Cleveland State University) · Chen Lv (Nanyang Technological University) · Jianru Xue (Xi'an Jiaotong University, Tsinghua University) · Tat-seng Chua (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Prompting Vision Foundation Models for Pathology Image Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            CHONG YIN (Hong Kong Baptist University) · Siqi Liu (Shenzhen Research Institute of Big Data) · Kaiyang Zhou (Hong Kong Baptist University) · Vincent Wong (The Chinese University of Hong Kong) · Pong C. Yuen (Hong Kong Baptist Unviersity)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Error Detection in Egocentric Procedural Task Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shih-Po Lee (Northeastern University) · Zijia Lu (Northeastern University) · Zekun Zhang (Stony Brook University) · Minh Hoai (State University of New York, Stony Brook) · Ehsan Elhamifar (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Patch2Self2: Self-supervised Denoising on Coresets via Matrix Sketching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shreyas Fadnavis (Johnson and Johnson) · Agniva Chowdhury (Oak Ridge National Laboratory) · Joshua Batson (Anthropic) · Petros Drineas (Purdue University) · Eleftherios Garyfallidis (Indiana University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Optimizing Diffusion Noise Can Serve As Universal Motion Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Korrawe Karunratanakul (ETH Zurich) · Konpat Preechakul (University of California, Berkeley) · Emre Aksan (Google) · Thabo Beeler (Google) · Supasorn Suwajanakorn (Vidyasirimedhi Institute of Science and Technology) · Siyu Tang (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Masking Clusters in Vision-language Pretraining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihao Wei (University of Michigan - Ann Arbor) · Zixuan Pan (University of Michigan - Ann Arbor) · Andrew Owens (University of Michigan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generalizable Whole Slide Image Classification with Fine-Grained Visual-Semantic Interaction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Li (Xiamen University) · Ying Chen (Xiamen University) · Yifei Chen (Huawei) · Rongshan Yu (National University of Singapore) · Wenxian Yang (Aginome Scientific) · Liansheng Wang (Xiamen University, Tsinghua University) · Bowen Ding (Shanghai Jiaotong University) · Yuchen Han (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>On The Vulnerability of Efficient Vision Transformers to Adversarial Computation Attacks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Navaneet K L (University of California, Davis) · Soroush Abbasi Koohpayegani (University of California, Davis) · Essam Sleiman (Harvard University, Harvard University) · Hamed Pirsiavash (University of California, Davis)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generative Unlearning for Any Identity</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juwon Seo (Kyung Hee University) · Sung-Hoon Lee (Kyung Hee University) · Tae-Young Lee (Kyung Hee University) · SeungJun Moon (KLleon) · Gyeong-Moon Park (Kyung Hee University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhancing Multimodal Cooperation via Sample-level Modality Valuation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yake Wei (Renmin University of China) · Ruoxuan Feng (Renmin University of China) · Zihe Wang (Renmin University of China) · Di Hu (Renmin University of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via Image-Informed Textual Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiongwei Wu (HyperGAI) · Sicheng Yu (Bytedance) · Ee-Peng Lim (Singapore Management University) · Chong Wah Ngo (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Trevine Oorloff (University of Maryland, College Park) · Surya Koppisetti (Reality Defender Inc) · Nicolo Bonettini (Reality Defender) · Divyaraj Solanki (Reality Defender Inc.) · Ben Colman (Reality Defender) · Yaser Yacoob (University of Maryland, College Park) · Ali Shahriyari (Reality Defender) · Gaurav Bharaj (Flawless AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CaKDP: Category-aware Knowledge Distillation and Pruning Framework for Lightweight 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haonan Zhang (Xi'an Jiaotong University) · Longjun Liu (Xi'an Jiaotong University) · Yuqi Huang (Xi'an Jiaotong University) · YangZhao (Xi'an Jiaotong University) · Xinyu Lei (Xi'an Jiaotong University) · Bihan Wen (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Regressor-Segmenter Mutual Prompt Learning for Crowd Counting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingyue Guo (University of Chinese Academy of Sciences) · Li Yuan (Peking University) · Zhaoyi Yan (PengCheng Laboratory) · Binghui Chen (Alibaba Group) · Yaowei Wang (Pengcheng Laboratory) · Qixiang Ye (University of Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SfmCAD: Unsupervised CAD Reconstruction by Learning Sketch-based Feature Modeling Operations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pu Li (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Jianwei Guo (Institute of Automation, Chinese Academy of Sciences) · HUIBIN LI (Institute of Automation, Chinese Academy of Sciences) · Bedrich Benes (Purdue University) · Dong-Ming Yan (Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongliang Cao (University of Bonn) · Marvin Eisenberger (Technical University Munich) · Nafie El Amrani (Rheinische Friedrich-Wilhelms Universität Bonn) · Daniel Cremers (Technical University Munich) · Florian Bernard (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Neural Redshift: Random Networks are not Random Functions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Damien Teney (Idiap Research Institute) · Armand Nicolicioiu (ETHZ - ETH Zurich) · Valentin Hartmann (EPFL) · Ehsan Abbasnejad (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Sculpt3D: Multi-View Consistent Text-to-3D Generation with Sparse 3D Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Cheng (Nanyang Technological University) · Xiaofeng Yang (Nanyang Technological University) · Fan Yang (None) · Chengzeng Feng (Nanyang Technological University) · ZHOUJIE FU (Nanyang Technological University) · Chuan-Sheng Foo (Centre for Frontier AI Research, A*STAR) · Guosheng Lin (Nanyang Technological University) · Fayao Liu (Institute for Infocomm Research, A*STAR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu-Bang Zheng (Southwest Jiaotong University) · Xile Zhao (University of Electronic Science and Technology of China) · Junhua Zeng (RIKEN) · Chao Li (RIKEN) · Qibin Zhao (RIKEN) · Heng-Chao Li (Southwest Jiaotong University) · Ting-Zhu Huang (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Open-Vocabulary HOI Detection via Conditional Multi-level Decoding and Fine-grained Semantic Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ting Lei (Peking University) · Shaofeng Yin (Peking University) · Yang Liu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CustomListener: Text-guided Responsive Interaction for User-friendly Listening Head Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xi Liu (University of Electronic Science and Technology of China) · Ying Guo (Meituan) · Cheng Zhen (Meituan) · Tong Li (Meituan) · Yingying Ao (Meituan) · Pengfei Yan (Meituan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature Refinement for Image Restoration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shihao Zhou (Nankai University) · Duosheng Chen (Nankai University) · Jinshan Pan (Nanjing University of Science and Technology) · Jinglei Shi (Nankai University) · Jufeng Yang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Fully Convolutional Slice-to-Volume Reconstruction for Single-Stack MRI</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sean I. Young (Harvard Medical School / MIT) · Yaël Balbastre (Massachusetts General Hospital, Harvard Medical School) · Bruce Fischl (Massachusetts General Hospital, Harvard University) · Polina Golland (Massachusetts Institute of Technology) · Juan Iglesias (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifei Li (Massachusetts Institute of Technology) · Hsiaoyu Chen (Meta) · Egor Larionov (Meta) · Nikolaos Sarafianos (Meta Reality Labs) · Wojciech Matusik (Massachusetts Institute of Technology) · Tuur Stuyck (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OED: Towards One-stage End-to-End Dynamic Scene Graph Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guan Wang (Peking University) · Zhimin Li (Tencent Data Platform) · Qingchao Chen (Peking University) · Yang Liu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>On the Estimation of Image-matching Uncertainty in Visual Place Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mubariz Zaffar (Delft University of Technology) · Liangliang Nan (Delft University of Technology) · Julian F. P. Kooij (Delft University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Transform Dynamically for Better Adversarial Transferability</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rongyi Zhu (None) · Zeliang Zhang (University of Rochester) · Susan Liang (University of Rochester) · Zhuo Liu (University of Rochester) · Chenliang Xu (University of Rochester)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SEAS: ShapE-Aligned Supervision for Person Re-Identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haidong Zhu (University of Southern California) · Pranav Budhwant (University of Southern California) · Zhaoheng Zheng (University of Southern California) · Ram Nevatia (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LidaRF: Delving into Lidar for Neural Radiance Field on Street Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            shanlin sun (University of California, Irvine) · Bingbing Zhuang (NEC Labs America) · Ziyu Jiang (Texas A&amp;M) · Buyu Liu (NEC-Labs) · Xiaohui Xie (University of California, Irvine) · Manmohan Chandraker (UC San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AEROBLADE: Training-Free Detection of Latent Diffusion Images Using Autoencoder Reconstruction Error</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jonas Ricker (Ruhr University Bochum) · Denis Lukovnikov (Ruhr University Bochum) · Asja Fischer (Ruhr-Universität Bochum)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surfaces</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linyi Jin (None) · Nilesh Kulkarni (None) · David Fouhey (New York University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Unified Framework for Microscopy Defocus Deblur with Multi-Pyramid Transformer and Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuelin Zhang (The Chinese University of Hong Kong) · Pengyu Zheng (The Chinese University of Hong Kong) · Wanquan Yan (The Chinese University of Hong Kong) · Chengyu Fang (Tsinghua University, Tsinghua University) · Shing Shin Cheng (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Task2Box: Box Embeddings for Modeling Asymmetric Task Relationships</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rangel Daroya (University of Massachusetts Amherst) · Aaron Sun (University of Massachusetts Amherst) · Subhransu Maji (University of Massachusetts, Amherst)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UniGS: Unified Representation for Image Generation and Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lu Qi (University of California, Merced) · Lehan Yang (University of Sydney) · Weidong Guo (Tencent) · Yu Xu (University of Waterloo) · Bo Du (Wuhan University) · Varun Jampani (Google Research) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LIVE: Online Large Video-Language Model for Streaming Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Joya Chen (National University of Singapore) · Zhaoyang Lv (None) · Shiwei Wu (University of Science and Technology of China) · Kevin Qinghong Lin (national university of singaore, National University of Singapore) · Chenan Song (national university of singaore, National University of Singapore) · Difei Gao (None) · Jia-Wei Liu (National University of Singapore) · Ziteng Gao (National University of Singapore) · Dongxing Mao (SUTD) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Physical Property Understanding from Language-Embedded Feature Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Albert J. Zhai (University of Illinois at Urbana-Champaign) · Yuan Shen (University of Illinois at Urbana-Champaign) · Emily Y. Chen (University of Illinois Urbana Champaign) · Gloria Wang (Department of Computer Science) · Xinlei Wang (University of Illinois Urbana-Champaign) · Sheng Wang (University of Illinois Urbana-Champaign) · Kaiyu Guan (University of Illinois, Urbana Champaign) · Shenlong Wang (University of Illinois, Urbana Champaign)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Training Generative Image Super-Resolution Models by Wavelet-Domain Losses Enables Better Control of Artifacts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cansu Korkmaz (Koc University) · Ahmet Murat Tekalp (Koç University) · Zafer Dogan (Koc University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SAFDNet: A Simple and Effective Network for Fully Sparse 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gang Zhang (Tsinghua University) · Chen Junnan (Huazhong University of Science and Technology) · Guohuan Gao (Beijing Institute of Technology) · Jianmin Li (Department of computer science and technology, Tsinghua University) · Si Liu (Beihang University) · Xiaolin Hu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruiyang Hao (Institute for AI Industry Research, Tsinghua University) · Siqi Fan (Institute for AI Industry Research, Tsinghua University) · Yingru Dai (Tsinghua University, Tsinghua University) · Zhenlin Zhang (China Automotive Innovation Corporation) · Chenxi Li (CAIC) · YuntianWang (China Automotive Innovation Corporation) · Haibao Yu (University of Hong Kong) · Wenxian Yang (Tsinghua University, Tsinghua University) · Jirui Yuan (Tsinghua University, Tsinghua University) · Zaiqing Nie (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Spatial Adaptation and Temporal Coherence in Diffusion Models for Video Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhikai Chen (None) · Fuchen Long (JD.com) · Zhaofan Qiu (University of Science and Technology of China) · Ting Yao (JD AI Research) · Wengang Zhou (University of Science and Technology of China) · Jiebo Luo (University of Rochester) · Tao Mei (JD Explore Academy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Le Xue (None) · Ning Yu (Salesforce Research) · Shu Zhang (SalesForce.com) · Artemis Panagopoulou (University of Pennsylvania) · Junnan Li (None) · Roberto Martín-Martín (University of Texas at Austin) · Jiajun Wu (Stanford University) · Caiming Xiong (Salesforce Research) · Ran Xu (SalesForce.com) · Juan Carlos Niebles (Salesforce Research) · Silvio Savarese (Salesforce)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Phillip Howard (Intel Labs) · Avinash Madasu (None) · Tiep Le (Intel) · Gustavo Lujan-Moreno (Intel) · Anahita Bhiwandiwalla (Intel) · Vasudev Lal (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EvalCrafter: Benchmarking and Evaluating Large Video Generation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yaofang Liu (City University of Hong Kong) · Xiaodong Cun (Tencent AI Lab) · Xuebo Liu (Harbin Institute of Technolgy, Shenzhen) · Xintao Wang (Tencent) · Yong Zhang (Tencent AI Lab) · Haoxin Chen (Tencent AI Lab) · Yang Liu (National University of Defense Technology) · Tieyong Zeng (The Chinese University of Hong Kong) · Raymond Chan (City University of Hong Kong) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CFAT: Unleashing Triangular Windows for Image Super-resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Abhisek Ray (Indian Institute of Technology, Patna) · Gaurav Kumar (Indian Institute of Technology (IIT), Patna) · Maheshkumar Kolekar (Indian Institute of Technology, Patna)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Convolutional Prompting meets Language Models for Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            ANURAG Roy (IIT Kharagpur) · Riddhiman Moulick (Indian Institute of Technology Kharagpur) · Vinay Verma Verma (None) · Saptarshi Ghosh (Indian Institute of Technology Kharagpur) · Abir Das (Indian Institute of Technology Kharagpur)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mitigating Noisy Correspondence by Geometrical Structure Consistency Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihua Zhao (Shanghai Jiao Tong University) · Mengxi Chen (Shanghai Jiaotong University) · Tianjie Dai (Shanghai Jiao Tong University) · Jiangchao Yao (Shanghai Jiaotong University) · Bo Han (HKBU) · Ya Zhang (Shanghai Jiao Tong University) · Yanfeng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Intraoperative 2D/3D Image Registration via Differentiable X-ray Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vivek Gopalakrishnan (MIT) · Neel Dey (Massachusetts Institute of Technology) · Polina Golland (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Depth  Information Assisted Collaborative Mutual Promotion Network for Single Image Dehazing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yafei Zhang (Kunmimg University of Science and Technology) · Shen Zhou (Kunmimg University of Science and Technology) · Huafeng Li (Kunmimg University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhongcong Xu (national university of singaore, National University of Singapore) · Jianfeng Zhang (NUS) · Jun Hao Liew (ByteDance) · Hanshu Yan (ByteDance) · Jia-Wei Liu (National University of Singapore) · Chenxu Zhang (Bytedance) · Jiashi Feng (ByteDance) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Integrating Efficient Optimal Transport and Functional Maps For Unsupervised Shape Correspondence Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tung Le (University of California, Irvine) · Khai Nguyen (UT Austin) · shanlin sun (University of California, Irvine) · Nhat Ho (University of Texas, Austin) · Xiaohui Xie (University of California, Irvine)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SuperPrimitive: Scene Reconstruction at a Primitive Level</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kirill Mazur (Imperial College London) · Gwangbin Bae (Imperial College London) · Andrew J. Davison (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Renshuai Liu (Xiamen University) · Bowen Ma (NetEase, Inc.) · Wei Zhang (None) · Zhipeng Hu (Leihuo Game, NetEase) · Changjie Fan (Netease, Fuxi AI Lab) · Tangjie Lv (NetEase, Inc.) · Yu Ding (Fuxi AI Lab in Netease) · Xuan Cheng (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Relaxed Contrastive Learning for Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seonguk Seo (Seoul National University) · Jinkyu Kim (Seoul National University) · Geeho Kim (Seoul National University) · Bohyung Han (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GlitchBench: Can large multimodal models detect video game glitches?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammad Reza Taesiri (University of Alberta) · Tianjun Feng (University of Alberta) · Cor-Paul Bezemer (University of Alberta) · Anh Nguyen (Auburn University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LLM4SGG: Large Language Models for Weakly Supervised Scene Graph Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kibum Kim (Korea Advanced Institute of Science &amp;amp; Technology) · Kanghoon Yoon (Korea Advanced Institute of Science &amp; Technology) · Jaehyeong Jeon (Korea Advanced Institute of Science and Technology) · Yeonjun In (Korea Advanced Institute of Science &amp; Technology) · Jinyoung Moon (ETRI) · Donghyun Kim (Korea University) · Chanyoung Park (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weirong Chen (Technical University of Munich) · Le Chen (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Rui Wang (Microsoft) · Marc Pollefeys (ETH Zurich / Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Geometrically-informed aggregation for zero-shot point cloud understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guofeng Mei (Fondazione Bruno Kessler) · Luigi Riz (Fondazione Bruno Kessler) · Yiming Wang (Fondazione Bruno Kessler) · Fabio Poiesi (Fondazione Bruno Kessler)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://tianhao-qi.github.io/DEADiff/" target="_blank">DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianhao Qi (University of Science and Technology of China) · Shancheng Fang (University of Science and Technology of China) · Yanze Wu (ByteDance Inc.) · Hongtao Xie (University of Science and Technology of China) · Jiawei Liu (ByteDance Inc.) · Lang chen (ByteDance) · Qian HE (Institute of Remote Sensing Application, Chinese Academic of Sciences) · Yongdong Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixiang Zhou (xiaobing.ai) · Yu Wan () · Baoyuan Wang (Xiaobing.ai)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Degradation-unaware Representation with Prior-based Latent Transformations for Blind Face Restoration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lianxin Xie (South China University of Technology) · csbingbing zheng (South China University of Technology) · Wen Xue (South China University of Technology) · Le Jiang (South China University of Technology) · Cheng Liu (Shantou University) · Si Wu (South China University of Technology) · Hau San Wong (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CycleINR: Cycle Implicit Neural Representation for Arbitrary-Scale Volumetric Super-Resolution of Medical Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei Fang (Alibaba Group) · Yuxing Tang (Alibaba Group) · Heng Guo (Alibaba Group) · Mingze Yuan (Peking University) · Tony C. W. MOK (Alibaba DAMO Academy) · Ke Yan (Alibaba DAMO Academy) · Jiawen Yao (Alibaba Group) · Xin Chen (Guangzhou First People's Hospital) · Zaiyi Liu (Guangdong General Hospital) · Le Lu (Alibaba Group) · Ling Zhang (Alibaba Group) · Minfeng Xu (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Beyond Textual Constraints: Learning Novel Diffusion Conditions with Fewer Examples</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuyang Yu (South China University of Technology) · Bangzhen Liu (South China University of Technology) · Chenxi Zheng (None) · Xuemiao Xu (South China University of Technology) · Huaidong Zhang (South China University of Technology) · Shengfeng He (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning from Synthetic Human Group Activities</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Che-Jui Chang (Rutgers University) · Danrui Li (Rutgers University) · Deep Patel (NEC Laboratories America) · Parth Goel (Oracle) · Seonghyeon Moon (Roblox) · Samuel Sohn (Rutgers University) · Honglu Zhou (Rutgers University) · Sejong Yoon (The College of New Jersey) · Vladimir Pavlovic (Rutgers University) · Mubbasir Kapadia (Rutgers University )
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Can’t make an Omelette without Breaking some Eggs: Plausible Action Anticipation using Large Video-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Himangi Mittal (Carnegie Mellon University) · Nakul Agarwal (None) · Shao-Yuan Lo (Johns Hopkins University) · Kwonjoon Lee (Honda Research Institute USA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FreeKD: Knowledge Distillation via Semantic Frequency Prompt</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Zhang (Peking University) · Tao Huang (The University of Sydney) · Jiaming Liu (Peking University) · Tao Jiang (Zhejiang University) · Kuan Cheng (Peking University) · Shanghang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongchi Xia (Shanghai Jiaotong University) · Chih-Hao Lin (None) · Wei-Chiu Ma (Cornell University) · Shenlong Wang (University of Illinois, Urbana Champaign)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qian Wang (Peking University) · Weiqi Li (Peking University) · Chong Mou (Peking University) · Xinhua Cheng (Peking University) · Jian Zhang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Modal Proxy Learning Towards Personalized Visual Multiple Clustering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Yao (University of Washington) · Qi Qian (Alibaba Group) · Juhua Hu (University of Washington)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ContextSeg: Sketch Semantic Segmentation by Querying the Context with Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawei Wang (Shandong University) · Changjian Li (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyan Wang (University of New South Wales) · Zhenhong Sun (University of New South Wales) · Stewart Tan (Alibaba DAMO Academy) · Xuanbai Chen (Carnegie Mellon University) · Weihua Chen (Alibaba Group) · li (None) · Cheng Zhang (Carnegie Mellon University) · Yang Song (University of New South Wales)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adaptive Bidirectional Displacement for Semi-Supervised Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanyang Chi (None) · Jian Pang (China University of Petroleum (East China)) · Bingfeng Zhang (China University of Petroleum (East China)) · Weifeng Liu (China University of Petroleum (East China))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Targeted Representation Alignment for Open-World Semi-Supervised Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruixuan Xiao (Zhejiang University) · Lei Feng (Nanyang Technological University) · Kai Tang (Zhejiang University) · Junbo Zhao (Zhejiang University) · Yixuan Li (University of Wisconsin Madison) · Gang Chen (College of Computer Science and Technology, Zhejiang University) · Haobo Wang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Contrasting intra-modal and ranking cross-modal hard negatives to enhance visio-linguistic compositional understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Le Zhang (Mila-Quebec AI Institute) · Rabiul Awal (None) · Aishwarya Agrawal (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://thaoshibe.github.io/edit-one-for-all" target="_blank">Edit One for All: Interactive Batch Image Editing</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Thao Nguyen (UW-Madison 🦡) · Utkarsh Ojha (University of Wisconsin - Madison) · Yuheng Li (University of Wisconsin - Madison) · Haotian Liu (University of Wisconsin-Madison) · Yong Jae Lee (Department of Computer Sciences, University of Wisconsin - Madison)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Genuine Knowledge from Practice: Diffusion Test-Time Adaptation for Video Adverse Weather Removal</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yijun Yang (None) · Hongtao Wu (The Hong Kong University of Science and Technology (Guangzhou)) · Angelica I. Aviles-Rivero (University of Cambridge) · Yulun Zhang (Shanghai Jiao Tong University) · Jing Qin (Hong Kong Polytechnic University) · Lei Zhu (Hong Kong University of Science and Technology (Guangzhou) &amp; HKUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeyuan Yang (, Tsinghua University) · LIU JIAGENG (None) · Peihao Chen (South China University of Technology) · Anoop Cherian (Mitsubishi Electric Research Labs (MERL)) · Tim Marks (None) · Jonathan Le Roux (Mitsubishi Electric Research Labs) · Chuang Gan (MIT-IBM Watson AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FlashAvatar: High-fidelity Head Avatar with Efficient Gaussian Embedding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jun Xiang (University of Science and Technology of China) · Xuan Gao (University of Science and Technology of China) · Yudong Guo (Image Derivative Inc) · Juyong Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Driving-Video Dehazing with Non-Aligned Regularization for Safety Assistance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junkai Fan (Nanjing University of Science and Technology) · Jiangwei Weng (Nanjing University of Science and Technology) · Kun Wang (Nanjing University of Science and Technology) · Yijun Yang (None) · Jianjun Qian (Nanjing University of Science and Techonology) · Jun Li (Nanjing University of Science and Technology) · Jian Yang (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rich Human Feedback for Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youwei Liang (University of California, San Diego) · Junfeng He (Google) · Gang Li (Google) · Peizhao Li (GE HealthCare) · Arseniy Klimovskiy (Google) · Nicholas Carolan (Google) · Jiao Sun (University of Southern California) · Jordi Pont-Tuset (Google Research) · Sarah Young (Google) · Feng Yang (Google Research) · Junjie Ke (None) · Krishnamurthy Dvijotham (Google DeepMind) · Katherine Collins (University of Cambridge) · Yiwen Luo (Research, Google) · Yang Li (Google) · Kai Kohlhoff (Google Research) · Deepak Ramachandran (Google) · Vidhya Navalpakkam (Research, Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaben Chen (University of California, San Diego) · Huaizu Jiang (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Sharingan: A Transformer Architecture for Multi-Person Gaze Following</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Samy Tafasca (EPFL) · Anshul Gupta (None) · Jean-marc Odobez (Swiss Federal Institute of Technology Lausanne)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DAP: A Dynamic Adversarial Patch for Evading Person Detectors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Amira Guesmi (New York University, Abu Dhabi) · Ruitian Ding (New York University) · Muhammad Abdullah Hanif (New York University, Abu Dhabi) · Ihsen Alouani (The Queen's University Belfast) · Muhammad Shafique (New York University Abu Dhabi)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Exploring Region-Word Alignment in Built-in Detector for Open-Vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Heng Zhang (Gaoling School of Artificial Intelligence, Renmin University of China) · Qiuyu Zhao (JD) · Linyu Zheng (JD) · Hao Zeng (JD.com) · Zhiwei Ge (JD) · Tianhao Li (JD) · Sulong Xu (JD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generative 3D Part Assembly via Part-Whole-Hierarchy Message Passing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bi'an Du (None) · Xiang Gao (Peking University) · Wei Hu (None) · Renjie Liao (University of British Columbia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>T4P: Test-Time Training of Trajectory Prediction via Masked Autoencoder and Actor-specific Token Memory</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daehee Park (KAIST) · Jaeseok Jeong (KAIST) · Sung-Hoon Yoon (KAIST) · Jaewoo Jeong (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dynamic Support Information Mining for Category-Agnostic Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pengfei Ren (Beijing University of Posts and Telecommunications) · Yuanyuan Gao (Beijing University of Posts and Telecommunications) · Haifeng Sun (Beijing University of Posts and Telecommunications) · Qi Qi (Beijing University of Posts and Telecommunications) · Jingyu Wang (Beijing University of Post and Telecommunication, Tsinghua University) · Jianxin Liao (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/matteo-bastico/CoupLap" target="_blank">Coupled Laplacian Eigenmaps for Locally-Aware 3D Rigid Point Cloud Matching</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Matteo Bastico (Mines Paris - PSL) · Etienne Decencière (Mines Paris) · Laurent Corté (Mines ParisTech) · Yannick TILLIER (Mines Paris - PSL) · David Ryckelynck (Mines Paris PSL University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Orthogonal Adaptation for Modular Customization of Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ryan Po (Stanford University) · Guandao Yang (None) · Kfir Aberman (Google) · Gordon Wetzstein (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MART: Masked Affective RepresenTation Learning via Masked Temporal Distribution Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhicheng Zhang (Nankai University) · Pancheng Zhao (Nankai University) · Eunil Park (Sungkyunkwan University) · Jufeng Yang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Context-based and Diversity-driven Specificity in Compositional Zero-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yun Li (CSIRO's Data61) · Zhe Liu (Tiktok) · Hang Chen (Snap Inc.) · Lina Yao (CSIRO's Data61 and University of New South Wales)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lorenzo Bianchi (CNR-ISTI) · Fabio Carrara (CNR-ISTI) · Nicola Messina (Institute of Information Science and Technologies - National Research Council (ISTI-CNR)) · Claudio Gennaro (CNR) · Fabrizio Falchi (CNR)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/SerCharles/CN-RMA" target="_blank">CN-RMA: Combined Network with Ray Marching Aggregation for 3D Indoor Object Detection from Multi-view Images</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guanlin Shen (Tsinghua University) · Jingwei Huang (Huawei Technologies Ltd.) · Zhihua Hu (Nanjing University of Information Science and Technology) · Bin Wang (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yining Hong () · Zishuo Zheng (None) · Peihao Chen (South China University of Technology) · Yian Wang (Department of Computer Science, University of Massachusetts at Amherst) · Junyan Li (Zhejiang University) · Chuang Gan (MIT-IBM Watson AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Clustering based Visual Representation Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guikun Chen (Zhejiang University) · Xia Li (Department of Computer Science, ETH Zurich) · Yi Yang (Zhejiang University) · Wenguan Wang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jeong-gi Kwak (Korea University) · Erqun Dong (University of British Columbia) · Yuhe Jin (University of British Columbia) · Hanseok Ko (Korea University) · Shweta Mahajan (University of British Columbia) · Kwang Moo Yi (University Of British Columbia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BEHAVIOR Vision Suite: Customizable Dataset Generation via Simulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhao Ge (University of Southern California) · Yihe Tang (Stanford University) · Jiashu Xu (University of Southern California) · Cem Gokmen (Stanford University) · Chengshu Li (Stanford University) · Wensi Ai (Stanford University) · Benjamin Martinez (Stanford University) · Arman Aydin (Stanford University) · Mona Anvari (Computer Science Department, Stanford University) · Ayush Chakravarthy (Stanford University) · Hong-Xing Yu (Computer Science Department, Stanford University) · Josiah Wong (Stanford University) · Sanjana Srivastava (Stanford University) · Sharon Lee (Stanford University) · Shengxin Zha (Meta GenAI) · Laurent Itti (USC) · Yunzhu Li (University of Illinois Urbana-Champaign) · Roberto Martín-Martín (University of Texas at Austin) · Miao Liu (META AI) · Pengchuan Zhang (Meta AI) · Ruohan Zhang (Stanford University) · Li Fei-Fei (Stanford University) · Jiajun Wu (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CDMAD: Class-Distribution-Mismatch-Aware Debiasing for Class-Imbalanced Semi-Supervised Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyuck Lee (Korea Advanced Institute of Science and Technology) · Heeyoung Kim (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NeRF Director: Revisiting View Selection in Neural Volume Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenhui Xiao (Queensland University of Technology) · Rodrigo Santa Cruz (CSIRO) · David Ahmedt-Aristizabal (CSIRO) · Olivier Salvado (CSIRO) · Clinton Fookes (Queensland University of Technology) · Leo Lebrat (CSIRO / QUT)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Map-Relative Pose Regression for Visual Re-Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuai Chen (University of Oxford) · Tommaso Cavallari (Niantic Inc.) · Victor Adrian Prisacariu (None) · Eric Brachmann (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MANUS: Markerless Grasp Capture using Articulated 3D Gaussians</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chandradeep Pokhariya (International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad) · Ishaan Shah (International Institute of Information Technology, Hyderabad, International Institute of Information Technology Hyderabad) · Angela Xing (Brown University) · Zekun Li (Tencent AI Lab) · Kefan Chen (Brown University) · Avinash Sharma (International Institute of Information Technology Hyderabad) · Srinath Sridhar (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bridging the Gap: A Unified Video Comprehension Framework for Moment Retrieval and Highlight Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yicheng Xiao (Tsinghua University, Tsinghua University) · Zhuoyan Luo (Tsinghua University) · Yong Liu (None) · Yue Ma (Tsinghua University, Tsinghua University) · Hengwei Bian (Carnegie Mellon University) · Yatai Ji (None) · Yujiu Yang (Tsinghua University) · Xiu Li (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ChAda-ViT : Channel Adaptive Attention for Joint Representation Learning of Heterogeneous Microscopy Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nicolas Bourriez (Ecole Normale Supérieure de Paris) · Ihab Bendidi (Ecole Normale Superieure) · Cohen Ethan (Ecole Normale Supérieure de Paris) · Gabriel Watkinson (Ecole Normale Supérieure de Paris) · Maxime Sanchez (IBENS) · Guillaume Bollot (Synsight company) · Auguste Genovesio (Ecole Normale Supérieure de Paris)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HIR-Diff: Unsupervised Hyperspectral Image Restoration Via Improved Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Pang (Xi'an Jiaotong University) · Xiangyu Rui (Xi'an Jiaotong University) · Long Cui (Xi'an Jiaotong University) · Hongzhong Wang (Xi'an Jiaotong University) · Deyu Meng () · Xiangyong Cao (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Generalizable Tumor Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qi Chen (University of Science and Technology of China) · Xiaoxi Chen (None) · Haorui Song (Johns Hopkins University) · Alan L. Yuille (Johns Hopkins University) · Zhiwei Xiong (None) · Chen Wei (Johns Hopkins University) · Zongwei Zhou (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sherwin Bahmani (None) · Ivan Skorokhodov (KAUST) · Victor Rong (University of Toronto) · Gordon Wetzstein (Stanford University) · Leonidas Guibas (Stanford University) · Peter Wonka (KAUST) · Sergey Tulyakov (Snap Inc.) · Jeong Joon Park (Stanford University) · Andrea Tagliasacchi (Simon Fraser University, Google Brain) · David B. Lindell (University of Toronto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diversified and Personalized Multi-rater Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yicheng Wu (Monash University) · Xiangde Luo (University of Electronic Science and Technology of China) · Zhe Xu (The Chinese University of Hong Kong; Harvard Medical School) · Xiaoqing Guo (University of Oxford, University of Oxford) · Lie Ju (Monash University) · Zongyuan Ge (Monash University) · Wenjun Liao (University of Electronic Science and Technology of China) · Jianfei Cai (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ANIM: Accurate Neural Implicit Model for Human Reconstruction from a single RGB-D image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Marco Pesavento (University of Surrey) · Yuanlu Xu (Meta Reality Labs Research) · Nikolaos Sarafianos (Meta Reality Labs) · Robert Maier (Meta) · Ziyan Wang (Carnegie Mellon University) · Chun-Han Yao (University of California at Merced) · Marco Volino (University of Surrey) · Edmond Boyer (INRIA Grenoble Rhône-Alpes) · Adrian Hilton (University of  Surrey) · Tony Tung (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VicTR: Video-conditioned Text Representations for Activity Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kumara Kahatapitiya (Stony Brook University) · Anurag Arnab (Google) · Arsha Nagrani (Google ) · Michael Ryoo (Stony Brook University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/Pointcept/PointTransformerV3" target="_blank">Point Transformer V3: Simpler, Faster, Stronger</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyang Wu (The University of Hong Kong) · Li Jiang (Max Planck Institute for Informatics) · Peng-Shuai Wang (Peking University) · Zhijian Liu (Massachusetts Institute of Technology) · Xihui Liu (The University of Hong Kong) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Wanli Ouyang (University of Sydney) · Tong He (Shanghai AI Lab) · Hengshuang Zhao (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Bi-SSC: Geometric-Semantic Bidirectional Fusion for Camera-based 3D Semantic Scene Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujie Xue (HNU) · Ruihui Li (Hunan University) · F anWu (Wuhan University) · Zhuo Tang (Hunan University) · Kenli Li (Hunan University) · Duan Mingxing (Hunan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FC-GNN: Recovering Reliable and Accurate Correspondences from Interferences</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haobo Xu (None) · Jun Zhou (Shanghai Jiaotong University) · Hua Yang (Shanghai Jiaotong University) · Renjie Pan (Shanghai Jiaotong University) · Cunyan Li (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Gradient-based Parameter Selection for Efficient Fine-Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhi Zhang (Institute for Logic, Language and Computation, University of Amsterdam) · Qizhe Zhang (Peking University) · Zijun Gao (Shandong University) · Renrui Zhang (MMLab of CUHK &amp;amp;amp; Shanghai AI Laboratory) · Ekaterina Shutova (University of Amsterdam) · Shiji Zhou (Tsinghua University, Tsinghua University) · Shanghang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UniRepLKNet: A Universal Perception Large-Kernel ConvNet for Audio, Video, Point Cloud, Time-Series and Image Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaohan Ding (Tencent AI Lab) · Yiyuan Zhang (The Chinese University of Hong Kong) · Yixiao Ge (Tencent) · Sijie Zhao (Tencent AI Lab) · Lin Song (Tencent AI Lab) · Xiangyu Yue (The Chinese University of Hong Kong) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mining Supervision for Dynamic Regions in Self-Supervised Monocular Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hoang Chuong Nguyen (Australian National University) · Tianyu Wang (Australian National University) · Jose M. Alvarez (NVIDIA) · Miaomiao Liu (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiran Qin (The Chinese University of Hong Kong(Shenzhen)) · Enshen Zhou (Shanghai AI Laboratory) · Qichang Liu (None) · Zhenfei Yin (University of Sydney) · Lu Sheng (Beihang University) · Ruimao Zhang (The Chinese University of Hong Kong (Shenzhen)) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Jing Shao (Shanghai AI Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Stationary Representations: Optimally Approximating Compatibility and Implications for Improved Model Replacements</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Niccolò Biondi (University of Florence, Italy) · Federico Pernici (University of Florence, Italy) · Simone Ricci (University of Florence, Italy) · Alberto Del Bimbo (Università degli Studi di Firenze)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A General and Efficient Training for Transformer via Token Expansion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenxuan Huang (East China Normal University) · Yunhang Shen (Tencent) · Jiao Xie (Xiamen University) · Baochang Zhang (Beihang University) · Gaoqi He (East China Normal University) · Ke Li (Tencent) · Xing Sun (Tencent YouTu Lab) · Shaohui Lin (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Li (Tencent Youtu Lab) · Yunfei Wu (Tencent YouTu Lab) · Xinghua Jiang (None) · ZhiHao Guo (Tencent YOUTU Lab) · Mingming Gong (Tencent YouTu Lab) · Haoyu Cao (Tencent Youtu Lab) · Yinsong Liu (Tencent Youtu Lab) · Deqiang Jiang (Tencent YouTu Lab) · Xing Sun (Tencent YouTu Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Language-Driven Anchors for Zero-Shot Adversarial Robustness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao Li (Tsinghua University) · Wei Zhang (Department of Computer Science and Technology, Tsinghua University) · Yining Liu (Harbin Institute of Technology at Weihai) · Zhanhao Hu (UC Berkeley) · Bo Zhang (Tsinghua University, Tsinghua University) · Xiaolin Hu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Vision from Models Rivals Learning Vision from Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yonglong Tian (Google) · Lijie Fan (Massachusetts Institute of Technology) · Kaifeng Chen (Google) · Dina Katabi (Massachusetts Institute of Technology) · Dilip Krishnan (Google) · Phillip Isola (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Byzantine-robust Decentralized Federated Learning via Dual-domain Clustering and Trust Bootstrapping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Sun (Hunan University) · Xinyang Liu (Hong Kong Polytechnic University) · Zhibo Wang (Zhejiang University) · Bo Liu (Shenzhen Institute of Artificial Intelligence and Robotics for Society)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MotionEditor: Editing Video Motion via Content-Aware Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuyuan Tu (Fudan University) · Qi Dai (Microsoft Research Asia) · Zhi-Qi Cheng (Carnegie Mellon University) · Han Hu (Microsft Research Asia) · Xintong Han (Huya Inc) · Zuxuan Wu (Fudan University) · Yu-Gang Jiang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EVS-assisted joint Deblurring, Rolling-Shutter Correction and Video Frame Interpolation through Sensor Inverse Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui Jiang (OMNIVISION) · Fangwen Tu (OMNIVISION) · Yixuan Long (OMNIVISION) · Aabhaas Vaish (OMNIVISION) · Bowen Zhou (OMNIVISION) · Qinyi Wang (OmniVision) · Wei Zhang (OMNIVISION) · Yuntan Fang (OMNIVISION) · Luis Eduardo García Capel (OMNIVISION) · Bo Mu (OMNIVISION) · Tiejun Dai (OMNIVISION) · Andreas Suess (OMNIVISION)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Open-World Semantic Segmentation Including Class Similarity</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Matteo Sodano (Institute of Photogrammetry and Robotics, University of Bonn (Germany)) · Federico Magistri (Rheinische Friedrich-Wilhelms Universität Bonn) · Lucas Nunes (University of Bonn) · Jens Behley (University of Bonn) · Cyrill Stachniss (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MindBridge: A Cross-Subject Brain Decoding Framework</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shizun Wang (National University of Singapore) · Songhua Liu (None) · Zhenxiong Tan (National University of Singapore) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Calibrated Multi-label Deep Neural Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiacheng Cheng (University of California, San Diego) · Nuno Vasconcelos (University of California San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Distilled Datamodel with Reverse Gradient Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingwen Ye (National University of Singapore) · Ruonan Yu (national university of singaore, National University of Singapore) · Songhua Liu (None) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NoiseCollage: A Layout-Aware Text-to-Image Diffusion Model Based on Noise Cropping and Merging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Takahiro Shirakawa (Kyushu University) · Seiichi Uchida (Kyushu University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OmniMedVQA: A New Large-Scale Comprehensive  Evaluation Benchmark for Medical LVLM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yutao Hu (University of Hong Kong) · Tianbin (None) · Quanfeng Lu (Shanghai AI Laboratory) · Wenqi Shao (The Chinese University of Hong Kong) · Junjun He (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Ping Luo (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Visual Anagrams: Synthesizing Multi-View Optical Illusions with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daniel Geng (University of Michigan) · Inbum Park (University of Michigan) · Andrew Owens (University of Michigan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Collaborating Foundation models for Domain Generalized Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yasser Benigmim (Telecom Paris) · Subhankar Roy (University of Aberdeen) · Slim Essid (Télécom Paris) · Vicky Kalogeiton (Ecole polytechnique, IP Paris) · Stéphane Lathuilière (Télécom ParisTech)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Attribute-Guided Pedestrian Retrieval: Bridging Person Re-ID with Internal Attribute Variability</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Huang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Qiang Wu (University of Technology Sydney) · yi zhong (Beijing Institute of Technology) · Liang Wang (CASIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Recipe for Scaling up Text-to-Video Generation with Text-free Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiang Wang (Huazhong University of Science and Technology) · Shiwei Zhang (Alibaba Group) · Hangjie Yuan (Zhejiang University) · Zhiwu Qing (Huazhong University of Science and Technology, Tsinghua University) · Biao Gong (Alibaba Group) · Yingya Zhang (Alibaba Group) · Yujun Shen (The Chinese University of Hong Kong) · Changxin Gao (Huazhong University of Science and Technology) · Nong Sang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DreamVideo: Composing Your Dream Videos with Customized Subject and Motion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujie Wei (Fudan University) · Shiwei Zhang (Alibaba Group) · Zhiwu Qing (Huazhong University of Science and Technology, Tsinghua University) · Hangjie Yuan (Zhejiang University) · Zhiheng Liu (University of Science and Technology of China) · Yu Liu (Alibaba Group) · Yingya Zhang (Alibaba Group) · Jingren Zhou (Alibaba Group) · Hongming Shan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>In2SET: Intra-Inter Similarity Exploiting Transformer for  Dual-Camera Compressive Hyperspectral Imaging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Wang (Beijing Institute of Technology) · Lizhi Wang (None) · Xiangtian Ma (Beijing Institute of Technology) · Maoqing Zhang (Beijing Institute of Technoloy) · Lin Zhu (Beijing Institute of Technology) · Hua Huang (Beijing Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MPOD123: One Image to 3D Content Generation Using Mask-enhanced Progressive Outline-to-Detail Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jimin Xu (Zhejiang University) · Tianbao Wang (Zhejiang University) · Tao Jin (Zhejiang University) · Shengyu Zhang (Zhejiang University) · Dongjie Fu (Zhejiang University) · Zhe Wang (Alibaba) · Jiangjing Lyu (None) · Chengfei Lv (Zhejiang University) · Chaoyue Niu (Shanghai Jiaotong University) · Zhou Yu (Hangzhou Dianzi University) · Zhou Zhao (Zhejiang University, Tsinghua University) · Fei Wu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/wang-zidu/3DDFA-V3" target="_blank">3D Face Reconstruction with the Geometric Guidance of Facial Part Segmentation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zidu Wang (Institute of automation, Chinese Academy of Sciences) · Xiangyu Zhu (None) · Tianshuo Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · baiqin wang (None) · Zhen Lei (Institute of Automation,  Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HanDiffuser: Text-to-Image Generation With Realistic Hand Appearances</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Supreeth Narasimhaswamy (Stony Brook University, New York) · Uttaran Bhattacharya (Adobe Inc.) · Xiang Chen (Adobe Research) · Ishita Dasgupta (Department of Computer Science, University of Massachusetts at Amherst) · Saayan Mitra (Adobe Research) · Minh Hoai (State University of New York, Stony Brook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DL3DV-10K: A Large-Scale Scene Dataset for Deep Learning-based 3D Vision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lu Ling (Purdue University) · Yichen Sheng (Purdue University) · Zhi Tu (Purdue University) · Wentian Zhao (Adobe Systems) · Cheng Xin (Rutgers University) · Kun Wan (Adobe Inc.) · Lantao Yu (Adobe Inc.) · Qianyu Guo (None) · Zixun Yu (Purdue University) · Yawen Lu (Purdue University) · Xuanmao Li (Huazhong University of Science and Technology) · Xingpeng Sun (Purdue University) · Rohan Ashok (Purdue University) · Aniruddha Mukherjee (Purdue University) · Hao Kang (Wormpex AI Research) · Xiangrui Kong (Purdue University) · Gang Hua (Wormpex AI Research) · Tianyi Zhang (Purdue University) · Bedrich Benes (Purdue University) · Aniket Bera (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OCAI: Improving Optical Flow Estimation by Occlusion and Consistency Aware Interpolation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jisoo Jeong (Qualcomm AI Research) · Hong Cai (Qualcomm AI Research) · Risheek Garrepalli (Qualcomm Inc, QualComm) · Jamie Lin (Qualcomm) · Munawar Hayat (Monash University) · Fatih Porikli (QualComm)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Open-Vocabulary Attention Maps with Token Optimization for Semantic Segmentation in Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pablo Marcos-Manchón (University of Barcelona) · Roberto Alcover-Couso (Universidad Autónoma de Madrid) · Juan SanMiguel (Universidad Autónoma de Madrid) · Jose M. Martinez (Universidad Autónoma de Madrid)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://airlabkhu.github.io/A2XP/" target="_blank">A2XP: Towards Private Domain Generalization</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Geunhyeok Yu (Kyung Hee University) · Hyoseok Hwang (Kyung Hee University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>BoQ: A Place is Worth a Bag of learnable Queries</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Amar Ali-bey (Université Laval) · Brahim Chaib-draa (Laval university) · Philippe Giguère (Université Laval)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://plustwo0.github.io/project-face-landmarker/" target="_blank">Generalizable Face Landmarking Guided by Conditional Face Warping</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiayi Liang (Beijing Institute of Technology) · Haotian Liu (Beijing Institute of Technology) · Hongteng Xu (Renmin University of China) · Dixin Luo (Beijing Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Inhee Lee (Seoul National University) · Byungjun Kim (Seoul National University) · Hanbyul Joo (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FreeCustom: Tuning-Free Customized Image Generation for Multi-Concept Composition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ganggui Ding (Zhejiang University) · Canyu Zhao (Zhejiang University) · Wen Wang (Zhejiang University) · Zhen Yang (Zhejiang University) · Zide Liu (Zhejiang University) · Hao Chen (Zhejiang University) · Chunhua Shen (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Semantic Human Mesh Reconstruction with Textures</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            xiaoyu zhan (Nanjing University) · Jianxin Yang (nanjing university) · Yuanqi Li (Nanjing University) · Jie Guo (Nanjing University) · Yanwen Guo (Nanjing University) · Wenping Wang (Texas A&amp;M University - College Station)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RoDLA: Benchmarking the Robustness of Document Layout Analysis Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufan Chen (Karlsruhe Institute of Technology (KIT)) · Jiaming Zhang (KIT) · Kunyu Peng (KIT) · Junwei Zheng (Karlsruhe Institute of Technology) · Ruiping Liu (Karlsruher Institut für Technologie) · Philip H.S. Torr (University of Oxford) · Rainer Stiefelhagen (Karlsruhe Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://zzcheng.top/ExtDM/" target="_blank">ExtDM: Distribution Extrapolation Diffusion Model for Video Prediction</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhicheng Zhang (Nankai University) · Junyao Hu (Nankai University) · Wentao Cheng (Nankai University) · Danda Paudel (INSAIT, Sofia University) · Jufeng Yang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TASeg: Temporal Aggregation Network for LiDAR Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaopei Wu (Zhejiang University) · Yuenan Hou (Shanghai AI Laboratory) · Xiaoshui Huang (Shanghai AI Laboratory) · Binbin Lin (Zhejiang University) · Tong He (Shanghai AI Lab) · Xinge Zhu (The Chinese University of Hong Kong) · Yuexin Ma (ShanghaiTech University) · Boxi Wu (Zhejiang University) · Haifeng Liu (Zhejiang University) · Deng Cai (Zhejiang University) · Wanli Ouyang (University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum Computing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jan-Nico Zaech (INSAIT Sofia, ETH Zürich) · Martin Danelljan (ETH Zurich) · Tolga Birdal (Imperial College London) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Robust Image Denoising through Adversarial Frequency Mixup</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Donghun Ryou (Seoul National University) · Inju Ha (Seoul National University) · Hyewon Yoo (Seoul National University) · Dongwan Kim (Seoul National University) · Bohyung Han (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu-Ying Yeh (University of California, San Diego) · Jia-Bin Huang (University of Maryland, College Park) · Changil Kim (Facebook) · Lei Xiao (None) · Thu Nguyen-Phuoc (Reality Labs Research, Meta) · Numair Khan (None) · Cheng Zhang (Facebook) · Manmohan Chandraker (UC San Diego) · Carl Marshall (Reality Labs Research) · Zhao Dong (Meta RL Research) · Zhengqin Li (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SC-GS: Sparse-Controlled Gaussian Splatting for Editable Dynamic Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yihua Huang (None) · Yangtian Sun (None) · Ziyi Yang (None) · Xiaoyang Lyu (University of Hong Kong) · Yan-Pei Cao (Tencent ARC Lab) · Xiaojuan Qi (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Occupancy for Monocular 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liang Peng (FABU Inc) · Junkai Xu (Zhejiang University) · Haoran Cheng (College of Computer Science and Technology, Zhejiang University) · Zheng Yang (Fabu Inc) · Xiaopei Wu (Zhejiang University) · Wei Qian (Fabu Inc.) · Wenxiao Wang (Zhejiang University) · Boxi Wu (Zhejiang University) · Deng Cai (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SeeSR: Towards Semantics-Aware Real-World Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rongyuan Wu (Hong Kong Polytechnic University) · Tao Yang (Tsinghua University, Tsinghua University) · Lingchen Sun (Hong Kong Polytechnic University) · Zhengqiang ZHANG (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) · Shuai Li (The Hong Kong Polytechnic University) · Lei Zhang (The Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PEGASUS: Personalized Generative 3D Avatars with Composable Attributes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyunsoo Cha (Seoul National University) · Byungjun Kim (Seoul National University) · Hanbyul Joo (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://lia-ditella.github.io/DUDF/" target="_blank">DUDF: Differentiable Unsigned Distance Fields with Hyperbolic Scaling</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Miguel Fainstein (Universidad de Buenos Aires) · Viviana Siless (Universidad Torcuato di Tella) · Emmanuel Iarussi (Universidad Torcuato di Tella / Conicet)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Meta-Point Learning and Refining for Category-Agnostic Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junjie Chen (Jiangxi University of Finance and Economics) · Jiebin Yan (Jiangxi University of Finance and Economics) · Yuming Fang (Jiangxi University of Finance and Economics) · Li Niu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Not All Prompts Are Secure: A Switchable Backdoor Attack Against Pre-trained Vision Transfomers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sheng Yang () · Jiawang Bai (None) · Kuofeng Gao (Tsinghua University, Tsinghua University) · Yong Yang (Tencent Security) · Yiming Li (Zhejiang University) · Shu-Tao Xia (Shenzhen International Graduate School, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Unified Framework for Human-centric Point Cloud Video Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiteng Xu () · Kecheng Ye (None) · xiao han (ShanghaiTech University) · yiming ren (None) · Xinge Zhu (The Chinese University of Hong Kong) · Yuexin Ma (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://zhangce01.github.io/HiKER-SGG/" target="_blank">HiKER-SGG: Hierarchical Knowledge Enhanced Robust Scene Graph Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ce Zhang (Carnegie Mellon University) · Simon Stepputtis (Carnegie Mellon University) · Joseph Campbell (Carnegie Mellon University) · Katia Sycara (Carnegie Mellon University) · Yaqi Xie (CMU)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zifan Wang (Tsinghua University) · Junyu Chen (Tsinghua University, Tsinghua University) · Ziqing Chen (Tsinghua University, Tsinghua University) · Pengwei Xie (Electronic Engineering, Tsinghua University, Tsinghua University) · Rui Chen (Tsinghua University, Tsinghua University) · Li Yi ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffSCI: Zero-Shot Snapshot Compressive Imaging via Iterative Spectral Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenghao Pan (Harbin Institute of Technology (Shenzhen)) · Haijin Zeng (IMEC &amp; Universiteit Gent) · Jiezhang Cao (ETH Zürich) · Kai Zhang (None) · Yongyong Chen (Harbin Institute of Technology (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Relightable Gaussian Codec Avatars</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shunsuke Saito (Reality Labs Research) · Gabriel Schwartz (Meta) · Tomas Simon (Meta) · Junxuan Li (Meta Reality Labs) · Giljoo Nam (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Misalignment-Robust Frequency Distribution Loss for Image Transformation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhangkai Ni (Tongji University) · Juncheng Wu (Tongji University) · Zian Wang (Tongji University) · Wenhan Yang (Peng Cheng Lab) · Hanli Wang (Tongji University) · Lin Ma (Meituan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WildlifeMapper:  Aerial Image Analysis for Multi-Species Detection and Identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Satish Kumar (None) · Bowen Zhang (University of California, Santa Barbara) · Chandrakanth Gudavalli (University of California, Santa Barbara) · Connor Levenson (University of California, Santa Barbara) · Lacey Hughey (Smithsonian National Zoo and Conservation Biology Institute) · Jared Stabach (Smithsonian Conservation Biology Institute) · Irene Amoke (Kenya Wildlife Trust) · Gordon Ojwang (University of Groningen) · Joseph Mukeka (Wildlife Reserach and Training Institute) · Howard Frederick (Tanzania Wildlife Research Institute) · Stephen Mwiu (Wildlife Research and Training Institute) · Joseph Ochieng Ogutu (Universität Hohenheim) · B S Manjunath (UC Santa Barbara)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Pre-training Vision Models with Mandelbulb Variations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Benjamin N. Chiche (Rist Inc.) · Yuto Horikawa (Rist) · Ryo Fujita (Kyoto University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Luo Jiayun (Nanyang Technological University) · Siddhesh Khandelwal (None) · Leonid Sigal (University Of British Columbia) · Boyang Li (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SelfPose3d: Self-Supervised Multi-Person Multi-View 3d Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Keqi Chen (University of Strasbourg) · vinkle srivastav (University of Strasbourg) · Nicolas Padoy (University of Strasbourg)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Context-Aware Integration of Language and Visual References for Natural Language Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanyan Shao (None) · Shuting He (Nanyang Technological University) · Qi Ye (Zhejiang University) · Yuchao Feng (None) · Wenhan Luo (SUN YAT-SEN UNIVERSITY) · Jiming Chen (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Directional Encoding for Efficient and Accurate View-Dependent Appearance Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liwen Wu (Computer Science and Engineering Department, University of California, San Diego) · Sai Bi (Adobe Systems) · Zexiang Xu (Adobe Research) · Fujun Luan (Adobe Systems) · Kai Zhang (Adobe Systems) · Iliyan Georgiev (Adobe) · Kalyan Sunkavalli (Adobe Research) · Ravi Ramamoorthi (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OAKINK2: A Dataset of Bimanual Hands-Object Manipulation in Complex Task Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyu Zhan (Shanghai Jiaotong University) · Lixin Yang (Shanghai Jiao Tong University) · Yifei Zhao (Shanghai Jiaotong University) · Kangrui Mao (Shanghai Jiao Tong University) · Hanlin Xu (Shanghai Jiaotong University) · Zenan Lin (South China University of Technology) · Kailin Li (Shanghai Jiaotong University) · Cewu Lu (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Video is Worth 256 Bases: Spatial-Temporal Expectation-Maximization Inversion for Zero-Shot Video Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Maomao (The University of HongKong) · Yu Li (International Digital Economy Academy) · Tianyu Yang (IDEA) · Yunfei Liu (International Digital Economy Academy (IDEA)) · Dongxu Yue (Peking University) · Zhihui Lin (Xverse) · Dong Xu (University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://kwanyun.github.io/lego/" target="_blank">LeGO: Leveraging a Surface Deformation Network for Animatable Stylized Face Generation with One Example</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Soyeon Yoon (Korea Advanced Institute of Science &amp;amp;amp; Technology) · Kwan Yun (Korea Advanced Institute of Science &amp; Technology) · Kwanggyoon Seo (KAIST) · Sihun Cha (Korea Advanced Institute of Science and Technology) · Jung Eun Yoo (Korea Advanced Institute of Science &amp; Technology) · Junyong Noh (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ReGenNet: Towards Human Action-Reaction Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liang Xu (Shanghai Jiao Tong University) · Yizhou Zhou (WeChat AI) · Yichao Yan (Shanghai Jiao Tong University) · Xin Jin (Eastern Institute of Technology, Ningbo) · Wenhan Zhu (None) · Fengyun Rao (WeChat, Tencent Inc.) · Xiaokang Yang (Shanghai Jiao Tong University, China) · Wenjun Zeng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adapting Visual-Language Models for Generalizable Anomaly Detection in Medical Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaoqin Huang (Shanghai Jiao Tong University) · Aofan Jiang (Shanghai Jiao Tong University) · Jinghao Feng (Shanghai Jiao Tong University) · Ya Zhang (Shanghai Jiao Tong University) · Xinchao Wang (National University of Singapore) · Yanfeng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GSNeRF: Generalizable Semantic Neural Radiance Fields with Enhanced 3D Scene Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zi-Ting Chou (National Taiwan University) · Sheng-Yu Huang (National Taiwan University) · I-Jieh Liu (National Taiwan University) · Yu-Chiang Frank Wang (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LTA-PCS: Learnable Task-Agnostic Point Cloud Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaheng Liu (Beihang University) · Jianhao Li (Beihang University) · Kaisiyuan Wang (University of Sydney) · Hongcheng Guo (Beihang University) · Jian Yang (Alibaba Group) · Junran Peng (Institute of automation, Chinese academy of science) · Ke Xu (Beijing University of Aeronautics and Astronautics) · Xianglong Liu (BUAA) · Jinyang Guo (Beijing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Breathing Life Into Sketches Using Text-to-Video Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rinon Gal (Tel Aviv University, NVIDIA) · Yael Vinker (Tel Aviv University) · Yuval Alaluf (Tel Aviv University) · Amit H. Bermano (Tel Aviv University, Technion) · Daniel Cohen-Or (Google) · Ariel Shamir (Reichman University) · Gal Chechik (NVIDIA, Bar-Ilan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffForensics: Leveraging Diffusion Prior to Image Forgery Detection and Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeqin Yu (Sun Yat-Sen University) · Jiangqun Ni (Sun Yat-Sen University) · Yuzhen Lin (Shenzhen University) · Haoyi Deng (Shenzhen University) · Bin Li (Shenzhen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Initialization Matters for Adversarial Transfer Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andong Hua (University of California, Santa Barbara) · Jindong Gu (University of Oxford &amp; Google Research) · Zhiyu Xue (University of California, Santa Barbara) · Nicholas Carlini (None) · Eric Wong (University of Pennsylvania) · Yao Qin (University of California, Santa Barbara)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Self-Discovering Interpretable Diffusion Latent Directions for Responsible Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hang Li (University of Munich) · Chengzhi Shen (Technische Universität München) · Philip H.S. Torr (University of Oxford) · Volker Tresp (Ludwig-Maximilians-Universität München) · Jindong Gu (University of Oxford &amp; Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Universal Segmentation at Arbitrary Granularity with Language Instruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yong Liu (None) · Cairong Zhang (ByteDance Inc.) · Yitong Wang (ByteDance Inc) · Jiahao Wang (Shanghai AI Lab) · Yujiu Yang (Tsinghua University) · Yansong Tang (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Sun (Zhejiang University &amp; Westlake University) · Bei Shi (Northwestern Polytechnical University, Northwest Polytechnical University Xi'an) · Daiwei Yu (Hangzhou City University) · Tao Lin (Westlake University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dr. Bokeh: DiffeRentiable Occlusion-aware Bokeh Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichen Sheng (Purdue University) · Zixun Yu (Purdue University) · Lu Ling (Purdue University) · Zhiwen Cao (Adobe Systems) · Xuaner Zhang (Adobe) · Xin Lu (Adobe Inc.) · Ke Xian (Nanyang Technological University) · Haiting Lin (Adobe Systems) · Bedrich Benes (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>BrainWash: A Poisoning Attack to Forget in Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ali Abbasi (Vanderbilt University) · Parsa Nooralinejad (University of California, Davis) · Hamed Pirsiavash (University of California, Davis) · Soheil Kolouri (Vanderbilt University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mehmet Saygin Seyfioglu (University of Washington) · Wisdom Ikezogwo (University of Washington) · Fatemeh Ghezloo (University of Washington) · Ranjay Krishna (University of Washington) · Linda Shapiro (UW Reality Lab University of Washington)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Segment and Caption Anything</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoke Huang (Shenzhen International Graduate School, Tsinghua University) · Jianfeng Wang (Microsoft) · Yansong Tang (Tsinghua University) · Zheng Zhang (Microsoft) · Han Hu (Microsft Research Asia) · Jiwen Lu (Tsinghua University) · Lijuan Wang (Microsoft) · Zicheng Liu (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Selective nonlinearities removal from digital signals</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Krzysztof Maliszewski (University of Canterbury) · Magdalena Urbanska (Massey University) · Varvara Vetrova (University of Canterbury) · Sylwia Kolenderska (University of Canterbury)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://song-jingyu.github.io/CRKD" target="_blank">CRKD: Enhanced Camera-Radar Object Detection with Cross-modality Knowledge Distillation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lingjun Zhao (University of Michigan - Ann Arbor) · Jingyu Song (University of Michigan - Ann Arbor) · Katherine Skinner (University of Michigan - Ann Arbor)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rotation-Agnostic Image Representation Learning for Digital Pathology</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Saghir Alfasly (Mayo Clinic) · Abubakr Shafique (Mayo Clinic) · Peyman Nejat (Mayo Clinic) · Jibran Khan (Luther College) · Areej Alsaafin (Mayo Clinic) · Ghazal Alabtah (Mayo Clinic) · Hamid Tizhoosh (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Lift3D: Zero-Shot Lifting of Any 2D Vision Model to 3D</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mukund Varma T (University of California, San Diego) · Peihao Wang (University of Texas, Austin) · Zhiwen Fan (University of Texas, Austin) · Zhangyang Wang (University of Texas at Austin) · Hao Su (UCSD) · Ravi Ramamoorthi (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>From a Bird’s Eye View to See: Joint Camera and Subject Registration without the Camera Calibration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zekun Qian (Tianjin University) · Ruize Han (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences) · Wei Feng (Tianjin University) · Song Wang (University of South Carolina)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Inversion-Free Image Editing with Language-Guided Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sihan Xu (University of Michigan - Ann Arbor) · Yidong Huang (University of Michigan - Ann Arbor) · Jiayi Pan (University of California, Berkeley) · Ziqiao Ma (University of Michigan) · Joyce Chai (University of Michigan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adaptive Fusion of Single-View and Multi-View Depth for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            JunDa Cheng (Huazhong University of Science and Technology) · Wei Yin ( Shenzhen DJI Sciences and Technologies Ltd.) · Kaixuan Wang (Hong Kong University of Science and Technology) · Xiaozhi Chen (DJI Innovations) · Shijie Wang (Huazhong University of Science and Technology) · Xin Yang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://self-correcting-llm-diffusion.github.io/" target="_blank">Self-correcting LLM-controlled Diffusion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tsung-Han Wu (University of California, Berkeley) · Long Lian (University of California, Berkeley) · Joseph Gonzalez (University of California - Berkeley) · Boyi Li (UC Berkeley / NVIDIA) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Active Open-Vocabulary Recognition: Let Intelligent Moving Mitigate CLIP Limitations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lei Fan (Northwestern University) · Jianxiong Zhou (Northwestern University) · Xiaoying Xing (Northwestern University) · Ying Wu (Northwestern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/zkawfanx/Atlantis" target="_blank">Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fan Zhang (Beijing Institute of Technology) · Shaodi You (Kyushu University) · Yu Li (International Digital Economy Academy) · Ying Fu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NeRF On-the-go: Exploiting Uncertainty for Distractor-free NeRFs in the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            weining ren (ETHz) · Zihan Zhu (ETHZ - ETH Zurich) · Boyang Sun (ETH Zurich) · Jiaqi Chen (ETHZ - ETH Zurich) · Marc Pollefeys (ETH Zurich / Microsoft) · Songyou Peng (ETH Zurich &amp; MPI Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/molden/atomlenz" target="_blank">Atom-Level Optical Chemical Structure Recognition with Limited Supervision</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Martijn Oldenhof (KU Leuven) · Edward De Brouwer (Yale University) · Adam Arany (KU Leuven) · Yves Moreau (University of Leuven)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Scalable 3D Registration via Truncated Entry-wise Absolute Residuals</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianyu Huang (The Chinese University of Hong Kong) · Liangzu Peng (Johns Hopkins University) · Rene Vidal (Johns Hopkins University) · Yun-Hui Liu (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Asymmetric Masked Distillation for Pre-Training Small Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyu Zhao (Nanjing University) · Bingkun Huang (Nanjing University) · Sen Xing (Tsinghua University, Tsinghua University) · Gangshan Wu (Nanjing University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Limin Wang (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Faces that Speak: Jointly Synthesising Talking Face and Speech from Text</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youngjoon Jang (Korea Advanced Institute of Science &amp; Technology) · Jihoon Kim (None) · Junseok Ahn (Korea Advanced Institute of Science and Technology) · Doyeop Kwak (Korea Advanced Institute of Science &amp; Technology) · Hongsun Yang (42dot) · Yooncheol Ju (42dot) · ILHWAN KIM (None) · Byeong-Yeol Kim (42dot) · Joon Chung (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generative Image Dynamics</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengqi Li (Google) · Richard Tucker (Google) · Noah Snavely (Google / Cornell) · Aleksander Holynski (UC Berkeley &amp; Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/bjzhb666/GS-LoRA" target="_blank">Continual Forgetting for Pre-trained Vision Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongbo Zhao (Institute of Automation, Chinese Academy of Sciences) · Bolin Ni (Institute of Automation, Chinese Academy of Sciences) · Junsong Fan (Centre for Artificial Intelligence and Robotics (CAIR) Hong Kong Institute of Science &amp; Innovation Chinese Academy of Sciences) · Yuxi Wang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Yuntao Chen (CAIR, HKISI, CAS) · Gaofeng Meng (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Distributionally Generative Augmentation for Fair Facial Attribute Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fengda Zhang (Zhejiang University) · Qianpei He (Zhejiang University) · Kun Kuang (Zhejiang University) · Jiashuo Liu (Tsinghua University, Tsinghua University) · Long Chen (HKUST) · Chao Wu (Zhejiang University) · Jun Xiao (Zhejiang University) · Hanwang Zhang (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CVT-xRF: Contrastive In-Voxel Transformer for 3D Consistent Radiance Fields from Sparse Inputs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yingji Zhong (None) · Lanqing Hong (Huawei Technologies Ltd.) · Zhenguo Li (Huawei) · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Adaptive Spatial Coherent Correlations for Speech-Preserving Facial Expression Manipulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianshui Chen (Guangdong University of Technology) · jianman lin (Guangdong University of Technology) · Zhijing Yang (Guangdong University of Technology) · Chunmei Qing (South China University of Technology) · Liang Lin (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Semi-supervised Nighttime Dehazing Baseline with Spatial-Frequency Aware and Realistic Brightness Constraint</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaofeng Cong (Southeast University) · Jie Gui (Southeast University) · Jing Zhang (The University of Sydney) · Junming Hou (Southeast University) · Hao Shen (Hefei University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bootstrapping SparseFormers from Vision Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziteng Gao (National University of Singapore) · Zhan Tong (Ant Group) · Kevin Qinghong Lin (national university of singaore, National University of Singapore) · Joya Chen (National University of Singapore) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>THRONE: A Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Prannay Kaul (University of Oxford, University of Oxford) · Zhizhong Li (Amazon) · Hao Yang (Amazon) · Yonatan Dukler (AWS AI) · Ashwin Swaminathan (University of Maryland, College Park) · CJ Taylor (Amazon AWS) · Stefano Soatto (AWS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Clockwork Diffusion: Efficient Generation With Model-Step Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Amirhossein Habibian (Qualcomm AI Research) · Amir Ghodrati (QualComm AI Research) · Noor Fathima (Qualcomm Inc, QualComm) · Guillaume Sautiere (Qualcomm Inc, QualComm) · Risheek Garrepalli (Qualcomm Inc, QualComm) · Fatih Porikli (QualComm) · Jens Petersen (Qualcomm AI Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huan Ling (Nvidia, University of Toronto) · Seung Wook Kim (NVIDIA) · Antonio Torralba (MIT) · Sanja Fidler (Department of Computer Science, University of Toronto) · Karsten Kreis (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Inlier Confidence Calibration for Point Cloud Registration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yongzhe Yuan (Xidian University) · Yue Wu (Xidian University) · Xiaolong Fan (Xidian University) · Maoguo Gong (Xidian University) · Qiguang Miao (Xidian University) · Wenping Ma (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Memory-Scalable and Simplified Functional Map Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Robin Magnet (École Polytechnique) · Maks Ovsjanikov (Ecole Polytechnique, France)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ADFactory: An Effective Framework for Generalizing Optical Flow with NeRF</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Han Ling (Nanjing University of Science and Technology) · Quansen Sun (Nanjing University of Science and Technology) · Yinghui Sun (Nanjing University of Science and Technology) · Xian Xu (Southeast Community College Area) · Xingfeng Li (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IReNe: Instant Recoloring of Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alessio Mazzucchelli (Arquimea Research Center) · Adrian Garcia-Garcia (Arquimea Research Center) · Elena Garces (Universidad Rey Juan Carlos) · Fernando Rivas-Manzaneque (None) · Francesc Moreno-Noguer (Universidad Politécnica de Cataluna) · Adrian Penate-Sanchez (Universidad de Las Palmas de Gran Canaria)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HardMo: A Large-Scale Hardcase Dataset for Motion Capture</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaqi Liao (None) · Chuanchen Luo (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Yinuo Du (Beijing University of Posts and Telecommunications) · Yuxi Wang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xu-Cheng Yin (University of Science and Technology Beijing) · Man Zhang (None) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Junran Peng (Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Xu (None) · Li Haipeng (None) · Yinqiao Wang (The Chinese University of Hong Kong) · Shuaicheng Liu (None) · Chi-Wing Fu (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>An Empirical Study of the Generalization Ability of Lidar 3D Object Detectors to Unseen Domains</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            George Eskandar (Universität Stuttgart)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Constrained Layout Generation with Factor Graphs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammed Haroon Dupty (National University of Singapore) · Yanfei Dong (PayPal Inc.) · Sicong Leng (Nanyang Technological University) · Guoji Fu (National University of Singapore) · Yong Liang Goh (National University of Singapore) · Wei Lu (Singapore University of Technology and Design) · Wee Sun Lee (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/Forrest-110/FastMAC" target="_blank">FastMAC: Stochastic Spectral Sampling of Correspondence Graph</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifei Zhang (University of Chinese Academy of Sciences) · Hao Zhao (Tsinghua University, Tsinghua University) · Hongyang Li (Shanghai AI Lab) · Siheng Chen (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improved Zero-Shot Classification by Adapting VLMs with Text Descriptions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Oindrila Saha (University of Massachusetts at Amherst) · Grant Horn (University of Massachusetts at Amherst) · Subhransu Maji (University of Massachusetts, Amherst)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Harnessing the Power of MLLMs for Transferable Text-to-Image Person ReID</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wentao Tan (South China University of Technology) · Changxing Ding (South China University of Technology) · Jiayu Jiang (South China University of Technology) · Fei Wang (South China University of Technology) · Yibing Zhan (JD Explore Academy) · Dapeng Tao (Yunnan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Focus on Your Instruction: Fine-grained and Multi-instruction Image Editing by Attention Modulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            guo (None) · Tianwei Lin (Horizon Robotics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jialin Wu (Google) · Xia Hu (Research, Google) · Yaqing Wang (Research, Google) · Bo Pang (Google) · Radu Soricut (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Distilling CLIP with Dual Guidance for  Learning Discriminative Human Body Shape Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feng Liu (Michigan State University) · Minchul Kim (Michigan State University) · Zhiyuan Ren (Michigan State University) · Xiaoming Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Observation-Guided Diffusion Probabilistic Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junoh Kang (Seoul National University) · Jinyoung Choi (Seoul National University) · Sungik Choi (LG AI Research) · Bohyung Han (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rotated Multi-Scale Interaction Network for Referring Remote Sensing Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sihan liu (Xiamen University) · Yiwei Ma (Xiamen University) · Xiaoqing Zhang (Xiamen University) · Haowei Wang (Xiamen University) · Jiayi Ji (Xiamen University) · Xiaoshuai Sun (Xiamen University) · Rongrong Ji (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/dvlab-research/GroupContrast" target="_blank">GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengyao Wang (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Li Jiang (Max Planck Institute for Informatics) · Xiaoyang Wu (The University of Hong Kong) · Zhuotao Tian (The Chinese University of Hong Kong) · Bohao Peng (The Chinese University of Hong Kong) · Hengshuang Zhao (The University of Hong Kong) · Jiaya Jia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fully Exploiting Every Real Sample: Super-Pixel Sample Gradient Model Stealing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunlong Zhao () · Xiaoheng Deng (Central South University) · Yijing Liu (Zhejiang University) · Xinjun Pei (None) · Jiazhi Xia (Central South University) · Wei Chen (State key laboratory of CAD&amp;CG)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LED: A Large-scale Real-world Paired Dataset for Event Camera Denoising</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxing Duan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided Diffusion with Visual Invariant</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenlu Zhan (None) · Gaoang Wang (Zhejiang University) · Yu LIN (Zhejiang University) · Hongwei Wang (Zhejiang University) · Jian Wu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DePT: Decoupled Prompt Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ji Zhang (University of Electronic Science and Technology of China) · Shihan Wu (University of Electronic Science and Technology of China) · Lianli Gao (University of Electronic Science and Technology of China, Tsinghua University) · Heng Tao Shen (University of Electronic Science and Technology of China) · Jingkuan Song (University of Electronic Science and Technology of China,)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Subspace-Constrained Tyler's Estimator and its Applications to Structure from Motion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feng Yu (University of Minnesota - Twin Cities) · Teng Zhang (University of Central Florida) · Gilad Lerman (University of Minnesota, Minneapolis)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bi-level Learning of Task-Specific Decoders for Joint Registration and One-Shot Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Fan (Dalian University of Technology) · Wang (Dalian University of Technology) · Jiaxin Gao (Dalian University of Technology) · Jia Wang (Dalian University of Technology) · Zhongxuan Luo (Dalian University of Technology) · Risheng Liu (Dalian University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Osprey: Pixel Understanding with Visual Instruction Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqian Yuan (Zhejiang University) · Wentong Li (College of Computer Science and Technology, Zhejiang University) · Jian liu (AntGroup) · Dongqi Tang (Ant Group) · Xinjie Luo (Zhejiang University) · Chi Qin (Microsoft) · Lei Zhang (The Hong Kong Polytechnic University) · Jianke Zhu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NeRFCodec: Neural Feature Compression Meets Neural Radiance Fields for Memory-Efficient Scene Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sicheng Li (Zhejiang University) · Hao Li (None) · Yiyi Liao (Zhejiang University) · Lu Yu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diffusion Reflectance Map: Single-Image Stochastic Inverse Rendering of Illumination and Reflectance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuto Enyo (Kyoto University) · Ko Nishino (Kyoto University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Contrastive Denoising Score for Text-guided Latent Diffusion Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyelin Nam (Korea Advanced Institute of Science &amp; Technology) · Gihyun Kwon (Korea Advanced Institute of Science &amp; Technology) · Geon Yeong Park (Korea Advanced Institute of Science and Technology) · Jong Chul Ye (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Domain Prompt Learning with Quaternion Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qinglong Cao (Shanghai Jiao Tong University) · Zhengqin Xu (Shanghai Jiaotong University) · Yuntian Chen (Eastern Institute for Advanced Study) · Chao Ma (Shanghai Jiao Tong University) · Xiaokang Yang (Shanghai Jiao Tong University, China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards More Unified In-context Visual Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dianmo Sheng (University of Science and Technology of China) · Dongdong Chen (Microsoft Research) · Zhentao Tan (Alibaba DAMO Academy; University of Science and Technology of China) · Qiankun Liu (Beijing Institute of Technology) · Qi Chu (University of Science and Technology of China) · Jianmin Bao (Microsoft) · Tao Gong (University of Science and Technology of China) · Bin Liu (None) · Shengwei Xu (Beijing Electronic Science and Technology Institute) · Nenghai Yu (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="http://www.svcl.ucsd.edu/projects/ltad/" target="_blank">Long-Tailed Anomaly Detection with Learnable Class Names</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chih-Hui Ho (University of California San Diego) · Kuan-Chuan Peng (Mitsubishi Electric Research Laboratories (MERL)) · Nuno Vasconcelos (University of California San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DetCLIPv3: Towards Versatile Generative Open-vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lewei Yao (Harbin Institute of Technology) · Renjie Pi (None) · Jianhua Han (Huawei Technologies Ltd.) · Xiaodan Liang (Sun Yat-sen University) · Hang Xu (Huawei Noah‘s Ark Lab) · Wei Zhang (Huawei Technologies Ltd.) · Zhenguo Li (Huawei) · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Uncertainty-Driven Continual Learning for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lei Lai (Boston University, Boston University) · Eshed Ohn-Bar (Boston University, Boston University) · Sanjay Arora (Red Hat, Inc.) · John Yi (Boston University, Boston University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://platonerf.github.io/" target="_blank">PlatoNeRF: 3D Reconstruction in Plato’s Cave via Single-View Two-Bounce Lidar</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tzofi Klinghoffer (Massachusetts Institute of Technology) · Xiaoyu Xiang (Meta) · Siddharth Somasundaram (Massachusetts Institute of Technology) · Yuchen Fan (Facebook) · Christian Richardt (Meta Reality Labs) · Ramesh Raskar (Massachusetts Institute of Technology) · Rakesh Ranjan ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VideoMosaic: Connecting the Temporal Dots in Long Videos for LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Reuben Tan (Boston University) · Ximeng Sun (Boston University) · Ping Hu (University of Electronic Science and Technology of China) · Jui-Hsien Wang (Adobe Systems) · Hanieh Deilamsalehy (None) · Bryan A. Plummer (None) · Bryan Russell (Adobe Research) · Kate Saenko (Meta / Boston University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffusionGAN3D: Boosting Text-guided 3D Generation and Domain Adaptation by Combining 3D GANs and Diffusion Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Biwen Lei (Alibaba Group) · Kai Yu (None) · Mengyang Feng (Alibaba Group) · Miaomiao Cui (Alibaba Group) · Xuansong Xie (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ZeroShape: Regression-based Zero-shot Shape Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixuan Huang (University of Illinois Urbana-Champaign) · Stefan Stojanov (Georgia Institute of Technology) · Anh Thai (Georgia Institute of Technology) · Varun Jampani (Google Research) · James Rehg (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Your Transferability Barrier is Fragile: Free-Lunch for Transferring the Non-Transferable Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziming Hong (The University of Sydney) · Li Shen (JD Explore Academy) · Tongliang Liu (Mohamed bin Zayed University of Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifan Bai (Xi’an Jiaotong University) · Zeyang Zhao (Xi'an Jiaotong University) · Yihong Gong (Xi'an Jiaotong University) · Xing Wei (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DPHMs: Diffusion Parametric Head Models for Depth-based Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiapeng Tang (Technische Universität München) · Angela Dai () · Yinyu Nie (Huawei Technologies Ltd.) · Lev Markhasin (None) · Justus Thies (Max-Planck Institute for Intelligent Systems) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CNC-Net: Self-Supervised Learning for CNC Machining Operations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohsen Yavartanoo (None) · Sangmin Hong (Seoul National University) · Reyhaneh Neshatavar (None) · Kyoung Mu Lee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>High-Quality Facial Geometry and Appearance Capture at Home</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxuan Han (Tsinghua University) · Junfeng Lyu (School of Software, Tsinghua University) · Feng Xu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qinghao Ye (Alibaba Group) · Haiyang Xu (Alibaba Group) · Jiabo Ye (East China Normal University) · Ming Yan (Alibaba Group) · Anwen Hu (Alibaba Group) · Haowei Liu (Institute of Automation, Chinese Academy of Sciences) · Qi Qian (Alibaba Group) · Ji Zhang (Alibaba Group) · Fei Huang (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Portrait4D: Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu Deng (Xiaobing.ai) · Duomin Wang () · Xiaohang Ren (xiaobing) · Xingyu Chen (Xiaobing.AI) · Baoyuan Wang (Xiaobing.ai)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Efficient Scene Recovery Using Luminous Flux Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            ZhongYu Li (University of Science and Technology of China) · Lei Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Insect-Foundation: A Foundation Model and Large-scale 1M Dataset for Visual Insect Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hoang-Quan Nguyen (University of Arkansas - Fayetteville) · Thanh-Dat Truong (University of Arkansas) · Xuan-Bac Nguyen (None) · Ashley Dowling (University of Arkansas - Fayetteville) · Xin Li (State University of New York at Albany) · Khoa Luu (University of Arkansas)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IMPRINT: Generative Object Compositing by Learning Identity-Preserving Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yizhi Song (Purdue University) · Zhifei Zhang (Adobe Research) · Zhe Lin (Adobe Research) · Scott Cohen (Adobe Systems) · Brian Price (Adobe Research) · Jianming Zhang (Adobe Systems) · Soo Ye Kim (Adobe Systems) · He Zhang (Adobe Systems) · Wei Xiong (Adobe Systems) · Daniel Aliaga (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning without Exact Guidance: Updating Large-scale High-resolution Land Cover Maps from Low-resolution Historical Labels</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhuohong Li () · Wei He (Wuhan University) · Jiepan Li (None) · Fangxiao Lu (Wuhan University) · Hongyan Zhang (China University of Geosciences Wuhan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Troika: Multi-Path Cross-Modal Traction for Compositional Zero-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siteng Huang (Zhejiang University &amp; Westlake University) · Biao Gong (Alibaba Group) · Yutong Feng (Alibaba Group) · Zhang Min (Westlake University) · Yiliang Lv (Gientech AIL) · Donglin Wang (Westlake University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hyperbolic Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huimin Li (Beihang University) · Zhentao Chen (Beihang University) · Yunhao Xu (Beihang University) · Junlin Hu (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multiple View Geometry Transformers for 3D Human Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziwei Liao (University of Toronto) · jialiang zhu (Southeast University) · Chunyu Wang (Microsoft) · Han Hu (Microsft Research Asia) · Steven L. Waslander (University of Toronto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>H-ViT: A Hierarchical Vision Transformer for Deformable Image Registration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            MORTEZA GHAHREMANI (Technische Universität München) · Mohammad Khateri (University of Eastern Finland) · Bailiang Jian (Technische Universität München) · Benedikt Wiestler (Technical University Munich) · Ehsan Adeli (Stanford University) · Christian Wachinger (Technische Universität München)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinting Liao (Zhejiang Univerisity) · Weiming Liu (Zhejiang University) · Chaochao Chen (Zhejiang University) · Pengyang Zhou (Zhejiang University) · Fengyuan Yu (Zhejiang University) · Huabin Zhu (Zhejiang University) · Binhui Yao (University of Canberra) · Tao Wang (Midea Group) · Xiaolin Zheng (Zhejiang University) · Yanchao Tan (Fuzhou University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SCULPT: Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Soubhik Sanyal (None) · Partha Ghosh (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Jinlong Yang (Google) · Michael J. Black (University of Tübingen) · Justus Thies (Max-Planck Institute for Intelligent Systems) · Timo Bolkart (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Contrastive Pre-Training with Multi-View Fusion for No-Reference Point Cloud Quality Assessment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyu Shan (Shanghai Jiao Tong University) · Yujie Zhang (Shanghai Jiao Tong University) · Qi Yang (Tencent MediaLab) · Haichen Yang (Shanghai Jiaotong University) · Yiling Xu (None) · Jenq-Neng Hwang (None) · Xiaozhong Xu (Tencent Media Lab) · Shan Liu (Tencent Media Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Training-free Pretrained Model Merging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengqi Xu (Zhejiang University) · Ke Yuan (Zhejiang University) · Huiqiong Wang (Zhejiang University) · Yong Wang (State Grid Shandong Electronic Power Company) · Mingli Song (Zhejiang University) · Jie Song (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Anatomically Constrained Implicit Face Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Prashanth Chandran (None) · Gaspard Zoss (Disney Research, Disney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Revisiting Global Translation Estimation with Feature Tracks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peilin Tao (None) · Hainan Cui (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Mengqi Rong (, Institute of automation, Chinese academy of science) · Shuhan Shen (Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LoCoNet: Long-Short Context Network for Active Speaker Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xizi Wang (Indiana University, Bloomington) · Feng Cheng (University of North Carolina at Chapel Hill) · Gedas Bertasius (UNC Chapel Hill)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WinSyn: A High Resolution Testbed for Synthetic Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tom Kelly (King Abdullah University of Science and Technology) · John Femiani (None) · Peter Wonka (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huan Liu (Beijing Jiaotong University) · Zichang Tan (Baidu) · Chuangchuang Tan (Beijing Jiaotong University) · Yunchao Wei (Beijing Jiaotong University) · Jingdong Wang (Baidu) · Yao Zhao (Beijing Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Flatten Long-Range Loss Landscapes for Cross-Domain Few-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixiong Zou (Huazhong University of Science and Technology) · Yicong Liu (Huazhong University of Science and Technology) · Yiman Hu (Huazhong University of Science and Technology) · Yuhua Li (Huazhong University of Science and Technology) · Ruixuan Li (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Super-Resolution for Real-time Rendering with Radiance Demodulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jia Li (Shandong University) · Ziling Chen (Shandong University) · Xiaolong Wu (None) · Lu Wang (Shandong University) · Beibei Wang (Nankai University) · Lei Zhang (The Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Noisy One-point Homographies are Surprisingly Good</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yaqing Ding (None) · Jonathan Astermark (Lund University) · Magnus Oskarsson (Lund University) · Viktor Larsson (Lund University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Alchemist: Parametric Control of Material Properties with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Prafull Sharma (Massachusetts Institute of Technology) · Varun Jampani (Google Research) · Yuanzhen Li (Massachusetts Institute of Technology) · Xuhui Jia (Google) · Dmitry Lagun (Google) · Fredo Durand (Massachusetts Institute of Technology) · William Freeman (MIT and Google) · Mark Matthews (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DisCo: Disentangled Control for Realistic Human Dance Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tan Wang (Nanyang Technological University) · Linjie Li (Microsoft) · Kevin Lin (Microsoft) · Yuanhao Zhai (State University of New York at Buffalo) · Chung-Ching Lin (Microsoft) · Zhengyuan Yang (Microsoft) · Hanwang Zhang (Nanyang Technological University) · Zicheng Liu (Microsoft) · Lijuan Wang (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PaReNeRF: Toward Fast Large-scale Dynamic NeRF with Patch-based Reference</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao Tang (None) · Min Yang (None) · Penghui Sun (Samsung R&amp;D Institute) · Hui Li (Samsung R&amp;amp;amp;D Institute China Xi’an (SRCX)) · Yuchao Dai (Northwestern Polytechnical University) · feng zhu (None) · Hojae Lee (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FLHetBench: Benchmarking Device and State Heterogeneity in Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyuan Zhang (University of Hong Kong) · Shuang Zeng (The University of Hong Kong) · Miao Zhang (New York University) · Runxi Wang (Beijing University of Aeronautics and Astronautics) · Feifei Wang (Stanford University) · Yuyin Zhou (UC Santa Cruz) · Paul Pu Liang (Carnegie Mellon University) · Liangqiong Qu (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiamian Wang (Rochester Institute of Technology) · Guohao Sun (Rochester Institute of Technology) · Pichao Wang (Amazon) · Dongfang Liu (Rochester Institute of Technology) · Sohail Dianat (Rochester Institute of Technology) · MAJID RABBANI (Rochester Institute of Technology) · Raghuveer Rao (DEVCOM Army Research Laboratory) · ZHIQIANG TAO (Rochester Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>JRDB-Social: A Multifaceted Robotic Dataset for Understanding of Context and Dynamics of Human Interactions Within Social Groups</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Simindokht Jahangard (None) · Zhixi Cai (None) · Shiki Wen (Monash University) · Hamid Rezatofighi (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Jin (Peking University) · Ryuichi Takanobu (miHoYo) · Cai Zhang (Nanrui Group Co., Ltd) · Xiaochun Cao (SUN YAT-SEN UNIVERSITY) · Li Yuan (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Suppress and Rebalance: Towards Generalized Multi-Modal Face Anti-Spoofing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xun Lin (Beihang University) · Shuai Wang (Beihang University) · RIZHAO CAI (Nanyang Technological University) · Yizhong Liu (Beihang University) · Ying Fu (None) · Wenzhong Tang (Beihang University) · Zitong YU (Nanyang Technological University) · Alex C. Kot (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qinghe Ma (Nanjing University) · Jian Zhang (Nanjing university) · Lei Qi (Southeast University) · Qian Yu (Shandong Women's University) · Yinghuan Shi (Nanjing University) · Yang Gao (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Universal Novelty Detection through Adaptive Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hossein Mirzaei (Sharif University of Technology, Sharif University of Technology) · Mojtaba Nafez (Sharif University of Technology) · Mohammad Jafari (Sharif University of Technology) · Mohammad Soltani (Sharif University of Technology) · Mohammad Azizmalayeri (Amsterdam UMC) · Jafar Habibi (Sharif University of Technology) · Mohammad Sabokrou (Okinawa Institute of Science and Technology (OIST)) · Mohammad Rohban (Sharif University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://rq-wu.github.io/projects/LAMP/index.html" target="_blank">LAMP: Learn A Motion Pattern for Few-Shot Video Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rui-Qi Wu (Nankai University) · Liangyu Chen (Megvii Technology Inc.) · Tong Yang (Fudan University) · Chun-Le Guo (None) · Chongyi Li () · Xiangyu Zhang (MEGVII Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CLiC: Concept Learning in Context</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mehdi Safaee (SFU GrUVi Lab) · Aryan Mikaeili (Simon Fraser University) · Or Patashnik (Tel Aviv University) · Daniel Cohen-Or (Google) · Ali Mahdavi Amiri (Simon Fraser University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dynamic Graph Representation with Knowledge-aware Attention for Histopathology Whole Slide Image Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawen Li (Tsinghua University) · Yuxuan Chen (Tsinghua University) · Hongbo Chu (None) · Sun Qiehe (Tsinghua University) · Tian Guan (Graduate School at Shenzhen, Tsinghua University) · Anjia Han (SUN YAT-SEN UNIVERSITY) · Yonghong He (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Fei (National University of Singapore) · Shengqiong Wu (National University of Singapore) · Wei Ji (None) · Hanwang Zhang (Nanyang Technological University) · Tat-seng Chua (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LEAD: Exploring Logit Space Evolution for Model Selection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixuan Hu (None) · Xiaotong Li (Peking University) · SHIXIANG TANG (The Chinese University of Hong Kong) · Jun Liu () · Yichun Hu (Peking University) · Ling-Yu Duan (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards CLIP-driven Language-free 3D Visual Grounding via 2D-3D Relational Enhancement and Consistency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqi Zhang (Sichuan University) · Han Luo (Sichuan University) · Yinjie Lei (Sichuan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MR-VNet: Media Restoration using Volterra Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siddharth Roheda (Samsung Research) · Amit Unde (SRIB Bangalore) · Loay Rashid (Samsung Research Institute Bangalore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WonderJourney: Going from Anywhere to Everywhere</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hong-Xing Yu (Computer Science Department, Stanford University) · Haoyi Duan (Stanford University) · Junhwa Hur (Google) · Kyle Sargent (Computer Science Department, Stanford University) · Michael Rubinstein (Google) · William Freeman (MIT and Google) · Forrester Cole (Google) · Deqing Sun (Google) · Noah Snavely (Google / Cornell) · Jiajun Wu (Stanford University) · Charles Herrmann (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UFORecon: Generalizable Sparse-View Surface Reconstruction from Arbitrary and Unfavorable Sets</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youngju Na (KAIST) · Woo Jae Kim (Korea Advanced Institute of Science and Technology (KAIST)) · Kyu Han (Korea Advanced Institute of Science &amp; Technology) · Suhyeon Ha (Korea Advanced Institute of Science and Technology) · Sung-Eui Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxuan Zhang (Shanghai Jiao Tong University) · Yiren Song (Shanghai Jiaotong University) · Jiaming Liu (Xiaohongshu) · Rui Wang (Beijing University of Posts and Telecommunications) · Jinpeng Yu (None) · Hao Tang (ETH Zurich and CMU) · Huaxia Li (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Xu Tang (Shanghaitech University) · Yao Hu (Zhejiang University, Tsinghua University) · Han Pan (Shanghai Jiao Tong University) · Zhongliang Jing (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Few-shot Learner Parameterization by Diffusion Time-steps</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhongqi Yue (Nanyang Technological University) · Pan Zhou (Sea Group) · Richang Hong (Hefei University of Technology) · Hanwang Zhang (Nanyang Technological University) · Qianru Sun (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Global and Hierarchical Geometry Consistency Priors for Few-shot NeRFs in Indoor Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaotian Sun (Xiamen University) · Qingshan Xu (Nanyang Technological University) · Xinjie Yang (Xiamen University) · Yu Zang (Xiamen University) · Cheng Wang (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Compressed 3D Gaussian Splatting for Accelerated Novel View Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Simon Niedermayr (Technical University of Munich) · Josef Stumpfegger (Technische Universität München) · rüdiger westermann (Technische Universität München)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>The STVchrono Dataset: Towards Continuous Change Recognition in Time</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanjun Sun (Keio University) · Yue Qiu (AIST, National Institute of Advanced Industrial Science and Technology) · Mariia Khan (Edith Cowan University) · Fumiya Matsuzawa (AIST, University of Tsukuba) · Kenji Iwata (AIST, National Institute of Advanced Industrial Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SPIN: Simultaneous Perception, Interaction and Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shagun Uppal (Carnegie Mellon University) · Ananye Agarwal (Carnegie Mellon University) · Haoyu Xiong (CMU, Carnegie Mellon University) · Kenneth Shaw (Carnegie Mellon University) · Deepak Pathak (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SED: A Simple Encoder-Decoder for Open-Vocabulary Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Xie (Tianjin University) · Jiale Cao (Tianjin University) · Jin Xie (Chongqing University) · Fahad Shahbaz Khan (MBZUAI; Linköping University) · Yanwei Pang (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unleashing Channel Potential: Space-Frequency Selection Convolution for SAR Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ke Li (Xidian University) · Di Wang (Xidian University) · Zhangyuan Hu (Xidian University) · Wenxuan Zhu (Xidian University) · Shaofeng Li (None) · Quan Wang (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Motion Blur Decomposition with Cross-shutter Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiang Ji (The University of Tokyo) · Haiyang Jiang (None) · Yinqiang Zheng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Real-time Acquisition and Reconstruction of Dynamic Volumes with Neural Structured Illumination</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixin Zeng (Zhejiang University) · Zoubin Bi (State Key Laboratory of CAD&amp;CG, Zhejiang Univerisity) · Yin Mingrui (Zhejiang University) · Xiang Feng (Zhejiang University) · Kun Zhou (Zhejiang University) · Hongzhi Wu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MV-Adapter: Exploring Parameter Efficient Learning for Video Text Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            bowen zhang (Bytedance) · Xiaojie Jin (ByteDance Inc./TikTok) · Weibo Gong (ByteDance) · Kai Xu (University of Chinese Academy of Sciences) · Xueqing Deng (ByteDance Research) · Peng Wang (Bytedance US AILab) · Zhao Zhang (Hefei University of Technology) · Xiaohui Shen (ByteDance) · Jiashi Feng (ByteDance)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mind marginal non-crack regions: Clustering-inspired representation learning for crack segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            zhuangzhuang chen (shenzhen university) · Zhuonan Lai (Shenzhen University) · Jie Chen (Shenzhen University) · Jianqiang Li (Shenzhen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SpatialTracker: Tracking Any 2D Pixels in 3D Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxi Xiao (Zhejiang University) · Qianqian Wang (Cornell University) · Shangzhan Zhang (Zhejiang University) · Nan Xue (Ant Group) · Sida Peng (None) · Yujun Shen (The Chinese University of Hong Kong) · Xiaowei Zhou (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FreePoint: Unsupervised Point Cloud Instance Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhikai Zhang (Wuhan University) · Jian Ding (None) · Li Jiang (Max Planck Institute for Informatics) · Dengxin Dai () · Gui-Song Xia (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Perceptual Assessment and Optimization of HDR Image Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peibei Cao (City University of Hong Kong) · Rafal Mantiuk (University of Cambridge) · Kede Ma (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Programmable Motion Generation for Open-set Motion Control Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanchao Liu (Tsinghua University) · Xiaohang Zhan (Tencent) · Shaoli Huang (Tencent AI Lab) · Tai-Jiang Mu (Tsinghua University, Tsinghua University) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Degradation Independent Representations for Camera ISP Pipelines</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanhui Guo (McMaster University) · Fangzhou Luo (McMaster University) · Xiaolin Wu (McMaster University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Projecting Trackable Thermal Patterns for Dynamic Computer Vision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mark Sheinin (Weizmann Institute of Science) · Aswin C. Sankaranarayanan (Carnegie Mellon University) · Srinivasa G. Narasimhan (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MMVP: A Multimodal MoCap Dataset with Vision and Pressure Sensors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            He Zhang (Beihang University) · Shenghao Ren (Nanjing University) · Haolei Yuan (Beijing University of Aeronautics and Astronautics) · Jianhui Zhao (Beijing University of Aeronautics and Astronautics) · Fan Li (Beijing University of Aeronautics and Astronautics) · Shuangpeng Sun (Tsinghua University, Tsinghua University) · Zhenghao Liang (Tsinghua University, Tsinghua University) · Tao Yu (Tsinghua University, Tsinghua University) · Qiu Shen (Nanjing University) · Xun Cao (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Overcoming Generic Knowledge Loss with Selective Parameter Update</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenxuan Zhang (King Abdullah University of Science and Technology) · Paul Janson (Concordia University/ MILA) · Rahaf Aljundi (Toyota Motor Europe) · Mohamed Elhoseiny (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EventPS: Real-Time Photometric Stereo Using an Event Camera</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bohan Yu (None) · Jieji Ren (Shanghai Jiao Tong University) · Jin Han () · Feishi Wang (Peking University) · Jinxiu Liang (None) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Kernel Adaptive Convolution for Scene Text Detection via Distance Map Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinzhi Zheng (University of Chinese Academy of Sciences) · Heng Fan (University of North Texas) · Libo Zhang (Institute of Software Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Open-Vocabulary 3D Semantic Segmentation with Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Jiang (Max Planck Institute for Informatics) · Shaoshuai Shi (Saarland Informatics Campus, Max-Planck Institute) · Bernt Schiele (Max Planck Institute for Informatics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Pick-or-Mix: Dynamic Channel Sampling for ConvNets</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ashish Kumar (Indian Institute of Technology,  Kanpur) · Daneul Kim (Seoul National University) · Jaesik Park (Seoul National University) · Laxmidhar Behera (Indian Institute of Technology , Kanpur)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeyu Han (Sichuan University) · Fangrui Zhu (Northeastern University) · Qianru Lao (Harvard University) · Huaizu Jiang (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Triplane Meets Gaussian Splatting: Fast and Generalizable Single-View 3D Reconstruction with Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zi-Xin Zou (None) · Zhipeng Yu (University of the Chinese Academy of Sciences) · Yuan-Chen Guo (Tsinghua University) · Yangguang Li (Shanghai AI Laboratory) · Yan-Pei Cao (Tencent ARC Lab) · Ding Liang (Tsinghua University, Tsinghua University) · Song-Hai Zhang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CAMEL: CAusal Motion Enhancement tailored for Lifting Text-driven Video Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guiwei Zhang (Beijing University of Aeronautics and Astronautics) · Tianyu Zhang (Du Xiaoman Financial) · Guanglin Niu (Beihang University) · Zichang Tan (Baidu) · Yalong Bai (JD AI Research) · Qing Yang (Du Xiaoman Technology(BeiJing))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking Transformers Pre-training for Multi-Spectral Satellite Imagery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mubashir Noman (MBZUAI) · Muzammal Naseer (MBZUAI) · Hisham Cholakkal (MBZUAI) · Rao Anwer (Mohamed bin Zayed University of Artificial Intelligence) · Salman Khan (Mohamed bin Zayed University of Artificial Intelligence) · Fahad Shahbaz Khan (MBZUAI; Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Real-World HDR Video Reconstruction: A Large-Scale Benchmark Dataset and A Two-Stage Alignment Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yong Shu (Shanghai University) · Liquan Shen (Shanghai University) · Xiangyu Hu (Shanghai University) · Mengyao Li (Shanghai University) · Zihao Zhou (Shanghai University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ViTamin: Designing Scalable Vision Models in the Vision-Language Era</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jieneng Chen (Johns Hopkins University) · Qihang Yu (Johns Hopkins University) · Xiaohui Shen (ByteDance) · Alan L. Yuille (Johns Hopkins University) · Liang-Chieh Chen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GraCo: Granularity-Controllable Interactive Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yian Zhao (Peking University) · Kehan Li (Peking University) · Zesen Cheng (Peking University) · Pengchong Qiao (Peking University) · Xiawu Zheng (Xiamen University) · Rongrong Ji (Xiamen University) · Chang Liu (Tsinghua University, Tsinghua University) · Li Yuan (Peking University) · Jie Chen (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mocap Everyone Everywhere: Lightweight Motion Capture With Smartwatches and a Head-Mounted Camera</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiye Lee (Seoul National University) · Hanbyul Joo (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DuPL: Dual Student with Trustworthy Progressive Learning for Robust Weakly Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanchen Wu (Shanghai University) · Xichen Ye (Shanghai University) · KequanYang (Shanghai University) · Jide Li () · Xiaoqiang Li (shanghai university)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Image Neural Field Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yinbo Chen (University of California, San Diego) · Oliver Wang (Adobe Research) · Richard Zhang (Adobe Systems) · Eli Shechtman (Adobe) · Xiaolong Wang (UCSD) · Michaël Gharbi (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Segment Every Out-of-Distribution Object</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenjie Zhao (Univeristy of Texas at Dallas) · Jia Li (None) · Xin Dong (Harvard University) · Yu Xiang (University of Texas, Dallas) · Yunhui Guo (The University of Texas at Dallas)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Upscale-A-Video: Temporal-Consistent Diffusion Model for Real-World Video Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shangchen Zhou (Nanyang Technological University) · Peiqing Yang (S-Lab, Nanyang Technological University) · Jianyi Wang (Nanyang Technological University) · Yihang Luo (Nanyang Technological University) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Physics-informed Low-rank Deep Neural Network for Blind and Universal Lens Aberration Correction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jin Gong (Tsinghua University) · Runzhao Yang (Department of Automation, Tsinghua University) · Weihang Zhang (Tsinghua University) · Jinli Suo (Tsinghua University, Tsinghua University) · Qionghai Dai (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DIRECT-3D: Learning Direct Text-to-3D Generation on Massive Noisy 3D Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qihao Liu (Johns Hopkins University) · Yi Zhang (Sony Corporation of America) · Song Bai (ByteDance) · Adam Kortylewski (University of Freiburg &amp; MPI-INF) · Alan L. Yuille (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>An Interactive Navigation Method with Effect-oriented Affordance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            XIAOHAN Wang (Xi'an Jiaotong University) · Yuehu LIU (College of Artificial Intelligence, Xi'an Jiaotong University) · Xinhang Song (None) · Yuyi Liu (Institute of Computing Technology,University of the Chinese Academy of Sciences) · Sixian Zhang (None) · Shuqiang Jiang (Institute of Computing Technology, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NAPGuard: Towards Detecting Naturalistic Adversarial Patches</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siyang Wu (None) · Jiakai Wang (Zhongguancun Laboratory) · Jiejie Zhao (Zhongguancun Laboratory) · Yazhe Wang (Zhongguancun Laboratory) · Xianglong Liu (BUAA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Stealthy Wrongdoer: Feature-Oriented Reconstruction Attack against Split Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyang Xu (Wuhan University) · Mengda Yang (None) · Wenzhe Yi (Wuhan University) · Ziang Li (None) · Juan Wang (None) · Hongxin Hu (State University of New York, Buffalo) · Yong ZHUANG (Wuhan University) · Yaxin Liu (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Flattening the Parent Bias: Hierarchical Semantic Segmentation in the Poincaré Ball</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Simon Weber (Technische Universität München) · Barış Zöngür (Technische Universität München) · Nikita Araslanov (TU Munich) · Daniel Cremers (Technical University Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generative Region-Language Pretraining for Open-Ended Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chuang Lin (None) · Yi Jiang (bytedance) · Lizhen Qu (Monash University) · Zehuan Yuan (Nanjing University) · Jianfei Cai (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jun-Kun Chen (None) · Samuel Rota Bulò (Meta) · Norman Müller (Meta) · Lorenzo Porzi (Facebook) · Peter Kontschieder (Meta) · Yu-Xiong Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Psychometry: An Omnifit Model for Image Reconstruction from Human Brain Activity</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruijie Quan (Zhejiang University) · Wenguan Wang (Zhejiang University) · Zhibo Tian (Lanzhou University) · Fan Ma (None) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking Multi-domain Generalization with A General Learning Objective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaorui Tan (None) · Xi Yang (Xi'an Jiaotong-Liverpool University) · Kaizhu Huang (Duke Kunshan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Theory of Joint Light and Heat Transport for Lambertian Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mani Ramanagopal (Carnegie Mellon University) · Sriram Narayanan (Carnegie Mellon University) · Aswin C. Sankaranarayanan (Carnegie Mellon University) · Srinivasa G. Narasimhan (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Spectral and Polarization Vision: Spectro-polarimetric Real-world Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujin Jeon (Pohang University of Science and Technology) · Eunsue Choi (Pohang University of Science and Technology) · Youngchan Kim (Pohang University of Science and Technology) · Yunseong Moon (Pohang University of Science and Technology) · Khalid Omer (Meta Reality Labs) · Felix Heide (Department of Computer Science, Princeton University) · Seung-Hwan Baek (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Text-guided 3D Scene Composition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qihang Zhang (The Chinese University of Hong Kong) · Chaoyang Wang (Snap Inc) · Aliaksandr Siarohin (Snap Inc.) · Peiye Zhuang (Snap Inc.) · Yinghao Xu (Chinese University of Hong Kong) · Ceyuan Yang (The Chinese University of Hong Kong) · Dahua Lin (The Chinese University of Hong Kong) · Bolei Zhou (University of California, Los Angeles) · Sergey Tulyakov (Snap Inc.) · Hsin-Ying Lee (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Efficient Stitchable Task Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyu He (Monash University) · Zizheng Pan (None) · Jing Liu () · Jianfei Cai (Monash University) · Bohan Zhuang (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MeaCap: Memory-Augmented Zero-shot Image Captioning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zequn Zeng (None) · Yan Xie (None) · Hao Zhang (Xidian University, Xi'an, China) · Chiyu Chen (Xi'an University of Electronic Science and Technology) · Zhengjue Wang (Xidian University) · Bo Chen (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MuGE: Multiple Granularity Edge Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Caixia Zhou (None) · Yaping Huang (Beijing Jiaotong University) · Mengyang Pu (North China Electric Power University) · Qingji Guan (Beijing Jiaotong University) · Ruoxi Deng (Wenzhou University) · Haibin Ling (State University of New York, Stony Brook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Multitask Dense Predictor via Binarization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuzhang Shang (Illinois Institute of Technology) · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology) · Gaowen Liu (None) · Ramana Kompella (Cisco) · Yan Yan (Illinois Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Novel View Synthesis with View-Dependent Effects from a Single Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juan Luis Gonzalez Bello (KAIST) · Munchurl Kim (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://dreamwireart.github.io/" target="_blank">Wired Perspectives: Multi-View Wire Art Embraces Generative AI</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyu Qu (University of Surrey) · LAN YANG (Beijing University of Posts and Telecommunications) · Honggang Zhang (Beijing University of Posts and Telecommunications) · Tao Xiang (University of Surrey) · Kaiyue Pang (SketchX AI) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Orchestrate Latent Expertise: Advancing Online Continual Learning with Multi-Level Supervision and Reverse Self-Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongwei Yan (Tsinghua University) · Liyuan Wang (Tsinghua University) · Kaisheng Ma (Institute for Interdisciplinary Information Sciences (IIIS), Tsinghua University) · Yi Zhong (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Small Scale Data-Free Knowledge Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            He Liu (None) · Yikai Wang (Tsinghua University) · Huaping Liu (Tsinghua University, Tsinghua University) · Fuchun Sun (Tsinghua University) · Anbang Yao (Intel)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FSRT: Facial Scene Representation Transformer for Face Reenactment from Factorized Appearance, Head-pose, and Facial Expression Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andre Rochow (University of Bonn) · Max Schwarz (University of Bonn) · Sven Behnke (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-image Alignment for Diffusion-based Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Neehar Kondapaneni (California Institute of Technology) · Markus Marks (None) · Manuel Knott (ETHZ - ETH Zurich) · Rogério Guimarães (California Institute of Technology) · Pietro Perona (California Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuqi Wang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Yuntao Chen (CAIR, HKISI, CAS) · Xingyu Liao (University of Science and Technology of China) · Lue Fan (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cheeun Hong (Seoul National University) · Kyoung Mu Lee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Domain Separation Graph Neural Networks for Saliency Object Ranking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zijian Wu (Nanjing University of Science and Technology) · Jun Lu (Nanjing University of Science and Technology) · Jing Han (Nanjing University Of Science And Technology) · Lianfa Bai (Nanjing University of Science and Technology) · Yi Zhang (Nanjing University of Science and Technology) · Zhuang Zhao (Nanjing University of Science and Technology) · Siyang Song (University of Leicester)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Solving the Catastrophic Forgetting Problem in Generalized Category Discovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinzi Cao (Sun Yat-Sen University) · Xiawu Zheng (Xiamen University) · Guanhong Wang (Zhejiang University) · Weijiang Yu (SUN YAT-SEN UNIVERSITY) · Yunhang Shen (Tencent) · Ke Li (Tencent) · Yutong Lu (SUN YAT-SEN UNIVERSITY) · Yonghong Tian (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Image Restoration through Removing Degradations in Textual Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingbo Lin (Harbin Institute of Technology) · Zhilu Zhang (Harbin Institute of Technology) · Yuxiang Wei (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) · Dongwei Ren (Harbin Institute of Technology) · Dongsheng Jiang (Huawei Technologies Ltd.) · Qi Tian (Huawei Technologies Ltd.) · Wangmeng Zuo (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Activity-Biometrics: Person Identification from Daily Activities</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shehreen Azad (University of Central Florida) · Yogesh S. Rawat (University of Central Florida)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Temporally Consistent Unbalanced Optimal Transport for Unsupervised Action Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ming Xu (The Australian National University) · Stephen Gould (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Instance-level Expert Knowledge and Aggregate Discriminative Attention for Radiology Report Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shenshen Bu (Sun Yat-sen University) · Taiji Li (SUN YAT-SEN UNIVERSITY) · Zhiming Dai (SUN YAT-SEN UNIVERSITY) · Yuedong Yang (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HyperSDFusion: Bridging Hierarchical Structures in Language and Geometry for Enhanced 3D Text2Shape Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiying Leng (Beihang University) · Tolga Birdal (Imperial College London) · Xiaohui Liang (Zhongguancun Laboratory) · Federico Tombari (Google, TUM)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MatchU: Matching Unseen Objects for 6D Pose Estimation from RGB-D Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junwen Huang (Technische Universität München) · Hao Yu (Technical University Munich) · Kuan-Ting Yu (XYZ Robotics) · Nassir Navab (TU Munich) · Slobodan Ilic (Technical University Munich) · Benjamin Busam (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Resource-Efficient Transformer Pruning for Finetuning of Large Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fatih Ilhan (Georgia Institute of Technology) · Gong Su (IBM, International Business Machines) · Selim Tekin (College of Computing, Georgia Institute of Technology) · Tiansheng Huang (Georgia Institute of Technology) · Sihao Hu (Georgia Institute of Technology) · Ling Liu (Georgia Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Variable and Coordinated Holistic Co-Speech Motion Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifei Liu (South China University of Technology) · Qiong Cao (JD Explore Academy) · Yandong Wen (Max Planck Institute for Intelligent Systems) · Huaiguang Jiang (South China University of Technology) · Changxing Ding (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fast ODE-based Sampling for Diffusion Models in Around 5 Steps</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenyu Zhou (Zhejiang University) · Defang Chen (Zhejiang University) · Can Wang (Zhejiang University) · Chun Chen (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Going Beyond Multi-Task Dense Prediction with Synergy Embedding Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huimin Huang (Zhejiang University) · Yawen Huang (None) · Lanfen Lin (Zhejiang University) · Ruofeng Tong (None) · Yen-Wei Chen (Ritsumeikan University) · Hao Zheng (Tencent) · Yuexiang Li (Tencent Jarvis Lab) · Yefeng Zheng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>WWW: A Unified Framework for Explaining What, Where and Why of Neural Networks by Interpretation of Neuron Concept</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yong Hyun Ahn (Kyung Hee University) · Hyeon Kim (Kyunghee University) · Seong Tae Kim (Kyung Hee University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ToonerGAN: Reinforcing GANs for Obfuscating Automated Facial Indexing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kartik Thakral (IIT Jodhpur) · Shashikant Prasad (Indian Institute of Technology, Jodhpur, Dhirubhai Ambani Institute Of Information and Communication Technology) · Stuti Aswani (Indian Institute of Technology, Jodhpur) · Mayank Vatsa (IIT Jodhpur) · Richa Singh (IIT Jodhpur)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hybrid Functional Maps for Crease-Aware Non-Isometric Shape Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lennart Bastian (None) · Yizheng Xie (Technische Universität München) · Nassir Navab (TU Munich) · Zorah Lähner (Rheinische Friedrich-Wilhelms Universität Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LowRankOcc: Tensor Decomposition and Low-Rank Recovery for Vision-based 3D Semantic Occupancy Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linqing Zhao (Tianjin University, Tsinghua University) · Xiuwei Xu (Tsinghua University, Tsinghua University) · Ziwei Wang (Tsinghua University, Tsinghua University) · Yunpeng Zhang (PhiGent Robotics) · Borui Zhang (Tsinghua University, Tsinghua University) · Wenzhao Zheng (Tsinghua University, Tsinghua University) · Dalong Du (PhiGent Robotics) · Jie Zhou (None) · Jiwen Lu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unveiling Parts Beyond Objects: Towards Finer-Granularity Referring Expression Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenxuan Wang (National Lab of Pattern Recognition, Institute of Automation,Chinese Academy of Sciences) · Tongtian Yue (, Institute of automation, Chinese academy of science) · Yisi Zhang (University of Science and Technology Beijing) · Longteng Guo (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xingjian He (, Institute of automation, Chinese academy of science) · Xinlong Wang (Beijing Academy of Artificial Intelligence) · Jing Liu (Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junuk Cha (UNIST) · Jihyeon Kim (Ulsan National Institute of Science and Technology) · Jae Shin Yoon (Adobe Systems) · Seungryul Baek (UNIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BT-Adapter: Video Conversation is Feasible Without Video Instruction Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruyang Liu (Peking University) · Chen Li (Tencent ARC Lab) · Yixiao Ge (Tencent) · Thomas H. Li (AIIT, Peking University) · Ying Shan (Tencent) · Ge Li (Peking University Shenzhen Graduate School)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Video Frame Interpolation via Direct Synthesis with the Event-based Reference</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhan Liu () · Yongjian Deng (Beijing University of Technology) · Hao Chen (Southeast University) · Zhen Yang (Beijing University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Instance-Adaptive and Geometric-Aware Keypoint Learning for Category-Level 6D Object Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao Lin (University of Science and Technology of China) · Wenfei Yang (University of Science and Technology of China) · Yuan Gao (University of Science and Technology of China) · Tianzhu Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/BBBBchan/CorrMatch" target="_blank">CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bo-Yuan Sun (Nankai University) · Yuqi Yang (Nankai University) · Le Zhang (University of Electronic Science and Technology of China) · Ming-Ming Cheng (Nankai University, Tsinghua University) · Qibin Hou (Nankai University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MCNet: Rethinking the Core Ingredients for Accurate and Efficient Homography Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haokai Zhu (Zhejiang University) · Si-Yuan Cao (Zhejiang University) · Jianxin Hu (Zhejiang University) · Sitong Zuo (Beijing University of Posts and Telecommunications) · Beinan Yu (Zhejiang University) · Jiacheng Ying (Zhejiang University) · Junwei Li (Zhejiang University) · Hui-Liang Shen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jielin Qiu (Carnegie Mellon University) · Jiacheng Zhu (Massachusetts Institute of Technology) · William Han (Carnegie Mellon University) · Aditesh Kumar (Carnegie Mellon University) · Karthik Mittal (School of Computer Science, Carnegie Mellon University) · Claire Jin (School of Computer Science, Carnegie Mellon University) · Zhengyuan Yang (Microsoft) · Linjie Li (Microsoft) · Jianfeng Wang (Microsoft) · DING ZHAO (Carnegie Mellon University) · Bo Li (UIUC) · Lijuan Wang (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Open Set Domain Adaptation for Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seun-An Choe (Kyung Hee University) · Ah-Hyung Shin (Kyung Hee University) · Keon Hee Park (Kyung Hee University) · Jinwoo Choi (Kyung Hee University) · Gyeong-Moon Park (Kyung Hee University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gongwei Chen (Harbin Institute of Technology) · Leyang Shen (Harbin Institute of Technology) · Rui Shao (Harbin Institute of Technology) · Xiang Deng (Harbin Institute of Technology (Shenzhen)) · Liqiang Nie (Harbin Institute of Technology (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Evonne Ng (University of California, Berkeley) · Javier Romero (None) · Timur Bagautdinov (Reality Labs Research) · Shaojie Bai (Meta) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Angjoo Kanazawa (UC Berkeley) · Alexander Richard (Reality Labs Research, Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://jerryxu.net/PixelLLM/" target="_blank">Pixel Aligned Language Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiarui Xu (University of California, San Diego) · Xingyi Zhou (Google) · Shen Yan (Google Research) · Xiuye Gu (None) · Anurag Arnab (Google) · Chen Sun (Brown University) · Xiaolong Wang (UCSD) · Cordelia Schmid (Inria / Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Transcending Forgery Specificity with Latent Space Augmentation for Generalizable Deepfake Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyuan Yan (Tencent YouTu Lab) · Yuhao Luo (The Chinese University of Hong Kong, Shenzhen) · Siwei Lyu (State University of New York, Buffalo) · Qingshan Liu (Nanjing University of Posts and Telecommunications) · Baoyuan Wu (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking the Evaluation Protocol of Domain Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Han Yu (Tsinghua University) · Xingxuan Zhang (Tsinghua University) · Renzhe Xu (Tsinghua University) · Jiashuo Liu (Tsinghua University, Tsinghua University) · Yue He (Tsinghua University, Tsinghua University) · Peng Cui (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PFStorer: Personalized Face Restoration and Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tuomas Varanka (University of Oulu) · Tapani Toivonen (Huawei Technologies Ltd.) · Soumya Tripathy (Huawei Technologies Ltd. Finland) · Guoying Zhao (None) · Erman Acar (Huawei Technologies)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adapters Strike Back</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jan-Martin Steitz (None) · Stefan Roth (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Eclipse: Disambiguating Illumination and Materials using Unintended Shadows</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dor Verbin (None) · Ben Mildenhall (Google) · Peter Hedman (Google) · Jonathan T. Barron (Google) · Todd Zickler (Harvard University) · Pratul P. Srinivasan (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ASAM: Boosting Segment Anything Model with Adversarial Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bo Li (vivo Mobile Communication Co.,Ltd.) · Haoke Xiao (Xiamen University) · Lv Tang (vivo Mobile Communication Co., Ltd)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ConvoFusion: Multi-Modal Conversational Diffusion for Co-Speech Gesture Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Muhammad Hamza Mughal (Max-Planck Institute for Informatics) · Rishabh Dabral (Saarland Informatics Campus, Max-Planck Institute) · Ikhsanul Habibie (Saarland Informatics Campus, Max-Planck Institute) · Lucia Donatelli (Vrije Universiteit Amsterdam) · Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) · Christian Theobalt (MPI Informatik)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bowen Wen (NVIDIA) · Wei Yang (NVIDIA) · Jan Kautz (NVIDIA) · Stan Birchfield (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Beichen Zhang (Shanghai Jiaotong University) · Xiaoxing Wang (Shanghai Jiao Tong University) · Xiaohan Qin (None) · Junchi Yan (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Yang (South China University of Technology) · Changxing Ding (South China University of Technology) · Zhibin Hong (HeyGen) · Junhao Huang (HeyGen) · Jin Tao (South China University of Technology) · Xiangmin Xu (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ScanFormer: Referring Expression Comprehension by Iteratively Scanning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei Su (Zhejiang University) · Peihan Miao (Zhejiang University) · Huanzhang Dou (Zhejiang University) · Xi Li (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Make-It-Vivid: Dressing Your Animatable Biped Cartoon Characters from Text</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junshu Tang (None) · Yanhong Zeng (None) · Ke Fan (Shanghai Jiaotong University) · Xuheng Wang (Tsinghua University, Tsinghua University) · Bo Dai (Shanghai AI Laboratory) · Kai Chen (Shanghai AI Laboratory) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://shinying.github.io/dmp" target="_blank">Exploiting Diffusion Prior for Generalizable Dense Prediction</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hsin-Ying Lee (University of California, Merced) · Hung-Yu Tseng (Meta) · Hsin-Ying Lee (Snap Inc.) · Ming-Hsuan Yang (University of California at Merced)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GSVA: Generalized Segmentation via Multimodal Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhuofan Xia (Tsinghua University) · Dongchen Han (Tsinghua University) · Yizeng Han (Tsinghua University, Tsinghua University) · Xuran Pan (Tsinghua University, Tsinghua University) · Shiji Song (Tsinghua University, Tsinghua University) · Gao Huang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ElasticDiffusion: Training-free Arbitrary Size Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Moayed Haji Ali (Rice University) · Guha Balakrishnan (Rice University) · Vicente Ordonez (Rice University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Instance-Aware Correspondences for Robust Multi-Instance Point Cloud Registration in Cluttered Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyuan Yu (Na) · Zheng Qin (National University of Defense Technology) · lintao zheng (National University of Defense Technology) · Kai Xu (National University of Defense Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fine-grained Prototypical Voting with Heterogeneous Mixup for Semi-supervised 2D-3D Cross-modal Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fan Zhang (Georgia Institute of Technology) · Xian-Sheng Hua (Terminus Group) · Chong Chen (Terminus Group) · Xiao Luo (University of California, Los Angeles)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Uncertainty Visualization via Low-Dimensional Posterior Projections</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Omer Yair (Technion) · Tomer Michaeli (Technion) · Elias Nehme (Electrical Engineering Department, Technion – Israel Institute of Technology, Technion - Israel Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Young Kyun Jang (Meta AI) · Donghyun Kim (Korea University) · Zihang Meng (Meta) · Dat Huynh (Meta) · Ser-Nam Lim (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Penghao Wu (University of California, San Diego) · Saining Xie (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Real-Time Neural BRDF with Spherically Distributed Primitives</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yishun Dou (Huawei) · Zhong Zheng (huawei.com) · Qiaoqiao Jin (Shanghai Jiao Tong University) · Bingbing Ni (Shanghai Jiao Tong University) · Yugang Chen (Hisilicon) · Junxiang Ke (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RCL: Reliable Continual Learning for Unified Failure Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fei Zhu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhen Cheng (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xu-Yao Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Cheng-Lin Liu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Correlation-Decoupled Knowledge Distillation for Multimodal Sentiment Analysis with Incomplete Modalities</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingcheng Li (Fudan University) · Dingkang Yang (Fudan University) · Xiao Zhao (None) · Shuaibing Wang (Fudan University) · Yan Wang (Fudan University) · Kun Yang (Fudan University) · Mingyang Sun (Fudan University) · Dongliang Kou (Academy for Engineering and Technology, Fudan University, Shanghai, China.) · Qian (Fudan University) · Lihua Zhang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://rave-video.github.io/" target="_blank">RAVE: Randomized Noise Shuffling for Fast and Consistent Video Editing with Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ozgur Kara (Georgia Institute of Technology) · Bariscan Kurtkaya (Koc University) · Hidir Yesiltepe (Virginia Polytechnic Institute and State University) · James Rehg (None) · Pinar Yanardag (Virginia Polytechnic Institute and State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Geometry Transfer for Stylizing Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyunyoung Jung (Seoul National University) · Seonghyeon Nam (Facebook) · Nikolaos Sarafianos (Meta Reality Labs) · Sungjoo Yoo (None) · Alexander Sorkine-Hornung (Meta) · Rakesh Ranjan ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Diffusion Model Alignment Using Direct Preference Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bram Wallace (SalesForce.com) · Meihua Dang (Stanford University) · Rafael Rafailov (Stanford University) · Linqi Zhou (Stanford University) · Aaron Lou (Stanford University) · Senthil Purushwalkam (None) · Stefano Ermon (Stanford University) · Caiming Xiong (Salesforce Research) · Shafiq Joty (SalesForce.com) · Nikhil Naik (MIT)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CSTA: CNN-based Spatiotemporal Attention for Video Summarization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaewon Son (Sungkyunkwan University) · Jaehun Park (Sung Kyun Kwan University) · Kwangsu Kim (Department of Computer Science &amp; Engineering, College of Computing, Sungkyunkwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Robust Emotion Recognition in Context Debiasing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dingkang Yang (Fudan University) · Kun Yang (Fudan University) · Mingcheng Li (Fudan University) · Shunli Wang (Fudan University) · Shuaibing Wang (Fudan University) · Lihua Zhang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Sieve: Multimodal Dataset Pruning using Image-Captioning Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anas Mahmoud (University of Toronto) · Mostafa Elhoushi (Meta, FAIR) · Amro Abbas (Meta) · Yu Yang (University of California, Los Angeles) · Newsha Ardalani (Facebook) · Hugh Leather (Facebook) · Ari Morcos (Meta AI (FAIR))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AMU-Tuning: Learning Effective Bias for CLIP-based Few-shot Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuwei Tang (Tianjin University) · ZhenYi Lin (TianJin University) · Qilong Wang (university  of tianjin of china) · Pengfei Zhu (Tianjin University) · Qinghua Hu (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Not All Voxels Are Equal: Hardness-Aware Semantic Scene Completion with Self-Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Song Wang (Zhejiang University) · Jiawei Yu (Zhejiang University) · Wentong Li (College of Computer Science and Technology, Zhejiang University) · Wenyu Liu (Zhejiang University) · Xiaolu Liu (Zhejiang University) · Junbo Chen (UDEER AI PTE.LTD) · Jianke Zhu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Fairness-Aware Adversarial Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanghao Zhang (University of Liverpool) · Tianle Zhang (University of Liverpool) · Ronghui Mu (Lancaster University) · Xiaowei Huang (University of Liverpool) · Wenjie Ruan (University of Exeter)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Retrieval-Augmented Egocentric Video Captioning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jilan Xu (None) · Yifei Huang (The University of Tokyo) · Junlin Hou (Hong Kong University of Science and Technology) · Guo Chen (Nanjing University) · Yuejie Zhang (Fudan University) · Rui Feng (Fudan University) · Weidi Xie (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Low-Rank Knowledge Decomposition for Medical Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhang Zhou () · Haolin li (Fudan University) · Siyuan Du (Fudan University) · Jiangchao Yao (Shanghai Jiaotong University) · Ya Zhang (Shanghai Jiao Tong University) · Yanfeng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shivangi Aneja (Technical University of Munich) · Justus Thies (Max-Planck Institute for Intelligent Systems) · Angela Dai () · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Pixel-level Semantic Correspondence through Layout-aware Representation Learning and Multi-scale Matching Integration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixuan Sun (Fudan University) · Zhangyue Yin (Fudan University) · Haibo Wang (None) · Yan Wang (Fudan University) · Xipeng Qiu (Fudan University) · Weifeng Ge (Fudan University) · Wenqiang Zhang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Understanding Video Transfomers via Universal Concept Discovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            MATTHEW KOWAL (None) · Achal Dave (None) · Rares Andrei Ambrus (Toyota Research Institute) · Adrien Gaidon (Toyota Research Institute (TRI)) · Kosta Derpanis (York University/Samsung) · Pavel Tokmakov (Toyota Research Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CPR: Retrieval Augmented Generation for Copyright Protection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aditya Golatkar (University of California, Los Angeles) · Alessandro Achille (California Institute of Technology) · Luca Zancato (AWS AI Labs) · Yu-Xiang Wang (UC Santa Barbara / Amazon) · Ashwin Swaminathan (University of Maryland, College Park) · Stefano Soatto (AWS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generative Proxemics: A Prior for 3D Social Interaction from Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lea Müller (University of California, Berkeley) · Vickie Ye (University of California, Berkeley) · Georgios Pavlakos (University of Texas at Austin) · Michael J. Black (University of Tübingen) · Angjoo Kanazawa (UC Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Event-assisted Low-Light Video Object Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Hebei (University of Science and Technology of China) · Jin Wang (University of Science and Technology of China) · Jiahui Yuan (University of Science and Technology of China) · Yue Li (None) · Wenming Weng (None) · Yansong Peng (None) · Yueyi Zhang (University of Science and Technology of China) · Zhiwei Xiong (None) · Xiaoyan Sun (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/wjpoom/SPEC" target="_blank">Synthesize, Diagnose, and Optimize: Towards Fine-Grained Vision-Language Understanding</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wujian Peng (Fudan University) · Sicheng Xie (Fudan University) · Zuyao You (Fudan University) · Shiyi Lan (NVIDIA CORPORATION) · Zuxuan Wu (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Animating General Image with Large Visual Motion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dengsheng Chen (Meituan) · Xiaoming Wei (Meituan) · Xiaolin Wei (Meituan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DeIl: Direct and Inverse CLIP for Open-World Few-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuai Shao (Zhejiang Lab) · Yu Bai (China University of Petroleum（East China）) · Yan WANG (Beihang University) · Bao-di Liu (China University of Petroleum (East China)) · Yicong Zhou (University of Macau)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/xiyuanyang45/FedAS" target="_blank">FedAS: Bridging Inconsistency in Personalized Federated Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiyuan Yang (Wuhan University) · Wenke Huang (Wuhan University) · Mang Ye (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GPT4Point: A Unified Framework for Point-Language Understanding and Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhangyang Qi (None) · Ye Fang (None) · Zeyi Sun (Shanghai Jiao Tong University) · Xiaoyang Wu (The University of Hong Kong) · Tong Wu (None) · Jiaqi Wang (Shanghai AI Laboratory) · Dahua Lin (The Chinese University of Hong Kong) · Hengshuang Zhao (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Scene Adaptive Sparse Transformer for Event-based Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yansong Peng (None) · Li Hebei (University of Science and Technology of China) · Yueyi Zhang (University of Science and Technology of China) · Xiaoyan Sun (University of Science and Technology of China) · Feng Wu (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Transcending the Limit of Local Window: Advanced Super-Resolution Transformer with Adaptive Token Dictionary</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leheng Zhang (University of Electronic Science and Technology of China) · Yawei Li (ETH Zurich) · Xingyu Zhou (University of Electronic Science and Technology of China) · Xiaorui Zhao (None) · Shuhang Gu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rendering Every Pixel for High-Fidelity Geometry in 3D GANs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alex Trevithick (None) · Matthew Chan (NVIDIA) · Towaki Takikawa (NVIDIA) · Umar Iqbal (None) · Shalini De Mello (NVIDIA Research) · Manmohan Chandraker (UC San Diego) · Ravi Ramamoorthi (None) · Koki Nagano (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://spatial-vlm.github.io/" target="_blank">SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boyuan Chen (MIT) · Zhuo Xu (Google Deepmind) · Sean Kirmani (Google DeepMind) · brian ichter (Google) · Dorsa Sadigh (Google) · Leonidas Guibas (Stanford University) · Fei Xia (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Residual Learning in Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhang Junyu (Central South University) · Daochang Liu (University of Sydney) · Eunbyung Park (SKKU) · Shichao Zhang (Central South University) · Chang Xu (University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Blur2Blur: Blur Conversion for Unsupervised Image Deblurring on Unknown Domains</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bang-Dang Pham (VinAI Research) · Phong Tran (MBZUAI) · Anh Tran (VinAI Research) · Cuong Pham (Posts &amp; Telecommunications Institute of Technology and VinAI Research) · Rang Nguyen (VinAI Research) · Minh Hoai (State University of New York, Stony Brook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FreGS: 3D Gaussian Splatting with Progressive Frequency Regularization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahui Zhang (Nanyang Technological University) · Fangneng Zhan (None) · MUYU XU (Nanyang Technological University) · Shijian Lu (Nanyang Technological University) · Eric P. Xing (Mohamed bin Zayed Univeristy of AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PICTURE: PhotorealistIC virtual Try-on from UnconstRained dEsigns</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuliang Ning (The Chinese University of HongKong, ShenZhen) · Duomin Wang () · Yipeng Qin (Cardiff University) · Zirong Jin () · Baoyuan Wang (Xiaobing.ai) · Xiaoguang Han (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Stable Neighbor Denoising for Source-free Domain Adaptive Segmentation.</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dong Zhao (Xi'an University of Electronic Science and Technology) · Shuang Wang (Xidian University) · Qi Zang (Xidian University) · Licheng Jiao (Xidian University) · Nicu Sebe (University of Trento) · Zhun Zhong (University of Nottingham)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Revisiting Sampson Approximations for Geometric Estimation Problems</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Felix Rydell (KTH Royal Institute of Technology) · Angelica Torres (Max Planck Institute for Mathematics in the Sciences) · Viktor Larsson (Lund University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Neural 3D Strokes: Creating Stylized 3D Scenes with Vectorized 3D Strokes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haobin Duan (Beihang University) · Miao Wang (Beihang University) · Yanxun Li (Buaa Software Engineering) · Yong-Liang Yang (University of Bath)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-modal Instruction Tuned LLMs with Fine-grained Visual Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junwen He (Dalian University of Technology) · Yifan Wang (Dalian University of Technology) · Lijun Wang (Dalian University of Technology) · Huchuan Lu (Dalian University of Technology) · Bin Luo (Alibaba Group) · Jun-Yan He (DAMO Academy, Alibaba Group) · Jin-Peng Lan (Alibaba Group) · Xuansong Xie (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://subhadeepkoley.github.io/DiffusionZSSBIR/" target="_blank">Text-to-Image Diffusion Models are Great Sketch-Photo Matchmakers</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Subhadeep Koley (University of Surrey) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Aneeshan Sain (University of Surrey) · Pinaki Nath Chowdhury (University of Surrey) · Tao Xiang (University of Surrey) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Flexible Depth Completion for Sparse and Varying Point Densities</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinhyung Park (Carnegie Mellon University) · Yu-Jhe Li (Carnegie Mellon University) · Kris Kitani (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion Latent Aligners</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yazhou Xing (The Hong Kong University of Science and Technology) · Yingqing He (HKUST) · Zeyue Tian (Hong Kong University of Science and Technology) · Xintao Wang (Tencent) · Qifeng Chen (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Sparse Global Matching for Video Frame Interpolation with Large Motion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chunxu Liu (Nanjing University) · Guozhen Zhang (Nanjing University) · Rui Zhao (Qing Yuan Research Institute, Shanghai Jiao Tong University) · Limin Wang (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PIGEON: Predicting Image Geolocations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lukas Haas (Stanford University) · Michal Skreta (Stanford University) · Silas Alberti (Stanford University) · Chelsea Finn (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Generalization via Meta-Learning on Hard Samples</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nishant Jain (Indian Institute of Technology, Roorkee, Dhirubhai Ambani Institute Of Information and Communication Technology) · Arun Suggala (Google) · Pradeep Shenoy (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Action-slot: Visual Action-centric Representations for Multi-label Atomic Activity Recognition in Traffic Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chi-Hsi Kung (National Yang Ming Chiao Tung University) · 書緯 呂 (National Yang Ming Chiao Tung University) · Yi-Hsuan Tsai (Google) · Yi-Ting Chen (National Yang Ming Chiao Tung University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CLIP as RNN:  Segment Countless Visual Concepts without Training Endeavor</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuyang Sun (University of Oxford) · Runjia Li (University of Oxford) · Philip H.S. Torr (University of Oxford) · Xiuye Gu (None) · Siyang Li (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>M&amp;M VTO: Multi-Garment Virtual Try-On and Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Luyang Zhu (Department of Computer Science, University of Washington) · Yingwei Li (Google) · Nan Liu (Google) · Hao Peng (Google) · Dawei Yang (Google Inc.) · Ira Kemelmacher-Shlizerman (University of Washington)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhonglin Sun (Queen Mary University of London) · Chen Feng (Queen Mary University of London) · Ioannis Patras (Queen Mary University of London) · Georgios Tzimiropoulos (Queen Mary University London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SinSR: Diffusion-Based Image Super-Resolution in a Single Step</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufei Wang (Nanyang Technological University) · Wenhan Yang (Peng Cheng Lab) · Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) · Yaohui Wang (Shanghai AI Laboratory) · Lanqing Guo (Nanyang Technological University) · Lap-Pui Chau (The Hong Kong Polytechnic University) · Ziwei Liu (Nanyang Technological University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Alex C. Kot (Nanyang Technological University) · Bihan Wen (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Tuning Stable Rank Shrinkage: Aiming at the Overlooked Structural Risk in Fine-tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sicong Shen (Beihang University) · Yang Zhou (Beihang University) · Bingzheng Wei (Xiaomi Corporation) · Eric Chang (Massachusetts Institute of Technology) · Yan Xu (Beijing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiSR-NeRF: Diffusion-Guided View-Consistent Super-Resolution NeRF</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jie Long Lee (None) · Chen Li (National University of Singapore) · Gim Hee Lee (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Taming Mode Collapse in Score Distillation for Text-to-3D Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peihao Wang (University of Texas, Austin) · Dejia Xu (University of Texas at Austin) · Zhiwen Fan (University of Texas, Austin) · Dilin Wang (Facebook) · Sreyas Mohan (Meta) · Forrest Iandola (Meta) · Rakesh Ranjan () · Yilei Li (Facebook) · Qiang Liu (University of Texas, Austin) · Zhangyang Wang (University of Texas at Austin) · Vikas Chandra (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Relightable and Animatable Neural Avatar from Sparse-View Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Xu (Zhejiang University) · Sida Peng (None) · Chen Geng (Stanford University) · Linzhan Mou (Zhejiang University) · Zihan Yan (University of Illinois Urbana-Champaign) · Jiaming Sun (Image Derivative Inc.) · Hujun Bao (Zhejiang University) · Xiaowei Zhou (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DVMNet: Computing Relative Pose for Unseen Objects Beyond Hypotheses</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Zhao (EPFL) · Tong Zhang (EPFL) · Zheng Dang (None) · Mathieu Salzmann (EPFL)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PostureHMR: Posture Transformation for 3D Human Mesh Recovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yupei Song (None) · Xiao WU (Southwest Jiaotong University) · Zhaoquan Yuan (None) · Jian-Jun Qiao (Southwest Jiaotong University) · Qiang Peng (Southwest Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaqi Lin (Tsinghua University) · Zhihao Li (Huawei Noah's Ark Lab) · Xiao Tang (Huawei Technologies Ltd.) · Jianzhuang Liu (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Shiyong Liu (Huawei Noah's Ark Lab) · Jiayue Liu (Tsinghua University, Tsinghua University) · Yangdi Lu (Huawei Technologies Ltd.) · Xiaofei Wu (Huawei Technologies Ltd.) · Songcen Xu (Huawei Noah's Ark Lab) · Youliang Yan (Huawei Technologies Ltd.) · Wenming Yang (Tsinghua University,)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WANDR: Intention-guided Human Motion Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Markos Diomataris (None) · Nikos Athanasiou (None) · Omid Taheri () · Xi Wang (None) · Otmar Hilliges (None) · Michael J. Black (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Scaffold-GS: Structured 3D Gaussians for View-Adaptive Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tao Lu (Nanjing University) · Mulin Yu (Shanghai AI Laboratory) · Linning Xu (The Chinese University of Hong Kong) · Yuanbo Xiangli (None) · Limin Wang (Nanjing University) · Dahua Lin (The Chinese University of Hong Kong) · Bo Dai (Shanghai AI Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SimDA: Simple Diffusion Adapter for Efficient Video Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Xing (Fudan University) · Qi Dai (Microsoft Research Asia) · Han Hu (Microsft Research Asia) · Zuxuan Wu (Fudan University) · Yu-Gang Jiang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GART: Gaussian Articulated Template Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahui Lei (University of Pennsylvania) · Yufu Wang (University of Pennsylvania) · Georgios Pavlakos (University of Texas at Austin) · Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) · Kostas Daniilidis (University of Pennsylvania)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning from Observer Gaze: Zero-shot Attention Prediction Oriented by Human-Object Interaction Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuchen Zhou (Sun Yat-Sen University) · Linkai Liu (Sun Yat-Sen University) · Chao Gou (Sun Yat-Sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Spatio-Temporal Turbulence Mitigation: A Translational Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingguang Zhang (Purdue University) · Nicholas M Chimitt () · Yiheng Chi (Purdue University) · Zhiyuan Mao (Samsung Research America) · Stanley H. Chan (Purdue University, USA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Anchor-based Robust Finetuning of Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinwei Han (Wuhan University) · Zhiwen Lin (Tencent) · Zhongyisun Sun (Tencent Youtu Lab) · Yingguo Gao (Tencent Youtu Lab) · Ke Yan () · Shouhong Ding (Tencent Youtu Lab) · Yuan Gao (Wuhan University) · Gui-Song Xia (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Denoising Point Cloud in Latent Space via Graph Convolution and Invertible Neural Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aihua Mao (South China University of Technology) · Biao Yan (None) · Zijing Ma (South China University of Technology) · Ying He (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Composing Object Relations and Attributes for Image-Text Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Khoi Pham (University of Maryland, College Park) · Chuong Huynh (University of Maryland, College Park) · Ser-Nam Lim (Meta AI) · Abhinav Shrivastava (University of Maryland)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diffusion Handles: Enabling 3D Edits for Diffusion Models by Lifting Activations to 3D</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Karran Pandey (University of Toronto) · Paul Guerrero (Adobe Systems) · Matheus Gadelha (Adobe Systems) · Yannick Hold-Geoffroy (Adobe Research) · Karan Singh (Department of Computer Science) · Niloy J. Mitra (University College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PSDPM: Prototype-based Secondary Discriminative Pixels Mining for Weakly Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinqiao Zhao (Xi’an Jiaotong-Liverpool University) · Yang (None) · Tianhong Dai (University of Aberdeen) · Bingfeng Zhang (China University of Petroleum (East China)) · Jimin Xiao (Xi'an Jiaotong-Liverpool University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DreamMatcher: Appearance Matching Self-Attention for Semantically-Consistent Text-to-Image Personalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jisu Nam (Korea University) · Heesu Kim (NAVER) · DongJae Lee (KAIST) · Siyoon Jin (Korea University) · Seungryong Kim (Korea University) · Seunggyu Chang (NAVER Cloud)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>COTR: Compact Occupancy TRansformer for Vision-based 3D Occupancy Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qihang Ma (East China Normal Universitry) · Xin Tan (East China Normal University) · Yanyun Qu (Xiamen University) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University) · Zhizhong Zhang (East China Normal University) · Yuan Xie (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generalizable Novel-View Synthesis using a Stereo Camera</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haechan Lee (Pohang University of Science and Technology) · Wonjoon Jin (Pohang University of Science and Technology) · Seung-Hwan Baek (POSTECH) · Sunghyun Cho (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Prompt3D: Random Prompt Assisted Weakly-Supervised 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaohong Zhang (None) · Huisheng Ye (Nanjing University) · Jingwen Li (Nanjing University) · Qinyu Tang (Nanjing University) · Yuanqi Li (Nanjing University) · Yanwen Guo (Nanjing University) · Jie Guo (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Language-driven All-in-one Adverse Weather Removal</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Yang (Beijing Institute of Technology) · Liyuan Pan (Beijing Institute of Technology) · Yan Yang (ANU) · Wei Liang (Beijing Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Meshflow and Optical Flow Estimation from Event Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinglong Luo (None) · Ao Luo (Megvii Technology Inc.) · Zhengning Wang (University of Electronic Science and Technology of China) · Chunyu Lin (Beijing Jiaotong University) · Bing Zeng (None) · Shuaicheng Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Volumetric Environment Representation for Vision-Language Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liu (None) · Wenguan Wang (Zhejiang University) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LiDAR4D: Dynamic Neural Fields for Novel Space-time View LiDAR Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zehan Zheng (Tongji University) · Fan Lu (Tongji University) · Weiyi Xue (Tongji University) · Guang Chen (Tongji University) · Changjun Jiang (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/ispc-lab/LEAD" target="_blank">LEAD: Learning Decomposition for Source-free Universal Domain Adaptation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sanqing Qu (Tongji University) · Tianpei Zou (Tongji University) · Lianghua He (Tongji University) · Florian Röhrbein (Chemnitz University of Technology) · Alois Knoll (Technical University Munich) · Guang Chen (Tongji University) · Changjun Jiang (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CG-HOI: Contact-Guided 3D Human-Object Interaction Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Christian Diller (Technische Universität München) · Angela Dai ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Contrastive Mean-Shift Learning for Generalized Category Discovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sua Choi (POSTECH) · Dahyun Kang (POSTECH) · Minsu Cho (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Federated Generalized Category Discovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nan Pu (University of Trento) · Wenjing Li (University of Science and Technology of China) · Xinyuan Ji (Leiden University) · Yalan Qin (Shanghai University) · Nicu Sebe (University of Trento) · Zhun Zhong (University of Nottingham)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Is Vanilla MLP in Neural Radiance Field Enough for Few-shot View Synthesis?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanxin Zhu (University of Science and Technology of China) · Tianyu He (None) · Xin Li (None) · Bingchen Li (University of Science and Technology of China) · Zhibo Chen (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiazuo Yu (Dalian University of Technology) · Yunzhi Zhuge (Dalian University of Technology) · Lu Zhang (Dalian University of Technology) · Ping Hu (University of Electronic Science and Technology of China) · Dong Wang (Dalian University of Technology) · Huchuan Lu (Dalian University of Technology) · You He (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://subhadeepkoley.github.io/AbstractAway/" target="_blank">How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval?</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Subhadeep Koley (University of Surrey) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Aneeshan Sain (University of Surrey) · Pinaki Nath Chowdhury (University of Surrey) · Tao Xiang (University of Surrey) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chong Mou (Peking University) · Xintao Wang (Tencent) · Jiechong Song (None) · Ying Shan (Tencent) · Jian Zhang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Iterated Learning Improves Compositionality in Large Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenhao Zheng (University of Michigan) · Jieyu Zhang (Department of Computer Science, University of Washington) · Aniruddha Kembhavi (Allen Institute for Artificial Intelligence) · Ranjay Krishna (University of Washington)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Detours for Navigating Instructional Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kumar Ashutosh (UT Austin &amp; FAIR, Meta) · Zihui Xue (None) · Tushar Nagarajan (Meta) · Kristen Grauman (University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Domain Gap Embeddings for Generative Dataset Augmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yinong Wang (Carnegie Mellon University) · Younjoon Chung (Carnegie Mellon University) · Chen Henry Wu (Carnegie Mellon University) · Fernando De la Torre (Carnegie Mellon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Domain-Agnostic Mutual Prompting for Unsupervised Domain Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhekai Du (University of Electronic Science and Technology of China) · Xinyao Li (University of Electronic Science and Technology of China) · Fengling Li (University of Technology Sydney) · Ke Lu (University of Electronic Science and Technology of China) · Lei Zhu (Shandong Normal University) · Jingjing Li (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TransLoc4D: Transformer-based 4D-Radar Place Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guohao Peng (Nanyang Technological University) · Heshan Li (Nanyang Technological University) · Yangyang Zhao (Nanyang Technological University) · Jun Zhang (Nanyang Technological University) · Zhenyu Wu (Nanyang Technological University) · Pengyu Zheng (Chinese University of Hong Kong) · Danwei Wang (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Higher-order Relational Reasoning for Pedestrian Trajectory Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sungjune Kim (Korea University) · Hyung-gun Chi (Purdue University) · Hyerin Lim (Hyundai Motor Company) · Karthik Ramani (Purdue University) · Jinkyu Kim (Korea University) · Sangpil Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learn to Rectify the Bias of CLIP for Unsupervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingyun Wang (None) · Guoliang Kang (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Leveraging Vision-Language Models for Improving Domain Generalization in Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sravanti Addepalli (Indian Institute of Science) · Ashish Asokan (Indian Institute of Science, Indian institute of science, Bangalore) · Lakshay Sharma (Indian Institute of Science, Indian institute of science, Bangalore) · R. Venkatesh Babu (Indian Institute of Science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Learning a Generalist Model for Embodied Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Duo Zheng (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Shijia Huang (The Chinese University of Hong Kong) · Lin Zhao (Beijing Institute of Technology) · Yiwu Zhong (University of Wisconsin, Madison) · Liwei Wang (CUHK)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Small Steps and Level Sets: Fitting Neural Surface Models with Point Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chamin Hewa Koneputugodage (Australian National University) · Yizhak Ben-Shabat (Technion, Israel Institute of Technology) · Dylan Campbell (Australian National University) · Stephen Gould (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Absolute Pose from One or Two Scaled and Oriented Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jonathan Ventura (None) · Zuzana Kukelova (Czech Technical University in Prague) · Torsten Sattler (Czech Technical University in Prague) · Daniel Barath (ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://zeeshanhayder.github.io/DSGG" target="_blank">DSGG: Dense Relation Transformer for an End-to-end Scene Graph Generation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeeshan Hayder (CSIRO) · Xuming He (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://corleone-huang.github.io/realcustom/" target="_blank">\emph{RealCustom}: Narrowing Real Text Word for Real-Time Open-Domain Text-to-Image Customization</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengqi Huang (University of Science and Technology of China) · Zhendong Mao (None) · Mingcong Liu (ByteDance Inc.) · Qian HE (Institute of Remote Sensing Application, Chinese Academic of Sciences) · Yongdong Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Driving Everywhere with Large Language Model Policy Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boyi Li (UC Berkeley / NVIDIA) · Yue Wang (Massachusetts Institute of Technology) · Jiageng Mao (CUHK) · Boris Ivanovic (NVIDIA) · Sushant Veer (NVIDIA) · Karen Leung (University of Washington) · Marco Pavone (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SANeRF-HQ: Segment Anything for NeRF in High Quality</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichen Liu (HKUST) · Benran Hu (The Hong Kong University of Science and Technology) · Chi-Keung Tang (The Hong Kong University of Science and Technology) · Yu-Wing Tai (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>APSeg: Auto-Prompt Network for Cross-Domain Few-Shot Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weizhao He (None) · Yang Zhang (Shenzhen University) · Wei Zhuo (Shenzhen University) · Linlin Shen (None) · Jiaqi Yang (University of Nottingham) · Songhe Deng (None) · Liang Sun (Shenzhen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ECLIPSE: Efficient Continual Learning in Panoptic Segmentation with Visual Prompt Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Beomyoung Kim (NAVER Cloud / KAIST) · Joonsang Yu (NAVER) · Sung Ju Hwang (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>InstanceDiffusion: Instance-level Control for Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xudong Wang (Electrical Engineering &amp; Computer Science Department, University of California Berkeley) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Sai Saketh Rambhatla (Meta) · Rohit Girdhar (Meta) · Ishan Misra (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Shadow Generation for Composite Image Using Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qingyang Liu (Shanghai Jiao Tong University) · Junqi You (Shanghai Jiaotong University) · Jian-Ting Wang (Shanghai JiaoTong University) · Xinhao Tao (Shanghai Jiaotong University) · Bo Zhang (Shanghai Jiao Tong University) · Li Niu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://haoyan14.github.io/DS-NeRV/" target="_blank">DS-NeRV: Implicit Neural Video Representation with Decomposed Static and Dynamic Codes</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Yan (Tianjin University) · Zhihui Ke (Tianjin University) · Xiaobo Zhou (Tianjin University) · Tie Qiu (Tianjin University) · Xidong Shi (Tianjin University) · DaDong Jiang (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OVER-NAV: Elevating Iterative Vision-and-Language Navigation with Open-Vocabulary Detection and StructurEd Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ganlong Zhao (University of Hong Kong) · Guanbin Li (Sun Yat-sen University) · Weikai Chen (Tencent America) · Yizhou Yu (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rolling Shutter Correction with Intermediate Distortion Flow Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingdeng Cao (The University of Tokyo) · Sidi Yang (Shenzhen International Graduate School, Tsinghua University) · Yujiu Yang (Tsinghua University) · Yinqiang Zheng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Transferable Targeted 3D Adversarial Attack in the Physical World</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yao Huang (Beihang University) · Yinpeng Dong (Tsinghua University) · Shouwei Ruan (None) · Xiao Yang (Tsinghua University, Tsinghua University) · Hang Su (Tsinghua University) · Xingxing Wei (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AnyDoor: Zero-shot Object-level Image Customization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xi Chen (the University of Hong Kong, University of Hong Kong) · Lianghua Huang (Alibaba Group) · Yu Liu (Alibaba Group) · Yujun Shen (The Chinese University of Hong Kong) · Deli Zhao (Alibaba Group) · Hengshuang Zhao (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GraphDreamer: Compositional 3D Scene Synthesis from Scene Graphs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gege Gao (ETH Zürich) · Weiyang Liu (University of Cambridge) · Anpei Chen (Department of Computer Science, ETHZ - ETH Zurich) · Andreas Geiger (University of Tübingen) · Bernhard Schölkopf (ELLIS Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Revisiting Spatial-Frequency Information Integration from a Hierarchical Perspective for Panchromatic and Multi-Spectral Image Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiangtong Tan (None) · Jie Huang (University of Science and Technology of China) · Naishan Zheng (University of Science and Technology of China) · Man Zhou (University of Science and Technology of China) · Keyu Yan (University of Science and Technology of China) · Danfeng Hong (Chinese Academy of Sciences, Aerospace Information Research Institute) · Feng Zhao (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3D Facial Expressions through Analysis-by-Neural-Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            George Retsinas (None) · Panagiotis Filntisis (None) · Radek Danecek (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Victoria Abrevaya (None) · Anastasios Roussos (Foundation for Research and Technology - Hellas) · Timo Bolkart (Google) · Petros Maragos (National Technical University of Athens)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Exploring the Transferability of Visual Prompting for Multimodal Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichi Zhang (Tsinghua University) · Yinpeng Dong (Tsinghua University) · Siyuan Zhang (None) · Tianzan Min (Tsinghua University, Tsinghua University) · Hang Su (Tsinghua University) · Jun Zhu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unified Language-driven Zero-shot Domain Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Senqiao Yang (Harbin Institute of Technology) · Zhuotao Tian (The Chinese University of Hong Kong) · Li Jiang (Max Planck Institute for Informatics) · Jiaya Jia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Aligning Logits Generatively for Principled Black-Box Knowledge Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jing Ma (None) · Xiang Xiang (None) · Ke Wang (Alibaba Group) · Yuchuan Wu (Alibaba Group) · Yongbin Li (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HomoFormer: Homogenized Transformer for Image Shadow Removal</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jie Xiao (University of Science and Technology of China) · Xueyang Fu (University of Science and Technology of China) · Yurui Zhu (University of Science and Technology of China) · Dong Li (University of Science and Technology of China) · Jie Huang (University of Science and Technology of China) · Kai Zhu (University of Science and Technology of China) · Zheng-Jun Zha (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Narges Norouzi (None) · Svetlana Orlova (Eindhoven University of Technology) · Daan de Geus (Eindhoven University of Technology) · Gijs Dubbelman (Eindhoven University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Efficient LoFTR: Semi-Dense Local Feature Matching with Sparse-Like Speed</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifan Wang (Zhejiang University) · Xingyi He (Zhejiang University) · Sida Peng (None) · Dongli Tan (Zhejiang University) · Xiaowei Zhou (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Language-guided Image Reflection Separation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haofeng Zhong (Peking University) · Yuchen Hong (Peking University) · Shuchen Weng (Peking University) · Jinxiu Liang (None) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiming Zhang (None) · Zhening Xing (Shanghai AI Laboratory) · Yanhong Zeng (None) · Youqing Fang (Anhui University) · Kai Chen (Shanghai AI Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Motion Diversification Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hee Jae Kim (Boston University, Boston University) · Eshed Ohn-Bar (Boston University, Boston University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>On the Scalability of Diffusion-based Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Li (AWS AI Labs) · Yang Zou (Amazon) · Ying Wang (Amazon) · Orchid Majumder (Amazon Web Services) · Yusheng Xie (Amazon) · R. Manmatha (Amazon) · Ashwin Swaminathan (University of Maryland, College Park) · Zhuowen Tu (University of California, San Diego) · Stefano Ermon (Stanford University) · Stefano Soatto (AWS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FaceLift: Semi-supervised 3D Facial Landmark Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            David Ferman (Flawless AI) · Pablo Garrido (Flawless AI) · Gaurav Bharaj (Flawless AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BSNet: Box-Supervised Simulation-assisted Mean Teacher for 3D Instance Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahao Lu (University of Science and Technology of China) · Jiacheng Deng (University of Science and Technology of China) · Tianzhu Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unlocking Pretrained Image Backbones for Semantic Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tariq Berrada (Meta) · Jakob Verbeek (Meta AI) · camille couprie (Facebook) · Karteek Alahari (Inria)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HarmonyView: Harmonizing Consistency and Diversity in One-Image-to-3D</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sangmin Woo (Korea Advanced Institute of Science &amp; Technology) · byeongjun park () · Hyojun Go (Twelvelabs) · Jin-Young Kim (Yonsei University) · Changick Kim (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Infer from What You Have Seen Before: Temporally-dependent Classifier for Semi-supervised Video Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiafan Zhuang (Shantou University) · Zilei Wang (University of Science and Technology of China) · Yixin Zhang (University of Science and Technology of China) · Zhun Fan (Shantou University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adapt Before Comparison: A New Perspective on Cross-Domain Few-Shot Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jonas Herzog (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://chenyangsi.top/FreeU/" target="_blank">FreeU: Free Lunch in Diffusion U-Net</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenyang Si (Nanyang Technological University  Singapore) · Ziqi Huang (Nanyang Technological University) · Yuming Jiang (Nanyang Technological University) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>From Variance to Veracity: Unbundling and Mitigating Gradient Variance in Differentiable Bundle Adjustment Layers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Swaminathan Gurumurthy (School of Computer Science, Carnegie Mellon University) · Karnik Ram (Technische Universität München) · Bingqing Chen (Bosch) · Zachary Manchester (Carnegie Mellon University) · Zico Kolter (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Image Restoration by Denoising Diffusion Models With Iteratively Preconditioned Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tomer Garber (Open University of Israel) · Tom Tirer (Bar-Ilan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mean-Shift Feature Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Takumi Kobayashi (National Institute of Advanced Industrial Science and Technology (AIST))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SFOD: Spiking Fusion Object Detector</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yimeng Fan (School of Microelectronics, Tianjin University) · Wei Zhang (None) · Changsong Liu (Tianjin University) · Mingyang Li (Tianjin University) · Wenrui Lu (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RegionGPT: Towards Region Understanding Vision Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiushan Guo (The University of Hong Kong) · Shalini De Mello (NVIDIA Research) · Danny Yin (NVIDIA) · Wonmin Byeon (NVIDIA) · Ka Chun Cheung (NVIDIA) · Yizhou Yu (The University of Hong Kong) · Ping Luo (The University of Hong Kong) · Sifei Liu (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unlocking the Potential of Pre-trained Vision Transformers for Few-Shot Semantic Segmentation through Relationship Descriptors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziqin Zhou (None) · Hai-Ming Xu (The University of Adelaide) · Yangyang Shu (None) · Lingqiao Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Relational Matching for Weakly Semi-Supervised Oriented Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenhao Wu (City University of Hong Kong) · Hau San Wong (City University of Hong Kong) · Si Wu (South China University of Technology) · Tianyou Zhang (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>JointSQ: Joint Sparsification-Quantization for Distributed Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weiying Xie (None) · Haowei Li (None) · Ma Jitao (None) · Yunsong Li () · Jie Lei (Xi'an University of Electronic Science and Technology) · donglai Liu (Xi'an University of Electronic Science and Technology) · Leyuan Fang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Endow SAM with Keen Eyes: Temporal-spatial Prompt Learning for Video Camouflaged Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenjun Hui (None) · Zhenfeng Zhu (Beijing Jiaotong University) · Shuai Zheng (Beijing Jiaotong University) · Yao Zhao (Beijing Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NICE: Neurogenesis Inspired Contextual Encoding for Replay-free Class Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mustafa B Gurbuz (Georgia Institute of Technology) · Jean Moorman (Georgia Institute of Technology) · Constantine Dovrolis (Georgia Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Axel Barroso-Laguna (None) · Sowmya Munukutla (None) · Victor Adrian Prisacariu (None) · Eric Brachmann (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning for Transductive Threshold Calibration in Open-World Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qin ZHANG (Amazon) · DONGSHENG An (Amazon) · Tianjun Xiao (Amazon) · Tong He (Amazon Web Services) · Qingming Tang (Amazon, Alexa) · Ying Nian Wu (UCLA) · Joseph Tighe (Meta) · Yifan Xing (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FocusMAE: Gallbladder Cancer Detection from Ultrasound Videos with Focused Masked Autoencoders</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Soumen Basu (Indian Institute of Technology Delhi) · Mayuna Gupta (Indian Institute of Technology, Delhi) · Chetan Madan (Indian Institute of Technology, Delhi) · Pankaj Gupta (PGIMER Chandigarh) · Chetan Arora (Indian Institute of Technology Delhi)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LightOctree: Lightweight 3D Spatially-Coherent Indoor Lighting Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuecan Wang (None) · Shibang Xiao (Beijing University of Aeronautics and Astronautics) · Xiaohui Liang (Zhongguancun Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>pix2gestalt: Amodal Segmentation by Synthesizing Wholes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ege Ozguroglu () · Ruoshi Liu (Columbia University) · Dídac Surís (Columbia University) · Dian Chen (Toyota Research Institute) · Achal Dave (None) · Pavel Tokmakov (Toyota Research Institute) · Carl Vondrick (Columbia University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Navigate Beyond Shortcuts: Debiased Learning through the Lens of Neural Collapse</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yining Wang (Fudan University) · Junjie Sun (Fudan University) · Chenyue Wang (Fudan University) · Mi Zhang (Fudan University) · Min Yang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SD4Match: Learning to Prompt Stable Diffusion Model for Semantic Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinghui Li (University of Oxford) · Jingyi Lu (University of Hong Kong) · Kai Han (The University of Hong Kong) · Victor Adrian Prisacariu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://sjojok.github.io/3dgstream/" target="_blank">3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming of Photo-Realistic Free-Viewpoint Videos</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiakai Sun (Zhejiang University) · Han Jiao (Zhejiang University) · Guangyuan Li (Zhejiang University) · Zhanjie Zhang (Zhejiang University) · Lei Zhao (Zhejiang University) · Wei Xing (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TextCraftor: Your Text Encoder Can be Image Quality Controller</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanyu Li (Northeastern University) · Xian Liu (The Chinese University of Hong Kong) · Anil Kag (Snap Inc.) · Ju Hu (Snap Inc.) · Yerlan Idelbayev (Snap Inc.) · Dhritiman Sagar (Snap Inc.) · Yanzhi Wang (Northeastern University) · Sergey Tulyakov (Snap Inc.) · Jian Ren (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>3D Human Pose Perception from Egocentric Stereo Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hiroyasu Akada (Max Planck Institute for Informatics) · Jian Wang (Max Planck Institute for Informatics) · Vladislav Golyanik (MPI for Informatics) · Christian Theobalt (MPI Informatik)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generalized Large-Scale Data Condensation via Various Backbone and Statistical Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shitong Shao (Southeast University) · Zeyuan Yin (Mohamed bin Zayed University of Artificial Intelligence) · Muxin Zhou (Mohamed bin Zayed University of Artificial Intelligence) · Xindong Zhang (The Hong Kong Polytechnic University, Hong Kong Polytechnic University) · Zhiqiang Shen (Mohamed bin Zayed University of Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AAMDM: Accelerated Auto-regressive Motion Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianyu Li (Georgia Institute of Technology) · Calvin Zhuhan Qiao (University of British Columbia) · Ren Guanqiao (Beijing University of Aeronautics and Astronautics) · KangKang Yin (Simon Fraser University) · Sehoon Ha (Georgia Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TexOct: Generating Textures of 3D Models with Octree-based Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jialun Liu (Baidu) · Chenming Wu (None) · Xinqi Liu (Baidu Inc) · Xing Liu (Baidu) · Jinbo Wu (Baidu) · Haotian Peng (Baidu) · Chen Zhao (None) · Haocheng Feng (Baidu) · Jingtuo Liu (Baidu) · Errui Ding (Baidu Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OTE: Exploring Accurate Scene Text Recognition Using One Token</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianjun Xu (University of Science and Technology of China) · Yuxin Wang (University of Science and Technology of China) · Hongtao Xie (University of Science and Technology of China) · Yongdong Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OmniVid: A Generative Framework for Universal Video Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junke Wang (None) · Dongdong Chen (Microsoft Research) · Chong Luo (Microsoft Research Asia) · Bo He (None) · Lu Yuan (Microsoft) · Zuxuan Wu (Fudan University) · Yu-Gang Jiang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Check, Locate, Rectify: A Training-Free Layout Calibration System for Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Biao Gong (Alibaba Group) · Siteng Huang (Zhejiang University &amp; Westlake University) · Yutong Feng (Alibaba Group) · Shiwei Zhang (Alibaba Group) · Yuyuan Li (Zhejiang University) · Yu Liu (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-24-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;L&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;Q&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;M&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;F&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;o&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;r&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;m&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;e&lt;/mi&gt;&lt;mi mathvariant=&quot;sans-serif&quot;&gt;r&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-136" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-137" class="mjx-mrow"><span id="MJXc-Node-138" class="mjx-texatom"><span id="MJXc-Node-139" class="mjx-mrow"><span id="MJXc-Node-140" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.503em; padding-bottom: 0.253em;">L</span></span><span id="MJXc-Node-141" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.503em; padding-bottom: 0.378em;">Q</span></span><span id="MJXc-Node-142" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.503em; padding-bottom: 0.253em;">M</span></span><span id="MJXc-Node-143" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.441em; padding-bottom: 0.253em;">F</span></span><span id="MJXc-Node-144" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.253em; padding-bottom: 0.316em;">o</span></span><span id="MJXc-Node-145" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.253em; padding-bottom: 0.253em;">r</span></span><span id="MJXc-Node-146" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.253em; padding-bottom: 0.253em;">m</span></span><span id="MJXc-Node-147" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.253em; padding-bottom: 0.316em;">e</span></span><span id="MJXc-Node-148" class="mjx-mi"><span class="mjx-char MJXc-TeX-sans-R" style="padding-top: 0.253em; padding-bottom: 0.253em;">r</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mi mathvariant="sans-serif">L</mi><mi mathvariant="sans-serif">Q</mi><mi mathvariant="sans-serif">M</mi><mi mathvariant="sans-serif">F</mi><mi mathvariant="sans-serif">o</mi><mi mathvariant="sans-serif">r</mi><mi mathvariant="sans-serif">m</mi><mi mathvariant="sans-serif">e</mi><mi mathvariant="sans-serif">r</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-24">\mathsf{LQMFormer}</script>:~Language-aware Query Mask Transformer for Referring Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nisarg Shah (Johns Hopkins University) · Vibashan VS (Johns Hopkins University) · Vishal M. Patel (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Latent Modulated Function for Computational Optimal Continuous Image Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zongyao He (Sun Yat-sen University) · Zhi Jin (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bingxin Ke (ETH Zurich) · Anton Obukhov (None) · Shengyu Huang (None) · Nando Metzger (ETH Zürich) · Rodrigo Caye Daudt (ETH Zurich) · Konrad Schindler (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LiDAR-based Person Re-identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenxuan Guo (Tsinghua University) · Zhiyu Pan (Department of Automation, Tsinghua University) · Yingping Liang (None) · Ziheng Xi (Tsinghua University, Tsinghua University) · Zhi Chen Zhong (Tsinghua University, Tsinghua University) · Jianjiang Feng (Tsinghua University) · Jie Zhou (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Shallow-Deep Collaborative Learning for Unsupervised Visible-Infrared Person Re-Identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Yang (Wuhan University) · Jun Chen (Wuhan University) · Mang Ye (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Spherical Mask: Coarse-to-Fine 3D Point Cloud Instance Segmentation with Spherical Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sangyun Shin (University of Oxford) · Kaichen Zhou (Department of Computer Science, University of Oxford) · Madhu Vankadari (Department of Computer Science, University of Oxford) · Andrew Markham (University of Oxford) · Niki Trigoni (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Spline Fields for Burst Image Fusion and Layer Separation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ilya Chugunov (Princeton University) · David Shustin (Princeton University) · Ruyu Yan (Princeton University) · Chenyang Lei (The Hong Kong University of Science and Technology) · Felix Heide (Department of Computer Science, Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>L2B: Learning to Bootstrap Robust Models for Combating Label Noise</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuyin Zhou (UC Santa Cruz) · Xianhang li (University of California, Santa Cruz) · Fengze Liu (ByteDance) · Qingyue Wei (Stanford University) · Xuxi Chen (University of Texas at Austin) · Lequan Yu (The University of Hong Kong) · Cihang Xie (University of California, Santa Cruz) · Matthew P. Lungren (Microsoft) · Lei Xing (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deep Video Inverse Tone Mapping Based on Temporal Clues</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuyao Ye (Peking University) · Ning Zhang (None) · Yang Zhao (Hefei University of Technology) · Hongbin Cao (ByteDance) · Ronggang Wang (Peking University Shenzhen Graduate School)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://ziqiaopeng.github.io/synctalk/" target="_blank">SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziqiao Peng (Renmin University of China) · Wentao Hu (Beijing University of Posts and Telecommunications) · Yue Shi (Psyche AI Inc.) · Xiangyu Zhu (None) · Xiaomei Zhang (None) · Hao Zhao (Tsinghua University, Tsinghua University) · Jun He (Renmin University of China) · Hongyan Liu (Tsinghua University, Tsinghua University) · Zhaoxin Fan (Renmin University of China, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Attack To Defend: Exploiting Adversarial Attacks for Detecting Poisoned Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Samar Fares (None) · Karthik Nandakumar (Mohamed Bin Zayed University of Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Non-autoregressive Sequence-to-Sequence Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kunyu Shi (Amazon) · Qi Dong (Amazon) · Luis Goncalves (California Institute of Technology) · Zhuowen Tu (University of California, San Diego) · Stefano Soatto (AWS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Seeing the Unseen: Visual Common Sense for Semantic Placement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ram Ramrakhya (None) · Aniruddha Kembhavi (Allen Institute for Artificial Intelligence) · Dhruv Batra (FAIR (Meta) and Georgia Tech) · Zsolt Kira (Georgia Institute of Technology) · Kuo-Hao Zeng (Allen Institute for Artificial Intelligence) · Luca Weihs (Allen Institute for Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://www.whyy.site/paper/nep" target="_blank">Inverse Rendering of Glossy Objects via the Neural Plenoptic Function and Radiance Fields</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyuan Wang (City University of Hong Kong) · Wenbo Hu (Tencent AI Lab) · Lei Zhu (City University of Hong Kong) · Rynson W.H. Lau (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/PRBonn/4dNDF" target="_blank">3D LiDAR Mapping in Dynamic Environments using a 4D Implicit Neural Representation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingguang Zhong (Rheinische Friedrich-Wilhelms Universität Bonn) · Yue Pan (University of Bonn) · Cyrill Stachniss (University of Bonn) · Jens Behley (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanhui Huang (Tsinghua University) · Wenzhao Zheng (Tsinghua University, Tsinghua University) · Borui Zhang (Tsinghua University, Tsinghua University) · Jie Zhou (None) · Jiwen Lu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SUGAR: Pre-training 3D Visual Representation for Robotics</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shizhe Chen (INRIA) · Ricardo Garcia Pinel (INRIA) · Ivan Laptev (INRIA Paris) · Cordelia Schmid (Inria / Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GaussianDreamer: Fast Generation from Text to 3D Gaussians by Bridging 2D and 3D Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taoran Yi (Huazhong University of Science and Technology) · Jiemin Fang (Huawei Technologies Ltd.) · Junjie Wang (None) · Guanjun Wu (None) · Lingxi Xie (Huawei Technologies Ltd.) · Xiaopeng Zhang (Huawei Technologies Ltd.) · Wenyu Liu (Huazhong University of Science and Technology) · Qi Tian (Huawei Technologies Ltd.) · Xinggang Wang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Active Generalized Category Discovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shijie Ma (Institute of Automation, Chinese Academy of Sciences) · Fei Zhu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Zhun Zhong (University of Nottingham) · Xu-Yao Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Cheng-Lin Liu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CoG-DQA: Chain-of-Guiding Learning with Large Language Models for Diagram Question Answering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shaowei Wang (Xi'an Jiaotong University) · Lingling Zhang (Xi'an Jiaotong University) · Longji Zhu (Xi'an Jiaotong University) · Tao Qin (Xi'an Jiaotong University) · Kim-Hui Yap (Nanyang Technological University) · Xinyu Zhang (None) · Jun Liu (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Coherence As Texture -- Passive Textureless 3D Reconstruction by Self-interference</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei-Yu Chen (Carnegie Mellon University) · Aswin C. Sankaranarayanan (Carnegie Mellon University) · Anat Levin (Weizmann Institute of Science) · Matthew O’Toole (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Backpack Full of Skills: Egocentric Video Understanding with Diverse Task Perspectives</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Simone Peirone (Polytechnic Institute of Turin) · Francesca Pistilli (Polytechnic Institute of Turin) · Antonio Alliegro (Politecnico di Torino) · Giuseppe Averta (Polytechnic of Turin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://maincold2.github.io/c3dgs/" target="_blank">Compact 3D Gaussian Representation for Radiance Field</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Joo Chan Lee (Sungkyunkwan University) · Daniel Rho (Korea Telecom Research) · Xiangyu Sun (None) · Jong Hwan Ko (Sungkyunkwan University (SKKU)) · Eunbyung Park (SKKU)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Christian Diller (Technische Universität München) · Thomas Funkhouser (Princeton University) · Angela Dai ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FlowIE：Efficient Image Enhancement via Rectified Flow</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixuan Zhu (Tsinghua University) · Wenliang Zhao (Automation, Tsinghua University, Tsinghua University) · Ao Li (Tsinghua University) · Yansong Tang (Tsinghua University) · Jie Zhou (None) · Jiwen Lu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Combining Frame and GOP Embeddings for Neural Video Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jens Eirik Saethre (ETH Zurich &amp; Disney Research|Studios) · Roberto Azevedo (Disney Research, Disney) · Christopher Schroers (Disney Research|Studios, Disney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qidong Huang (University of Science and Technology of China) · Xiaoyi Dong (Microsoft) · Pan Zhang (Shanghai Artificial Intelligence Laboratory) · Bin Wang (Shanghai AI Laboratory) · Conghui He (None) · Jiaqi Wang (Shanghai AI Laboratory) · Dahua Lin (The Chinese University of Hong Kong) · Weiming Zhang (University of Science and Technology of China) · Nenghai Yu (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yushi Huang (SenseTime) · Ruihao Gong (SenseTime) · Jing Liu () · Tianlong Chen (Massachusetts Institute of Technology) · Xianglong Liu (BUAA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Not All Classes Stand on Same Embeddings: Calibrating a Semantic Distance with Metric Tensor</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jae Hyeon Park (None) · Gyoomin Lee (Dongguk University) · Seunggi Park (Dongguk University) · Sung In Cho (Dongguk University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shengqu Cai (ETH Zurich &amp; Stanford University) · Duygu Ceylan (Adobe Systems) · Matheus Gadelha (Adobe Systems) · Chun-Hao P. Huang (Adobe Systems) · Tuanfeng Y. Wang (None) · Gordon Wetzstein (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Out-of-Distribution Generalization in Graphs via Hierarchical Semantic Environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yinhua Piao (Seoul National University) · Sangseon Lee (Seoul National University) · Yijingxiu Lu (Seoul National University) · Sun Kim (Seoul National University, Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Understanding and Improving Adversarial Robustness of Vision Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Samyak Jain () · Tanima Dutta (IIT BHU)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fang Kaipeng (None) · Jingkuan Song (University of Electronic Science and Technology of China,) · Lianli Gao (University of Electronic Science and Technology of China, Tsinghua University) · Pengpeng Zeng (University of Electronic Science and Technology of China) · Zhi-Qi Cheng (Carnegie Mellon University) · Xiyao LI (Kuaishou Technology) · Heng Tao Shen (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yankai Jiang (Shanghai Artificial Intelligence Laboratory) · Zhongzhen Huang (None) · Rongzhao Zhang (Shanghai Artificial Intelligence Laboratory) · Xiaofan Zhang (Shanghai Jiao Tong University) · Shaoting Zhang (University of North Carolina at Charlotte)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improved Self-Training for Test-Time Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jing Ma (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NTO3D: Neural Target Object 3D Reconstruction with Segment Anything</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaobao Wei (University of the Chinese Academy of Sciences) · Renrui Zhang (MMLab of CUHK &amp;amp;amp; Shanghai AI Laboratory) · Jiarui Wu (Beijing University of Aeronautics and Astronautics) · Jiaming Liu (Peking University) · Ming Lu (Intel Labs China) · Yandong Guo (OPPO Research Institute) · Shanghang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Structure-Aware Sparse-View X-ray 3D Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanhao Cai (Johns Hopkins University) · Jiahao Wang (Johns Hopkins University) · Alan L. Yuille (Johns Hopkins University) · Zongwei Zhou (Johns Hopkins University) · Angtian Wang (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LangSplat: 3D Language Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minghan Qin (Tsinghua University) · Wanhua Li (Harvard University) · Jiawei ZHOU (Tsinghua University) · Haoqian Wang (Tsinghua University, Tsinghua University) · Hanspeter Pfister (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Retrieval-Augmented Embodied Agents</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichen Zhu (Midea Group) · Zhicai Ou (AI Innovation Center, Midea Group) · Xiaofeng Mou (Midea Group) · Jian Tang (Midea Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bidirectional Multi-Scale Implicit Neural Representations for Image Deraining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiang Chen (Nanjing University of Science and Technology) · Jinshan Pan (Nanjing University of Science and Technology) · Jiangxin Dong (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Positive-Unlabeled Learning by Latent Group-Aware Meta Disambiguation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lin Long (Zhejiang University) · Haobo Wang (Zhejiang University) · Zhijie Jiang (Zhejiang University) · Lei Feng (Nanyang Technological University) · Chang Yao (Zhejiang University) · Gang Chen (College of Computer Science and Technology, Zhejiang University) · Junbo Zhao (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Contextrast: Contextual Contrastive Learning for Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changki Sung (Korea Advanced Institute of Science &amp; Technology) · Wanhee Kim (Korea Advanced Institute of Science &amp; Technology) · Jungho An (Korea Advanced Institute of Science &amp; Technology) · WooJu Lee (KAIST) · Hyungtae Lim (KAIST) · Hyun Myung (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffusionMTL: Learning Multi-Task Denoising Diffusion Model from Partially Annotated Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanrong Ye () · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-conditional Attribute Alignment across Latent Spaces for 3D Controllable Face Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            FeiFan Xu (None) · Rui Li (Shantou University) · Si Wu (South China University of Technology) · Yong Xu (Peng Cheng Laboratory) · Hau San Wong (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MonoCD: Monocular 3D Object Detection with Complementary Depths</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Longfei Yan (Huazhong University of Science and Technology) · Pei Yan (Huazhong University of Science and Technology) · Shengzhou Xiong (Huazhong University of Science and Technology) · Xuanyu Xiang (Huazhong University of Science and Technology) · Yihua Tan (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu Zeng (None) · Vishal M. Patel (Johns Hopkins University) · Haochen Wang (Toyota Technological Institute at Chicago) · Xun Huang (NVIDIA) · Ting-Chun Wang (NVIDIA) · Ming-Yu Liu (NVIDIA) · Yogesh Balaji (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Linear N-Point Solver for Line and Motion Estimation with Event Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ling Gao (ShanghaiTech University) · Daniel Gehrig (None) · Hang Su (None) · Davide Scaramuzza (University of Zurich) · Laurent Kneip (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zilin Du (Nanyang Technological University) · Haoxin Li (Nanyang Technological University) · Xu Guo (Nanyang Technological University) · Boyang Li (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HAVE-FUN: Human Avatar Reconstruction from Few-Shot Unconstrained Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xihe Yang (The Chinese University of Hong Kong, Shenzhen) · Xingyu Chen (Xiaobing.AI) · Daiheng Gao () · Finn Wong (Xiaobing.AI) · Xiaoguang Han (The Chinese University of Hong Kong, Shenzhen) · Baoyuan Wang (Xiaobing.ai)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>4D Gaussian Splatting for Real-Time Dynamic Scene Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guanjun Wu (None) · Taoran Yi (Huazhong University of Science and Technology) · Jiemin Fang (Huawei Technologies Ltd.) · Lingxi Xie (Huawei Technologies Ltd.) · Xiaopeng Zhang (Huawei Technologies Ltd.) · Wei Wei (Huazhong University of Science and Technology) · Wenyu Liu (Huazhong University of Science and Technology) · Qi Tian (Huawei Technologies Ltd.) · Xinggang Wang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Differentiable Information Bottleneck for Deterministic Multi-view Clustering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoqiang Yan () · Zhixiang Jin (Zhengzhou University) · Fengshou Han (Zhengzhou University) · Yangdong Ye (Zhengzhou University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Antoine Guédon (Ecole des Ponts ParisTech) · Vincent Lepetit (Ecole des Ponts ParisTech)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>R-Cyclic Diffuser: Reductive and Cyclic Latent Diffusion for 3D Clothed Human Digitalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kennard Chan (, A*STAR) · Fayao Liu (Institute for Infocomm Research, A*STAR) · Guosheng Lin (Nanyang Technological University) · Chuan-Sheng Foo (Centre for Frontier AI Research, A*STAR) · Weisi Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhang Li (None) · Biao Yang (Huazhong University of Science and Technology) · Qiang Liu (Kingsoft Office) · Zhiyin Ma (Huazhong University of Science and Technology) · Shuo Zhang (Huazhong University of Science and Technology) · Jingxu Yang (Kingsoft Office Corporation Limited) · Yabo Sun (Kingsoft Office) · Yuliang Liu (Huazhong University of Science and Technology) · Xiang Bai (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Zero-Reference Low-Light Enhancement via Physical Quadruple Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenjing Wang (Peking University) · Huan Yang (01.AI) · Jianlong Fu (Microsoft) · Jiaying Liu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hybrid Proposal Refiner: Revisiting DETR Series from the Faster R-CNN Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinjing Zhao (The University of Sydney) · Fangyun Wei (None) · Chang Xu (University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffusionPoser: Real-time Human Motion Reconstruction From Arbitrary Sparse Sensors Using Autoregressive Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tom Van Wouwe (Stanford University) · Seunghwan Lee (Stanford University) · Antoine Falisse (Stanford University) · Scott Delp (Stanford University) · Karen Liu (Computer Science Department, Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingbo Zhang (City University of Hong Kong) · Xiaoyu Li (Tencent AI Lab) · Qi Zhang (Tencent AI Lab) · Yan-Pei Cao (Tencent ARC Lab) · Ying Shan (Tencent) · Jing Liao (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CurveCloudNet: Processing Point Clouds with 1D Structure</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Colton Stearns (None) · Alex Fu (Illumix) · Jiateng Liu (Department of Computer Science) · Jeong Joon Park (Stanford University) · Davis Rempe (NVIDIA) · Despoina Paschalidou (Stanford) · Leonidas Guibas (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://vlislab22.github.io/eg-lowlight/" target="_blank">Towards Robust Event-guided Low-Light Image Enhancement: A Large-Scale Real-World Event-Image Dataset and Novel Approach</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guoqiang Liang (Hong Kong University of Science and Technology) · Kanghao Chen (Hong Kong University of Science and Technology) · Hangyu Li (Hong Kong University of Science and Technology) · Yunfan Lu (Hong Kong University of Science and Technology(GuangZhou)) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Visual Prompt for Gait Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kang Ma (Beijing Institute of Technology) · Ying Fu (None) · Chunshui Cao (Watrix Technology) · Saihui Hou (Beijing Normal University) · Yongzhen Huang (Beijing Normal University) · Dezhi Zheng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FCS: Feature Calibration and Separation for Non-Exemplar Class Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiwei Li (Peking University) · Yuxin Peng (Peking University) · Jiahuan Zhou (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Physical 3D Adversarial Attacks against Monocular Depth Estimation in Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junhao Zheng (Xi'an Jiaotong University) · Chenhao Lin (Xi'an Jiaotong University) · Jiahao Sun (Xi'an Jiaotong University) · Zhengyu Zhao (Xi'an Jiaotong University) · Qian Li (Xi'an Jiaotong University) · Chao Shen (Xi’an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Discovering and Mitigating Visual Biases through Keyword Explanation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Younghyun Kim (KAIST) · Sangwoo Mo (University of Michigan) · Minkyu Kim (KRAFTON, Inc.) · Kyungmin Lee (Korea Advanced Institute of Science &amp; Technology) · Jaeho Lee (POSTECH) · Jinwoo Shin (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>XFibrosis: Explicit Vessel-Fiber Modeling for Fibrosis Staging from Liver Pathology Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            CHONG YIN (Hong Kong Baptist University) · Siqi Liu (Shenzhen Research Institute of Big Data) · Fei Lyu (Hong Kong Baptist University) · Jiahao Lu (Copenhagen University) · Sune Darkner (Copenhagen University) · Vincent Wong (The Chinese University of Hong Kong) · Pong C. Yuen (Hong Kong Baptist Unviersity)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaoyi Zhang (The University of Sydney, University of Sydney) · Kevin Lin (Microsoft) · Zhengyuan Yang (Microsoft) · Jianfeng Wang (Microsoft) · Linjie Li (Microsoft) · Chung-Ching Lin (Microsoft) · Zicheng Liu (Microsoft) · Lijuan Wang (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GreedyViG: Dynamic Axial Graph Construction for Efficient Vision GNNs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mustafa Munir (The University of Texas at Austin) · William Avery (None) · Md Mostafijur Rahman (University of Texas at Austin) · Radu Marculescu (University of Texas, Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MoML: Online Meta Adaptation for 3D Human Motion Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoning Sun (Nanjing University of Science and Technology) · Huaijiang Sun (Nanjing University of Science and Technology) · Bin Li (Nanjing University of Science and Technology) · Dong Wei (Nanjing University of Science and Technology) · Weiqing Li (Nanjing University of Science and Technology) · Jianfeng Lu (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zan Wang (Beijing Institute of Technology) · Yixin Chen (BIGAI) · Baoxiong Jia (University of California, Los Angeles) · Puhao Li (Department of Automation, Tsinghua University) · Jinlu Zhang (Peking University) · Jingze Zhang (Tsinghua University, Tsinghua University) · Tengyu Liu (None) · Yixin Zhu (Peking University) · Wei Liang (Beijing Institute of Technology) · Siyuan Huang (Beijing Institute of General Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Exploring Regional Clues in CLIP for Zero-Shot Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yi Zhang (Beihang University) · Meng-Hao Guo (Tsinghua University, Tsinghua University) · Miao Wang (Beihang University) · Shi-Min Hu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Graph Contrastive Learning via Adaptive Positive Sampling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaming Zhuo (Hebei University of Technology) · Feiyang Qin (None) · Can Cui (Hebei University of Technology) · Kun Fu (Hebei University of Technology) · Bingxin Niu (Hebei University of Techonology) · Mengzhu Wang (Hebei University of Technology) · Yuanfang Guo (Beihang University) · Chuan Wang (institute of information engineering) · Zhen Wang (None) · Xiaochun Cao (SUN YAT-SEN UNIVERSITY) · Liang Yang (Hebei University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://niccolocavagnero.github.io/PEM/" target="_blank">PEM: Prototype-based Efficient MaskFormer for Image Segmentation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Niccolò Cavagnero (Polytechnic Institute of Turin) · Gabriele Rosi (Polytechnic Institute of Turin) · Claudia Cuttano (Polytechnic Institute of Turin) · Francesca Pistilli (Polytechnic Institute of Turin) · Marco Ciccone (Politecnico di Torino) · Giuseppe Averta (Polytechnic of Turin) · Fabio Cermelli (Politecnico di Torino)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VILA: On Pre-training for Visual Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ji Lin (Massachusetts Institute of Technology) · Danny Yin (NVIDIA) · Wei Ping (NVIDIA) · Pavlo Molchanov (NVIDIA) · Mohammad Shoeybi (NVIDIA) · Song Han (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dr2Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Zhao (King Abdullah University of Science and Technology (KAUST)) · Shuming Liu (KAUST) · Karttikeya Mangalam (University of California Berkeley) · Guocheng Qian (KAUST) · Fatimah Zohra (King Abdullah University of Science and Technology) · Abdulmohsen Alghannam (University of Virginia, Charlottesville) · Jitendra Malik (University of California at Berkeley) · Bernard Ghanem (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Vision-and-Language Navigation via Causal Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liuyi Wang (Tongji University) · Zongtao He (Tongji University) · Ronghao Dang (Tongji University) · mengjiao shen (Tongji University) · Chengju Liu (Tongji University) · Qijun Chen (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/Godheritage/BOTH2Hands" target="_blank">BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenqian Zhang (ShanghaiTech University) · Molin Huang (Shanghaitech University) · Yuxuan Zhou (None) · Juze Zhang (ShanghaiTech University) · Jingyi Yu (ShanghaiTech University) · Jingya Wang (ShanghaiTech University) · Lan Xu (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A noisy elephant in the room: Is your out-of-distribution detector robust to label noise?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Galadrielle Humblot-Renaux (Aalborg University) · Sergio Escalera (Computer Vision Center) · Thomas B. Moeslund (Aalborg University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning with Structural Labels for Learning with Noisy Labels</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Noo-ri Kim (Sungkyunkwan University) · Jin-Seop Lee (Sungkyunkwan University) · Jee-Hyong Lee (Sungkyunkwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Letian Zhang (Tongji University) · Xiaotong Zhai (University of Warwick) · Zhongkai Zhao (National University of Singapore) · Yongshuo Zong (School of Informatics, University of Edinburgh) · Xin Wen (The University of Hong Kong) · Bingchen Zhao (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bayesian Exploration of Pre-trained Models for Low-shot Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yibo Miao (Shanghai Jiaotong University) · Yu lei (Shanghai Jiao Tong University) · Feng Zhou (Renmin University of China) · Zhijie Deng (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PLGSLAM: Progressive Neural Scene Represenation with Local to Global Bundle Adjustment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianchen Deng (None) · Guole Shen (None) · Tong Qin (Shanghai Jiaotong University) · jianyu wang (Shanghai Jiao Tong University) · Wentao Zhao (Shanghai Jiao Tong University) · Jingchuan Wang (None) · Danwei Wang (Nanyang Technological University) · Weidong Chen (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RecDiffusion: Rectangling for Image Stitching with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianhao Zhou (University of Electronic Science and Technology of China) · Li Haipeng (None) · Ziyi Wang (University of Electronic Science and Technology of China) · Ao Luo (Megvii Technology Inc.) · Chenlin Zhang (Moonshot AI, Ltd) · Jiajun Li (4Paradigm Technology Inc.) · Bing Zeng (None) · Shuaicheng Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Incremental Nuclei Segmentation from Histopathological Images via Future-class Awareness and Compatibility-inspired Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huyong Wang (Shenzhen University) · Huisi Wu (Shenzhen University) · Jing Qin (Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Arbitrary-Scale Image Generation and Upsampling using Latent Diffusion Model and Implicit Neural Decoder</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinseok Kim (KAIST / LG Electronics) · Tae-Kyun Kim (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HashPoint: Accelerated Point Searching and Sampling for Neural Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahao Ma () · Miaomiao Liu (Australian National University) · David Ahmedt-Aristizabal (CSIRO) · Chuong Nguyen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Three Pillars improving Vision Foundation Model Distillation for Lidar</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gilles Puy (valeo.ai) · Spyros Gidaris (Valeo.ai) · Alexandre Boulch (valeo.ai) · Oriane Siméoni (Valeo.ai) · Corentin Sautier (ENPC, Ecole Nationale des Ponts et Chausees) · Patrick Pérez (None) · Andrei Bursuc (valeo.ai) · Renaud Marlet (INRIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Retraining-free Model Quantization via One-Shot Weight-Coupling Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Tang (Tsinghua University) · Yuan Meng (Tsinghua University, Tsinghua University) · Jiacheng Jiang (Tsinghua University, Tsinghua University) · Shuzhao Xie (Tsinghua University, Tsinghua University) · Rongwei Lu (Tsinghua University, Tsinghua University) · Xinzhu Ma (University of Sydney) · Zhi Wang (SIGS, Tsinghua University) · Wenwu Zhu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Model Inversion Robustness: Can Transfer Learning Help?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sy-Tuyen Ho (Singapore University of Technology and Design) · Koh Jun Hao (Singapore University of Technology and Design) · Keshigeyan Chandrasegaran (Stanford University) · Ngoc-Bao Nguyen (Singapore University of Technology and Design) · Ngai-Man Cheung (Singapore University of Technology and Design)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://barquerogerman.github.io/FlowMDM/" target="_blank">Seamless Human Motion Composition with Blended Positional Encodings</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            German Barquero (Universitat de Barcelona) · Sergio Escalera (Computer Vision Center) · Cristina Palmero (Universitat de Barcelona)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Single Domain Generalization for Crowd Counting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhuoxuan Peng (The Hong Kong University of Science and Technology) · S.-H. Gary Chan (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Motion2VecSets: 4D Latent Vector Set Diffusion for Non-rigid Shape Reconstruction and Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei Cao (Technische Universität München) · Chang Luo (Technical University of Munich) · Biao Zhang (KAUST) · Matthias Nießner (Technical University of Munich) · Jiapeng Tang (Technische Universität München)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Collaborative Learning of Anomalies with Privacy (CLAP) for Unsupervised Video Anomaly Detection: A New Baseline</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anas Al-lahham (Mohamed bin Zayed University of Artificial Intelligence) · Muhammad Zaigham Zaheer (Mohamed bin Zayed University of Artificial Intelligence) · Nurbek Tastan (Mohamed bin Zayed University of Artificial Intelligence) · Karthik Nandakumar (Mohamed Bin Zayed University of Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SG-BEV: Satellite-Guided BEV Fusion for Cross-View Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyan Ye (SUN YAT-SEN UNIVERSITY) · Qiyan Luo () · Jinhua Yu (Sun Yat-sen University, School of Geospatial Engineering and Science) · Huaping Zhong (SenseTime) · Zhimeng Zheng (Zhejiang University) · Conghui He (None) · Weijia Li (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GLaMM: Pixel Grounding Large Multimodal Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanoona Rasheed (Mohamed bin Zayed University of Artificial Intelligence) · Muhammad Maaz (Mohamed Bin Zayed University of Artificial Intelligence) · Sahal Shaji Mullappilly (Mohamed bin Zayed University of Artificial Intelligence) · Abdelrahman Shaker (Mohamed Bin Zayed University of Artificial Intelligence) · Salman Khan (Mohamed bin Zayed University of Artificial Intelligence) · Hisham Cholakkal (MBZUAI) · Rao Anwer (Mohamed bin Zayed University of Artificial Intelligence) · Eric P. Xing (Mohamed bin Zayed Univeristy of AI) · Ming-Hsuan Yang (University of California at Merced) · Fahad Shahbaz Khan (MBZUAI; Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Pre-trained Vision and Language Transformers Are Few-Shot Incremental Learners</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Keon Hee Park (Kyung Hee University) · Kyungwoo Song (Yonsei University) · Gyeong-Moon Park (Kyung Hee University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SHiNe: Semantic Hierarchy Nexus for Open-vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingxuan Liu (University of Trento) · Tyler Hayes (Naver Labs Europe) · Elisa Ricci (University of Trento) · Gabriela Csurka (None) · Riccardo Volpi (Naver Labs Europe)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Large-Factor EM Image Super-Resolution with Generative Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiateng Shou (University of Science and Technology of China) · Zeyu Xiao (None) · Shiyu Deng (University of Science and Technology of China) · Wei Huang (University of Science and Technology of China) · ShiPeiyao (Suzhou Institute of Biomedical Engineering and Technology, Chinese Academy of Sciences) · Ruobing Zhang (Suzhou Institute of Biomedical Engineering and Technology) · Zhiwei Xiong (None) · Feng Wu (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Knowledge-Enhanced Dual-stream Zero-shot Composed Image Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yucheng Suo (Zhejiang University) · Fan Ma (None) · Linchao Zhu (None) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenyu Li (King Abdullah University of Science and Technology) · Shariq Bhat (King Abdullah University of Science and Technology (KAUST)) · Peter Wonka (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Functional Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Biao Zhang (KAUST) · Peter Wonka (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VideoRF: Rendering Dynamic Radiance Fields as 2D Feature Video Streams</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liao Wang () · Kaixin Yao (ShanghaiTech University) · Chengcheng Guo (ShanghaiTech University) · Zhirui Zhang (ShanghaiTech University) · Qiang Hu (Shanghai Jiaotong University) · Jingyi Yu (Shanghai Tech University) · Lan Xu (ShanghaiTech University) · Minye Wu (KU Leuven)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nataniel Ruiz (Boston University) · Yuanzhen Li (Massachusetts Institute of Technology) · Varun Jampani (Google Research) · Wei Wei (Google) · Tingbo Hou (Google Research) · Yael Pritch (Google Research) · Neal Wadhwa (Google) · Michael Rubinstein (Google) · Kfir Aberman (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/SSDUT-Caiyq/UFG-NCD" target="_blank">Novel Class Discovery for Ultra-Fine-Grained Visual Categorization</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu Liu (None) · Yaqi Cai (Dalian University of Technology) · Qi Jia (Dalian University of Technology) · Binglin Qiu (Dalian University of Technology) · Weimin Wang (Dalian University of Techonoly) · Nan Pu (University of Trento)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Clustering Propagation for Universal Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhang Ding (University of Technology Sydney) · Liulei Li (Zhejiang University) · Wenguan Wang (Zhejiang University) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Content-Adaptive Non-Local Convolution for Remote Sensing Pansharpening</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yule Duan (University of Electronic Science and Technology of China) · Xiao Wu (University of Electronic Science and Technology of China) · Haoyu Deng (University of Electronic Science and Technology of China) · Liang-Jian Deng (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Versatile Framework for Continual Test-Time Domain Adaptation: Balancing Discriminability and Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Yang (Xi'an University of Electronic Science and Technology) · Xuan chen (Xi'an University of Electronic Science and Technology) · Moqi Li (Xi'an University of Electronic Science and Technology) · Kun Wei (Xidian University) · Cheng Deng (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Gradient Reweighting: Towards Imbalanced Class-Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiangpeng He (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Can I Trust Your Answer? Visually Grounded Video Question Answering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junbin Xiao (None) · Angela Yao (National University of Singapore) · Yicong Li (national university of singaore, National University of Singapore) · Tat-seng Chua (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Device-Wise Federated Network Pruning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shangqian Gao (University of Pittsburgh) · Junyi Li (University of Maryland, College Park) · Zeyu Zhang (Amazon AGI) · Yanfu Zhang (College of William and Mary) · Weidong Cai (The University of Sydney) · Heng Huang (University of Pittsburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>D<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-25-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-149" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-150" class="mjx-mrow"><span id="MJXc-Node-151" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-152" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-153" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">4</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>4</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-25">^4</script>M: Dataset Distillation via Disentangled Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Duo Su (University of Chinese Academy of Sciences) · Junjie Hou (None) · Weizhi Gao (North Carolina State University) · Yingjie Tian (, Chinese Academy of Sciences) · Bowen Tang (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Face2Diffusion for Fast and Editable Face Personalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kaede Shiohara (None) · Toshihiko Yamasaki (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Logarithmic Lenses: Exploring Log RGB Data for Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bruce Maxwell (Northeastern University) · Sumegha Singhania (Northeastern University) · Avnish Patel (Northeastern University) · Rahul Kumar (Northeastern University) · Heather Fryling (Northeastern University) · Sihan Li (None) · Haonan Sun (Northeastern University) · Ping He (Northeastern University) · Zewen Li (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://statho.github.io/ScoreHMR/" target="_blank">Score-Guided Diffusion for 3D Human Recovery</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anastasis Stathopoulos (Rutgers University) · Ligong Han (Rutgers University) · Dimitris N. Metaxas (Rutgers)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Draw Step by Step Like Human: Reconstructing CAD Construction Sequences from Point Clouds via Multimodal Diffusion.</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weijian Ma (Fudan University) · Shuaiqi Chen (Fudan University) · Yunzhong Lou (Fudan University) · Xueyang Li (Fudan University) · Xiangdong Zhou (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>StreamingFlow: Streaming Occupancy Forecasting with Asynchronous Multi-modal Data Streams via Neural Ordinary Differential Equation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yining Shi (Tsinghua University) · Kun JIANG (Tsinghua University) · Ke Wang (Didi Research) · Jiusi Li (Tongji University) · Yunlong Wang (Tsinghua University) · Mengmeng Yang (None) · Diange Yang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andrew Song (Brigham and Women's hospital) · Richard J. Chen (Harvard University) · Tong Ding (Harvard University) · Drew F. K. Williamson (Massachusetts General Hospital, Harvard University) · Guillaume Jaume (Harvard University) · Faisal Mahmood (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Specularity Factorization for Low Light Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Saurabh Saini (International Institute of Information Technology Hyderabad) · P. J. Narayanan (IIIT Hyderabad)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CLIP-KD: An Empirical Study of CLIP Model Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chuanguang Yang (Institute of Computing Technology, Chinese Academy of Sciences) · Zhulin An (Institute of Computing Technology, Chinese Academy of Sciences) · Libo Huang (None) · Junyu Bi () · XinQiang Yu (Institute of Computing Technology, Chinese Academy of Sciences) · Han Yang (University of the Chinese Academy of Sciences) · boyu diao (None) · Yongjun Xu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhance Image Classification Via Inter-Class Image Mixup With Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhicai Wang (Univerisity of Science and Technology of China) · Longhui Wei (Huawei Cloud Technologies Ltd.) · Tan Wang (Nanyang Technological University) · Heyu Chen (None) · Yanbin Hao () · Xiang Wang (University of Science and Technology of China) · Xiangnan He (University of Science and Technology of China) · Qi Tian (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SignGraph: A Sign Sequence is Worth Graphs of Nodes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiwei Gan (None) · Yafeng Yin (Nanjing University) · Zhiwei Jiang (Nanjing University) · Hongkai Wen (University of Warwick) · Lei Xie (Nanjing University) · Sanglu Lu (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiyu Tian (Chongqing University) · Hongxin Wei (Southern University of Science and Technology) · Yiqun Wang (Chongqing University) · Lei Feng (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Just Add <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-26-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C0;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-154" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-155" class="mjx-mrow"><span id="MJXc-Node-156" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.191em; padding-bottom: 0.316em; padding-right: 0.003em;">π</span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>π</mi></math></span></span><script type="math/tex" id="MathJax-Element-26">\pi</script>! Pose Induced Video Transformers for Understanding Activities of Daily Living</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dominick Reilly (UNC Charlotte) · Srijan Das (University of North Carolina at Charlotte)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3DInAction: Understanding Human Actions in 3D Point Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yizhak Ben-Shabat (Technion, Israel Institute of Technology) · Oren Shrout (Faculty of Electrical And Computer Engineering - Technion, Israel) · Stephen Gould (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VideoDistill: Language-aware Vision Distillation for Video Question Answering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bo Zou (Computer Science, Tsinghua University, Tsinghua University) · Chao Yang (Shanghai AI Laboratory) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Chengbin Quan (Tsinghua University, Tsinghua University) · Youjian Zhao (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Decoupling Static and Hierarchical Motion Perception for Referring Video Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuting He (Nanyang Technological University) · Henghui Ding (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Embracing Unimodal Aleatoric Uncertainty for Robust Multimodal Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixian Gao (University of Electronic Science and Technology of China) · Xun Jiang (University of Electronic Science and Technology of China) · Xing Xu (University of Electronic Science and Technology of China) · Fumin Shen (UESTC) · Yujie Li (Yangzhou University) · Heng Tao Shen (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiLiGenRT: A Photometric Stereo Dataset with Quantified Roughness and Translucency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Heng Guo (Beijing University of Posts and Telecommunications) · Jieji Ren (Shanghai Jiao Tong University) · Feishi Wang (Peking University) · Boxin Shi (Peking University) · Mingjun Ren (Shanghai Jiaotong University) · Yasuyuki Matsushita (Osaka University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DGC-GNN: Leveraging Geometry and Color Cues for Visual Descriptor-Free 2D-3D Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuzhe Wang (Aalto University) · Juho Kannala (University of Oulu) · Daniel Barath (ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multiplane Prior Guided Few-Shot Aerial Scene Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihan Gao (Xidian University) · Licheng Jiao (Xi'an University of Electronic Science and Technology) · Lingling Li (Xidian University) · Xu Liu (Xidian University) · Fang Liu (Xidian University) · Puhua Chen () · Yuwei Guo (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>4K4D: Real-Time 4D View Synthesis at 4K Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Xu (Zhejiang University) · Sida Peng (None) · Haotong Lin (None) · Guangzhao He (Zhejiang University) · Jiaming Sun (Image Derivative Inc.) · Yujun Shen (The Chinese University of Hong Kong) · Hujun Bao (Zhejiang University) · Xiaowei Zhou (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MCPNet: An Interpretable Classifier via Multi-Level Concept Prototypes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bor Shiun Wang (None) · Chien-Yi Wang (NVIDIA) · Wei-Chen Chiu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PELA: Learning Parameter-Efficient Models with Low-Rank Approximation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yangyang Guo (National University of Singapore) · Guangzhi Wang (National University of Singapore) · Mohan Kankanhalli (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Context-Guided Spatio-Temporal Video Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Gu (None) · Heng Fan (University of North Texas) · Yan Huang (, University of North Texas) · Tiejian Luo (University of the Chinese Academy of Sciences) · Libo Zhang (Institute of Software Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Zhou (Huazhong University of Science and Technology) · Dingkang Liang (Huazhong University of Science and Technology) · Wei Xu (Huazhong University of Science and Technology) · Xingkui Zhu (Huazhong University of Science and Technology) · Yihan Xu (Huazhong University of Science and Technology) · Zhikang Zou (Huazhong University of Science and Technology) · Xiang Bai (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Reconstruction-free Cascaded Adaptive Compressive Sensing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenxi Qiu (Nanjing University) · Tao Yue (Nanjing University) · Xuemei Hu (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Progressive Divide-and-Conquer via Subsampling Decomposition for Accelerated MRI</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chong Wang (Nanyang Technological University) · Lanqing Guo (Nanyang Technological University) · Yufei Wang (Nanyang Technological University) · Hao Cheng (Nanyang Technological University) · Yi Yu (Nanyang Technological University, Singapore) · Bihan Wen (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Unified Approach for Text- and Image-guided 4D Scene Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufeng Zheng (ETH Zurich, MPI-IS) · Xueting Li (NVIDIA) · Koki Nagano (None) · Sifei Liu (NVIDIA) · Otmar Hilliges (None) · Shalini De Mello (NVIDIA Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Intrinsic Image Diffusion for Indoor Single-view Material Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peter Kocsis (None) · Vincent Sitzmann (Massachusetts Institute of Technology) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/cnulab/RealNet" target="_blank">RealNet: A Feature Selection Network with Realistic Synthetic Anomaly for Anomaly Detection</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ximiao Zhang (Capital Normal University) · Min Xu (Capital Normal University) · Xiuzhuang Zhou (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Sequential Modeling Enables Scalable Learning for Large Vision Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yutong Bai (Johns Hopkins University) · Xinyang Geng (University of California Berkeley) · Karttikeya Mangalam (University of California Berkeley) · Amir Bar (TAU / UC Berkeley) · Alan L. Yuille (Johns Hopkins University) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Jitendra Malik (University of California at Berkeley) · Alexei A. Efros (UC Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Can Protective Perturbation Safeguard Personal Data from Being Exploited by Stable Diffusion?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengyue Zhao (University of Chinese Academy of Sciences) · Jinhao Duan (Drexel University) · Kaidi Xu (Drexel University) · Chenan Wang (Drexel University) · Rui Zhang (None) · Zidong Du (Institute of Computing Technology, Chinese Academy of Sciences) · Qi Guo (Institute of Computing Technology, Chinese Academy of Sciences) · Xing Hu (, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NetTrack: Tracking Highly Dynamic Objects with a Net</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangze Zheng (The University of Hong Kong) · Shijie Lin (None) · Haobo Zuo (University of Hong Kong) · Changhong Fu (Tongji University) · Jia Pan (University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CLIPtone: Unsupervised Learning for Text-based Image Tone Adjustment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyeongmin Lee (Pohang University of Science and Technology) · Kyoungkook Kang (Pohang University of Science and Technology) · Jungseul Ok (POSTECH) · Sunghyun Cho (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VGGSfM: Visual Geometry Grounded Deep Structure From Motion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianyuan Wang (Oxford VGG) · Nikita Karaev (University of Oxford) · Christian Rupprecht (University of Oxford) · David Novotny (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CPP-Net: Embracing Multi-Scale Feature Fusion into Deep Unfolding CP-PPA Network for Compressive Sensing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Guo (Northwestern Polytechnical University) · Hongping Gan (Northwest Polytechnical University Xi'an)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Video Recognition in Portrait Mode</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingfei Han (University of Technology Sydney) · Linjie Yang (ByteDance Inc.) · Xiaojie Jin (ByteDance Inc./TikTok) · Jiashi Feng (ByteDance) · Xiaojun Chang (University of Technology Sydney) · Heng Wang (Bytedance)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FADES: Fair Disentanglement with Sensitive Relevance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Taeuk Jang (Purdue University) · Xiaoqian Wang (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Versatile Navigation under Partial Observability via Value-Guided Diffusion Policy</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gengyu Zhang (Illinois Institute of Technology) · Hao Tang (ETH Zurich and CMU) · Yan Yan (Illinois Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Point, Segment and Count: A Generalized Framework for Object Counting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhizhong Huang (Fudan University) · Mingliang Dai (Fudan University) · Yi Zhang (Sichuan University) · Junping Zhang (Fudan University) · Hongming Shan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CPR-Coach: Recognizing Composite Error Actions based on Single-class Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shunli Wang (Fudan University) · Shuaibing Wang (Fudan University) · Dingkang Yang (Fudan University) · Mingcheng Li (Fudan University) · Haopeng Kuang (Fudan University) · Xiao Zhao (None) · Liuzhen Su (Fudan University) · Peng Zhai (Fudan University) · Lihua Zhang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Generative Approach for Wikipedia-Scale Visual Entity Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mathilde Caron (Google) · Ahmet Iscen (Google) · Alireza Fathi (Google) · Cordelia Schmid (Inria / Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Normalizing Flows on the Product Space of SO(3) Manifolds for Probabilistic Human Pose Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Olaf Dünkel (Saarland Informatics Campus, Max-Planck Institute) · Tim Salzmann (Technische Universität München) · Florian Pfaff (University of Stuttgart)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Demin Yu (Harbin Institute of Technology) · Xutao Li (Harbin Institute of Technology, Shenzhen) · Yunming Ye (Harbin Institute of Technology, Shenzhen) · Baoquan Zhang (, Harbin Institute of Technology (shenzhen)) · Luo Chuyao (None) · Kuai Dai (Harbin Institute of Technology) · wangrui (Meteorological Bureau of Shenzhen Municipality) · Chenxunlai (shenzhen Meteorological Bureau)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GEARS: Local Geometry-aware Hand-object Interaction Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Keyang Zhou (Eberhard-Karls-Universität Tübingen) · Bharat Lal Bhatnagar (Eberhard-Karls-Universität Tübingen) · Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) · Gerard Pons-Moll (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junjie Wang (None) · Jiemin Fang (Huawei Technologies Ltd.) · Xiaopeng Zhang (Huawei Technologies Ltd.) · Lingxi Xie (Huawei Technologies Ltd.) · Qi Tian (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yawar Siddiqui (Technical University Munich) · Antonio Alliegro (Politecnico di Torino) · Alexey Artemov (Technische Universität München) · Tatiana Tommasi (Politecnico di Torino) · Daniele Sirigatti (Audi AG) · Vladislav Rosov (AUDI AG) · Angela Dai () · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3D-Aware Face Editing via Warping-Guided Latent Direction Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhao Cheng (Shanghai Jiaotong University) · Zhuo Chen (Shanghai Jiaotong University) · Xingyu Ren (Shanghai Jiao Tong University) · Wenhan Zhu (None) · Zhengqin Xu (Shanghai Jiaotong University) · Di Xu (Huawei Technologies Ltd.) · Yang Changpeng (Huawei Cloud) · Yichao Yan (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RadarDistill: Boosting Radar-based Object Detection Performance via Knowledge Distillation from LiDAR Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Geonho Bang (Hanyang University) · Kwangjin Choi (Hanyang University) · Jisong Kim (Hanyang University) · Dongsuk Kum (Korea Advanced Institute of Science and Technology) · Jun Won Choi (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://gvecchio.com/matfuse/" target="_blank">MatFuse: Controllable Material Generation with Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Giuseppe Vecchio (University of Catania) · Renato Sortino (University of Catania) · Simone Palazzo (University of Catania) · Concetto Spampinato (University of Catania)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Global Latent Neural Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Thomas Tanay (Huawei Technologies Ltd.) · Matteo Maggioni (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Token Transformation Matters: Towards Faithful Post-hoc Explanation for Vision Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyi Wu (None) · Bin Duan (Illinois Tech) · Weitai Kang (None) · Hao Tang (ETH Zurich and CMU) · Yan Yan (Illinois Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Cache Me if You Can: Accelerating Diffusion Models through Block Caching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Felix Wimbauer (Technical University of Munich) · Bichen Wu (Facebook) · Edgar Schoenfeld (None) · Xiaoliang Dai (Facebook) · Ji Hou (Facebook) · Zijian He (None) · Artsiom Sanakoyeu (RL) · Peizhao Zhang (Facebook) · Sam Tsai (Meta) · Jonas Kohler (Facebook) · Christian Rupprecht (University of Oxford) · Daniel Cremers (Technical University Munich) · Peter Vajda (Facebook) · Jialiang Wang (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://subhadeepkoley.github.io/StableSketching/" target="_blank">It's All About Your Sketch: Democratising Sketch Control in Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Subhadeep Koley (University of Surrey) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Deeptanshu Sekhri (University of Surrey) · Aneeshan Sain (University of Surrey) · Pinaki Nath Chowdhury (University of Surrey) · Tao Xiang (University of Surrey) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ESR-NeRF: Emissive Source Reconstruction Using LDR Multi-view Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinseo Jeong (Seoul National University) · Junseo Koo (Seoul National University) · Qimeng Zhang (Korea University) · Gunhee Kim (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Epistemic Uncertainty Quantification For Pre-trained Neural Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanjing Wang (Rensselaer Polytechnic Institute) · Qiang Ji (Rensselaer Polytechnic Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haiyang Ying (None) · Yixuan Yin (Tsinghua University) · Jinzhi Zhang (Electronic Engineering, Tsinghua University, Tsinghua University) · Fan Wang (Alibaba Group) · Tao Yu (Tsinghua University, Tsinghua University) · Ruqi Huang (Tsinghua Shenzhen International Graduate School/Tsinghua Berkeley Shenzhen Institute ) · Lu Fang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MRFS: Mutually Reinforcing Image Fusion and Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Zhang (Wuhan University) · Xuhui Zuo (Wuhan University) · Jie Jiang (Tencent AI Lab) · Chunchao Guo (SUN YAT-SEN UNIVERSITY) · Jiayi Ma (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuanyu Zhang (Peking University Shenzhen Graduate School) · Runyi Li (Peking University) · Jiwen Yu (Peking University) · Youmin Xu (Peking University) · Weiqi Li (Peking University) · Jian Zhang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3D Paintbrush: Local Stylization of 3D Shapes with Cascaded Score Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dale Decatur (University of Chicago) · Itai Lang (University of Chicago &amp; Tel Aviv University) · Kfir Aberman (Google) · Rana Hanocka (University of Chicago)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Design2Cloth: 3D Cloth Generation from 2D Masks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiali Zheng (Imperial College London) · Rolandos Alexandros Potamias (Imperial College London) · Stefanos Zafeiriou (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3D-LFM: Lifting Foundation Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mosam Dabhi () · László A. Jeni (Carnegie Mellon University) · Simon Lucey (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Adam Lilja (None) · Junsheng Fu (Zenseact) · Erik Stenborg (Chalmers University) · Lars Hammarstrand (Chalmers University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>USE: Universal Segment Embeddings for Open-Vocabulary Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoqi Wang (Ohio State University, Columbus) · Wenbin He (Bosch) · Xiwei Xuan (Bosch) · Clint Sebastian (Bosch) · Jorge Piazentin Ono (Bosch) · Xin Li (Bosch Reserach) · Sima Behpour (Bosch Center for Artificial Intelligence (BCAI)) · Thang Doan (Bosch Center for Artificial Intelligence) · Liang Gou (Bosch) · Shen (Ohio State University) · Liu Ren (Bosch Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Masked AutoDecoder is Effective Multi-Task Vision Generalist</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Han Qiu (Nanyang Technological University) · Jiaxing Huang (Nanyang Technological University) · Peng Gao (The Chinese University of Hong Kong) · Lewei Lu (SenseTime) · Xiaoqin Zhang (Wenzhou University) · Shijian Lu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UniMix: Towards Domain Adaptive and Generalizable LiDAR Semantic Segmentation in Adverse Weather</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haimei Zhao (The University of Sydney) · Jing Zhang (The University of Sydney) · Zhuo Chen (Tsinghua University, Tsinghua University) · Shanshan Zhao (JD Explore Academy) · Dacheng Tao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Fourier-basis functions to bridge augmentation gap: Rethinking frequency augmentation in image classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mei Vaish (TNO) · Shunxin Wang (University of Twente) · Nicola Strisciuglio (University of Twente)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dynamic Policy-Driven Adaptive Multi-Instance Learning for Whole Slide Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tingting Zheng (Harbin Institute of Technology) · Kui Jiang (Harbin Institute of Technology) · Hongxun Yao (Harbin Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PerceptionGPT: Effectively Fusing Visual Perception into LLM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Renjie Pi (None) · Lewei Yao (Harbin Institute of Technology) · Jiahui Gao (The University of Hong Kong) · Jipeng Zhang (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology) · Tong Zhang (UIUC)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Probing the 3D Awareness of Visual Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohamed El Banani (University of Michigan) · Amit Raj (Google ) · Kevis-kokitsi Maninis (Google) · Abhishek Kar (Google) · Yuanzhen Li (Massachusetts Institute of Technology) · Michael Rubinstein (Google) · Deqing Sun (Google) · Leonidas Guibas (Stanford University) · Justin Johnson (University of Michigan) · Varun Jampani (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>View-Category Interactive Sharing Transformer for Incomplete Multi-View Multi-Label Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shilong Ou (Beijing University of Posts and Telecommunications) · Zhe Xue (Beijing University of Posts and Telecommunications) · Yawen Li (Beijing University of Posts and Telecommunications) · Meiyu Liang (Beijing University of Posts and Telecommunications) · Yuanqiang Cai (Beijing University of Posts and Telecommunications) · junjiang wu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://yifanlu0227.github.io/ChatSim/" target="_blank">Editable Scene Simulation for Autonomous Driving via LLM-Agent Collaboration</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxi Wei (None) · Zi Wang (CMU, Carnegie Mellon University) · Yifan Lu (Shanghai Jiaotong University) · Chenxin Xu (Shanghai Jiao Tong University &amp; National University of Singapore) · Changxing Liu (Shanghai Jiaotong University) · Hao Zhao (Tsinghua University, Tsinghua University) · Siheng Chen (Shanghai Jiao Tong University) · Yanfeng Wang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PAD: Patch-Agnostic Defense against Adversarial Patch Attacks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lihua Jing (University of the Chinese Academy of Sciences) · Rui Wang (Institute of Information Engineering) · Wenqi Ren (Sun Yat-Sen University) · Xin Dong (University of the Chinese Academy of Sciences) · Cong Zou (Institute of Information Engineering, CAS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unleashing Unlabeled Data: A Paradigm for Cross-View Geo-Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guopeng Li (Wuhan University) · Ming Qian (None) · Gui-Song Xia (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhi Gao (Peking University) · Yuntao Du. (Nanjing University) · Xintong Zhang (Beijing Institute for General Artificial Intelligence) · Xiaojian Ma (University of California, Los Angeles) · Wenjuan Han (Beijing Jiaotong University) · Song-Chun Zhu (UCLA) · Qing Li (Beijing Institute for General Artificial Intelligence (BIGAI))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EasyDrag: Efficient Point-based Manipulation on Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingzhong Hou (University of Chinese Academy of Sciences) · Boxiao Liu (Sensetime Research) · Yi Zhang (The Chinese University of Hong Kong) · Jihao Liu (The Chinese University of Hong Kong) · Yu Liu (The Chinese University of Hong Kong) · Haihang You (Institute of Computing Technology, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generating Illustrated Instructions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sachit Menon (Columbia University) · Ishan Misra (Facebook) · Rohit Girdhar (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LASIL: Learner-Aware Supervised Imitation Learning For Long-term Microscopic Traffic Simulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ke Guo (HKU) · Zhenwei Miao (Alibaba Group) · Wei Jing (NetEase, Inc.) · Weiwei Liu (Huzhou Institute of Zhejiang University) · Weizi Li (University of Tennessee, Knoxville) · Dayang Hao (Cainiao) · Jia Pan (University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ye Yuan (NVIDIA Research) · Xueting Li (NVIDIA) · Yangyi Huang (Zhejiang University) · Shalini De Mello (NVIDIA Research) · Koki Nagano (None) · Jan Kautz (NVIDIA) · Umar Iqbal (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://mslab.es/projects/TexTile/" target="_blank">TexTile: A Differentiable Metric for Texture Tileability</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Carlos Rodriguez-Pardo (Politecnico di Milano) · Dan Casas (Universidad Rey Juan Carlos) · Elena Garces (Universidad Rey Juan Carlos) · Jorge Lopez-Moreno (Universidad Rey Juan Carlos)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Image Processing GNN: Breaking Rigidity in Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuchuan Tian (Peking University) · Hanting Chen (Huawei Technologies Ltd.) · Chao Xu (Peking University) · Yunhe Wang (Huawei Noah's Ark Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jihan Yang (The University of Hong Kong) · Runyu Ding (Electrical and Electronic Engineering, University of Hong Kong) · Weipeng DENG (University of Hong Kong) · Zhe Wang (Sensetime Group Limited) · Xiaojuan Qi (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Yang (Beijing Institute of Technology) · Liyuan Pan (Beijing Institute of Technology) · Yan Yang (ANU) · Richard Hartley (ANU / Google) · Miaomiao Liu (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>X-MIC: Cross-Modal Instance Conditioning for Egocentric Action Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anna Kukleva (MPII) · Fadime Sener () · Edoardo Remelli (EPFL - EPF Lausanne) · Bugra Tekin (Meta) · Eric Sauser (Meta) · Bernt Schiele (Max Planck Institute for Informatics) · Shugao Ma (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiyi Chen (ETH Zürich) · Marko Mihajlovic (Swiss Federal Institute of Technology) · Shaofei Wang (None) · Sergey Prokudin (ETHZ - ETH Zurich) · Siyu Tang (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Prompt-Driven Referring Image Segmentation with Instance Contrasting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chao Shang (None) · Zichen Song (University of Electronic Science and Technology of China) · Heqian Qiu (University of Electronic Science and Technology of China) · Lanxiao Wang (University of Electronic Science and Technology of China) · Fanman Meng (University of Electronic Science and Technology of China) · Hongliang Li (University of Electronic Science and Technology of China, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MTMMC: A Large-Scale Real-World Multi-Modal Camera Tracking Benchmark</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sanghyun Woo (New York University) · Kwanyong Park (	Electronics and Telecommunication Research Institute) · Inkyu Shin (Korea Advanced Institute of Science &amp; Technology) · Myungchul Kim (Korea Advanced Institute of Science &amp; Technology) · In So Kweon (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LTGC: Long-tail Recognition via Leveraging LLMs-driven Generated Content</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qihao Zhao (Beijing University of Chemical Technology) · Yalun Dai (Nanyang Technological University) · Hao Li (Northwest Polytechnical University) · Wei Hu (Beijing Univeristy of Chemical Technology) · Fan Zhang (Beijing University of Chemical Technology) · Jun Liu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/GitZH-Chen/SPDMLR.git" target="_blank">Riemannian Multinomial Logistics Regression for SPD Neural Networks</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziheng Chen (University of Trento) · Yue Song (University of Trento) · Gaowen Liu (None) · Ramana Kompella (Cisco) · Xiaojun Wu (Jiangnan University) · Nicu Sebe (University of Trento)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Joint Reconstruction of 3D Human and Object via Contact-Based Refinement Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyeongjin Nam (None) · Daniel Jung (Seoul National University) · Gyeongsik Moon (None) · Kyoung Mu Lee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learned Scanpaths Aid Blind Panoramic Video Quality Assessment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kanglong FAN (City University of Hong Kong) · Wen Wen (City University of Hong Kong) · Mu Li (The Chinese University of Hong Kong, Shenzhen) · YIFAN PENG (University of Hong Kong) · Kede Ma (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>S<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-27-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-157" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-158" class="mjx-mrow"><span id="MJXc-Node-159" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-160" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-161" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.316em;">2</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>2</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-27">^2</script>MVTC: a Simple yet Efficient Scalable Multi-View Tensor Clustering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhen Long (University of Electronic Science and Technology of China) · Qiyuan Wang (None) · Yazhou Ren (University of Electronic Science and Technology of China) · Yipeng Liu (University of Electronic Science and Technology of China) · Ce Zhu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Call to Reflect on Evaluation Practices for Age Estimation: Comparative Analysis of the State-of-the-Art and a Unified Benchmark</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jakub Paplham (Czech Technical University in Prague) · Vojtech Franc (Czech Technical University in Prague, Faculty of Electrical Engineering)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Adapting Short-Term Transformers for Action Detection in Untrimmed Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Min Yang (None) · gaohuan (Inchitech Company) · Ping Guo (Intel) · Limin Wang (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>COSMO: Converting and Smoothing False Negatives for Vision-Language Pre-training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaeseok Byun (Seoul National University) · Dohoon Kim (Seoul National University) · Taesup Moon (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Efficiently Assemble Normalization Layers and Regularization for Federated Domain Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Khiem Le (University of Notre Dame) · Tuan Long Ho (VinUniversity) · Cuong Do (None) · Danh Le-Phuoc (TU Berlin) · KOK SENG WONG (VinUniversity)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unsupervised Gaze Representation Learning from Multi-view Face Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiwei Bao (Beihang University) · Feng Lu (Beihang University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PEEKABOO: Interactive Video Generation via Masked-Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yash Jain (Microsoft) · Anshul Nasery (University of Washington) · Vibhav Vineet (Microsoft) · Harkirat Behl (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Align and Aggregate: Compositional Reasoning with Video Alignment and Answer Aggregation for Video Question-Answering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaohe Liao (Shanghai Jiao Tong University) · Jiangtong Li (Shanghai Jiao Tong University) · Li Niu () · Liqing Zhang (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Roy Kapon (Tel Aviv University) · Guy Tevet (None) · Daniel Cohen-Or (Google) · Amit H. Bermano (Tel Aviv University, Technion)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Reg-PTQ: Regression-specialized Post-training Quantization for Fully Quantized Object Detector</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifu Ding (None) · Weilun Feng (Beijing University of Aeronautics and Astronautics) · Chuyan Chen (Beijing University of Aeronautics and Astronautics) · Jinyang Guo (Beijing University of Aeronautics and Astronautics) · Xianglong Liu (BUAA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>From Coarse to Fine-Grained Open-Set Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nico Lang (University of Copenhagen) · Vésteinn Snæbjarnarson (Copenhagen University) · Elijah Cole (Altos Labs) · Oisin Mac Aodha (University of Edinburgh) · Christian Igel (University of Copenhagen) · Serge Belongie (University of Copenhagen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DSL-FIQA: Assessing Facial Image Quality via Dual-Set Degradation Learning and Landmark-Guided Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei-Ting Chen (National Taiwan University) · Gurunandan Krishnan (Snap Inc.) · Qiang Gao (Snap Inc.) · Sy-Yen Kuo (National Taiwan University) · Sizhuo Ma (Snap Inc.) · Jian Wang (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Discriminative Pattern Calibration Mechanism for Source-Free Domain Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haifeng Xia (Southeast University) · Siyu Xia (Southeast University) · Zhengming Ding (Tulane University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RAM-Avatar: Real-time Photo-Realistic Avatar from Monocular Videos with Full-body Control</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            xiang deng (tsinghua university) · Zerong Zheng (Tsinghua University) · Yuxiang Zhang (Tsinghua University, Tsinghua University) · Jingxiang Sun (None) · Chao Xu (NNCosmos) · Xiaodong Yang (Li Auto) · Lizhen Wang (Tsinghua University, Tsinghua University) · Yebin Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Generalizable Multi-Object Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zheng Qin () · Le Wang (Xi'an Jiaotong University) · Sanping Zhou (Xi'an Jiaotong University) · Panpan Fu (Xi'an Jiaotong University) · Gang Hua (Wormpex AI Research) · Wei Tang (University of Illinois, Chicago)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EMCAD: Efficient Multi-scale Convolutional Attention Decoding for Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Md Mostafijur Rahman (University of Texas at Austin) · Mustafa Munir (The University of Texas at Austin) · Radu Marculescu (University of Texas, Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyi Yang (None) · Xinyu Gao (Zhejiang University) · Wen Zhou (University of Science and Technology of China) · Shaohui Jiao (Bytedance) · Yuqing Zhang (Zhejiang University) · Xiaogang Jin (State Key Lab of CAD&amp;CG, Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TokenHMR: Advancing Human Mesh Recovery with a Tokenized Pose Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sai Kumar Dwivedi (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Yu Sun (Harbin Institute of Technology) · Priyanka Patel (Max-Planck Institute) · Yao Feng (None) · Michael J. Black (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generate Like Experts: Multi-Stage Font Generation by Incorporating Font Transfer Process into Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Fu (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Fanghua Yu (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Anran Liu (None) · Zixuan Wang (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences) · Jie Wen (Harbin Institute of Technology, Shenzhen) · Junjun He (Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Chinese Academy of Sciences) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Unified Diffusion Framework for Scene-aware Human Motion Estimation from Sparse Signals</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiangnan Tang (ShanghaiTech University) · Jingya Wang (ShanghaiTech University) · Kaiyang Ji (None) · Lan Xu (ShanghaiTech University) · Jingyi Yu (Shanghai Tech University) · Ye Shi (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Surveillance Video-and-Language Understanding: New Dataset, Baselines, and Challenges</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tongtong Yuan (Beijing University of Technology) · Xuange Zhang (Beijing University Of Technology) · Kun Liu (Beijing University of Posts and Telecommunications) · Bo Liu (Beijing University of Technology) · Chen Chen () · Jian Jin (China Academy of information and communications technology) · Zhenzhen Jiao (Beijing Teleinfo Technology, CAICT)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>How to Make Cross Encoder a Good Teacher for Efficient Image-Text Retrieval?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxin Chen (None) · Zongyang Ma (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Ziqi Zhang (None) · Zhongang Qi (Tencent PCG ARC Lab) · Chunfeng Yuan (, Institute of automation, Chinese academy of science) · Bing Li (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Junfu Pu (Tencent ARC Lab) · Ying Shan (Tencent) · Xiaojuan Qi (University of Oxford) · Weiming Hu (Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Locally Adaptive Neural 3D Morphable Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Michail Tarasiou (Imperial College London) · Rolandos Alexandros Potamias (Imperial College London) · Eimear O' Sullivan (Huawei Technologies Ltd.) · Stylianos Ploumpis (Imperial College London) · Stefanos Zafeiriou (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Revisiting Adversarial Training at Scale</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeyu Wang (University of California, Santa Cruz) · Xianhang li (University of California, Santa Cruz) · Hongru Zhu (None) · Cihang Xie (University of California, Santa Cruz)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Benchmarking Segmentation Models with Mask-Preserved Attribute Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zijin Yin (None) · Kongming Liang (Beijing University of Posts and Telecommunications) · Bing Li (None) · Zhanyu Ma (Beijing University of Post and Telecommunication) · Jun Guo (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MaskCLR: Attention-Guided Contrastive Learning for Robust Action Representation Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohamed Abdelfattah (VITA, EPFL) · Mariam Hassan (EPFL - EPF Lausanne) · Alex Alahi (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Logit Standardization in Knowledge Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shangquan Sun (University of Chinese Academy of Sciences) · Wenqi Ren (Sun Yat-Sen University) · Jingzhi Li (Institute information of engineering, chinese academy of sciences) · Rui Wang (Institute of Information Engineering) · Xiaochun Cao (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://zc-alexfan.github.io/hold" target="_blank">HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zicong Fan (ETH Zürich) · Maria Parelli (ETH Zurich) · Maria Kadoglou (ETHZ - ETH Zurich) · Xu Chen (Google) · Muhammed Kocabas (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Michael J. Black (University of Tübingen) · Otmar Hilliges (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/yuhangchen0/FedHEAL" target="_blank">Fair Federated Learning under Domain Skew with Local Consistency and Domain Diversity</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhang Chen (Wuhan University) · Wenke Huang (Wuhan University) · Mang Ye (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learn from View Correlation: An Anchor Enhancement Strategy for Multi-view Clustering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Suyuan Liu (National University of Defense Technology) · KE LIANG (National University of Defense Technology) · Zhibin Dong (National University of Defense Technology) · Siwei Wang (National University of Defense Technology) · Xihong Yang (National University of Defense Technology) · sihang zhou (National University of Defense Technology) · En Zhu (National University of Defense Technology) · Xinwang Liu (National University of Defense Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Visual In-Context Prompting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feng Li (The Hong Kong University of Science and Technology) · Qing Jiang (South China University of Technology) · Hao Zhang (The Hong Kong University of Science and Technology) · Shilong Liu (Tsinghua University, Tsinghua University) · Huaizhe Xu (Hong Kong University of Science and Technology) · Xueyan Zou (None) · Tianhe Ren (The International Digital Economy Academy) · Hongyang Li (South China University of Technology) · Lei Zhang (International Digital Economy Academy (IDEA)) · Chunyuan Li (Microsoft Research, Redmond) · Jianwei Yang (Microsoft Research) · Jianfeng Gao (Microsoft Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Overload: Latency Attacks on Object Detection for Edge Devices</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Erh-Chung Chen (National Tsing Hua University) · Pin-Yu Chen (None) · I-Hsin Chung (IBM Research) · Che-Rung Lee (Department of Computer Science, National Tsing Hua University, National Tsing Hua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MonoNPHM: Dynamic Head Reconstruction from Monocular Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Simon Giebenhain (Technische Universität München) · Tobias Kirschstein (Department of Informatics, Technische Universität München) · Markos Georgopoulos (Synthesia) · Martin Rünz (Synthesia) · Lourdes Agapito (University College London) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dual DETRs for Multi-Label Temporal Action Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhan Zhu () · Guozhen Zhang (Nanjing University) · Jing Tan (The Chinese University of Hong Kong) · Gangshan Wu (Nanjing University) · Limin Wang (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UFC-Net: Unrolling Fixed-point Continuous Network for Deep Compressive Sensing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyang Wang (Northwest Polytechnical University Xi'an) · Hongping Gan (Northwest Polytechnical University Xi'an)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Symphonize 3D Semantic Scene Completion with Contextual Instance Queries</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyi Jiang (Huazhong University of Science and Technology) · Tianheng Cheng (Huazhong University of Science and Technology) · Naiyu Gao (HorizonRobotics Inc.) · Haoyang Zhang (Horizon Robotics) · Tianwei Lin (Horizon Robotics) · Wenyu Liu (Huazhong University of Science and Technology) · Xinggang Wang (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>End-to-End Temporal Action Detection with 1B Parameters Across 1000 Frames</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuming Liu (KAUST) · Chenlin Zhang (Moonshot AI, Ltd) · Chen Zhao (King Abdullah University of Science and Technology (KAUST)) · Bernard Ghanem (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unbiased Faster R-CNN for Single-source Domain Generalized Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yajing Liu (Shenyang Institute of Automation, Chinese Academy of Sciences) · Shijun Zhou (Shenyang Institute of Automation, Chinese Academy of Sciences) · Xiyao Liu (Shenyang Institute of Automation Chinese Academy of Sciences ) · chunhui Hao (None) · Baojie Fan (Nanjing University of Posts and Telecommunications) · Jiandong Tian (The Shenyang Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AlignSAM: Aligning Segment Anything Model to Open Context via Reinforcement Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Duojun Huang (SUN YAT-SEN UNIVERSITY) · Xinyu Xiong (SUN YAT-SEN UNIVERSITY) · Jie Ma (SUN YAT-SEN UNIVERSITY) · Jichang Li (The University of Hong Kong) · Zequn Jie (Meituan) · Lin Ma (Meituan) · Guanbin Li (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adaptive VIO: Deep Visual-Inertial Odometry with Online Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Youqi Pan (Peking University) · Wugen Zhou (Peking University) · Yingdian Cao (Peking University) · Hongbin Zha (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Tri-Modal Motion Retrieval by Learning a Joint Embedding Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kangning Yin (ShanghaiTech University) · Shihao Zou (University of Alberta) · Yuxuan Ge (ShanghaiTech University) · Zheng Tian (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Continual Segmentation with Disentangled Objectness Learning and Class Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yizheng Gong (Xi'an Jiaotong-Liverpool University) · Siyue Yu (Xi'an Jiaotong-Liverpool University) · Xiaoyang Wang () · Jimin Xiao (Xi'an Jiaotong-Liverpool University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Supervised Anomaly Detection for Complex Industrial Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aimira Baitieva (Valeo) · David Hurych (Valeo.ai) · Victor Besnier (Valeo.ai) · Olivier BERNARD (Valeo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>The Devil is in the Details: StyleFeatureEditor for Detail-Rich StyleGAN Inversion and High Quality Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Denis Bobkov (Higher School of Economics, Higher School of Economics) · Vadim Titov (ARTIFICIAL INTELLIGENCE RESEARCH INSTITUTE) · Aibek Alanov (Artificial Intelligence Research Institute) · Dmitry Vetrov (Constructor University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Interactive Continual Learning: Fast and Slow Thinking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Biqing Qi (Harbin Institute of Technology &amp; Tsinghua University &amp; Frontis.AI) · Xinquan Chen (Harbin Institute of Technology) · Junqi Gao (Harbin Institute of Technology) · Dong Li (Harbin Institute of Technology) · Jianxing Liu (Harbin Institute of Technology) · Ligang Wu (Harbin Institute of Technology) · Bowen Zhou (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Weakly Misalignment-free Adaptive Feature Alignment for UAVs-based Multimodal Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Chen (National University of Defense Technology) · Jiahao Qi (None) · Xingyue Liu (National University of Defense Technology) · Kangcheng Bin (National University of Defense Technology) · Ruigang Fu (None) · Xikun Hu (None) · Ping Zhong (National University of Defense Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Scaling Diffusion Models to Real-World 3D LiDAR Scene Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lucas Nunes (University of Bonn) · Rodrigo Marcuzzi (University of Bonn) · Benedikt Mersch (University of Bonn) · Jens Behley (University of Bonn) · Cyrill Stachniss (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Prompt-Driven Dynamic Object-Centric Learning for Single Domain Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Deng Li (Tianjin University) · Aming Wu (Xidian University) · Yaowei Wang (Pengcheng Laboratory) · Yahong Han (Tianjin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>In-Context Matting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            He Guo () · Zixuan Ye (None) · Zhiguo Cao () · Hao Lu (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/zhuangshaobin/Vlogger" target="_blank">Vlogger: Make Your Dream A Vlog</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shaobin Zhuang (Shanghai AI Laboratory) · Kunchang Li (SIAT, UCAS) · Xinyuan Chen (Shanghai Artificial Intelligence Laboratory) · Yaohui Wang (Shanghai AI Laboratory) · Ziwei Liu (Nanyang Technological University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Yali Wang (SIAT, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://kxhit.github.io/EscherNet" target="_blank">EscherNet: A Generative Model for Scalable View Synthesis</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Kong (Imperial College London) · Shikun Liu (Imperial College London) · Xiaoyang Lyu (University of Hong Kong) · Marwan Taher (The University of Sheffield) · Xiaojuan Qi (University of Oxford) · Andrew J. Davison (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FlowTrack: Revisiting Optical Flow for Long-Range Dense Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seokju Cho (Korea University) · Gabriel Huang (None) · Seungryong Kim (Korea University) · Joon-Young Lee (Adobe Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MVCPS-NeuS: Multi-view Constrained Photometric Stereo for Neural Surface Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hiroaki Santo (Osaka University) · Fumio Okura (Osaka University) · Yasuyuki Matsushita (Osaka University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LLaFS: When Large Language Models Meet Few-Shot Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lanyun Zhu (Singapore University of Technology and Design) · Tianrun Chen (Zhejiang University) · Deyi Ji (None) · Jieping Ye (Alibaba Group) · Jun Liu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Memorization-Free Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Chen (University of Sydney) · Daochang Liu (University of Sydney) · Chang Xu (University of Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ParameterNet: Parameters Are All You Need for Large-scale Visual Pretraining of Mobile Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kai Han (Huawei Noah's Ark Lab) · Yunhe Wang (Huawei Noah's Ark Lab) · Jianyuan Guo (University of Sydney) · Enhua Wu (University of Macau)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PH-Net: Semi-Supervised Breast Lesion Segmentation via Patch-wise Hardness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siyao Jiang (Shenzhen University) · Huisi Wu (Shenzhen University) · Junyang Chen () · Qin Zhang (Shenzhen University) · Jing Qin (Hong Kong Polytechnic University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RELI11D: A Comprehensive Multimodal Human Motion Dataset and Method</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ming Yan (Xiamen University) · Yan Zhang (Xiamen University) · Shuqiang Cai (Xiamen University) · Shuqi Fan (Xiamen University) · Xincheng Lin (Xiamen University) · Yudi Dai (Xiamen University) · Siqi Shen (Xiamen University) · Chenglu Wen (Xiamen University) · Lan Xu (ShanghaiTech University) · Yuexin Ma (ShanghaiTech University) · Cheng Wang (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Guided Slot Attention for Unsupervised Video Object Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minhyeok Lee (Yonsei University) · Suhwan Cho (Yonsei University) · Dogyoon Lee (Yonsei University) · Chaewon Park (Yonsei University) · Jungho Lee () · Sangyoun Lee (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Entangled View-Epipolar Information Aggregation for Generalizable Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyuan Min (Zhejiang University) · Yawei Luo (Zhejiang University) · Wei Yang (Huazhong University of Science and Technology) · Yuesong Wang (None) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unified Entropy Optimization for Open-Set Test-Time Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengqing Gao (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xu-Yao Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Cheng-Lin Liu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Distraction is All You Need: Memory-Efficient Image Immunization against Diffusion-Based Image Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lo (None) · Cheng Yeo (National Chiao Tung University) · Hong-Han Shuai (National Yang Ming Chiao Tung University) · Wen-Huang Cheng (National Taiwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SEED-Bench: Benchmarking Multimodal Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bohao Li (None) · Yuying Ge (University of Hong Kong) · Yixiao Ge (Tencent) · Guangzhi Wang (National University of Singapore) · Rui Wang (Fudan University) · Ruimao Zhang (The Chinese University of Hong Kong (Shenzhen)) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LSK3DNet: Towards Effective and Efficient 3D Perception with Large Sparse Kernels</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tuo Feng (University of Technology Sydney) · Wenguan Wang (Zhejiang University) · Fan Ma (None) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Parameter Efficient Fine-tuning via Cross Block Orchestration for Segment Anything Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zelin Peng (None) · Zhengqin Xu (Shanghai Jiaotong University) · Zhilin Zeng (Shanghai Jiaotong University) · Lingxi Xie (Huawei Technologies Ltd.) · Qi Tian (Huawei Technologies Ltd.) · Wei Shen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MGMap: Mask-Guided Learning for Online Vectorized HD Map Construction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaolu Liu (Zhejiang University) · Song Wang (Zhejiang University) · Wentong Li (College of Computer Science and Technology, Zhejiang University) · Ruizi Yang (Zhejiang University) · Junbo Chen (UDEER AI PTE.LTD) · Jianke Zhu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ViT-Lens: Towards Omni-modal Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Stan Weixian Lei (National University of Singapore) · Yixiao Ge (Tencent) · Kun Yi (Tencent ARC Lab) · Jianfeng Zhang (NUS) · Difei Gao (None) · Dylan Sun (University of Southern California) · Yuying Ge (University of Hong Kong) · Ying Shan (Tencent) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Leak and Learn: An Attacker's Cookbook to Train Using Leaked Data from Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Joshua C. Zhao (Purdue University) · Ahaan Dabholkar (Purdue University) · Atul Sharma (Purdue University) · Saurabh Bagchi (KeyByte LLC)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rewrite the stars</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Ma (Northeastern University) · Xiyang Dai (Microsoft) · Yue Bai (Northeastern University) · Yizhou Wang (Northeastern University) · Yun Fu (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MultiPhys: Multi-Person Physics-aware 3D Motion Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nicolás Ugrinovic (Universitat Politècnica de Catalunya) · Boxiao Pan (Stanford University) · Georgios Pavlakos (University of Texas at Austin) · Despoina Paschalidou (Stanford) · Bokui Shen (Stanford University) · Jordi Sanchez-Riera (IRI-CSIC - Institut de Robòtica i Informàtica Industrial) · Francesc Moreno-Noguer (Universidad Politécnica de Cataluna) · Leonidas Guibas (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LMDrive: Closed-Loop End-to-End Driving  with Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Shao (None) · Yuxuan Hu (The Chinese University of Hong Kong) · Letian Wang (University of Toronto) · Guanglu Song (Sensetime X-Lab) · Steven L. Waslander (University of Toronto) · Yu Liu (The Chinese University of Hong Kong) · Hongsheng Li (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A-Teacher: Asymmetric Network for 3D Semi-Supervised Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanshi Wang (Institute of automation, Chinese academy of science) · Zhipeng Zhang (Didi Research) · Jin Gao (Institute of automation, Chinese Academy of Sciences) · Weiming Hu (Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Xiao (Microsoft) · Haiping Wu (Microsoft) · Weijian Xu (Microsoft) · Xiyang Dai (Microsoft) · Houdong Hu (Microsoft) · Yumao Lu (Microsoft) · Michael Zeng (Microsoft) · Ce Liu (Microsoft) · Lu Yuan (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adversarial Score Distillation: When score distillation meets GAN</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Min Wei (Beijing University of Posts and Telecommunications) · Jingkai Zhou (Alibaba DAMO Academy) · Junyao Sun (South China University of Technology) · Xuesong Zhang (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BEVNeXt: Reviving Dense BEV Frameworks for 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenxin Li (Fudan University) · Shiyi Lan (NVIDIA CORPORATION) · Jose M. Alvarez (NVIDIA) · Zuxuan Wu (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with Diverse Poses</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Caoyuan Ma (Wuhan University) · Yu-Lun Liu (National Yang Ming Chiao Tung University) · Zhixiang Wang (The University of Tokyo) · Wu Liu (None) · Xinchen Liu (None) · Zheng Wang (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DMR: Decomposed Multi-Modality Representations for Frames and Events Fusion in Visual Reinforcement Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoran Xu () · Peixi Peng (Peking University) · Guang Tan (Sun Yat-sen University) · Yuan Li (Academy of Military Sciences) · Xinhai Xu (Academy of Military Sciences) · Yonghong Tian (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Communication-Efficient Collaborative Perception via Information Filling with Codebook</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yue Hu (Shanghai Jiao Tong University) · Juntong Peng (Shanghai Jiao Tong University) · Sifei Liu (Shanghai Jiao Tong University) · Junhao Ge (Shanghai Jiaotong University) · Si Liu (Beihang University) · Siheng Chen (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EventDance: Unsupervised Cross-modal Source-free Adaptation for Event-based Object Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Zheng (HKUST) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-IF: Leveraging Semantic Text Guidance for Degradation-Aware and Interactive Image Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xunpeng Yi (None) · Han Xu (None) · Hao Zhang (Wuhan University) · Linfeng Tang (Wuhan University) · Jiayi Ma (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Semantics-aware Motion Retargeting with Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haodong Zhang (None) · ZhiKe Chen () · Haocheng Xu (Zhejiang University) · Lei Hao (Huawei Noah's Ark Lab) · Xiaofei Wu (Huawei Technologies Ltd.) · Songcen Xu (Huawei Noah's Ark Lab) · Zhensong Zhang (Huawei Noah's Ark Lab) · Yue Wang (Zhejiang University) · Rong Xiong (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LeftRefill: Filling Right Canvas based on Left Reference through Generalized Text-to-Image Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenjie Cao (None) · Yunuo Cai (Fudan University) · Qiaole Dong (Fudan University) · Yikai Wang (None) · Yanwei Fu (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pengfei Xie (Southeast University) · Wenqiang Xu (Shanghai Jiao Tong University) · Tutian Tang (Shanghai Jiao Tong University) · Zhenjun Yu (Shanghai JiaoTong University) · Cewu Lu (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoqi Li (Peking University) · Mingxu Zhang (Beijing University of Posts and Telecommunications) · Yiran Geng (Peking University) · Haoran Geng (Peking University) · Yuxing Long (Beijing University of Posts and Telecommunications) · Yan Shen (Peking University) · Renrui Zhang (MMLab of CUHK &amp;amp;amp; Shanghai AI Laboratory) · Jiaming Liu (Peking University) · Hao Dong (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PanoPose: Self-supervised Relative Pose Estimation for Panoramic Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Diantao Tu (None) · Hainan Cui (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xianwei Zheng (Wuhan University) · Shuhan Shen (Institute of automation, Chinese academy of science)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Enhancing Post-training Quantization Calibration through Contrastive Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuzhang Shang (Illinois Institute of Technology) · Gaowen Liu (None) · Ramana Kompella (Cisco) · Yan Yan (Illinois Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://fictionarry.github.io/DNGaussian/" target="_blank">DNGaussian: Optimizing Sparse-View 3D Gaussian Radiance Fields with Global-Local Depth Normalization</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahe Li (Beijing University of Aeronautics and Astronautics) · Jiawei Zhang (Beijing University of Aeronautics and Astronautics) · Xiao Bai (Beijing University of Aeronautics and Astronautics) · Jin Zheng (Beijing University of Aeronautics and Astronautics) · Xin Ning (Institute of Semiconductors, Chinese Academy of Sciences) · Jun Zhou (Griffith University) · Lin Gu (RIKEN / the University of Tokyo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffHuman: Probabilistic Photorealistic 3D Reconstruction of Humans</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Akash Sengupta (University of Cambridge) · Thiemo Alldieck (Google) · NIKOS KOLOTOUROS (None) · Enric Corona (Google) · Andrei Zanfir (Google) · Cristian Sminchisescu (Lund University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mask4Align: Aligned Entity Prompting with Color Masks for Multi-Entity Localization Problem</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoquan Zhang (South China University of Technology) · Ronggang Huang (South China University of Technology) · Yi Xie (South China University of Technology) · Huaidong Zhang (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Global and Local Prompts Cooperation via Optimal Transport for Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongxia Li (ShanghaiTech University) · Wei Huang (RIKEN AIP) · Jingya Wang (ShanghaiTech University) · Ye Shi (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Classes Are Not Equal: An Empirical Study on Image Recognition Fairness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiequan Cui (Nanyang Technological University) · Beier Zhu (Nanyang Technological University) · Xin Wen (The University of Hong Kong) · Xiaojuan Qi (University of Oxford) · Bei Yu (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Hanwang Zhang (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/16lemoing/dot" target="_blank">Dense Optical Tracking: Connecting the Dots</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guillaume Le Moing (Inria) · Jean Ponce (Ecole Normale Supérieure de Paris) · Cordelia Schmid (Inria / Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-agent Collaborative Perception via Motion-aware Robust Communication Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shixin Hong (Tsinghua University, Tsinghua University) · Yu LIU (Tsinghua University, Tsinghua University) · Zhi Li (Shenzhen International Graduate School, Tsinghua University) · Shaohui Li ( Tsinghua University) · You He (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Precise Image Editing via Recognition and Generation Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shelly Sheynin (META) · Adam Polyak (Meta) · Uriel Singer (Meta) · Yuval Kirstain (Tel Aviv University) · Amit Zohar (Meta) · Oron Ashual (Meta) · Devi Parikh (Meta / Georgia Tech) · Yaniv Taigman (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fourier Priors-Guided Diffusion for Zero-Shot Joint Low-Light Enhancement and Deblurring</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoqian Lv (Harbin Institute of Technology) · Shengping Zhang (Harbin Institute of Technology) · Chenyang Wang (Harbin Institute of Technology) · Yichen Zheng (Huazhong University of Science and Technology) · Bineng Zhong (Guangxi Normal University) · Chongyi Li () · Liqiang Nie (Harbin Institute of Technology (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Focus on Hiders: Exploring Hidden Threats for Enhancing Adversarial Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qian Li (Dalian University of Technology) · Yuxiao Hu (None) · Yinpeng Dong (Tsinghua University) · Dongxiao Zhang (Eastern Institute for Advanced Study) · Yuntian Chen (Eastern Institute for Advanced Study)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ColorPCR: Color Point Cloud Registration with Multi-Stage Geometric-Color Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juncheng Mu (, Tsinghua University) · Lin Bie (Tsinghua University, Tsinghua University) · Shaoyi Du (Xi'an Jiaotong University) · Yue Gao (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Person-in-WiFi 3D: End-to-End Multi-Person 3D Pose Estimation with Wi-Fi</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kangwei Yan (Xi'an Jiaotong University) · Fei Wang (Xi'an Jiaotong University) · Bo Qian (None) · Han Ding (Xi'an Jiaotong University) · Jinsong Han (Zhejiang University) · Xing Wei (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://dqiaole.github.io/MemFlow/" target="_blank">MemFlow: Optical Flow Estimation and Prediction with Memory</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiaole Dong (Fudan University) · Yanwei Fu (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FREE: Faster and Better Data-Free Meta-Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yongxian Wei (Tsinghua University) · Zixuan Hu (Tsinghua University) · Zhenyi Wang (University of Maryland, College Park) · Li Shen (JD Explore Academy) · Chun Yuan (Tsinghua University, Tsinghua University) · Dacheng Tao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Open Vocabulary Semantic Scene Sketch Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ahmed Bourouis (None) · Judith Fan (Stanford University) · Yulia Gryaditskaya (CVSSP, PAI, University of Surrey)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unsupervised Feature Learning with Emergent Data-Driven Prototypicality</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhui Guo (The University of Texas at Dallas) · Youren Zhang (University of Michigan - Ann Arbor) · Yubei Chen (New York University) · Stella X. Yu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Low-Res Leads the Way: Improving Generalization for Super-Resolution by Self-Supervised Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyu Chen (Hong Kong University of Science and Technology (Guangzhou)) · Wenbo Li (Huawei Technologies Ltd.) · Jinjin Gu (University of Sydney) · Jingjing Ren (The Hong Kong University of Science and Technology (Guangzhou)) · Haoze Sun (Tsinghua University, Tsinghua University) · Xueyi Zou (Huawei Technologies Ltd.) · Youliang Yan (Huawei Technologies Ltd.) · Zhensong Zhang (Huawei Noah's Ark Lab) · Lei Zhu (Hong Kong University of Science and Technology (Guangzhou) &amp; HKUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Distilling ODE Solvers of Diffusion Models into Smaller Steps</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sanghwan Kim (ETHZ - ETH Zurich) · Hao Tang (ETH Zurich and CMU) · Fisher Yu (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3DiffTection: 3D Object Detection with Geometry-aware Diffusion Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenfeng Xu (University of California Berkeley) · Huan Ling (Nvidia, University of Toronto) · Sanja Fidler (Department of Computer Science, University of Toronto) · Or Litany (NVIDIA / Technion)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hierarchical Patch-wise Diffusion Models for High-Resolution Video Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ivan Skorokhodov (KAUST) · Willi Menapace (University of Trento) · Aliaksandr Siarohin (Snap Inc.) · Sergey Tulyakov (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>XCube: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuanchi Ren (University of Toronto) · Jiahui Huang (None) · Xiaohui Zeng (Department of Computer Science, University of Toronto) · Ken Museth (NVIDIA) · Sanja Fidler (Department of Computer Science, University of Toronto) · Francis Williams (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Probabilistic Human Mesh Estimation with Hypothesis Scoring</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Xu (Peking University) · Xiaoxuan Ma (Peking University) · Jiajun Su (None) · Wentao Zhu (None) · Yu Qiao (Shanghai Jiao Tong University) · Yizhou Wang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shitian Zhao (East China Normal University) · Zhuowan Li (Johns Hopkins University) · YadongLu (ECNU) · Alan L. Yuille (Johns Hopkins University) · Yan Wang (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GRAM: Global Reasoning for Multi-Page VQA</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Itshak Blau (Electrical Engineering Department, Technion – Israel Institute of Technology, Technion - Israel Institute of Technology) · Sharon Fogel (Amazon) · Roi Ronen (Technion - Israel Institute of Technology, Technion - Israel Institute of Technology) · Alona Golts (Amazon) · Shahar Tsiper (Amazon) · Elad Ben Avraham (Amazon) · Aviad Aberdam (Amazon AWS AI) · Roy Ganz (Technion - Israel Institute of Technology, Technion) · Ron Litman (Amazon AI Labs)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            wenlong deng (University of British Columbia) · Christos Thrampoulidis (None) · Xiaoxiao Li (University of British Columbia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qifan Yu (None) · Juncheng Li (Zhejiang University) · Longhui Wei (Huawei Cloud Technologies Ltd.) · Liang Pang (Institute of Computing Technology, Chinese Academy of Sciences) · Wentao Ye (Zhejiang University) · Bosheng Qin (Zhejiang University) · Siliang Tang (Zhejiang University) · Qi Tian (Huawei Technologies Ltd.) · Yueting Zhuang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VMINer: Versatile Multi-view Inverse Rendering with Near- and Far-field Light Sources</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fan Fei (Peking University) · Jiajun Tang (Peking University) · Ping Tan (Hong Kong University of Science and Technology) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>On the test-time zero-shot generalization of vision-language models: Do we really need prompt learning?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Maxime Zanella (Université Catholique de Louvain) · Ismail Ben Ayed (ETS Montreal)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FaceChain-SuDe: Building Derived Class to Inherit Category Attributes for One-shot Subject-Driven Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pengchong Qiao (Peking University) · Lei Shang (Alibaba Group) · Chang Liu (Tsinghua University, Tsinghua University) · Baigui Sun (Alibaba Group) · Xiangyang Ji (Tsinghua University) · Jie Chen (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SpikingResformer: Bridging ResNet and Vision Transformer in Spiking Neural Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyu Shi (None) · Zecheng Hao (Peking University) · Zhaofei Yu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pin Tang (Shanghai Jiao Tong University) · Zhongdao Wang (Huawei Technologies Ltd.) · Guoqing Wang (Shanghai Jiao Tong University) · Jilai Zheng (Shanghai Jiaotong University) · Xiangxuan Ren (Shanghai Jiao Tong University) · Bailan Feng (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Chao Ma (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards High-fidelity Artistic Image Vectorization via Texture-Encapsulated Shape Parameterization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ye Chen (Shanghai Jiao Tong University) · Bingbing Ni (Shanghai Jiao Tong University) · Jinfan Liu (Tongji University) · Xiaoyang Huang (Shanghai Jiao Tong University) · Xuanhong Chen (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OmniSDF: Scene Reconstruction using Omnidirectional Signed Distance Functions and Adaptive Binoctrees</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hakyeong Kim (Korea Advanced Institute of Science and Technology) · Andreas Meuleman (Korea Advanced Institute of Science and Technology) · Hyeonjoong Jang (None) · James Tompkin (Brown University) · Min H. Kim (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Extreme Point Supervised Instance Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyeonjun Lee (POSTECH) · Sehyun Hwang (POSTECH) · Suha Kwak (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Co-Speech Gesture Video Generation via Motion-Decoupled Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu He (Tsinghua University) · Qiaochu Huang (Tsinghua University, Tsinghua University) · Zhensong Zhang (Huawei Noah's Ark Lab) · Zhiwei Lin (Tsinghua Shenzhen International Graduate School) · Zhiyong Wu (Tsinghua University) · Sicheng Yang () · Minglei Li (Huawei Cloud Computing Technologies Ltd.) · Zhiyi Chen (Huawei Technologies Ltd.) · Songcen Xu (Huawei Noah's Ark Lab) · Xiaofei Wu (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DreamComposer: Controllable 3D Object Generation via Multi-View Conditions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhan Yang (The University of Hong Kong) · Yukun Huang (University of Science and Technology of China) · Xiaoyang Wu (The University of Hong Kong) · Yuan-Chen Guo (Tsinghua University) · Song-Hai Zhang (Tsinghua University, Tsinghua University) · Hengshuang Zhao (The University of Hong Kong) · Tong He (Shanghai AI Lab) · Xihui Liu (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Degree-of-Freedom Matters: Inferring Dynamics from Point Trajectories</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Zhang (ETH Zurich) · Sergey Prokudin (ETHZ - ETH Zurich) · Marko Mihajlovic (Swiss Federal Institute of Technology) · Qianli Ma (NVIDIA Research) · Siyu Tang (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ActiveDC: Distribution Calibration for Active Finetuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenshuai Xu () · Zhenghui Hu (Hangzhou Innovation Institute, Beihang University) · Yu Lu (Beijing University of Aeronautics and Astronautics) · Jinzhou Meng (Beijing University of Aeronautics and Astronautics) · Qingjie Liu (None) · Yunhong Wang (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>KVQ: Kwai Video Quality Assessment for Short-form Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiting Lu (University of Science and Technology of China) · Xin Li (None) · Yajing Pei (None) · Kun Yuan (Kuaishou Technology) · Qizhi Xie (None) · Yunpeng Qu (None) · Ming Sun (Kuaishou Tech) · Chao Zhou (Peking University) · Zhibo Chen (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bidirectional Autoregessive Diffusion Model for Dance Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Canyu Zhang (University of South Carolina) · Youbao Tang (PAII INC.) · NING Zhang (PAII Inc.) · Ruei-Sung Lin (PAII Inc) · Mei Han (PAII Inc.) · Jing Xiao (Pingan Group) · Song Wang (University of South Carolina)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CoSeR: Bridging Image and Language for Cognitive Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoze Sun (Tsinghua University, Tsinghua University) · Wenbo Li (Huawei Technologies Ltd.) · Jianzhuang Liu (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Haoyu Chen (Hong Kong University of Science and Technology (Guangzhou)) · Renjing Pei (Huawei Technologies Ltd.) · Xueyi Zou (Huawei Technologies Ltd.) · Youliang Yan (Huawei Technologies Ltd.) · Yujiu Yang (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://subhadeepkoley.github.io/Sketch2Word/" target="_blank">You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Subhadeep Koley (University of Surrey) · Ayan Kumar Bhunia (University of Surrey, United Kingdom) · Aneeshan Sain (University of Surrey) · Pinaki Nath Chowdhury (University of Surrey) · Tao Xiang (University of Surrey) · Yi-Zhe Song (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NeRF Analogies - Example-Based Visual Attribute Transfer for NeRFs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Michael Fischer (University College London) · Zhengqin Li (Facebook) · Thu Nguyen-Phuoc (Reality Labs Research, Meta) · Aljaž Božič (Facebook) · Zhao Dong (Meta RL Research) · Carl Marshall (Reality Labs Research) · Tobias Ritschel (University College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Interference-Free Low-Rank Adaptation for Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan-Shuo Liang (nanjing university) · Wu-Jun Li (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-Scale 3D Gaussian Splatting for Anti-Aliased Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiwen Yan (National University of Singapore) · Weng Fei Low () · Yu Chen (National University of Singapore) · Gim Hee Lee (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Style Injection in Diffusion: A Training-free Approach for Adapting Large-scale Diffusion Models for Style Transfer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiwoo Chung (Sungkyunkwan University) · Sangeek Hyun (Sungkyunkwan University) · Jae-Pil Heo (Sungkyunkwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Selective Hourglass Mapping for Universal Image Restoration Based on Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dian Zheng (None) · Xiao-Ming Wu (SUN YAT-SEN UNIVERSITY) · Shuzhou Yang (Peking University) · Jian Zhang (None) · Jian-Fang Hu (SUN YAT-SEN UNIVERSITY) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Skeleton-in-Context: Unified Skeleton Sequence Modeling with In-Context Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinshun Wang (Sun Yat-sen University) · Zhongbin Fang (Sun Yat-sen University) · Xia Li (Department of Computer Science, ETH Zurich) · Xiangtai Li (Nanyang Technological University) · Chen Chen () · Mengyuan Liu (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Arun Reddy (Johns Hopkins University) · William Paul (None) · Corban Rivera (Johns Hopkins University Applied Physics Laboratory) · Ketul Shah (Johns Hopkins University) · Celso M. de Melo (University of Southern California) · Rama Chellappa (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Fast Adaptation for Human Pose Estimation via Meta-Optimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shengxiang Hu (None) · Huaijiang Sun (Nanjing University of Science and Technology) · Bin Li (Nanjing University of Science and Technology) · Dong Wei (Nanjing University of Science and Technology) · Weiqing Li (Nanjing University of Science and Technology) · Jianfeng Lu (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>"Previously on ..." From Recaps to Story Summarization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aditya Kumar Singh (International Institute of Information Technology, Hyderabad) · Dhruv Srivastava (International Institute of Information Technology (IIIT-H), Hyderabad) · Makarand Tapaswi (IIIT Hyderabad, Wadhwani AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generating Non-Stationary Textures using Self-Rectification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Zhou (Shenzhen University) · Rongjun Xiao (None) · Dani Lischinski (The Hebrew University of Jerusalem, Israel) · Daniel Cohen-Or (Google) · Hui Huang (Shenzhen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SDDGR: Stable Diffusion-based Deep Generative Replay for Class Incremental Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            JUNSU KIM (Ulsan National Institute of Science and Technology) · Hoseong Cho (None) · Jihyeon Kim (Ulsan National Institute of Science and Technology) · Yihalem Tiruneh (Ulsan National Institute of Science and Technology) · Seungryul Baek (UNIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Frozen Feature Augmentation for Few-Shot Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andreas Bär (None) · Neil Houlsby (Google) · Mostafa Dehghani (Google DeepMind) · Manoj Kumar (Google Deepmind)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>1-Lipschitz Layers Compared: Memory, Speed, and Certifiable Robustness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bernd Prach (ISTA) · Fabio Brau (Scuola Superiore Sant'Anna Pisa) · Giorgio Buttazzo (Scuola Superiore Sant'Anna Pisa) · Christoph Lampert (Institute of Science and Technology Austria)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyeonho Jeong (Korea Advanced Institute of Science &amp; Technology) · Geon Yeong Park (Korea Advanced Institute of Science and Technology) · Jong Chul Ye (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Anomaly Heterogeneity Learning for Open-set Supervised Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawen Zhu (Singapore Management University) · Choubo Ding (University of Adelaide) · Yu Tian (None) · Guansong Pang (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>L4D-Track: Language-to-4D Modeling Towards 6-DoF Tracking and Shape Reconstruction in 3D Point Cloud Stream</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingtao Sun (National University of Singapore) · Yaonan Wang (Hunan University) · Mingtao Feng (Xidian University) · Yulan Guo (SUN YAT-SEN UNIVERSITY) · Ajmal Mian (University of Western Australia) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>BerfScene: Bev-conditioned Equivariant Radiance Fields for Infinite 3D Scene Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qihang Zhang (The Chinese University of Hong Kong) · Yinghao Xu (Chinese University of Hong Kong) · Yujun Shen (The Chinese University of Hong Kong) · Bo Dai (Shanghai AI Laboratory) · Bolei Zhou (University of California, Los Angeles) · Ceyuan Yang (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GaussianShader: 3D Gaussian Splatting with Shading Functions for Reflective Surfaces</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yingwenqi Jiang (None) · Jiadong Tu (None) · Yuan Liu (The University of Hong Kong) · Xifeng Gao (Tencent America) · Xiaoxiao Long (The University of Hong Kong) · Wenping Wang (Texas A&amp;M University - College Station) · Yuexin Ma (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>The Science of Data Filtering: Data Curation cannot be Compute Agnostic</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sachin Goyal (Carnegie Mellon University) · Pratyush Maini (Carnegie Mellon University) · Zachary Lipton (Carnegie Mellon University) · Aditi Raghunathan (Carnegie Mellon University) · Zico Kolter (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PoNQ: a Neural QEM-based Mesh Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nissim Maruani (Inria) · Maks Ovsjanikov (Ecole Polytechnique, France) · Pierre Alliez (INRIA) · Mathieu Desbrun (INRIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Representing Signs as Language: A New Method for Sign Language Translation from Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jia Gong (Singapore University of Technology and Design) · Lin Geng Foo (Singapore University of Technology and Design) · Yixuan He (Singapore University of Technology and Design) · Hossein Rahmani (Lancaster University) · Jun Liu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HIPTrack: Visual Tracking with Historical Prompts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenrui Cai (Beihang University) · Qingjie Liu (None) · Yunhong Wang (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CAM Back Again: Large Kernel CNNs from a Weakly Supervised Object Localization Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shunsuke Yasuki (Rikkyo University) · Masato Taki (Rikkyo University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Disentangled Identifiers for Action-Customized Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siteng Huang (Zhejiang University &amp; Westlake University) · Biao Gong (Alibaba Group) · Yutong Feng (Alibaba Group) · Xi Chen (the University of Hong Kong, University of Hong Kong) · Yuqian Fu (Fudan University) · Yu Liu (Alibaba Group) · Donglin Wang (Westlake University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Regularized Parameter Uncertainty for Improving Generalization in Reinforcement Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pehuen Moure (ETH Zurich) · Longbiao Cheng (Insititute of Neuroinformatics, University of Zurich and ETH Zurich) · Joachim Ott (Swiss Federal Institute of Technology) · Zuowen Wang (Institute of Neuroinformatics, University of Zurich and ETH Zurich) · Shih-Chii Liu (University of Zurich and ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Robust Noisy Correspondence Learning with Equivariant Similarity Consistency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuchen Yang (Xi'an University of Electronic Science and Technology) · Erkun Yang (None) · Likai Wang (Xi'an University of Electronic Science and Technology) · Cheng Deng (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PanoRecon: Real-Time Panoptic 3D Reconstruction from Monocular Video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dong Wu (None) · Zike Yan (Tsinghua University) · Hongbin Zha (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/liyuantsao/FlowSR-LP" target="_blank">Boosting Flow-based Generative Super-Resolution Models via Learned Prior</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li-Yuan Tsao (National Tsing Hua University) · Yi-Chen Lo (National Tsing Hua University) · Chia-Che Chang (MediaTek) · Hao-Wei Chen (National Tsing Hua University) · Roy Tseng (MediaTek) · Chien Feng (Department of Computer Science, National Tsing Hua University, National Tsinghua University) · Chun-Yi Lee (National Tsing Hua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Situational Awareness Matters in 3D Vision Language Reasoning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunze Man (Department of Computer Science, University of Illinois at Urbana-Champaign) · Liang-Yan Gui (UIUC) · Yu-Xiong Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Directed Decentralized Collaboration for Personalized Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yingqi Liu (None) · Yifan Shi (Tsinghua University, Tsinghua University) · Qinglun Li (National University of Defense Technology) · Baoyuan Wu (The Chinese University of Hong Kong, Shenzhen) · Xueqian Wang (Tsinghua University, Tsinghua University) · Li Shen (JD Explore Academy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Holo-Relighting: Controllable Volumetric Portrait Relighting from a Single Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiqun Mei (None) · Yu Zeng (None) · He Zhang (Adobe Systems) · Zhixin Shu (Adobe Systems) · Xuaner Zhang (Adobe) · Sai Bi (Adobe Systems) · Jianming Zhang (Adobe Systems) · HyunJoon Jung (Adobe Systems) · Vishal M. Patel (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Rank Patches for Unbiased Image Redundancy Reduction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Luo (Fudan University) · Zhineng Chen (Fudan University) · Peng Zhou (Amazon) · Zuxuan Wu (Fudan University) · Xieping Gao (None) · Yu-Gang Jiang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Task-Driven Wavelets using Constrained Empirical Risk Minimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Eric Marcus (Netherlands Cancer Institute) · Ray Sheombarsing (None) · Jan-Jakob Sonke (Netherlands Cancer Institute) · Jonas Teuwen (Netherlands Cancer Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Molecular Data Programming: Towards Molecule Pseudo-labeling with Systematic Weak Supervision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Juan (None) · Kaixiong Zhou (Rice University) · Ninghao Liu (University of Georgia) · Tianlong Chen (Massachusetts Institute of Technology) · Xin Wang (Jilin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AHIVE: Anatomy-aware Hierarchical Vision Encoding for Interactive Radiology Report Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sixing Yan (None) · William K. Cheung (Hong Kong Baptist University) · Ivor Tsang (A*STAR) · Wan Hang Keith Chiu (Queen Elizabeth Hospital) · Tong Terence (The Chinese University of Hong Kong) · Ka Chun Cheung (NVIDIA) · Simon See (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text-to-3D using Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zilong Chen (Tsinghua University) · Feng Wang (Tsinghua University, Tsinghua University) · Yikai Wang (Tsinghua University) · Huaping Liu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Probing Synergistic High-Order Interaction in Infrared and Visible Image Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Naishan Zheng (University of Science and Technology of China) · Man Zhou (University of Science and Technology of China) · Jie Huang (University of Science and Technology of China) · Junming Hou (Southeast University) · Haoying Li (Zhejiang University) · Yuan Xu (Nanyang Technological University) · Feng Zhao (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jihyun Lee (KAIST) · Shunsuke Saito (Reality Labs Research) · Giljoo Nam (Meta) · Minhyuk Sung (KAIST) · Tae-Kyun Kim (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Scaling Laws of Synthetic Images for Model Training ... for Now</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lijie Fan (Massachusetts Institute of Technology) · Kaifeng Chen (Google) · Dilip Krishnan (Google) · Dina Katabi (Massachusetts Institute of Technology) · Phillip Isola (None) · Yonglong Tian (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Egocentric Full Body Motion Capture with FisheyeViT and Diffusion-Based Motion Refinement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jian Wang (Max Planck Institute for Informatics) · Zhe Cao (None) · Diogo Luvizon (Saarland Informatics Campus, Max-Planck Institute) · Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) · Kripasindhu Sarkar (Google) · Danhang Tang (Google Inc.) · Thabo Beeler (Google) · Christian Theobalt (MPI Informatik)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MMA: Multi-Modal Adapter for Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lingxiao Yang (SUN YAT-SEN UNIVERSITY) · Ru-Yuan Zhang (None) · Yanchen Wang (Stanford University) · Xiaohua Xie (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Linguistic-Aware Patch Slimming Framework for Fine-grained Cross-Modal Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zheren Fu (University of Science and Technology of China) · Lei Zhang (University of Science and Technology of China) · Hou Xia (University of science and technology of China) · Zhendong Mao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Blind Image Quality Assessment Based on Geometric Order Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nyeong-Ho Shin (None) · Seon-Ho Lee (None) · Chang-Su Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unsupervised Deep Unrolling Networks for Phase Unwrapping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhile Chen (South China University of Technology) · Yuhui Quan (South China University of Technology) · Hui Ji (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Bayesian Diffusion Models for 3D Shape Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haiyang Xu (University of Science and Technology of China) · Yu lei (Shanghai Jiao Tong University) · Zeyuan Chen (University of California, San Diego) · Xiang Zhang (University of California, San Diego) · Yue Zhao (Tsinghua University ) · Yilin Wang (Tsinghua University, Tsinghua University) · Zhuowen Tu (University of California, San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Would Deep Generative Models Amplify Bias in Future Models?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianwei Chen (Osaka University) · Yusuke Hirota (Osaka University) · Mayu Otani (None) · Noa Garcia (Osaka University) · Yuta Nakashima (Osaka University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SPOC: Imitating Shortest Paths in Simulation Enables Effective Navigation and Manipulation in the Real World</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kiana Ehsani (Allen Institute for Artificial Intelligence) · Tanmay Gupta (Allen Institute for Artificial Intelligence) · Rose Hendrix (Allen Institute for Artificial Intelligence) · Jordi Salvador (Allen Institute for AI) · Luca Weihs (Allen Institute for Artificial Intelligence) · Kuo-Hao Zeng (Allen Institute for Artificial Intelligence) · Kunal Singh Singh (None) · Yejin Kim (Allen Institute for Artificial Intelligence) · Winson Han (Allen Institute for Artificial Intelligence) · Alvaro Herrasti (Allen Institute for Artificial Intelligence) · Ranjay Krishna (University of Washington) · Dustin Schwenk (Allen Institute for Artificial Intelligence) · Eli VanderBilt (University of Idaho) · Aniruddha Kembhavi (Allen Institute for Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>De-confounded Data-free Knowledge Distillation for Handling Distribution Shifts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuzheng Wang (Fudan University) · Dingkang Yang (Fudan University) · Zhaoyu Chen (Fudan University) · Yang Liu (Fudan University) · Siao Liu (Fudan University) · Wenqiang Zhang (None) · Lihua Zhang (Fudan University) · Lizhe Qi (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>What Do You See in Vehicle? Comprehensive Vision Solution for In-Vehicle Gaze Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yihua Cheng (University of Birmingham) · Yaning Zhu (Huazhong University of Science and Technology) · Zongji Wang (Key Laboratory of Network Information System Technology (NIST), Aerospace Information Research Institute, Chinese Academy of Sciences) · hongquan hao (calmcar) · Liu wei (Jiangsu University of Science and Technology) · Shiqing Cheng (Zhejiang University) · Xi Wang (CalmCar Vision System) · Hyung Jin Chang ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HUGS: Human Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Muhammed Kocabas (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Jen-Hao Rick Chang (Apple) · James Gabriel (Apple) · Oncel Tuzel (Apple) · Anurag Ranjan (Apple)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://shunyuanzheng.github.io/GPS-Gaussian" target="_blank">GPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shunyuan Zheng (Harbin Institute of Technology) · Boyao ZHOU (Tsinghua University) · Ruizhi Shao (Tsinghua University, Tsinghua University) · Boning Liu (Department of Automation, Tsinghua University) · Shengping Zhang (Harbin Institute of Technology) · Liqiang Nie (Harbin Institute of Technology (Shenzhen)) · Yebin Liu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Commonsense Prototype for Outdoor Unsupervised 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hai Wu (Xiamen University) · Shijia Zhao (Xiamen University) · Xun Huang (Xiamen University) · Chenglu Wen (Xiamen University) · Xin Li (None) · Cheng Wang (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rapid Motor Adaptation for Robotic Manipulator Arms</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yichao Liang (None) · Kevin Ellis (Cornell University) · João F. Henriques (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffSal: Joint Audio and Video Learning for Diffusion Saliency Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junwen Xiong (Northwestern Polytechnical University) · Peng Zhang (Northwest Polytechnical University Xi'an) · Tao You (Northwest Polytechnical University Xi'an) · Chuanyue Li (Northwestern Polytechnical University, Northwest Polytechnical University Xi'an) · Wei Huang (Nanchang University) · Yufei Zha (Northwestern Polytechinical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Chen (HiDream.ai) · Yingwei Pan (HiDream.ai) · haibo yang (Fudan University) · Ting Yao (JD AI Research) · Tao Mei (JD Explore Academy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kejia Yin (None) · Varshanth Rao (ModiFace - A L'Oreal Group Company) · Ruowei Jiang (ModiFace) · Xudong Liu (None) · Parham Aarabi (Toronto University) · David B. Lindell (University of Toronto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>TurboSL: Dense, Accurate and Fast 3D by Neural Inverse Structured Light</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Parsa Mirdehghan (University of Toronto) · Maxx Wu (University of Toronto) · Wenzheng Chen (University of Toronto) · David B. Lindell (University of Toronto) · Kiriakos Kutulakos (University of Toronto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haokun Lin (City University of Hong Kong) · Haoli Bai (Huawei Technologies Ltd.) · Zhili Liu (Hong Kong University of Science and Technology) · Lu Hou (Huawei Technologies Ltd.) · Muyi Sun (Institute of automation,  Chinese Academy of Sciences) · Linqi Song (City University of Hong Kong) · Ying Wei (City University of Hong Kong) · Zhenan Sun (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diffeomorphic Template Registration for Atmospheric Turbulence Mitigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dong Lao (University of California, Los Angeles) · Congli Wang (University of California, Berkeley) · Alex Wong (Yale University) · Stefano Soatto (UCLA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adapting to Length Shift: FlexiLength Network for Trajectory Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yi Xu (Northeastern University) · Yun Fu (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Revisiting Non-Autoregressive Transformers for Efficient Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zanlin Ni (Tsinghua University) · Yulin Wang (Tsinghua University, Tsinghua University) · Renping Zhou (, Tsinghua University) · Jiayi Guo (Tsinghua University, Tsinghua University) · Jinyi Hu (Tsinghua University, Tsinghua University) · Zhiyuan Liu (Tsinghua University) · Shiji Song (Tsinghua University, Tsinghua University) · Yuan Yao (Tsinghua University) · Gao Huang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Mixed-Precision Quantization for Federated Learning on Resource-Constrained Heterogeneous Devices</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huancheng Chen (University of Texas at Austin) · Haris Vikalo (University of Texas, Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MIGC: Multi-Instance Generation Controller for Text-to-Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dewei Zhou (None) · You Li (Zhejiang University) · Fan Ma (None) · Xiaoting Zhang (Huawei Technologies Ltd.) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CausalPC: Improving the Robustness of Point Cloud Classification by Causal Effect Identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanmin Huang (Fudan University) · Mi Zhang (Fudan University) · Daizong Ding (Huawei Technologies Ltd.) · Erling Jiang (Fudan University) · Zhaoxiang Wang (Fudan University) · Min Yang (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffPerformer: Iterative Learning of Consistent Latent Guidance for Diffusion-based Human Video Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenyang Wang (Harbin Institute of Technology) · Zerong Zheng (Tsinghua University) · Tao Yu (Tsinghua University, Tsinghua University) · Xiaoqian Lv (Harbin Institute of Technology) · Bineng Zhong (Guangxi Normal University) · Shengping Zhang (Harbin Institute of Technology) · Liqiang Nie (Harbin Institute of Technology (Shenzhen))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LiSA: LiDAR Localization with Semantic Awareness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bochun Yang (Xiamen University) · Zijun Li (Xiamen University) · Wen Li (schoold of informatics xiamen university) · zhipeng cai (Intel Labs) · Chenglu Wen (Xiamen University) · Yu Zang (Xiamen University) · Matthias Mueller (None) · Cheng Wang (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unknown Prompt, the only Lacuna: Unveiling CLIP's Potential for Open Domain Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mainak Singha (Indian Institute of Technology Bombay) · Ankit Jha (Indian Institute of Technology Bombay) · Shirsha Bose (Technische Universität München) · Ashwin Nair (Indian Institute of Science Education and Research Thiruvananthapuram) · Moloud Abdar (Deakin University) · Biplab Banerjee (IIT Bombay)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Tumor Micro-environment Interactions Guided Graph Learning for Survival Analysis of Human Cancers from Whole-slide Pathological Images.</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            WEI SHAO (Nanjing University of Aeronautics and Astronautics) · YangYang Shi (Nanjing University of Aeronautics and Astronautics) · Daoqiang Zhang (Nanjing University of Aeronautics and Astronautics) · Junjie Zhou (Nanjing University of Aeronautics and Astronautics) · Peng Wan (Nanjing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Perceptual-Oriented Video Frame Interpolation Via Asymmetric Synergistic Blending</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangyang Wu (Shanghai Jiaotong University) · Xin Tao (Kuaishou) · Changlin Li (SeeKoo) · Wenyi Wang (University of Electronic Science and Technology of China) · Xiaohong Liu (Shanghai Jiao Tong University) · Qingqing Zheng ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Diffusion-based Blind Text Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuzhe Zhang (None) · jiawei zhang (Sensetime) · Hao Li (Beihang University) · Zhouxia Wang (Nanyang Technological University) · Luwei Hou (Beihang University) · Dongqing Zou (Sensetime Research) · Liheng Bian (Beijing Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Coupled Dictionaries from Unpaired Data for Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Longguang Wang (National University of Defense Technology) · Juncheng Li (Shanghai University) · Yingqian Wang (None) · Qingyong Hu (University of Oxford) · Yulan Guo (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FlowDiffuser: Advancing Optical Flow Estimation with Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ao Luo (Megvii Technology Inc.) · XIN LI (G42) · Fan Yang (AIQ) · Jiangyu Liu (Megvii Technology Inc.) · Haoqiang Fan (Megvii Technology Inc.) · Shuaicheng Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking Human Motion Prediction with Symplectic Integral</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haipeng Chen (Jilin University) · Kedi L yu (None) · Zhenguang Liu (Zhejiang University) · Yifang Yin (I2R, A*STAR) · Xun Yang (University of Science and Technology of China) · Yingda Lyu (Jilin University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Holodeck: Language Guided Generation of 3D Embodied AI Environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yue Yang (University of Pennsylvania) · Fan-Yun Sun (None) · Luca Weihs (Allen Institute for Artificial Intelligence) · Eli VanderBilt (University of Idaho) · Alvaro Herrasti (Allen Institute for Artificial Intelligence) · Winson Han (Allen Institute for Artificial Intelligence) · Jiajun Wu (Stanford University) · Nick Haber (Stanford University) · Ranjay Krishna (University of Washington) · Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) · Chris Callison-Burch (University of Pennsylvania) · Mark Yatskar (Department of Computer and Information Science, School of Engineering and Applied Science) · Aniruddha Kembhavi (Allen Institute for Artificial Intelligence) · Christopher Clark (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unleashing Network Potentials for Semantic Scene Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fengyun Wang (None) · Qianru Sun (None) · Dong Zhang (The Hong Kong University of Science and Technology) · Jinhui Tang (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/DeepMed-Lab-ECNU/Single-Image-Deblur" target="_blank">AdaRevD: Adaptive Patch Exiting Reversible Decoder Pushes the Limit of Image Deblurring</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xintian Mao (East China Normal University) · Xiwen Gao (East China Normal University) · Yan Wang (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Video Super-Resolution Transformer with Masked Inter&amp;Intra-Frame Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xingyu Zhou (University of Electronic Science and Technology of China) · Leheng Zhang (University of Electronic Science and Technology of China) · Xiaorui Zhao (None) · Keze Wang (SUN YAT-SEN UNIVERSITY) · Leida Li (Xidian University) · Shuhang Gu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://82magnolia.github.io/fgpl/" target="_blank">Fully Geometric Panoramic Localization</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junho Kim (Seoul National University) · Jiwon Jeong (Stanford University) · Young Min Kim (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://yunminjin2.github.io/projects/bitt/" target="_blank">BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Minje Kim (KAIST) · Tae-Kyun Kim (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Robust 3D Pose Transfer with Adversarial Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoyu Chen (University of Oulu) · Hao Tang (ETH Zurich and CMU) · Ehsan Adeli (Stanford University) · Guoying Zhao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Building Vision-Language Models on Solid Foundations with Masked Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sepehr Sameni (University of Bern) · Kushal Kafle (Adobe Systems) · Hao Tan (Adobe Systems) · Simon Jenni (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AV2AV: Direct Audio-Visual Speech to Audio-Visual Speech Translation with Unified Audio-Visual Speech Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jeongsoo Choi (Korea Advanced Institute of Science and Technology) · Se Jin Park (KAIST) · Minsu Kim (None) · Yong Man Ro (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CogAgent: A Visual Language Model for GUI Agents</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenyi Hong (Tsinghua University) · Weihan Wang (Tsinghua University, Tsinghua University) · Qingsong Lv (Tsinghua University, Tsinghua University) · Jiazheng Xu (, Tsinghua University) · Wenmeng Yu (None) · Junhui Ji (Zhipu.AI) · Yan Wang (Zhipu AI) · Zihan Wang (Tsinghua University, Tsinghua University) · Yuxiao Dong (Tsinghua University) · Ming Ding (ZHIPU AI) · Jie Tang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lihe Yang (The University of Hong Kong) · Bingyi Kang (TikTok) · Zilong Huang (Tencent GY Lab) · Xiaogang Xu (Zhejiang Lab) · Jiashi Feng (ByteDance) · Hengshuang Zhao (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            wenqiao Li (ShanghaiTech University) · Xiaohao Xu (University of Michigan - Ann Arbor) · Yao Gu (Shanghaitech University) · BoZhong Zheng (None) · Shenghua Gao (ShanghaiTech University) · Yingna Wu (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Discontinuity-preserving Normal Integration with Auxiliary Edges</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyomin Kim (POSTECH) · Yucheol Jung (POSTECH) · Seungyong Lee (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning to navigate efficiently and precisely in real environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guillaume Bono (Naver Labs Europe) · Hervé Poirier (Naver Labs Europe) · Leonid Antsfeld (Naver Labs Europe) · Gianluca Monaci (Naver Labs Europe) · Boris Chidlovskii (Naver Labs Europe) · Christian Wolf (Naver Labs Europe)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PAPR in Motion: Seamless Point-level 3D Scene Interpolation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shichong Peng (None) · Yanshu Zhang (None) · Ke Li (Simon Fraser University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Modern Image Manipulation Localization: A Large-Scale Dataset and Novel Methods</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenfan Qu (South China University of Technology) · Yiwu Zhong (University of Wisconsin, Madison) · Chongyu Liu (South China University of Technology) · Guitao Xu (South China University of Technology) · Dezhi Peng (South China University of Technology) · Fengjun Guo (Shanghai Jiaotong University) · Lianwen Jin (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dense Vision Transformer Compression with Few Samples</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanxiao Zhang (Nanjing University) · Yifan Zhou (nanjing university) · Guo-Hua Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Weakly Supervised Monocular 3D Detection with a Single-View Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xueying Jiang (Nanyang Technological University) · Sheng Jin (Nanyang Technological University) · Lewei Lu (SenseTime) · Xiaoqin Zhang (Wenzhou University) · Shijian Lu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AM-RADIO: Agglomerative Models - Reduce All Domains Into One</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mike Ranzinger (NVIDIA Research) · Greg Heinrich (NVIDIA) · Jan Kautz (NVIDIA) · Pavlo Molchanov (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Tune-An-Ellipse: CLIP Has Potential to Find What You Want</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinheng Xie (None) · Songhe Deng (None) · Bing Li (King Abdullah University of Science and Technology) · Haozhe Liu (King Abdullah University of Science and Technology) · Yawen Huang (None) · Yefeng Zheng (None) · Jürgen Schmidhuber (King Abdullah University of Science and Technology) · Bernard Ghanem (KAUST) · Linlin Shen (None) · Mike Zheng Shou (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LISA: Reasoning Segmentation via Large Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Lai (None) · Zhuotao Tian (The Chinese University of Hong Kong) · Yukang Chen (None) · Yanwei Li (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Yuhui Yuan (Microsoft Research Asia) · Shu Liu (The Chinese University of Hong Kong) · Jiaya Jia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fantastic Animals and Where to Find Them: Segment Any Marine Animal with Dual SAM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pingping Zhang (Dalian University of Technology) · Tianyu Yan (Dalian University of Technology) · Yang Liu (Dalian University of Technology) · Huchuan Lu (Dalian University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>IntrinsicAvatar: Physically Based Inverse Rendering of Dynamic Humans from Monocular Videos via Explicit Ray Tracing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shaofei Wang (None) · Bozidar Antic (Eberhard-Karls-Universität Tübingen) · Andreas Geiger (University of Tübingen) · Siyu Tang (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Exploring Pose-Aware Human-Object Interaction via Hybrid Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            EASTMAN Z Y WU (Tsinghua University) · Yali Li (Tsinghua University) · Yuan Wang (None) · Shengjin Wang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multi-modal learning for geospatial vegetation forecasting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vitus Benson (Max-Planck-Institute for Biogeochemistry) · Claire Robin (Max Planck Institute for Biogeochemistry) · Christian Requena-Mesa (Max-Planck Institute for Biogeochemistry) · LAZARO ALONSO SILVA (Max-Planck Institute) · Mélanie Weynants (Max Planck Institute for Biogeochemistry) · Nora Linscheid (Max Planck Institute for Biogeochemistry) · Jose Cortes (Max-Planck Institute) · Zhihan Gao (The Hong Kong University of Science and Technology) · Nuno Carvalhais (Max-Planck Institute) · Markus Reichstein (Max-Planck Institute)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>All in One Framework for Multimodal Re-identification in the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            He Li (Wuhan University) · Mang Ye (Wuhan University) · Ming Zhang (Guangzhou Urban Planning &amp; Design Survey Research Institute) · Bo Du (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Leveraging Cross-Modal Neighbor Representation for Improved CLIP Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chao Yi (Nanjing University) · Lu Ren (nanjing university) · De-Chuan Zhan (Nanjing University) · Han-Jia Ye (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Bilateral Adaptation for Human-Object Interaction Detection with Occlusion-Robustness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangzhi Wang (National University of Singapore) · Yangyang Guo (National University of Singapore) · Ziwei Xu (National University of Singapore) · Mohan Kankanhalli (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>From Feature to Gaze: A Generalizable Replacement of Linear Layer for Gaze Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiwei Bao (Beihang University) · Feng Lu (Beihang University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Michael Dorkenwald (University of Amsterdam) · Nimrod Barazani (University of Amsterdam) · Cees G. M. Snoek (University of Amsterdam) · Yuki Asano (University of Amsterdam)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MVIP-NeRF: Multi-view 3D Inpainting on NeRF Scenes via Diffusion Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Honghua Chen (National Technological University) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY) · Xingang Pan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Composed Video Retrieval via Enriched Context and Discriminative Embeddings</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Omkar Thawakar (MBZUAI) · Muzammal Naseer (MBZUAI) · Rao Anwer (Mohamed bin Zayed University of Artificial Intelligence) · Salman Khan (Mohamed bin Zayed University of Artificial Intelligence) · Michael Felsberg (Linköping University) · Mubarak Shah (University of Central Florida) · Fahad Shahbaz Khan (MBZUAI; Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TCP: Textual-based Class-aware Prompt tuning for Visual-Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hantao Yao (None) · Rui Zhang (None) · Changsheng Xu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RMT: Retentive Networks Meet Vision Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qihang Fan (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Huaibo Huang (Institute of Automation, Chinese Academy of Sciences) · Mingrui Chen (Institute of Automation, Chinese Academy of Sciences (CASIA)) · Hongmin Liu (University of Science and Technology Beijing) · Ran He (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Low-Rank Approximation for Sparse Attention in Multi-Modal LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lin Song (Tencent AI Lab) · Yukang Chen (None) · Shuai Yang (Hong Kong University of Science and Technology (Guangzhou)) · Xiaohan Ding (Tencent AI Lab) · Yixiao Ge (Tencent) · Ying-Cong Chen (The Hong Kong University of Science and Technology) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zicheng Zhang (None) · RUOBING ZHENG (None) · Bonan Li (None) · Congying Han (University of  Chinese Academy of Sciences) · Tianqi Li (Ant Group) · Meng Wang (Ant Group) · Tiande Guo (University of the Chinese Academy of Sciences) · Jingdong Chen (Ant Group) · Ziwen Liu (University of the Chinese Academy of Sciences) · Ming Yang (Ant Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PairDETR : Joint Detection and Association of Human Bodies and Faces</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ammar Ali (ITMO University) · Georgii Gaikov (MTS AI) · Denis Rybalchenko (VisionLabs) · Alexander Chigorin (VisionLabs MENA) · Ivan Laptev (INRIA Paris) · Sergey Zagoruyko (MTS AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Language Models as Black-Box Optimizers for Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shihong Liu (Carnegie Mellon University) · Samuel Yu (Carnegie Mellon University) · Zhiqiu Lin (Carnegie Mellon University) · Deepak Pathak (Carnegie Mellon University) · Deva Ramanan (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GES: Generalized Exponential Splatting for Efficient Radiance Field Rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Abdullah J Hamdi (University of Oxford) · Luke Melas-Kyriazi (VGG, University of Oxford) · Jinjie Mai (KAUST) · Guocheng Qian (KAUST) · Ruoshi Liu (Columbia University) · Carl Vondrick (Columbia University) · Bernard Ghanem (KAUST) · Andrea Vedaldi (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Using Human Feedback to Fine-tune Diffusion Models  without Any Reward Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kai Yang (Tsinghua University, Tsinghua University) · Jian Tao (Tsinghua University, Tsinghua University) · Jiafei Lyu (Tsinghua University, Tsinghua University) · Chunjiang Ge (Control science and technology, Tsinghua University, Tsinghua University) · Jiaxin Chen (Parametrix.ai) · Weihan Shen (Parametrix) · Xiaolong Zhu (Parametrix) · Xiu Li (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Steerers: A framework for rotation equivariant keypoint descriptors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Georg Bökman (Chalmers University of Technology) · Johan Edstedt (Computer Vision Laboratory, Linköping University) · Michael Felsberg (Linköping University) · Fredrik Kahl (Chalmers University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Think Twice Before Selection: Federated Evidential Active Learning for Medical Image Analysis with Domain Shifts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiayi Chen (Northwestern Polytechnical University) · Benteng Ma (Hong Kong University of Science and Technology) · Hengfei Cui (Northwest Polytechnical University Xi'an) · Kwang-Ting Cheng (Hong Kong University of Science and Technology) · Yong Xia (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>On the Faithfulness of Vision Transformer Explanations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junyi Wu (None) · Weitai Kang (None) · Hao Tang (ETH Zurich and CMU) · Yuan Hong (University of Connecticut) · Yan Yan (Illinois Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Wavelet-based Fourier Information Interaction with Frequency Diffusion Adjustment for Underwater Image Restoration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Zhao (None) · Weiling Cai (Nanjing Normal University) · Chenyu Dong (Nanjing Normal University) · Chengwei Hu (Nanjing Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Transferable Negative Prompts for Out-of-Distribution Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianqi Li (None) · Guansong Pang (Singapore Management University) · wenjun miao (None) · Xiao Bai (Beijing University of Aeronautics and Astronautics) · Jin Zheng (Beijing University of Aeronautics and Astronautics)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>3D Multi-frame Fusion for Video Stabilization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhan Peng (None) · Xinyi Ye (School of Artificial Intelligence and Automation, Huazhong University of Science and Technology) · Weiyue Zhao (Shenzhen Dajiang Innovation Technology Co., Ltd) · TIANQI LIU (None) · Huiqiang Sun (None) · Baopu Li (Baidu) · Zhiguo Cao ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Expandable Subspace Ensemble for Pre-Trained Model-Based Class-Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Da-Wei Zhou (Nanjing University) · Hai-Long Sun (Nanjing University) · Han-Jia Ye (Nanjing University) · De-Chuan Zhan (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fun with Flags: Robust Principal Directions via Flag Manifolds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tolga Birdal (Imperial College London) · Nathan Mankovich (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Beyond Image Super-Resolution for Image Recognition with Task-Driven Perceptual Loss</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaeha Kim (Seoul National University) · Junghun Oh (None) · Kyoung Mu Lee (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Boosting Image Quality Assessment through Efficient Transformer Adaptation with Local Feature Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kangmin Xu (Wuhan University) · Liang Liao (Nanyang Technological University) · Jing Xiao (Wuhan University) · Chaofeng Chen (Nanyang Technological University) · Haoning Wu (Nanyang Technological University) · Qiong Yan (SenseTime Research) · Weisi Lin (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>COLMAP-Free 3D Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Fu (University of California San Diego) · Sifei Liu (NVIDIA) · Amey Kulkarni (NVIDIA) · Jan Kautz (NVIDIA) · Alexei A. Efros (UC Berkeley) · Xiaolong Wang (UCSD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Realistic Scene Generation with LiDAR Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoxi Ran (Carnegie Mellon University) · Vitor Guizilini (Toyota Research Institute) · Yue Wang (Massachusetts Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Point-VOS: Pointing Up Video Object Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sabarinath Mahadevan (RWTH Aachen University) · Idil Esen Zulfikar (RWTH Aachen University) · Paul Voigtlaender (None) · Bastian Leibe (RWTH Aachen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Instruct 4D-to-4D: Editing 4D Scenes as Pseudo-3D Scenes Using 2D Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linzhan Mou (Zhejiang University) · Jun-Kun Chen (None) · Yu-Xiong Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Carve3D: Improving Multiview Reconstruction Consistency for Diffusion Models with RL Finetuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Desai Xie (Stony Brook University) · Jiahao Li (Toyota Technological Institute at Chicago) · Hao Tan (Adobe Systems) · Xin Sun (Adobe Systems) · Zhixin Shu (Adobe Systems) · Yi Zhou (Adobe Systems) · Sai Bi (Adobe Systems) · Soeren Pirk (Adobe) · ARIE KAUFMAN (Stony Brook University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/feifeiobama/OrthogonalDet" target="_blank">Exploring Orthogonality in Open World Object Detection</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhicheng Sun (Peking University) · Jinghan Li (Peking University) · Yadong Mu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://micv-yonsei.github.io/eagle2024/" target="_blank">EAGLE: Eigen Aggregation Learning for Object-Centric Unsupervised Semantic Segmentation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chanyoung Kim (Yonsei University) · Woojung Han (Yonsei University) · Dayun Ju (Yonsei University) · Seong Jae Hwang (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Compositional Chain-of-Thought Prompting for Large Multimodal Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chancharik Mitra (University of California, Berkeley) · Brandon Huang (University of California, Berkeley) · Trevor Darrell (Electrical Engineering &amp; Computer Science Department) · Roei Herzig (Tel Aviv University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>As-Plausible-As-Possible: Plausibility-Aware Mesh Deformation Using 2D Diffusion Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seungwoo Yoo (Korea Advanced Institute of Science and Technology (KAIST)) · Kunho Kim (Korea Advanced Institute of Science &amp; Technology) · Vladimir G. Kim (Adobe Systems) · Minhyuk Sung (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unifying Automatic and Interactive Matting with Pretrained ViTs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixuan Ye (None) · Wenze Liu (Huazhong University of Science and Technology) · He Guo () · Yujia Liang (Huazhong University of Science and Technology) · Chaoyi Hong (Huazhong University of Science and Technology) · Hao Lu (Huazhong University of Science and Technology) · Zhiguo Cao ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking Interactive Image Segmentation with Low Latency, High Quality, and Diverse Prompts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qin Liu (Department of Computer Science, University of North Carolina, Chapel Hill) · Jaemin Cho (UNC Chapel Hill) · Mohit Bansal (University of North Carolina at Chapel Hill) · Marc Niethammer (The University of North Carolina at Chapel Hill)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NViST: In the Wild New View Synthesis from a Single Image with Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wonbong Jang (University College London) · Lourdes Agapito (University College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Authentic Hand Avatar from a Phone Scan via Universal Hand Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gyeongsik Moon (None) · Weipeng Xu (Meta Reality Labs Research) · Rohan Joshi (Facebook) · Chenglei Wu (Meta) · Takaaki Shiratori (Meta Reality Labs Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Visual Fact Checker: Enabling High Fidelity Detailed Caption Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhao Ge (University of Southern California) · Xiaohui Zeng (Department of Computer Science, University of Toronto) · Jacob Huffman (NVIDIA) · Tsung-Yi Lin (NVIDIA) · Ming-Yu Liu (NVIDIA) · Yin Cui (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Latency Correction for Event-guided Deblurring and Frame Interpolation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixin Yang (Peking University) · Jinxiu Liang (None) · Bohan Yu (None) · Yan Chen (Tsinghua University, Tsinghua University) · Jimmy S. Ren (SenseTime Research) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ZERO-IG: Zero-Shot Illumination-Guided Joint Denoising and Adaptive Enhancement for Low-Light Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiqi Shi (Harbin Engineering University) · Duo Liu (Harbin Engineering University) · Liguo Zhang (Harbin Engineering University) · Ye Tian (Xidian University) · Xuezhi Xia (Harbin Engineering University) · fuxiaojing (Harbin Engineering University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HINTED: Hard Instance Enhanced Detector with Mixed-Density Feature Fusion for Sparsely-Supervised 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiming Xia (XMU) · Wei Ye (Xiamen University) · Hai Wu (Xiamen University) · Shijia Zhao (Xiamen University) · Leyuan Xing (Xiamen University) · Xun Huang (Xiamen University) · Jinhao Deng (Xiamen University) · Xin Li (None) · Chenglu Wen (Xiamen University) · Cheng Wang (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Domain-Specific Block Selection and Paired-View Pseudo-Labeling for Online Test-Time Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yeonguk Yu (Gwangju Institute of Science and Technology) · Sungho Shin (None) · Seunghyeok Back (Gwangju Institute of Science and Technology) · Minhwan Ko (Gwangju Institute of Science and Technology) · Sangjun Noh (Gwangju Institute of Science and Technology) · Kyoobin Lee (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving Visual Recognition with Hyperbolical Visual Hierarchy Mapping</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyeongjun Kwon (None) · Jinhyun Jang (Yonsei University) · Jin Kim (Yonsei University, Seoul, South Korea) · Kwonyoung Kim (None) · Kwanghoon Sohn (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Self-supervised Representation Learning from Arbitrary Scenarios</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaowen Li (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Yousong Zhu (Institute of Automation, Chinese Academy of Sciences) · Zhiyang Chen (Institute of automation, Chinese academy of science) · Zongxin Gao (Beijing Institute Of Graphic Communication) · Rui Zhao (Qing Yuan Research Institute, Shanghai Jiao Tong University) · Chaoyang Zhao (, Institute of automation, Chinese academy of science) · Ming Tang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Jinqiao Wang (Institute of Automation, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NEAT: Distilling 3D Wireframes from Neural Attraction Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nan Xue (Ant Group) · Bin Tan (Wuhan University) · Yuxi Xiao (Zhejiang University) · Liang Dong (Google) · Gui-Song Xia (Wuhan University) · Tianfu Wu () · Yujun Shen (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuai Tan (Shanghai Jiaotong University) · Bin Ji (Shanghai Jiaotong University) · Ye Pan (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generating Content for HDR Deghosting from Frequency View</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tao Hu (Northwestern Polytechnical University) · Qingsen Yan (Northwest Polytechnical University Xi'an) · Yuankai Qi (The University of Adelaide) · Yanning Zhang (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinpeng Ding (The Hong Kong University of Science and Technology) · Jianhua Han (Huawei Technologies Ltd.) · Hang Xu (Huawei Noah‘s Ark Lab) · Xiaodan Liang (Sun Yat-sen University) · Wei Zhang (Huawei Technologies Ltd.) · Xiaomeng Li (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>End-to-End Spatio-Temporal Action Localisation with Video Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alexey Gritsenko (Google) · Xuehan Xiong (Google) · Josip Djolonga (Google) · Mostafa Dehghani (Google DeepMind) · Chen Sun (Brown University) · Mario Lučić (Google) · Cordelia Schmid (Inria / Google) · Anurag Arnab (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Querying as Prompt: Parameter-Efficient Learning for Multimodal Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tian Liang (None) · Jing Huang (Zhejiang University) · Ming Kong (None) · Luyuan Chen (Beijing Information Science and Technology University) · Qiang Zhu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Systematic comparison of semi-supervised and self-supervised learning for medical image classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhe Huang (Tufts University) · Ruijie Jiang (Tufts University) · Shuchin Aeron (Tufts University) · Michael C. Hughes (Tufts University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning Background Prompts to Discover Implicit Knowledge for Open Vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaming Li (Baidu) · Jiacheng Zhang (SUN YAT-SEN UNIVERSITY) · Jichang Li (The University of Hong Kong) · Ge Li (Peking University Shenzhen Graduate School) · Si Liu (Beihang University) · Liang Lin (Sun Yat-sen University) · Guanbin Li (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dual Prototype Attention for Unsupervised Video Object Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Suhwan Cho (Yonsei University) · Minhyeok Lee (Yonsei University) · Seunghoon Lee (Yonsei University) · Dogyoon Lee (Yonsei University) · Heeseung Choi (None) · Ig-Jae Kim (Korea Institute of Science and Technology) · Sangyoun Lee (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GeoChat: Grounded Large Vision-Language Model for Remote Sensing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kartik Kuckreja (BITS Pilani, Birla Institute of Technology and Science) · Muhammad Sohail Danish (Mohamed bin Zayed University of Artificial Intelligence) · Muzammal Naseer (MBZUAI) · Abhijit Das (BITS Pilani, Birla Institute of Technology and Science) · Salman Khan (Mohamed bin Zayed University of Artificial Intelligence) · Fahad Shahbaz Khan (MBZUAI; Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text Prompt with Normality Guidance for Weakly Supervised Video Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiwei Yang (Guangzhou Institute of Technology, Xidian University) · Jing Liu (Guangzhou Institute of Technology, Xidian University) · Peng Wu (Northwest Polytechnical University Xi'an)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>AirPlanes: Accurate Plane Estimation via 3D-Consistent Embeddings</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jamie Watson (Niantic) · Filippo Aleotti (Niantic, Inc.) · Mohamed Sayed (University College London, University of London) · Zawar Qureshi (None) · Oisin Mac Aodha (University of Edinburgh) · Gabriel J. Brostow (Department of Computer Science, University College London) · Michael Firman (Niantic, Inc.) · Sara Vicente (Niantic)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Prompt Learning via Meta-Regularization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinyoung Park (Korea University) · Juyeon Ko (Korea University) · Hyunwoo J. Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GARField: Group Anything with Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chung Min Kim (University of California, Berkeley) · Mingxuan Wu (None) · Justin Kerr (University of California Berkeley) · Ken Goldberg (University of California Berkeley) · Matthew Tancik (Luma AI) · Angjoo Kanazawa (UC Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Addressing Background Context Bias in Few-Shot Segmentation through Iterative Modulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lanyun Zhu (Singapore University of Technology and Design) · Tianrun Chen (Zhejiang University) · Jianxiong Yin (NVIDIA) · Simon See (NVIDIA) · Jun Liu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking the Region Classification in Open-Vocabulary Semantic Segmentation: An Image-to-Image View</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Wang (University of Science and Technology of China) · Rui Sun (University of Science and Technology of China) · Naisong Luo (University of Science and Technology of China) · Yuwen Pan (University of Science and Technology of China) · Tianzhu Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Local-consistent Transformation Learning for Rotation-invariant Point Cloud Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiyang Chen (South China University of Technology) · Lunhao Duan (Wuhan University) · Shanshan Zhao (JD Explore Academy) · Changxing Ding (South China University of Technology) · Dacheng Tao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>KITRO: Refining Human Mesh by 2D Clues and Kinematic-tree Rotation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fengyuan Yang (National University of Singapore) · Kerui Gu (National University of Singapore) · Angela Yao (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SuperNormal: Neural Surface Reconstruction via Multi-View Normal Integration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xu Cao (Osaka University) · Takafumi Taketomi (CyberAgent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Navigating Beyond Dropout: An Intriguing Solution towards Generalizable Image Super-Resolution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongjun Wang (None) · Jiyuan Chen (Hong Kong Polytechnic University) · Yinqiang Zheng (None) · Tieyong Zeng (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards the Uncharted: Density-Descending Feature Perturbation for Semi-supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyang Wang () · Huihui Bai (Beijing jiaotong university) · Limin Yu (Xi'an Jiaotong-Liverpool University) · Yao Zhao (Beijing Jiaotong University) · Jimin Xiao (Xi'an Jiaotong-Liverpool University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>General Object Foundation Model for Images and Videos at Scale</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junfeng Wu (Huazhong University of Science and Technology) · Yi Jiang (bytedance) · Qihao Liu (Johns Hopkins University) · Zehuan Yuan (Nanjing University) · Xiang Bai (Huazhong University of Science and Technology) · Song Bai (ByteDance)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Friendly Sharpness-Aware Minimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tao Li (Shanghai Jiao Tong University) · Pan Zhou (Sea Group) · Zhengbao He (Department of Automation, Shanghai Jiao Tong University) · Xinwen Cheng (Shanghai Jiaotong University) · Xiaolin Huang (Shanghai Jiao Tong University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Auto-Train-Once: Controller Network Guided Automatic Network Pruning from Scratch</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xidong Wu (University of Pittsburgh) · Shangqian Gao (University of Pittsburgh) · Zeyu Zhang (Amazon AGI) · Zhenzhen Li (Bosch) · Runxue Bao (GE Healthcare) · Yanfu Zhang (College of William and Mary) · Xiaoqian Wang (Purdue University) · Heng Huang (University of Pittsburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SCINeRF: Neural Radiance Fields from a Snapshot Compressive Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhao Li (Zhejiang University) · Xiaodong Wang (Zhejiang University) · Ping Wang (Zhejiang University) · Xin Yuan (Westlake University) · Peidong Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianping Jiang (Peking University) · xinyu zhou (Peking University) · Bingxuan Wang (Peking University) · Xiaoming Deng (Institute of Software Chinese Academy of Sciences) · Chao Xu (Peking University) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://amuse.is.tue.mpg.de/" target="_blank">Emotional Speech-Driven 3D Body Animation via Disentangled Latent Diffusion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kiran Chhatre (KTH Royal Institute of Technology) · Radek Danecek (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Nikos Athanasiou (None) · Giorgio Becherini (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Christopher Peters (KTH Royal Institute of Technology) · Michael J. Black (University of Tübingen) · Timo Bolkart (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Deciphering ‘What’ and ‘Where’ Visual Pathways from Spectral Clustering of Layer-Distributed Neural Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao Zhang (University of Chicago) · David Yunis (Toyota Technological Institute at Chicago) · Michael Maire (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Distribution-aware Knowledge Prototyping for Non-exemplar Lifelong Person Re-identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kunlun Xu (Peking University) · Xu Zou (Huazhong University of Science and Technology) · Yuxin Peng (Peking University) · Jiahuan Zhou (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>KTPFormer: Kinematics and Trajectory Prior Knowledge-Enhanced Transformer for 3D Human Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jihua Peng (The Hong Kong Polytechnic University) · Yanghong Zhou (The Hong Kong Polytechnic University) · Tracy Mok (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zaid Khan (Northeastern University) · Yun Fu (Northeastern University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Optimal Transport Aggregation for Visual Place Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sergio Izquierdo (I3A - University of Zaragoza) · Javier Civera (I3A, Universidad de Zaragoza)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HUGS: Holistic Urban 3D Scene Understanding via Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongyu Zhou (Zhejiang University) · Jiahao Shao (Zhejiang University) · Lu Xu (Zhejiang University) · Dongfeng Bai (Huawei Technologies Ltd.) · Weichao Qiu (Huawei Technologies Ltd.) · Bingbing Liu (Huawei Technologies Ltd.) · Yue Wang (Zhejiang University) · Andreas Geiger (University of Tübingen) · Yiyi Liao (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Human Motion Prediction under Unexpected Perturbation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiangbei Yue (University of Leeds) · Baiyi Li (University of Leeds) · Julien Pettré (INRIA) · Armin Seyfried (Forschungszentrum Jülich) · He Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LLM-AR: When Large Language Model Meets Skeleton-Based Action Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoxuan Qu (Singapore University of Technology and Design) · Yujun Cai (Meta) · Jun Liu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MFP: Making Full use of Probability Maps for Interactive Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaewon Lee (None) · Seon-Ho Lee (None) · Chang-Su Kim (Korea University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiG-IN: Diffusion Guidance for Investigating Networks - Uncovering Classifier Differences, Neuron Visualisations, and Visual Counterfactual Explanations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Maximilian Augustin (University of Tuebingen) · Yannic Neuhaus (Eberhard-Karls-Universität Tübingen) · Matthias Hein (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Instantaneous Perception of Moving Objects in 3D</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Di Liu (Rutgers University, New Brunswick) · Bingbing Zhuang (NEC Labs America) · Dimitris N. Metaxas (Rutgers) · Manmohan Chandraker (UC San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Cross-Domain Few-Shot Segmentation via Iterative Support-Query Correspondence Mining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahao Nie (Nanyang Technological University) · Yun Xing (Nanyang Technological University) · Gongjie Zhang (Black Sesame Tech.) · Pei Yan (Huazhong University of Science and Technology) · Aoran Xiao (Nanyang Technological University) · Yap-peng Tan (Nanyang Technological University) · Alex C. Kot (Nanyang Technological University) · Shijian Lu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Backpropagation-free Network for 3D Test-time Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            YANSHUO WANG (CSIRO) · Ali Cheraghian (CSIRO) · Zeeshan Hayder (CSIRO) · JIE HONG (Australian National University) · Sameera Ramasinghe (Amazon) · Shafin Rahman (North South University) · David Ahmedt-Aristizabal (CSIRO) · Xuesong Li (Australian National University) · Lars Petersson (CSIRO) · Mehrtash Harandi (Monash University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NeRFiller: Completing Scenes via Generative 3D Inpainting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ethan Weber (University of California Berkeley) · Aleksander Holynski (UC Berkeley &amp; Google Research) · Varun Jampani (Google Research) · Saurabh Saxena (None) · Noah Snavely (Google / Cornell) · Abhishek Kar (Google) · Angjoo Kanazawa (UC Berkeley)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Strong Transferable Adversarial Attacks via Ensembled Asymptotically Normal Distribution Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhengwei Fang (Beijing Jiaotong University) · Rui Wang (Beijing Jiaotong University) · Tao Huang (Beijing Jiaotong University) · Liping Jing (Beijing Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Diffuse, Attend, and Segment: Unsupervised Zero-Shot Segmentation using Stable Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junjiao Tian (Georgia Institute of Technology) · Lavisha Aggarwal (Amazon) · Andrea Colaco (Google) · Zsolt Kira (Georgia Institute of Technology) · Mar Gonzalez-Franco (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Customize your NeRF: Adaptive Source Driven 3D Scene Editing via Local-Global Iterative Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Runze He (Institute of Information Engineering, Chinese Academy of Sciences) · Shaofei Huang (Institute of Information Engineering, Chinese Academy of Sciences) · Xuecheng Nie (national university of singaore, National University of Singapore) · Tianrui Hui (Hefei University of Technology) · Luoqi Liu (None) · Jiao Dai (Institute of Information Engineering,Chinese Academy of Sciences) · Jizhong Han (Institute of Information Engineering) · Guanbin Li (Sun Yat-sen University) · Si Liu (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yizhi Wang (Simon Fraser University) · Wallace Lira (Simon Fraser University) · Wenqi Wang (Computing Science, Simon Fraser University) · Ali Mahdavi Amiri (Simon Fraser University) · Hao Zhang (Simon Fraser University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learning to Produce Semi-dense Correspondences for Visual Localization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Khang Truong Giang (Korea Advanced Institute of Science and Technology) · Soohwan Song (Dongguk University) · Sungho Jo (Korea Advanced Institute of Science &amp; Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Differentiable Neural Surface Refinement for Transparent Objects</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weijian Deng (The Australian National University) · Dylan Campbell (Australian National University) · Chunyi Sun (Australian National University) · Shubham Kanitkar (RIOS Intelligent Machines) · Matthew Shaffer (RIOS Intelligent Machines) · Stephen Gould (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EfficientSAM: Leveraged Masked Image Pretraining for Efficient Segment Anything</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunyang Xiong (Facebook) · Balakrishnan Varadarajan (Meta) · Lemeng Wu (University of Texas, Austin) · Xiaoyu Xiang (Meta) · Fanyi Xiao (Meta) · Chenchen Zhu (Meta AI) · Xiaoliang Dai (Facebook) · Dilin Wang (Facebook) · Fei Sun (Meta Inc.) · Forrest Iandola (Meta) · Raghuraman Krishnamoorthi (Facebook) · Vikas Chandra (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Look-Up Table Compression for Efficient Image Restoration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yinglong Li (University of Science and Technology of China) · Jiacheng Li (None) · Zhiwei Xiong (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenhao Li (Peking University) · Mengyuan Liu (SUN YAT-SEN UNIVERSITY) · Hong Liu (Peking University) · Pichao Wang (Amazon) · Jialun Cai (peking university) · Nicu Sebe (University of Trento)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RepAn: Enhanced Annealing through Re-parameterization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiang Fei (School of Informatics, Xiamen University) · Xiawu Zheng (Xiamen University) · Yan Wang (Samsara) · Fei Chao (Xiamen University) · Chenglin Wu (DeepWisdom) · Liujuan Cao (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Mitigating Object Dependencies: Improving Point Cloud Self-Supervised Learning through Object Exchange</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanhao Wu (Xi'an Jiaotong University) · Tong Zhang (EPFL) · Wei Ke (Xi'an Jiaotong University) · Congpei Qiu (Xi'an Jiaotong University) · Sabine Süsstrunk (None) · Mathieu Salzmann (EPFL)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Vector Graphics Generation via Mutually Impulsed Dual-domain Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhongyin Zhao (Shanghai Jiaotong University) · Ye Chen (Shanghai Jiao Tong University) · Zhangli Hu (Shanghai Jiaotong University) · Xuanhong Chen (Shanghai Jiao Tong University) · Bingbing Ni (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FocSAM: Delving Deeply into Focused Objects in Segmenting Anything</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            You Huang (Xiamen University) · Zongyu Lan (Xiamen University) · Liujuan Cao (Xiamen University) · Xianming Lin () · Shengchuan Zhang (None) · Guannan Jiang (Contemporary Amperex Technology Co., Limited) · Rongrong Ji (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanwu Xu (Boston University, Boston University) · Yang Zhao (Google) · Zhisheng Xiao (Google) · Tingbo Hou (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Understanding and Improving Source-free Domain Adaptation from a Theoretical Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu Mitsuzumi (None) · Akisato Kimura (NTT Corporation) · Hisashi Kashima (Kyoto University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ODM: A Text-Image Further Alignment Pre-training Approach for Scene Text Detection and Spotting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Duan (Meituan) · Pei Fu (meituan) · Shan Guo (Meituan) · Qianyi Jiang (meituan) · Xiaoming Wei (Meituan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CLIP-BEVFormer: Enhancing Multi-View Image-Based BEV Detector with Ground Truth Flow</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenbin Pan (Syracuse University) · Burhaneddin Yaman (Bosch Center for Artificial Intelligence) · Senem Velipasalar (Syracuse University) · Liu Ren (Bosch Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hyperspherical Classification with Dynamic Label-to-Prototype Assignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mohammad Saadabadi Saadabadi (None) · Ali Dabouei (None) · Sahar Rahimi Malakshan (West Virginia University) · Nasser Nasrabadi (West Virginia University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Toward Generalist Anomaly Detection via In-context Residual Learning with Few-shot Sample Prompts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiawen Zhu (Singapore Management University) · Guansong Pang (Singapore Management University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Open-Vocabulary Spatio-Temporal Video Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Syed Talal Wasim (None) · Muzammal Naseer (MBZUAI) · Salman Khan (Mohamed bin Zayed University of Artificial Intelligence) · Ming-Hsuan Yang (University of California at Merced) · Fahad Shahbaz Khan (MBZUAI; Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ODIN: A Single Model for 2D and 3D Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ayush Jain (Carnegie Mellon University) · Pushkal Katara (Carnegie Mellon University) · Nikolaos Gkanatsios (Carnegie Mellon University) · Adam Harley (Ryerson University) · Gabriel Sarch (Carnegie Mellon University) · Kriti Aggarwal (Microsoft) · Vishrav Chaudhary (Microsoft) · Katerina Fragkiadaki (CMU)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Prompt Augmentation for Self-supervised Text-guided Image Manipulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rumeysa Bodur (Imperial College London) · Binod Bhattarai (University of Aberdeen) · Tae-Kyun Kim (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Open-Vocabulary Segmentation with Semantic-Assisted Calibration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yong Liu (None) · Sule Bai (Tsinghua University, Tsinghua University) · Guanbin Li (Sun Yat-sen University) · Yitong Wang (ByteDance Inc) · Yansong Tang (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FinePOSE: Fine-Grained Prompt-Driven 3D Human Pose Estimation via Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jinglin Xu (University of Science and Technology Beijing) · Yijie Guo (Peking University) · Yuxin Peng (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MaskClustering:  View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mi Yan (Peking University) · Jiazhao Zhang (None) · Yan Zhu (Peking University) · He Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MemoNav: Working Memory Model for Visual Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongxin Li (Institute of Automation, Chinese Academy of Sciences) · Zeyu Wang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xu Yang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · yuran Yang (Tencent) · Shuqi Mei (Tencent T-Lab) · Zhaoxiang Zhang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PointBeV: A Sparse Approach for BeV Predictions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Loick Chambon (Valeo) · Éloi Zablocki (Valeo) · Mickaël Chen (Valeo) · Florent Bartoccioni (Valeo) · Patrick Pérez (None) · Matthieu Cord (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Ensemble Diversity Facilitates Adversarial Transferability</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bowen Tang (University of Electronic Science and Technology of China) · Zheng Wang (University of Electronic Science and Technology of China) · Yi Bin (National University of Singapore) · Qi Dou (The Chinese University of Hong Kong) · Yang Yang (University of Electronic Science and Technology of China) · Heng Tao Shen (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>POCE: Primal Policy Optimization with Conservative Estimation for Multi-constraint Offline Reinforcement Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiayi Guan (Tongji University) · Li Shen (JD Explore Academy) · Ao Zhou (Tongji University) · Lusong Li (JDT) · Han Hu (Beijing Institute of Technology) · Xiaodong He (JD AI Research) · Guang Chen (Tongji University) · Changjun Jiang (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/hoqolo/SDSTrack" target="_blank">SDSTrack: Self-Distillation Symmetric Adapter Learning for Multi-Modal Visual Object Tracking</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaojun Hou (Zhejiang University) · Jiazheng Xing (Zhejiang University) · Yijie Qian (Zhejiang University) · Yaowei Guo (Zhejiang University) · Shuo Xin (Zhejiang University of Technology) · Junhao Chen (Zhejiang University) · Kai Tang (Zhejiang University) · Mengmeng Wang (Zhejiang University) · Zhengkai Jiang (Tencent) · Liang Liu (Tencent Youtu Lab) · Yong Liu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Implicit Discriminative Knowledge Learning for Visible-Infrared Person Re-Identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            kaijie ren (Chongqing University) · Lei Zhang (Chongqing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>On the Content Bias in Frechet Video Distance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Songwei Ge (University of Maryland, College Park) · Aniruddha Mahapatra (CMU, Carnegie Mellon University) · Gaurav Parmar (Carnegie Mellon University) · Jun-Yan Zhu (Carnegie Mellon University) · Jia-Bin Huang (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Sheared Backpropagation for Finetuning Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyuan Yu (None) · Li Shen (JD Explore Academy) · Liang Ding (Zhejiang University) · Xinmei Tian (University of Science and Technology of China) · Yixin Chen (Washington University, Saint Louis) · Dacheng Tao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hyperbolic Learning with Synthetic Captions for Open-World Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fanjie Kong (Duke University) · Yanbei Chen (Amazon) · Jiarui Cai (Amazon) · Davide Modolo (Amazon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NeRF-HuGS: Improved Neural Radiance Fields in Non-static Scenes Using Heuristics-Guided Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahao Chen (SUN YAT-SEN UNIVERSITY) · Yipeng Qin (Cardiff University) · Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) · Jiangbo Lu (SmartMore Corporation) · Guanbin Li (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>In-N-Out: Faithful 3D GAN Inversion with Volumetric Decomposition for Face Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiran Xu (University of Maryland, College Park) · Zhixin Shu (Adobe Systems) · Cameron Smith (Adobe Systems) · Seoung Wug Oh (Adobe Systems) · Jia-Bin Huang (University of Maryland, College Park)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards Language-Driven Video Inpainting via Multimodal Large Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianzong Wu (Peking University) · Xiangtai Li (Nanyang Technological University) · Chenyang Si (Nanyang Technological University  Singapore) · Shangchen Zhou (Nanyang Technological University) · Jingkang Yang (Nanyang Technological University) · Jiangning Zhang (Tencent Youtu Lab) · Yining Li (Shanghai AI Laboratory) · Kai Chen (Shanghai AI Laboratory) · Yunhai Tong (Peking University) · Ziwei Liu (Nanyang Technological University) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>From Activation to Initialization: Scaling Insights for Optimizing Neural Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hemanth Saratchandran (University of Adelaide/Australian Institute of Machine Learning) · Sameera Ramasinghe (Amazon) · Simon Lucey (University of Adelaide)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multimodal Aerial Visual RECognition (MAVREC) Dataset: Can Multi-view Improve Aerial Visual Perception?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aritra Dutta (University of Central Florida) · Srijan Das (University of North Carolina at Charlotte) · Jacob Nielsen (University of Southern Denmark - SDU) · RAJATSUBHRA CHAKRABORTY (University of North Carolina at Charlotte) · Mubarak Shah (University of Central Florida)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>High Fidelity Person-centric Subject-to-Image Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yibin Wang (None) · Weizhong Zhang (Fudan University) · Jianwei Zheng (Zhejiang University of Technology) · Cheng Jin (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fixed Point Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Luke Melas-Kyriazi (VGG, University of Oxford) · Xingjian Bai (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Contextual Augmented Global Contrast for Multimodal Intent Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kaili Sun () · Zhiwen Xie (Central China Normal University) · Mang Ye (Wuhan University) · Huyin Zhang (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunfei Fan (PICO, ByteDance) · Tianyu Zhao (Bytedance) · Guidong Wang (PICO)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/Shilin-LU/MACE" target="_blank">MACE: Mass Concept Erasure in Diffusion Models</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shilin Lu (Nanyang Technological University) · Zilan Wang (Nanyang Technological University) · Leyang Li (Nanyang Technological University) · Yanzhu Liu (I2R, A*STAR) · Adams Wai-Kin Kong (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>XFeat: Accelerated Features for Lightweight Image Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guilherme Potje (Federal University of Minas Gerais) · Felipe Cadar (Universidade Federal de Minas Gerais, Universidade Federal de Minas Gerais) · André Araujo (Google Research) · Renato Martins (Université de Bourgogne) · Erickson R. Nascimento (Universidade Federal de Minas Gerais / Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>GoodSAM: Bridging Domain and Capacity Gaps via Segment Anything Model for Distortion-aware Panoramic Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            WEIMING ZHANG () · Yexin Liu (The Hong Kong University of Science and Technology) · Xu Zheng (HKUST) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VRP-SAM: SAM with Visual Reference Prompt</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanpeng Sun (Nanjing University of Science and Technology) · Jiahui Chen (Beihang University) · Shan Zhang (Australian National University) · Xinyu Zhang (None) · Qiang Chen (Baidu) · gang zhang (Baidu Inc.) · Errui Ding (Baidu Inc.) · Jingdong Wang (Baidu) · Zechao Li (Nanjing University of Science and Techonolgy)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MA-LMM: Memory-Augmented Large Multimodal Model for Long-Term Video Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bo He (None) · Hengduo Li (Meta AI) · Young Kyun Jang (Meta AI) · Menglin Jia (Facebook) · Xuefei Cao (Meta) · Ashish Shah (Meta) · Abhinav Shrivastava (University of Maryland) · Ser-Nam Lim (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VideoBooth: Diffusion-based Video Generation with Image Prompts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuming Jiang (Nanyang Technological University) · Tianxing Wu (Nanyang Technological University) · Shuai Yang (Nanyang Technological University) · Chenyang Si (Nanyang Technological University  Singapore) · Dahua Lin (The Chinese University of Hong Kong) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz continuity constrAIned Normalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yao Ni (Australian National University) · Piotr Koniusz (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Day-Night Cross-domain Vehicle Re-identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongchao Li (Anhui Normal University) · Jingong Chen (Anhui Normal University) · AIHUA ZHENG (Anhui University) · Yong Wu (Anhui Normal University) · YongLong Luo (Anhui Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffuScene: Denoising Diffusion Models for Generative Indoor Scene Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiapeng Tang (Technische Universität München) · Yinyu Nie (Huawei Technologies Ltd.) · Lev Markhasin (None) · Angela Dai () · Justus Thies (Max-Planck Institute for Intelligent Systems) · Matthias Nießner (Technical University of Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SaCo Loss: Sample-wise Affinity Consistency for Vision-Language Pre-training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            WU Sitong (Department of Computer Science and Engineering, The Chinese University of Hong Kong) · Haoru Tan (HKU) · Zhuotao Tian (The Chinese University of Hong Kong) · Yukang Chen (None) · Xiaojuan Qi (University of Oxford) · Jiaya Jia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>StrokeFaceNeRF: Stroke-based Facial Appearance Editing in Neural Radiance Field</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao-juan Li (University of the Chinese Academy of Sciences) · Dingxi Zhang (University of Chinese Academy of Science) · Shu-Yu Chen (Chinese Academy of Sciences) · Feng-Lin Liu (Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Structure-Guided Adversarial Training of Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ling Yang (Peking University) · Haotian Qian (Peking University) · Zhilong Zhang (Peking University) · Jingwei Liu (Peking University) · Bin CUI (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Pavan Kumar Anasosalu Vasu (Apple) · Hadi Pouransari (Apple) · Fartash Faghri (None) · Raviteja Vemulapalli (None) · Oncel Tuzel (Apple)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Modes: Self-supervised Learning of Nonlinear Modal Subspaces</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiahong Wang (None) · Yinwei DU (Department of Computer Science, ETHZ - ETH Zurich) · Stelian Coros (ETHZ - ETH Zurich) · Bernhard Thomaszewski (Swiss Federal Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Soyong Shin (None) · Juyong Kim (Carnegie Mellon University) · Eni Halilaj (Carnegie Mellon University) · Michael J. Black (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shengbang Tong (New York University / Meta AI) · Zhuang Liu (FAIR, Meta AI) · Yuexiang Zhai (University of California Berkeley) · Yi Ma (UC Berkeley) · Yann LeCun (Facebook) · Saining Xie (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>YOLO-World: Real-Time Open-Vocabulary Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tianheng Cheng (Huazhong University of Science and Technology) · Lin Song (Tencent AI Lab) · Yixiao Ge (Tencent) · Wenyu Liu (Huazhong University of Science and Technology) · Xinggang Wang (Huazhong University of Science and Technology) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Jointly Training and Pruning CNNs via Learnable Agent Guidance and Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alireza Ganjdanesh (University of Maryland, College Park) · Shangqian Gao (University of Pittsburgh) · Heng Huang (University of Pittsburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Direct2.5: Diverse Text-to-3D Generation via Multi-view 2.5D Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanxun Lu (Nanjing University) · Jingyang Zhang (Apple) · Shiwei Li (Apple) · Tian Fang (Hong Kong University of Science and Technology) · David McKinnon (Apple) · Yanghai Tsin (Apple) · Long Quan (The Hong Kong University of Science and Technology) · Xun Cao (Nanjing University) · Yao Yao (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Bézier Everywhere All at Once: Learning Drivable Lanes as Bézier Graphs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hugh Blayney (dRISK.ai) · Hanlin Tian (Imperial College London) · Hamish Scott (dRISK AI) · Nils Goldbeck (dRISK) · Chess Stetson (dRISK) · Panagiotis Angeloudis (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FedUV: Uniformity and Variance for Heterogeneous Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ha Min Son (University of California, Davis) · Moon-Hyun Kim (Sungkyunkwan University) · Tai-Myoung Chung (Sung Kyun Kwan University) · Chao Huang (University of California, Davis) · Xin Liu (University of California, Davis)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table Blendshapes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziqian Bai (Simon Fraser University &amp; Google) · Feitong Tan (Google) · Sean Fanello (Google) · Rohit Pandey (Google) · Mingsong Dou (Google) · Shichen Liu (Google) · Ping Tan (Hong Kong University of Science and Technology) · Yinda Zhang (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Explaining the Implicit Neural Canvas: Connecting Pixels to Neurons by Tracing their Contributions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Namitha Padmanabhan (University of Maryland) · Matthew A Gwilliam (University of Maryland, College Park) · Pulkit Kumar (None) · Shishira R Maiya (University of Maryland) · Max Ehrlich (University of Maryland, College Park) · Abhinav Shrivastava (University of Maryland)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FaceCom: Towards High-fidelity 3D Facial Shape Completion via Optimization and Inpainting Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yinglong Li (Beihang University) · Hongyu Wu (None) · Wang (None) · Qingzhao Qin (Peking University) · yijiao zhao (None) · Yong Wang (None) · Aimin Hao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RankMatch: Exploring the Better Consistency Regularization for Semi-supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huayu Mai (University of Science and Technology of China) · Rui Sun (University of Science and Technology of China) · Tianzhu Zhang (University of Science and Technology of China) · Feng Wu (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/NISPLab/AT-BSL" target="_blank">Revisiting Adversarial Training under Long-Tailed Distributions</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinli Yue (Wuhan University) · Ningping Mou (Wuhan University) · Qian Wang (Wuhan University) · Lingchen Zhao (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>From SAM to CAMs: Exploring Segment Anything Model for Weakly Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hyeokjun Kweon (KAIST) · Kuk-Jin Yoon (KAIST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VINECS: Video-based Neural Character Skinning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhouyingcheng Liao (the University of Hong Kong, University of Hong Kong) · Vladislav Golyanik (MPI for Informatics) · Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) · Christian Theobalt (MPI Informatik)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Generative Powers of Ten</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaojuan Wang (Department of Computer Science) · Janne Kontkanen (Research, Google) · Brian Curless (University of Washington) · Steve Seitz (University of Washington) · Ira Kemelmacher-Shlizerman (University of Washington) · Ben Mildenhall (Google) · Pratul P. Srinivasan (Google Research) · Dor Verbin (None) · Aleksander Holynski (UC Berkeley &amp; Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Plug and Play Active Learning for Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenhongyi Yang (University of Edinburgh, University of Edinburgh) · Lichao Huang (Horizon  robotics ) · Elliot Crowley (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Structure-from-Motion with Graph Attention Networks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lucas Brynte (None) · José Pedro Iglesias (None) · Carl Olsson (Lund University) · Fredrik Kahl (Chalmers University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruicong Liu (The University of Tokyo) · Takehiko Ohkawa (The University of Tokyo) · Mingfang Zhang (None) · Yoichi Sato (University of Tokyo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/Towers-D/NAS-Unseen-Datasets" target="_blank">Insights from the Use of Previously Unseen Neural Architecture Search Datasets</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rob Geada (University of Newcastle-upon-Tyne) · David Towers (Newcastle University) · Matthew Forshaw (Newcastle University, UK) · Amir Atapour-Abarghouei (Durham University) · Stephen McGough ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Joint-Task Regularization for Partially Labeled Multi-Task Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kento Nishi (Harvard University) · Junsik Kim (None) · Wanhua Li (Harvard University) · Hanspeter Pfister (Harvard University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Mind Artist: Creating Artistic Snapshots with Human Thought</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaxuan Chen (Zhejiang University) · Yu Qi (Zhejiang University) · Yueming Wang (Zhejiang University) · Gang Pan (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bohao Peng (The Chinese University of Hong Kong) · Xiaoyang Wu (The University of Hong Kong) · Li Jiang (Max Planck Institute for Informatics) · Yukang Chen (None) · Hengshuang Zhao (The University of Hong Kong) · Zhuotao Tian (The Chinese University of Hong Kong) · Jiaya Jia (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Unifying Top-down and Bottom-up Scanpath Prediction using Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhibo Yang (State University of New York, Stony Brook) · Sounak Mondal (State University of New York, Stony Brook) · Seoyoung Ahn (State University of New York, Stony Brook) · Ruoyu Xue (State University of New York at Stony Brook) · Gregory Zelinsky (State University of New York at Stony Brook) · Minh Hoai (State University of New York, Stony Brook) · Dimitris Samaras (Stony Brook University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-28-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-162" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-163" class="mjx-mrow"><span id="MJXc-Node-164" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-165" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em;">L</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-166" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">0</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-28">L_0</script>-Sampler: An <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-29-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-167" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-168" class="mjx-mrow"><span id="MJXc-Node-169" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-170" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.441em; padding-bottom: 0.253em;">L</span></span></span><span class="mjx-sub" style="font-size: 70.7%; vertical-align: -0.212em; padding-right: 0.071em;"><span id="MJXc-Node-171" class="mjx-texatom" style=""><span id="MJXc-Node-172" class="mjx-mrow"><span id="MJXc-Node-173" class="mjx-mn"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">0</span></span></span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>L</mi><mrow class="MJX-TeXAtom-ORD"><mn>0</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-29">L_{0}</script> Model Guided Volume Sampling for NeRF</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liangchen Li (University of Science and Technology of China) · Juyong Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SAI3D: Segment Any Instance in 3D Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yingda Yin (None) · Yuzheng Liu (Peking University) · Yang Xiao (Huawei Technologies Ltd.) · Daniel Cohen-Or (Google) · Jingwei Huang (Huawei Technologies Ltd.) · Baoquan Chen (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhipeng Hu (Leihuo Game, NetEase) · Minda Zhao (NetEase Fuxi AI Lab) · Chaoyi Zhao (Fuxi AI Lab, NetEase) · Xinyue Liang (nanjing university) · Lincheng Li () · Zeng Zhao (Fuxi AI Lab,NetEase, Inc.) · Changjie Fan (Netease, Fuxi AI Lab) · Xiaowei Zhou (None) · Xin Yu (University of Queensland)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Diffusion 3D Features (Diff3F): Decorating Untextured Shapes with Distilled Semantic Features</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Niladri Shekhar Dutt (Ready Player Me) · Sanjeev Muralikrishnan (University College London, University of London) · Niloy J. Mitra (University College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SGC-Occ: Semantic-Geometry Consistent 3D Occupancy Prediction for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiwen Yang (Peking University) · Xiangteng He (Peking University) · Yuxin Peng (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sibo Wang (Institute of Computing Technology, Chinese Academy of Sciences) · Jie Zhang (Institute of Computing Technology, Chinese Academy of Sciences) · Zheng Yuan (Institute of Computing Technology, Chinese Academy of Sciences) · Shiguang Shan (Institute of Computing Technology, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>De-Diffusion Makes Text a Strong Cross-Modal Interface</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Wei (Johns Hopkins University) · Chenxi Liu (Waymo) · Siyuan Qiao (Google) · Zhishuai Zhang (Google) · Alan L. Yuille (Johns Hopkins University) · Jiahui Yu (Google Brain)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/YanzuoLu/CFLD" target="_blank">Coarse-to-Fine Latent Diffusion for Pose-Guided Person Image Synthesis</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanzuo Lu (SUN YAT-SEN UNIVERSITY) · Manlin Zhang (SUN YAT-SEN UNIVERSITY) · Jinhua Ma (SUN YAT-SEN UNIVERSITY) · Xiaohua Xie (SUN YAT-SEN UNIVERSITY) · Jianhuang Lai (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unsupervised Occupancy Learning from Sparse Point Cloud</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Amine Ouasfi (INRIA) · Adnane Boukhayma (INRIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Benchmarking Implicit Neural Representation and Geometric Rendering in Real-Time RGB-D SLAM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tongyan Hua (HKUST(GZ)) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GLOW: Global Layout Aware Attacks on Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jun Bao (Hangzhou Dianzi University) · Buyu Liu (NEC-Labs) · Kui Ren (Zhejiang University) · Jun Yu (Hangzhou Dianzi University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DeepCache: Accelerating Diffusion Models for Free</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinyin Ma (National University of Singapore) · Gongfan Fang (None) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HPNet: Dynamic Trajectory Forecasting with Historical Prediction Attention</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaolong Tang (Institute of Computing Technoloy, Chinese Academy of Sciences) · Meina Kan (Institute of Computing Technoloy, Chinese Academy of Sciences) · Shiguang Shan (Institute of Computing Technology, Chinese Academy of Sciences) · Zhilong Ji (Tomorrow Advancing Life) · Jinfeng Bai (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Xilin Chen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://www.mikhailkennerley.com/cat" target="_blank">CAT: Exploiting Inter-Class Dynamics for Domain Adaptive Object Detection</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mikhail Kennerley (National University of Singapore) · Jian-Gang Wang (A*STAR) · Bharadwaj Veeravalli (NUS) · Robby T. Tan (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Neural Underwater Scene Representation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunkai Tang (Peking University) · Chengxuan Zhu (Peking University) · Renjie Wan (None) · Chao Xu (Peking University) · Boxin Shi (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Scale Decoupled Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shicai Wei (University of Electronic Science and Technology of China) · Chunbo Luo (University of Electronic Science and Technology of China) · Yang Luo (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>T-VSL: Text-Guided Visual Sound Source Localization in Mixtures</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tanvir Mahmud (University of Texas at Austin) · Yapeng Tian (University of Texas at Dallas) · Diana Marculescu (The University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PolarMatte: Fully Computational Ground-Truth-Quality Alpha Matte Extraction for Images and Video using Polarized Screen Matting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kenji Enomoto (Adobe Systems) · TJ Rhodes (Adobe Research) · Brian Price (Adobe Research) · Gavin Miller (Adobe)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Traceable Federated Continual Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiang Wang (Beijing University of Posts and Telecommunications) · Bingyan Liu (None) · Yawen Li (Beijing University of Posts and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CosalPure: Learning Concept from Group Images for Robust Co-Saliency Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiayi Zhu (East China Normal University) · Qing Guo (Institute of High Performance Computing, Singapore, A*STAR) · Felix Juefei Xu () · Yihao Huang (Nanyang Technological University) · Yang Liu (Nanyang Technological University) · Geguang Pu (East China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CrossMAE: Cross Modality Masked Autoencoders For Region-Aware Audio-Visual Pretraining</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuxin Guo (Institute of Automation, Chinese Academy of Sciences) · Siyang Sun (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Shuailei Ma () · Kecheng Zheng (Ant Group) · Xiaoyi Bao (CASIA) · Shijie Ma (Institute of Automation, Chinese Academy of Sciences) · Wei Zou (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Yun Zheng (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gihyun Kwon (Korea Advanced Institute of Science &amp; Technology) · Simon Jenni (Adobe Systems) · Ding Li (None) · Joon-Young Lee (Adobe Research) · Jong Chul Ye (Korea Advanced Institute of Science and Technology) · Fabian Caba Heilbron (Adobe Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CapHuman: Capture Your Moments in Parallel Universes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chao Liang (Zhejiang University) · Fan Ma (None) · Linchao Zhu (None) · Yingying Deng (None) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Vista-LLaMA: Reliable Video Teller via Equal Distance to Visual Tokens</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fan Ma (None) · Xiaojie Jin (ByteDance Inc./TikTok) · Heng Wang (Bytedance) · Yuchen Xian (Zhejiang University) · Jiashi Feng (ByteDance) · Yi Yang (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Low-Rank Rescaled Vision Transformer Fine-Tuning: A Residual Design Approach</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei Dong (Xi'an University of Architecture and Technology) · Xing Zhang (Xi'an University of Architecture and Technology) · Bihui Chen (Xi'an University of Architecture and Technology) · Dawei Yan (Xi'an University of Architecture and Technology) · Zhijun Lin (Northwest Polytechnical University Xi'an) · Qingsen Yan (Northwest Polytechnical University Xi'an) · Peng Wang (University of Wollonong) · Yang Yang (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Real-World Mobile Image Denoising Dataset with Efficient Baselines</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Roman Flepp (ETH Zurich) · Andrey Ignatov () · Radu Timofte (University of Würzburg) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PARA-Drive: Parallelized Architecture for Real-time Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xinshuo Weng (NVIDIA) · Boris Ivanovic (NVIDIA) · Yan Wang (NVIDIA) · Yue Wang (Massachusetts Institute of Technology) · Marco Pavone (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SRTube: Video-Language Pre-Training with Action-Centric Video Tube Features and Semantic Role Labeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Juhee Lee (None) · Jewon Kang (Ewha Womans Univrsity)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vanessa Skliarova (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Egor Zakharov (ETH Zurich) · Otmar Hilliges (None) · Michael J. Black (University of Tübingen) · Justus Thies (Max-Planck Institute for Intelligent Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Brain Decodes Deep Nets</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Huzheng Yang (University of Pennsylvania, University of Pennsylvania) · James Gee (University of Pennsylvania, University of Pennsylvania) · Jianbo Shi (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MoSAR: Monocular Semi-Supervised Model for Avatar Reconstruction using Differentiable Shading</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Abdallah Dib (Ubisoft) · Luiz Gustavo Hafemann (Ubisoft La Forge) · Emeline Got (La Forge - Ubisoft ) · Trevor Anderson (Ubisoft) · Amin Fadaeinejad (None) · Rafael M. O. Cruz (École de technologie supérieure, Université du Québec) · Marc-André Carbonneau (Ubisoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Defense without Forgetting: Continual Adversarial Defense with Anisotropic &amp; Isotropic Pseudo Replay</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhang Zhou (Harbin Institute of Technology) · Zhongyun Hua (Harbin Institute of Technology Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Extend Your Own Correspondences: Unsupervised Distant Point Cloud Registration by Progressive Distance Extension</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Quan Liu (Shanghai Jiaotong University) · Hongzi Zhu (Shanghai Jiao Tong University) · Zhenxi Wang (Shanghai Jiaotong University) · Yunsong Zhou (Shanghai Jiao Tong University) · Shan Chang (Donghua University, Shanghai) · Minyi Guo (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PoseIRM: Enhance 3D Human Pose Estimation on Unseen  Camera Settings via Invariant Risk Minimization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanlu Cai (Fudan University) · Weizhong Zhang (Fudan University) · Yuan Wu (Fudan University) · Cheng Jin (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UniHuman: A Unified Model For Editing Human Images in the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nannan Li (Boston University) · Qing Liu (Adobe Systems) · Krishna Kumar Singh (Adobe Systems) · Yilin Wang (Adobe Systems) · Jianming Zhang (Adobe Systems) · Bryan A. Plummer (None) · Zhe Lin (Adobe Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Select Views for Efficient Multi-View Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunzhong Hou (Australian National University) · Stephen Gould (Australian National University) · Liang Zheng (Australian National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            shiyu xuan (Peking University) · Qingpei Guo (Ant Group) · Ming Yang (Ant Group) · Shiliang Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Geometry-aware Reconstruction and Fusion-refined Rendering for Generalizable Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            TIANQI LIU (None) · Xinyi Ye (School of Artificial Intelligence and Automation, Huazhong University of Science and Technology) · Min Shi (None) · Zihao Huang (None) · Zhiyu Pan (None) · Zhan Peng (None) · Zhiguo Cao ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/Pointcept/Pointcept" target="_blank">Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyang Wu (The University of Hong Kong) · Zhuotao Tian (The Chinese University of Hong Kong) · Xin Wen (The University of Hong Kong) · Bohao Peng (The Chinese University of Hong Kong) · Xihui Liu (The University of Hong Kong) · Kaicheng Yu (Alibaba Group) · Hengshuang Zhao (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Point2CAD: Reverse Engineering CAD Models from 3D Point Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yujia Liu (ETH Zürich) · Anton Obukhov (None) · Jan D. Wegner (University of Zurich) · Konrad Schindler (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Active Object Detection with Knowledge Aggregation and Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dejie Yang (Peking University) · Yang Liu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/htyjers/StrDiffusion" target="_blank">Structure Matters: Tackling the Semantic Discrepancy in Diffusion Models for Image Inpainting</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haipeng Liu (Hefei University of Technology) · Yang Wang (Hefei University of Technology) · Biao Qian (None) · Meng Wang (Hefei University of Technology) · Yong Rui (Lenovo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ExMap: Leveraging Explainability Heatmaps for Unsupervised Group Robustness to Spurious Correlations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rwiddhi Chakraborty (None) · Adrian de Sena Sletten (University of Tromsø) · Michael C. Kampffmeyer (UiT The Arctic University of Norway)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RadSimReal: Bridging the Gap Between Synthetic and Real Data in Radar Object Detection With Simulation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Oded Bialer (General Motors) · Yuval Haitman (Ben-Gurion University of the Negev)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongyeong Hwang (Korea Advanced Institute of Science &amp; Technology) · Hyunju Kim (Korea Advanced Institute of Science &amp; Technology) · Sunwoo Kim (Korea Advanced Institute of Science &amp; Technology) · Kijung Shin (Korea Advanced Institute of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Mip-Splatting: Alias-free 3D Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zehao Yu (None) · Anpei Chen (Department of Computer Science, ETHZ - ETH Zurich) · Binbin Huang (ShanghaiTech University) · Torsten Sattler (Czech Technical University in Prague) · Andreas Geiger (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Text2QR:  Harmonizing Aesthetic Customization and Scanning Robustness for Text-Guided QR Code Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangyang Wu (Shanghai Jiaotong University) · Xiaohong Liu (Shanghai Jiao Tong University) · Jun Jia (Shanghai Jiaotong University) · Xuehao Cui (University of Michigan - Ann Arbor) · Guangtao Zhai (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>UV-IDM: Identity-Conditioned Latent Diffusion Model for Face UV-Texture Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hong Li (Beijing University of Aeronautics and Astronautics) · Yutang Feng (Beijing University of Aeronautics and Astronautics) · Song Xue (Baidu) · Xuhui Liu (Beihang University) · Boyu Liu (Beijing University of Aeronautics and Astronautics) · Bohan Zeng (Beijing University of Aeronautics and Astronautics) · Shanglin Li (Beijing University of Aeronautics and Astronautics) · Jianzhuang Liu (Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences) · Shumin Han (Baidu) · Baochang Zhang (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UniPTS: A Unified Framework for Proficient Post-Training Sparsity</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            JingJing Xie (Xiamen University) · Yuxin Zhang (Xiamen University) · Mingbao Lin (Xiamen University) · ZhiHang Lin (Xiamen University) · Liujuan Cao (Xiamen University) · Rongrong Ji (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PBWR: Parametric Building Wireframe Reconstruction from Aerial LiDAR Point Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shangfeng Huang (University of Calgary) · Ruisheng Wang (University of Calgary) · Bo Guo (Guangdong University of Technology) · Hongxin Yang (University of Calgary)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sijin Chen (Fudan University) · Xin Chen (University of Chinese Academy of Sciences, ShanghaiTech University) · Chi Zhang (Tencent ) · Mingsheng Li (Fudan University) · Gang Yu (Tencent) · Hao Fei (National University of Singapore) · Hongyuan Zhu (Institute for Infocomm Research) · Jiayuan Fan (Fudan University) · Tao Chen (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ProMark: Proactive Diffusion Watermarking for Causal Attribution</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vishal Asnani (Michigan State University) · John Collomosse (University of Surrey) · Tu Bui (Fujitsu Research and Development Center Co. Ltm.) · Xiaoming Liu (None) · Shruti Agarwal (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MMM: Generative Masked Motion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ekkasit Pinyoanuntapong (University of North Carolina at Charlotte) · Pu Wang (University of North Carolina at Charlotte) · Minwoo Lee (University of North Carolina, Charlotte) · Chen Chen ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bridging the Gap Between End-to-End and Two-Step Text Spotting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mingxin Huang (None) · Hongliang Li (South China University of Technology) · Yuliang Liu (Huazhong University of Science and Technology) · Xiang Bai (Huazhong University of Science and Technology) · Lianwen Jin (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GenNBV: Generalizable Next-Best-View Policy for Active 3D Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiao Chen (The Chinese University of Hong Kong) · Quanyi Li (University of Edinburgh) · Tai Wang (Shanghai AI Laboratory) · Tianfan Xue (The Chinese University of Hong Kong) · Jiangmiao Pang (Shanghai AI Laboratory )
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adaptive Hyper-graph Aggregation for Modality-Agnostic Federated Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fan Qi (Tianjin University of Technology) · Shuai Li (Tianjin University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VS: Reconstructing Clothed 3D Human from Single Image via Vertex Shift</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leyuan Liu (Central China Normal University) · Yuhan Li (Central China Normal University) · Yunqi Gao (Central China Normal University) · Changxin Gao (Huazhong University of Science and Technology) · Yuanyuan Liu (None) · Jingying Chen (Central China Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D Synthetic Data</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifang Men (Alibaba Group) · Biwen Lei (Alibaba Group) · Yuan Yao (Alibaba group) · Miaomiao Cui (Alibaba Group) · Zhouhui Lian (Peking University) · Xuansong Xie (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Wonder3D: Single Image to 3D using Cross-Domain Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoxiao Long (The University of Hong Kong) · Yuan-Chen Guo (Tsinghua University) · Cheng Lin (Tencent) · Yuan Liu (The University of Hong Kong) · Zhiyang Dou (The University of Hong Kong) · Lingjie Liu (Saarland Informatics Campus, Max-Planck Institute) · Yuexin Ma (ShanghaiTech University) · Song-Hai Zhang (Tsinghua University, Tsinghua University) · Marc Habermann (Saarland Informatics Campus, Max-Planck Institute) · Christian Theobalt (MPI Informatik) · Wenping Wang (Texas A&amp;M University - College Station)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Honeybee: Locality-enhanced Projector for Multimodal LLM</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Junbum Cha (Kakao Brain) · Woo-Young Kang (Kakaobrain) · Jonghwan Mun (KakaoBrain) · Byungseok Roh (Kakao Brain)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiqi Li (Nanjing University) · Zhiding Yu (NVIDIA) · Shiyi Lan (NVIDIA CORPORATION) · Jiahan Li (Nanjing University) · Jan Kautz (NVIDIA) · Tong Lu (Nanjing University) · Jose M. Alvarez (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Self-Training Large Language Models for Improved Visual Program Synthesis With Visual Reinforcement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zaid Khan (Northeastern University) · Vijay Kumar BG (NEC Laboratories America) · Samuel Schulter (NEC Laboratories America) · Yun Fu (Northeastern University) · Manmohan Chandraker (UC San Diego)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MoMask: Generative Masked Modeling of 3D Human Motions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            chuan guo (University of Alberta) · Yuxuan Mu (University of Alberta) · Muhammad Gohar Javed (University of Alberta) · Sen Wang (HoYoverse) · Li Cheng (University of Alberta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Text2Loc: 3D Point Cloud Localization from Natural Language</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yan Xia (Technical University of Munich) · Letian Shi (Technische Universität München) · Zifeng Ding (LMU Munich) · João F. Henriques (University of Oxford) · Daniel Cremers (Technical University Munich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Gaussian Shadow Casting for Neural Characters</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Luis Bolanos (The University of British Columbia) · Shih-Yang Su (University of British Columbia) · Helge Rhodin (UBC)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SleepVST: Sleep Staging from Near-Infrared Video Signals using Pre-Trained Transformers</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jonathan F. Carter (University of Oxford) · Joao Jorge (Oxehealth) · Oliver Gibson (Oxehealth Limited) · Lionel Tarassenko (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Enhancing 3D Fidelity of Text-to-3D using Cross-View Correspondences</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Seungwook Kim (POSTECH) · Kejie Li (University of Oxford) · Xueqing Deng (ByteDance Research) · Yichun Shi (ByteDance) · Minsu Cho (POSTECH) · Peng Wang (Bytedance US AILab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>BigGait: Learning Gait Representation You Want by Large Vision Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dingqiang Ye (Southern University of Science and Technology) · Chao Fan (None) · Jingzhe Ma (Southern University of Science and Technology) · Xiaoming Liu (None) · Shiqi Yu (Southern University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Event-based Visible and Infrared Fusion via Multi-task Collaboration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengyue Geng (Peking University) · Lin Zhu (Beijing Institute of Technology) · Lizhi Wang (None) · Wei Zhang (Peng Cheng Laboratory) · Ruiqin Xiong (Peking University) · Yonghong Tian (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haochen Han (Xi'an Jiaotong University) · Qinghua Zheng (Xi'an Jiaotong University) · Guang Dai (SGIT AI) · Minnan Luo (None) · Jingdong Wang (Baidu)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Gaussian Shell Maps for Efficient 3D Human Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rameen Abdal (Stanford University) · Wang Yifan (Stanford University) · Zifan Shi (HKUST) · Yinghao Xu (Chinese University of Hong Kong) · Ryan Po (Stanford University) · Zhengfei Kuang (Stanford University) · Qifeng Chen (Hong Kong University of Science and Technology) · Dit-Yan Yeung (Hong Kong University of Science and Technology) · Gordon Wetzstein (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AllSpark: Reborn Labeled Features from Unlabeled in Transformer for Semi-Supervised Semantic Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haonan Wang (The Hong Kong University of Science and Technology) · Qixiang ZHANG (Hong Kong University of Science and Technology) · Yi Li (Hong Kong University of Science and Technology) · Xiaomeng Li (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Living Scenes: Multi-object Relocalization and Reconstruction in Changing 3D Environments</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Liyuan Zhu (Stanford University) · Shengyu Huang (None) · Konrad Schindler (ETH Zurich) · Iro Armeni (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kyle Buettner (None) · Sina Malakouti (University of Pittsburgh) · Xiang Li (University of Pittsburgh) · Adriana Kovashka (University of Pittsburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffAM: Diffusion-based Adversarial Makeup Transfer for Facial Privacy Protection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhao Sun (University of Science and Technology of China) · Lingyun Yu (University of Science and Technology of China) · Hongtao Xie (University of Science and Technology of China) · Jiaming Li (University of Science and Technology of China) · Yongdong Zhang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Loopy-SLAM: Dense Neural SLAM with Loop Closures</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lorenzo Liso (ETHZ - ETH Zurich) · Erik Sandström (ETH Zürich) · Vladimir Yugay (University of Amsterdam) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Martin R. Oswald (University of Amsterdam)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffInDScene: Diffusion-based High-Quality 3D Indoor Scene Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoliang Ju (The Chinese University of Hong Kong) · Zhaoyang Huang (The Chinese University of Hong Kong) · Yijin Li (Zhejiang University) · Guofeng Zhang (Zhejiang University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Hongsheng Li (The Chinese University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Feedback-Guided Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jimuyang Zhang (None) · Zanming Huang (Boston University, Boston University) · Arijit Ray (Boston University) · Eshed Ohn-Bar (Boston University, Boston University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Empowering Resampling Operation for Ultra-High-Definition Image Enhancement with Model-Aware Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yu (None) · Jie Huang (University of Science and Technology of China) · Li (None) · Kaiwen Zheng (University of Science and Technology of China) · Qi Zhu (University of Science and Technology of China) · Man Zhou (University of Science and Technology of China) · Feng Zhao (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>READ: Retrieval-Enhanced Asymmetric Diffusion for Motion Planning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Takeru Oba (None) · Matthew Walter (Toyota Technological Institute at Chicago) · Norimichi Ukita (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>From Pixels to Graphs: Open-Vocabulary Scene Graph Generation with Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rongjie Li (SIST ,ShanghaiTech University) · Songyang Zhang (Shanghai AI Laboratory) · Dahua Lin (The Chinese University of Hong Kong) · Kai Chen (Shanghai AI Laboratory) · Xuming He (ShanghaiTech University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>LTM: Lightweight Textured Mesh Reconstruction of Unbounded Scenes Using Neural Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaehoon Choi (University of Maryland, College Park) · Rajvi Shah (Facebook) · Qinbo Li (Facebook) · Yipeng Wang (Meta Reality Labs) · Ayush Saraf (Meta Platforms, Inc.) · Changil Kim (Facebook) · Jia-Bin Huang (University of Maryland, College Park) · Dinesh Manocha (University of Maryland, College Park) · Suhib Alsisan (Meta) · Johannes Kopf (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Test-Time Linear Out-of-Distribution Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ke Fan (Fudan University) · Tong Liu (BOE Technology Group Co., Ltd) · Xingyu Qiu (Fudan University) · Yikai Wang (None) · Lian Huai (BOE Technology Group Co., Ltd) · Zeyu Shangguan (BOE TECHNOLOGY GROUP CO., LTD) · Shuang Gou (BOE Technology Group Co., Ltd) · FENGJIAN LIU (BOE Technology Group Co., Ltd ) · Yuqian Fu (Fudan University) · Yanwei Fu (Fudan University) · Xingqun Jiang (BOE Technology Group Co., LTD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Matching Anything by Segmenting Anything</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siyuan Li (ETH Zurich) · Lei Ke (HKUST &amp; ETH Zurich) · Martin Danelljan (ETH Zurich) · Luigi Piccinelli (ETH Zurich) · Mattia Segu (ETH Zurich - Swiss Federal Institute of Technology) · Luc Van Gool (ETH Zurich; KULeuven; INSAIT Sofia Un.) · Fisher Yu (ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>InstaGen: Enhancing Object Detection by Training on Synthetic Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengjian Feng (Meituan Inc.) · Yujie Zhong (Meituan Inc.) · Zequn Jie (Meituan) · Weidi Xie (Shanghai Jiaotong University) · Lin Ma (Meituan)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Narrative Action Evaluation with Prompt-Guided Multimodal Interaction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shiyi Zhang (None) · Sule Bai (Tsinghua University, Tsinghua University) · Guangyi Chen (MBZUAI, CMU) · Lei Chen (Beijing University of Science and Technology) · Jiwen Lu (Tsinghua University) · Junle Wang (Tencent) · Yansong Tang (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Multi-scale Dynamic and Hierarchical Relationship Modeling for Facial Action Units Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihan Wang (None) · Siyang Song (University of Leicester) · Cheng Luo (Shenzhen University) · Songhe Deng (None) · Weicheng Xie (Shenzhen University) · Linlin Shen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zijie Chen (Westlake University) · Lichao Zhang (Westlake University) · Fangsheng Weng (https://xinchenai.com/) · Lili Pan (University of Electronic Science and Technology of China) · ZHENZHONG Lan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Imagine Before Go: Self-Supervised Generative Map for Object Goal Navigation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sixian Zhang (None) · Xinyao Yu (University of the Chinese Academy of Sciences) · Xinhang Song (None) · XIAOHAN Wang (Xi'an Jiaotong University) · Shuqiang Jiang (Institute of Computing Technology, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multi-view Aggregation Network for Dichotomous Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qian Yu (Dalian University of Technology) · Xiaoqi Zhao (Dalian University of Technology) · Youwei Pang (Dalian University of Technology) · Lihe Zhang (Dalian University of Technology) · Huchuan Lu (Dalian University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>EVCap: Retrieval-Augmented Image Captioning with External Visual--Name Memory for Open-World Comprehension</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaxuan Li (The University of Tokyo) · Duc Minh Vo (The University of Tokyo) · Akihiro Sugimoto (NII) · Hideki Nakayama (The University of Tokyo)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Plug-and-Play Diffusion Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yi-Ting Hsiao (University of Michigan - Ann Arbor) · Siavash Khodadadeh (Adobe Systems) · Kevin Duarte (Adobe Systems) · Wei-An Lin (Adobe Systems) · Hui Qu (Adobe Inc.) · Mingi Kwon (None) · Ratheesh Kalarot (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CLIB-FIQA: Face Image Quality Assessment with Confidence Calibration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fu-Zhao Ou (City University of Hong Kong) · Chongyi Li () · Shiqi Wang (City University of Hong Kong) · Sam Kwong (Lingnan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhihao Zhang (Xi'an Jiaotong University) · Shengcao Cao (University of Illinois at Urbana-Champaign) · Yu-Xiong Wang (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://yuiga.dev/polos" target="_blank">Polos: Multimodal Metric Learning from Human Feedback for Image Captioning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuiga Wada (Keio University) · Kanta Kaneda (Keio University) · Daichi Saito (Keio University) · Komei Sugiura (Keio University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>XScale-NVS: Cross-Scale Novel View Synthesis with Hash Featurized Manifold</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guangyu Wang (Tsinghua University) · Jinzhi Zhang (Electronic Engineering, Tsinghua University, Tsinghua University) · Fan Wang (Alibaba Group) · Ruqi Huang (Tsinghua Shenzhen International Graduate School/Tsinghua Berkeley Shenzhen Institute ) · Lu Fang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Adrian Bulat (None) · Yassine Ouali (Samsung) · Georgios Tzimiropoulos (Queen Mary University London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Differentiable Micro-Mesh Construction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yishun Dou (Huawei) · Zhong Zheng (huawei.com) · Qiaoqiao Jin (Shanghai Jiao Tong University) · Rui Shi (Shanghai Jiao Tong University) · Yuhan Li (None) · Bingbing Ni (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>CPGA: Coding Priors-Guided Aggregation Network for Compressed Video Quality Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiang Zhu (University of Electronic Science and Technology of China) · Jinhua Hao (Kuaishou Tech) · Yukang Ding (Kuaishou Tech) · Yu Liu (University of Electronic Science and Technology of China) · Qiao Mo (University of Electronic Science and Technology of China) · Ming Sun (Kuaishou Tech) · Chao Zhou (kuaishou) · Shuyuan Zhu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Enhancing Vision-Language Pretraining with Rich Supervisions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Gao (Computer Science Department, Stanford University) · Kunyu Shi (Amazon) · Pengkai Zhu (Boston University) · Edouard Belval (Amazon) · Oren Nuriel (Amazon) · Srikar Appalaraju (Amazon) · Shabnam Ghadar (Amazon) · Zhuowen Tu (University of California, San Diego) · Vijay Mahadevan (Amazon) · Stefano Soatto (AWS)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>HOISDF: Constraining 3D Hand Object Pose Estimation with Global Signed Distance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haozhe Qi (EPFL - Switzerland) · Chen Zhao (EPFL) · Mathieu Salzmann (EPFL) · Alexander Mathis (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>On the Robustness of Large Multimodal Models Against Image Adversarial Attacks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuanming Cui (University of Central Florida) · Alejandro Aparcedo (University of Central Florida) · Young Kyun Jang (Meta AI) · Ser-Nam Lim (Meta AI)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Daan de Geus (Eindhoven University of Technology) · Gijs Dubbelman (Eindhoven University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Enhanced Motion-Text Alignment for Image-to-Video Transfer Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei Zhang (University of Science and Technology of China) · Chaoqun Wan (Alibaba Group) · Tongliang Liu (Mohamed bin Zayed University of Artificial Intelligence) · Xinmei Tian (University of Science and Technology of China) · Xu Shen (Alibaba Group) · Jieping Ye (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Identifying Important Group of Pixels using Interactions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kosuke Sumiyasu (Chiba University) · Kazuhiko Kawamoto (Chiba University) · Hiroshi Kera (Chiba University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient Multi-scale Network with Learnable Discrete Wavelet Transform for Blind Motion Deblurring</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xin Gao (None) · Tianheng Qiu (University of Science and Technology of China) · Xinyu Zhang (None) · Hanlin Bai (China University of Mining Technology - Beijing) · Kang Liu (None) · xuan huang (Chinese Academy of Sciences) · Hu Wei (Chinese Academy of Sciences) · Guoying Zhang (China University of Mining Technology - Beijing) · Huaping Liu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Countering Personalized Text-to-Image Generation with Influence Watermarks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanwen Liu (Peking University) · Zhicheng Sun (Peking University) · Yadong Mu (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>POPDG:Popular 3D Dance Generation with PopDanceSet</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            ZhenYe Luo (Beijing Normal University) · Min Ren (Beijing Normal University) · Xuecai Hu (Beijing Normal University) · Yongzhen Huang (Beijing Normal University) · Li Yao (Beijing Normal University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GOV-NeSF: Generalizable Open-Vocabulary Neural Semantic Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunsong Wang (National University of Singapore) · Hanlin Chen (National University of Singapore) · Gim Hee Lee (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SNIDA: Unlocking Few-Shot Object Detection with Non-linear Semantic Decoupling Augmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yanjie Wang (Huazhong University of Science and Technology) · Xu Zou (Huazhong University of Science and Technology) · Luxin Yan (Huazhong University of Science and Technology) · Sheng Zhong (Huazhong University of Science and Technology) · Jiahuan Zhou (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Automatic Controllable Colorization by Imagination</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyan Cong (Zhejiang University) · Yue Wu (Huawei Technologies Ltd.) · Qifeng Chen (Hong Kong University of Science and Technology) · Chenyang Lei (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DreamAvatar: Text-and-Shape Guided 3D Human Avatar Generation via Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yukang Cao (the University of Hong Kong) · Yan-Pei Cao (Tencent ARC Lab) · Kai Han (The University of Hong Kong) · Ying Shan (Tencent) · Kwan-Yee K. Wong (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Are Conventional SNNs Really Efficient? A Perspective from Network Quantization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Guobin Shen (None) · Dongcheng Zhao (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Tenglong Li (Institute of automation, Chinese Academy of Sciences) · Jindong Li (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Yi Zeng (Institute of automation, Chinese academy of science, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Adaptive Multi-Modal Cross-Entropy Loss for Stereo Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Peng Xu (Zhejiang University) · Zhiyu Xiang (None) · Chengyu Qiao (Zhejiang University) · Jingyun Fu (Zhejiang University) · Tianyu Pu (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>OpenEQA: Embodied Question Answering in the Era of Foundation Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Arjun Majumdar (Georgia Institute of Technology) · Anurag Ajay (Massachusetts Institute of Technology) · Xiaohan Zhang (State University of New York at Binghamton) · Sriram Yenamandra (Georgia Institute of Technology) · Mikael Henaff (Facebook) · Alexander Sax (University of California Berkeley) · Sneha Silwal (AI at Meta) · Paul McVay (Meta) · Oleksandr Maksymets (Facebook) · Sergio Arnaud (None) · Pranav Putta (Georgia Institute of Technology) · Karmesh Yadav (Meta AI) · Qiyang Li (University of California Berkeley) · Benjamin Newman (Meta Platforms) · Mohit Sharma (Carnegie Mellon University) · Vincent-Pierre Berges (Meta) · Shiqi Zhang (State University of New York at Binghamton) · Pulkit Agrawal (Massachusetts Institute of Technology) · Dhruv Batra (FAIR (Meta) and Georgia Tech) · Yonatan Bisk (Carnegie Mellon University) · Mrinal Kalakrishnan (Meta) · Franziska Meier (Facebook) · Chris Paxton (meta) · Aravind Rajeswaran (Facebook AI Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Prompting Hard or Hardly Prompting: Prompt Inversion for Text-to-Image Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shweta Mahajan (University of British Columbia) · Tanzila Rahman (University of British Columbia) · Kwang Moo Yi (University Of British Columbia) · Leonid Sigal (University Of British Columbia)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sebastian Koch (Ulm University / Bosch Center for AI) · Narunas Vaskevicius (Robert Bosch GmbH, Bosch) · Mirco Colosi (Robert Bosch GmbH) · Pedro Hermosilla (Technische Universität Wien) · Timo Ropinski (Ulm University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DeconfuseTrack：Dealing with Confusion for Multi-Object Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Cheng Huang (Huazhong University of Science and Technology) · Shoudong Han (Huazhong University of Science and Technology) · Mengyu He (Huazhong University of Science and Technology) · Wenbo Zheng (Huazhong University of Science and Technology) · Yuhao Wei (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PoseGPT: Chatting about 3D Human Pose</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yao Feng (None) · Jing Lin (Tsinghua University, Tsinghua University) · Sai Kumar Dwivedi (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Yu Sun (Harbin Institute of Technology) · Priyanka Patel (Max-Planck Institute) · Michael J. Black (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Rethinking Visual Instruction Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haotian Liu (University of Wisconsin-Madison) · Chunyuan Li (Microsoft Research, Redmond) · Yuheng Li (University of Wisconsin - Madison) · Yong Jae Lee (Department of Computer Sciences, University of Wisconsin - Madison)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DocRes: A Generalist Model Toward Unifying Document Image Restoration Tasks</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiaxin Zhang (South China University of Technology) · Dezhi Peng (South China University of Technology) · Chongyu Liu (South China University of Technology) · Peirong Zhang (South China University of Technology) · Lianwen Jin (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PHYSCENE: Physically Interactable 3D Scene Synthesis for Embodied AI</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yandan Yang (Beihang University) · Baoxiong Jia (University of California, Los Angeles) · Peiyuan Zhi (Beijing Institute for General Artificial Intelligence) · Siyuan Huang (Beijing Institute of General Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MoST: Motion Style Transformer between Diverse Action Contents</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boeun Kim (Seoul National University) · Jungho Kim (KETI) · Hyung Jin Chang () · Jin Young Choi (Seoul National University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Bilateral Propagation Network for Depth Completion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jie Tang (National University of Defense Technology) · Fei-Peng Tian (Light Illusions) · Boshi An (Peking University) · Jian Li (National University of Defense Technology) · Ping Tan (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zichen Miao (Purdue University) · Jiang Wang (Microsoft) · Ze Wang (Purdue University) · Zhengyuan Yang (Microsoft) · Lijuan Wang (Microsoft) · Qiang Qiu (Purdue University) · Zicheng Liu (Microsoft)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Mind The Edge: Refining Depth Edges in Sparsely-Supervised Monocular Depth Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lior Talker (Samsung R&amp;D Israel) · Aviad Cohen (Samsung) · Erez Yosef (Tel Aviv University) · Alexandra Dana (Samsung) · Michael Dinerstein (Samsung)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/OpenDriveLab/ViDAR" target="_blank">Visual Point Cloud Forecasting enables Scalable Autonomous Driving</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zetong Yang (The Chinese University of Hong Kong) · Li Chen (The University of Hong Kong) · Yanan Sun (The Hong Kong University of Science and Technology) · Hongyang Li (Shanghai AI Lab)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MaGGIe: Masked Guided Gradual Human Instance Matting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chuong Huynh (University of Maryland, College Park) · Seoung Wug Oh (Adobe Systems) · Abhinav Shrivastava (University of Maryland) · Joon-Young Lee (Adobe Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>On the Road to Portability: Compressing End-to-End Motion Planner for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kaituo Feng (Beijing Institute of Technology) · Changsheng Li (None) · Dongchun Ren (ALLRIDE.AI) · Ye Yuan (Beijing Institute of Technology) · Guoren Wang (Beijing Institute of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>NoiseCLR: A Contrastive Learning Approach for Unsupervised Discovery of Interpretable Directions in Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yusuf Dalva (Virginia Polytechnic Institute and State University) · Pinar Yanardag (Virginia Polytechnic Institute and State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Elite360D: Towards Efficient 360 Depth Estimation via Semantic- and Distance-Aware Bi-Projection Fusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hao Ai (The Hong Kong University of Science and Technology (Guangzhou Campus)) · Lin Wang (Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Your Student is Better Than Expected: Adaptive Teacher-Student Collaboration for Text-Conditional Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Nikita Starodubcev (Yandex) · Dmitry Baranchuk (Higher School of Economics) · Artem Fedorov (Moscow Institute of Physics and Technology) · Artem Babenko (Yandex)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LASA: Instance Reconstruction from Real Scans using A Large-scale Aligned Shape Annotation Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haolin Liu (The Chinese University of Hong Kong, Shenzhen) · Chongjie Ye (The Chinese University of Hong Kong, Shenzhen) · Yinyu Nie (Huawei Technologies Ltd.) · Yingfan He (Chinese University of Hong Kong, Shenzhen) · Xiaoguang Han (The Chinese University of Hong Kong, Shenzhen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fangfu Liu (Tsinghua University) · Diankun Wu (Tsinghua University, Tsinghua University) · Yi Wei (None) · Yongming Rao (Tsinghua University) · Yueqi Duan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generate Subgoal Images before Act: Unlocking the Chain-of-Thought Reasoning in Diffusion Model for Robot Manipulation with Multimodal Prompts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fei Ni (Tianjin University) · Jianye Hao (Tianjin University) · Shiguang Wu (Huawei Technologies Ltd.) · Longxin Kou (None) · Jiashun Liu (Tianjin University) · YAN ZHENG (Tianjin Unibersity, China) · Bin Wang (Huawei Noah's Ark Lab) · Yuzheng Zhuang (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Automatic Power Battery Detection:  New Challenge, Benchmark Dataset and Baseline</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoqi Zhao (Dalian University of Technology) · Youwei Pang (Dalian University of Technology) · Zhenyu Chen (Dalian University of Technology) · Qian Yu (Dalian University of Technology) · Lihe Zhang (Dalian University of Technology) · Hanqi Liu (Ohio State University, Columbus) · Jiaming Zuo (University of Southern California) · Huchuan Lu (Dalian University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Distant 3D Object Detection Using 2D Box Supervision</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zetong Yang (The Chinese University of Hong Kong) · Zhiding Yu (NVIDIA) · Christopher Choy (Stanford University) · Renhao Wang (University of California, Berkeley) · Anima Anandkumar (California Institute of Technology) · Jose M. Alvarez (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Efficient and Effective Weakly-Supervised Action Segmentation via Action-Transition-Aware Boundary Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Angchi Xu (SUN YAT-SEN UNIVERSITY) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Infrared Small Target Detection with Scale and Location Sensitivity</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qiankun Liu (Beijing Institute of Technology) · Rui Liu (Beijing Institute of Technology) · Bolun Zheng (Hangzhou Dianzi University) · Hongkui Wang (Hangzhou Dianzi University) · Ying Fu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Minimal Perspective Autocalibration</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andrea Porfiri Dal Cin (Polytechnic Institute of Milan) · Timothy Duff (University of Washington) · Luca Magri (Polytechnic Institute of Milan) · Tomas Pajdla (CIIRC - Czech Technical University in Prague)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SVGDreamer: Text Guided SVG Generation with Diffusion Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            XiMing Xing (Beihang University) · Chuang Wang (Beihang University) · Haitao Zhou (Beihang University) · Jing Zhang (Beihang University) · Dong Xu (University of Hong Kong) · Qian Yu (Beihang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Spanning Training Progress: Temporal Dual-Depth Scoring (TDDS) for Enhanced Dataset Pruning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            xin zhang (xidian university) · Jiawei Du (Centre for Frontier AI Research (CFAR), A*STAR, Singapore) · Weiying Xie (None) · Yunsong Li () · Joey Tianyi Zhou (National University of Singapore )
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GoMVS: Geometrically Consistent Cost Aggregation for Multi-View Stereo</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiang Wu (Northwest Polytechnical University Xi'an) · Rui Li (None) · Haofei Xu (ETH Zurich) · Wenxun Zhao (None) · Yu Zhu (Northwest Polytechnical University Xi'an) · Jinqiu Sun (Northwest Polytechnical University Xi'an) · Yanning Zhang (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Paint3D: Paint Anything 3D with Lighting-less Texture Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xianfang Zeng (Tencent PCG) · Xin Chen (University of Chinese Academy of Sciences, ShanghaiTech University) · Zhongqi Qi (Tencent PCG) · Wen Liu (Tencent PCG) · Zibo Zhao (None) · Zhibin Wang (Tencent LightAI Lab) · Bin Fu (Tencent) · Yong Liu (Zhejiang University) · Gang Yu (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/javrtg/C2P" target="_blank">From Correspondences to Pose: Non-minimal Certifiably Optimal Relative Pose without Disambiguation</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Javier Tirado-Garín (I3A, Universidad de Zaragoza) · Javier Civera (I3A, Universidad de Zaragoza)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Attentive Illumination Decomposition Model for Multi-Illuminant White Balancing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dongyoung Kim (None) · Jinwoo Kim (Yonsei University) · Junsang Yu (Samsung) · Seon Joo Kim (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Closely Interactive Human Reconstruction with Proxemics and Physics-Guided Adaption</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Buzhen Huang (None) · Chen Li (National University of Singapore) · Chongyang Xu (Sichuan University) · Liang Pan (Shanghai AI Laboratory) · Yangang Wang (Southeast University) · Gim Hee Lee (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VRetouchEr: Learning Cross-frame Feature Interdependence  with Imperfection Flow for Face Retouching in Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wen Xue (South China University of Technology) · Le Jiang (South China University of Technology) · Lianxin Xie (South China University of Technology) · Si Wu (South China University of Technology) · Yong Xu (Peng Cheng Laboratory) · Hau San Wong (City University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/Ruichen0424/AB-BNN" target="_blank">A&amp;B BNN: Add&amp;Bit-Operation-Only Hardware-Friendly Binary Neural Network</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ruichen Ma (University of Electronic Science and Technology of China) · Guanchao Qiao (University of Electronic Science and Technology of China) · Yian Liu (University of Electronic Science and Technology of China) · Liwei Meng (University of Electronic Science and Technology of China) · Ning Ning (University of Electronic Science and Technology of China) · Yang Liu (University of Electronic Science and Technology of China) · Shaogang Hu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CommonCanvas: Open Diffusion Models Trained on Creative-Commons Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aaron Gokaslan (Cornell University) · A. Feder Cooper (Cornell University) · Jasmine Collins (University of California Berkeley) · Landan Seguin (Databricks) · Austin Jacobson (Databricks) · Mihir Patel (Databricks MosaicML) · Jonathan Frankle (School of Engineering and Applied Sciences, Harvard University) · Cory Stephenson (Databricks) · Volodymyr Kuleshov (Cornell University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Choose What You Need: Disentangled Representation Learning for Scene Text Recognition, Removal and Editing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Boqiang Zhang (None) · Hongtao Xie (University of Science and Technology of China) · Zuan Gao (University of Science and Technology of China) · Yuxin Wang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Template Free Reconstruction of Human-object Interaction with Procedural Interaction Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xianghui Xie (University of Tübingen) · Bharat Lal Bhatnagar (Eberhard-Karls-Universität Tübingen) · Jan Lenssen (Saarland Informatics Campus, Max-Planck Institute) · Gerard Pons-Moll (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LEOD: Label-Efficient Object Detection for Event Cameras</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyi Wu (University of Toronto) · Mathias Gehrig (University of Zurich) · Qing Lyu (University of Toronto) · Xudong Liu (None) · Igor Gilitschenski (University of Toronto)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VAREN: Very Accurate and Realistic Equine Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Silvia Zuffi () · Ylva Mellbin (Swedish University of Agricultural Sciences) · Ci Li (None) · Markus Höschle (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Hedvig Kjellström (None) · Senya Polikovsky (Max Planck Institute for Intelligent Systems, Max-Planck Institute) · Elin Hernlund (Swedish University of Agricultural Sciences) · Michael J. Black (University of Tübingen)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Memory-based Adapters for Online 3D Scene Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiuwei Xu (Tsinghua University, Tsinghua University) · Chong Xia (Tsinghua University) · Ziwei Wang (Tsinghua University, Tsinghua University) · Linqing Zhao (Tianjin University, Tsinghua University) · Yueqi Duan (None) · Jie Zhou (None) · Jiwen Lu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Ranni: Taming Text-to-Image Diffusion for Accurate Instruction Following</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yutong Feng (Alibaba Group) · Biao Gong (Alibaba Group) · Di Chen (Alibaba Group) · Yujun Shen (The Chinese University of Hong Kong) · Yu Liu (Alibaba Group) · Jingren Zhou (Alibaba Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GeoAuxNet: Towards Universal 3D Representation Learning for Multi-sensor Point Clouds</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shengjun Zhang (Tsinghua University, Tsinghua University) · Xin Fei (Tsinghua University, Tsinghua University) · Yueqi Duan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SuperSVG: Superpixel-based Scalable Vector Graphics Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Teng Hu () · Ran Yi (Shanghai Jiao Tong University) · Baihong Qian (Shanghai Jiaotong University) · Jiangning Zhang (Tencent Youtu Lab) · Paul L. Rosin (Cardiff University) · Yu-Kun Lai (Cardiff University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Video ReCap: Recursive Captioning of Hour-Long Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Md Mohaiminul Islam (UNC Chapel Hill) · Vu Bao Ngan Ho (University of North Carolina at Chapel Hill) · Xitong Yang (Meta) · Tushar Nagarajan (Meta) · Lorenzo Torresani (Facebook) · Gedas Bertasius (UNC Chapel Hill)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Flexible Biometrics Recognition: Bridging the Multimodality Gap through Attention, Alignment and Prompt Tuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Leslie Ching Ow Tiong (Samsung Electronics) · Dick Sigmund (AIDOT Inc.) · Chen-Hui Chan (Korea Institute of Science and Technology) · Andrew Beng Jin Teoh (Yonsei University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yufei Ye (Carnegie Mellon University) · Abhinav Gupta (Carnegie Mellon University) · Kris Kitani (Carnegie Mellon University) · Shubham Tulsiani (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MVD-Fusion: Single-view 3D via Depth-consistent Multi-view Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanzhe Hu (Carnegie Mellon University) · Zhizhuo Zhou (Stanford University) · Varun Jampani (Google Research) · Shubham Tulsiani (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>IQ-VFI: Implicit Quadratic Motion Estimation for Video Frame Interpolation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengshun Hu (None) · Kui Jiang (Harbin Institute of Technology) · Zhihang Zhong (Shanghai AI Lab) · Zheng Wang (Wuhan University) · Yinqiang Zheng (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Part-aware Unified Representation of Language and Skeleton for Zero-shot Action Recognition</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Anqi Zhu (None) · Qiuhong Ke (Monash University) · Mingming Gong (University of Melbourne) · James Bailey (The University of Melbourne)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Semantic-aware SAM for Point-Prompted Instance Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhaoyang Wei (University of the Chinese Academy of Sciences) · Pengfei Chen (University of the Chinese Academy of Sciences) · Xuehui Yu (None) · Guorong Li (University of Chinese Academy of Sciences) · Jianbin Jiao () · Zhenjun Han (University of the Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CoGS: Controllable Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Heng Yu (Carnegie Mellon University) · Joel Julin (Carnegie Mellon University) · Zoltán Á. Milacski (Carnegie Mellon University) · Koichiro Niinuma (Fujitsu Research of America) · László A. Jeni (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>A Bayesian Approach to OOD Robustness in Image Classification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Prakhar Kaushik (Johns Hopkins University) · Adam Kortylewski (University of Freiburg &amp; MPI-INF) · Alan L. Yuille (Johns Hopkins University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multimodal Sense-Informed Prediction of 3D Human Motions</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenyu Lou (None) · Qiongjie Cui (Nanjing University of Science and Technology) · Haofan Wang (Xiaohongshu) · Xu Tang (Shanghaitech University) · Hong Zhou (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiasen Lu (None) · Christopher Clark (None) · Sangho Lee (Allen Institute for Artificial Intelligence) · Zichen Zhang (Allen Institute for Artificial Intelligence) · Savya Khosla (University of Illinois Urbana-Champaign) · Ryan Marten (None) · Derek Hoiem (University of Illinois at Urbana-Champaign) · Aniruddha Kembhavi (Allen Institute for Artificial Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>PTQ4SAM: Post-Training Quantization for Segment Anything</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chengtao Lv (None) · Hong Chen (Beijing University of Aeronautics and Astronautics) · Jinyang Guo (Beijing University of Aeronautics and Astronautics) · Yifu Ding (None) · Xianglong Liu (BUAA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Leveraging Predicate and Triplet Learning for Scene Graph Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiankai Li (Beihang University) · Yunhong Wang (Beihang University) · Xiefan Guo (Beihang University) · Ruijie Yang (Beihang University) · Weixin Li (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Semantic Shield: Defending Vision-Language Models Against Backdooring and Poisoning via Fine-grained Knowledge Alignment</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Alvi Md Ishmam (Virginia Polytechnic Institute and State University) · Chris Thomas (Virginia Polytechnic Institute and State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PixelRNN: In-pixel Recurrent Neural Networks for End-to-end-optimized Perception with Neural Sensors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haley So (Stanford University) · Laurie Bose (None) · Piotr Dudek (University of Manchester) · Gordon Wetzstein (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Baoquan Zhang (, Harbin Institute of Technology (shenzhen)) · Huaibin Wang (Harbin Institute of Technology，Shenzhen) · Luo Chuyao (None) · Xutao Li (Harbin Institute of Technology, Shenzhen) · Guotao liang (Harbin Institute of Technology(shenzhen)) · Yunming Ye (Harbin Institute of Technology, Shenzhen) · joeq (CEO) · Yao He (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Task-Adaptive Saliency Guidance for Exemplar-free Class Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xialei Liu (Nankai University) · Jiang-Tian Zhai (Nankai University) · Andrew Bagdanov (Università degli Studi di Firenze) · Ke Li (Tencent) · Ming-Ming Cheng (Nankai University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Action Detection via an Image Diffusion Process</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lin Geng Foo (Singapore University of Technology and Design) · Tianjiao Li (Singapore University of Technology and Design) · Hossein Rahmani (Lancaster University) · Jun Liu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Disentangled Prompt Representation for Domain Generalization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            De Cheng (Xidian University) · Zhipeng Xu (Xi'an University of Electronic Science and Technology) · XINYANG JIANG (Microsoft Research) · Nannan Wang (Xidian University) · Dongsheng Li (Microsoft Research Asia) · Xinbo Gao (Chongqing University of Post and Telecommunications)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Visual Concept Connectome (VCC): Open World Concept Discovery and their Interlayer Connections in Deep Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            MATTHEW KOWAL (None) · Richard P. Wildes (York University) · Kosta Derpanis (York University/Samsung)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UniMODE: Unified Monocular 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhuoling Li (University of Hong Kong) · Xiaogang Xu (Zhejiang Lab) · Ser-Nam Lim (Meta AI) · Hengshuang Zhao (The University of Hong Kong)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>A Pedestrian is Worth One Prompt: Towards Language Guidance Person Re-Identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zexian Yang (None) · Dayan Wu (iie,cas) · Chenming Wu (None) · Zheng Lin (Institute of Information Engineering, Chinese Academy of Sciences) · JingziGU (INSTATUTE OF INFORMATION ENGINEERING,CAS) · Weiping Wang (IIE)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jingbo Wang (Shanghai AI LAB) · Zhengyi Luo (Carnegie Mellon University) · Ye Yuan (NVIDIA Research) · Yixuan LI (The Chinese University of Hong Kong) · Bo Dai (Shanghai AI Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SAOR: Single-View Articulated Object Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mehmet Aygun (University of Edinburgh) · Oisin Mac Aodha (University of Edinburgh)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tomas Soucek (Czech Technical University of Prague) · Dima Damen (University of Bristol and Google DeepMind) · Michael Wray (University of Bristol) · Ivan Laptev (INRIA Paris) · Josef Sivic (Czech Technical University in Prague)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TULIP: Transformer for Upsampling of LiDAR Point Cloud</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bin Yang (ETHZ - ETH Zurich) · Patrick Pfreundschuh (ETHZ - ETH Zurich) · Roland Siegwart (Swiss Federal Institute of Technology) · Marco Hutter (ETHZ - ETH Zurich) · Peyman Moghadam (None) · Vaishakh Patil (ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Incremental Residual Concept Bottleneck Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenming Shang (Tsinghua University) · Shiji Zhou (Tsinghua University, Tsinghua University) · Hengyuan Zhang (Tsinghua University, Tsinghua University) · Xinzhe Ni (Tsinghua University) · Yujiu Yang (Tsinghua University) · Yuwang Wang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yijia Weng (Stanford University) · Bowen Wen (NVIDIA) · Jonathan Tremblay (NVIDIA) · Valts Blukis (NVIDIA) · Dieter Fox (University of Washington) · Leonidas Guibas (Stanford University) · Stan Birchfield (NVIDIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improving Transferable Targeted Adversarial Attacks with Model Self-Enhancement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Han Wu (Sun Yat-sen University) · Guanyan Ou (Sun Yat-sen University) · Weibin Wu (SUN YAT-SEN UNIVERSITY) · Zibin Zheng (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jin-Chuan Shi (Beihang University) · Miao Wang (Beihang University) · Haobin Duan (Beihang University) · Shaohua Guan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/vimar-gu/MinimaxDiffusion" target="_blank">Efficient Dataset Distillation via Minimax Diffusion</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianyang Gu (Zhejiang University) · Saeed Vahidian (Duke University) · Vyacheslav Kungurtsev (Czech Technical Univeresity in Prague, Czech Technical University of Prague) · Haonan Wang (national university of singaore, National University of Singapore) · Wei Jiang (Zhejiang University) · Yang You (National University of Singapore) · Yiran Chen (Duke University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lirui Zhao (Xiamen University) · Yue Yang (Shanghai Jiaotong University) · Kaipeng Zhang (Shanghai AI Laboratory) · Wenqi Shao (The Chinese University of Hong Kong) · Yuxin Zhang (Xiamen University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory) · Ping Luo (The University of Hong Kong) · Rongrong Ji (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Density-Adaptive Model Based on Motif Matrix for Multi-Agent Trajectory Prediction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Di Wen (None) · Haoran Xu () · Zhaocheng He (SUN YAT-SEN UNIVERSITY) · Zhe Wu (Pengcheng Laboratory) · Guang Tan (Sun Yat-sen University) · Peixi Peng (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Accurate Post-training Quantization for Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Changyuan Wang (Tsinghua University) · Ziwei Wang (Tsinghua University, Tsinghua University) · Xiuwei Xu (Tsinghua University, Tsinghua University) · Yansong Tang (Tsinghua University) · Jie Zhou (None) · Jiwen Lu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chi Yan (Shanghai AI Laboratory) · Delin Qu (Fudan University) · Dong Wang (Shanghai AI Laboratory) · Dan Xu (Department of Computer Science and Engineering, The Hong Kong University of Science and Technology) · Zhigang Wang (Shanghai AI  Lab) · Bin Zhao (Northwest Polytechnical University Xi'an) · Xuelong Li (Northwestern Polytechnical University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Open-Vocabulary Semantic Segmentation with Image Embedding Balancing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiangheng Shan (Huazhong University of Science and Technology) · Dongyue Wu (None) · Guilin Zhu (Huazhong University of Science and Technology) · Yuanjie Shao (Huazhong University of Science and Technology) · Nong Sang (Huazhong University of Science and Technology) · Changxin Gao (Huazhong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>View-decoupled Transformer for Person Re-identification under Aerial-ground Camera Network</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Quan Zhang (SUN YAT-SEN UNIVERSITY) · Lei Wang (SUN YAT-SEN UNIVERSITY) · Vishal M. Patel (Johns Hopkins University) · Xiaohua Xie (SUN YAT-SEN UNIVERSITY) · Jianhuang Lai (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>EgoExoLearn: A Dataset for Bridging Asynchronous Ego- and Exo-centric View of Procedural Activities in Real World</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yifei Huang (The University of Tokyo) · Guo Chen (Nanjing University) · Jilan Xu (None) · Mingfang Zhang (None) · Lijin Yang (The University of Tokyo) · Baoqi Pei (Zhejiang University) · Hongjie Zhang (Shanghai Artificial Intelligence Laboratory) · Lu Dong (University of Science and Technology of China) · Yali Wang (SIAT, Chinese Academy of Sciences) · Limin Wang (Nanjing University) · Yu Qiao (Shanghai Aritifcal Intelligence Laboratory)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>VLP: Vision Language Planning for Autonomous Driving</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chenbin Pan (Syracuse University) · Burhaneddin Yaman (Bosch Center for Artificial Intelligence) · Tommaso Nesti (None) · Abhirup Mallik (Bosch) · Alessandro G Allievi (Bosch / University of Texas at Austin) · Senem Velipasalar (Syracuse University) · Liu Ren (Bosch Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DUSt3R: Geometric 3D Vision Made Easy</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuzhe Wang (Aalto University) · Vincent Leroy (Naver Labs Europe) · Yohann Cabon (Naver Labs Europe) · Boris Chidlovskii (Naver Labs Europe) · Jerome Revaud (Naver Labs Europe)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>InceptionNeXt: When Inception Meets ConvNeXt</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Weihao Yu (National University of Singapore) · Pan Zhou (Sea Group) · Shuicheng Yan (National University of Singapore, Department of Electrical and Computer Engineering) · Xinchao Wang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MultiPly: Reconstruction of Multiple People from Monocular Video in the Wild</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zeren Jiang (ETHZ - ETH Zurich) · Chen Guo (ETH Zurich) · Manuel Kaufmann (ETH Zurich) · Tianjian Jiang (None) · Julien Valentin (Microsoft) · Otmar Hilliges (None) · Jie Song (ETHZ - ETH Zurich)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MultiDiff: Consistent Novel View Synthesis from a Single Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Norman Müller (Meta) · Katja Schwarz (University of Tuebingen) · Barbara Roessle (Technische Universität München) · Lorenzo Porzi (Facebook) · Samuel Rota Bulò (Meta) · Matthias Nießner (Technical University of Munich) · Peter Kontschieder (Meta)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Dual Pose-invariant Embeddings: Learning Category and Object-specific Discriminative Representations for Recognition and Retrieval</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rohan Sarkar (Purdue University) · Avinash Kak (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Siamese Learning with Joint Alignment and Regression for Weakly-Supervised Video Paragraph Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaolei Tan (SUN YAT-SEN UNIVERSITY) · Jianhuang Lai (SUN YAT-SEN UNIVERSITY) · Wei-Shi Zheng (SUN YAT-SEN UNIVERSITY) · Jian-Fang Hu (SUN YAT-SEN UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Improving the Generalization of Segmentation Foundation Model under Distribution Shift via Weakly Supervised Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haojie Zhang (South China University of Technology) · Yongyi Su (South China University of Technology) · Xun Xu (A*STAR) · Kui Jia (South China University of Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>The Audio-Visual Conversational Graph: From an Egocentric-Exocentric Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenqi Jia (None) · Miao Liu (META AI) · Hao Jiang (Facebook) · Ishwarya Ananthabhotla (Meta Reality Labs Research) · James Rehg (None) · Vamsi Krishna Ithapu (Facebook Reality Labs) · Ruohan Gao (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TIGER: Time-Varying Denoising Model for 3D Point Cloud Generation with Diffusion Process</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhiyuan Ren (Michigan State University) · Minchul Kim (Michigan State University) · Feng Liu (Michigan State University) · Xiaoming Liu (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>MLP Can Be A Good Transformer Learner</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sihao Lin (Royal Melbourne Institute of Technology) · Pumeng Lyu (Shanghai AI Laboratory) · Dongrui Liu (None) · Tao Tang (SYSU) · Xiaodan Liang (Sun Yat-sen University) · Andy Song (Royal Melbourne Institute of Technology) · Xiaojun Chang (University of Technology Sydney)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhenyu Cui (Peking University) · Jiahuan Zhou (Peking University) · Xun Wang (ByteDance Inc) · Manyu Zhu (bytedance) · Yuxin Peng (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>InstantBooth: Personalized Text-to-Image Generation without Test-Time Finetuning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jing Shi (Adobe Systems) · Wei Xiong (Adobe Systems) · Zhe Lin (Adobe Research) · HyunJoon Jung (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards a Perceptual Evaluation Framework for Lighting Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Justine Giroux (Université Laval) · Mohammad Reza Karimi Dastjerdi (None) · Yannick Hold-Geoffroy (Adobe Research) · Javier Vazquez-Corral (Computer Vision Center / Autonomous University of Barcelona) · Jean-François Lalonde (Université Laval)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hongchi Xia (Shanghai Jiaotong University) · Yang Fu (University of California San Diego) · Sifei Liu (NVIDIA) · Xiaolong Wang (UCSD)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Aligning and Prompting Everything All at Once for Universal Visual Perception</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhang Shen (Tencent) · Chaoyou Fu (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Peixian Chen (Xiamen University) · Mengdan Zhang (Tencent Youtu Lab) · Ke Li (Tencent) · Xing Sun (Tencent YouTu Lab) · Yunsheng Wu (Tencent YouTu Lab) · Shaohui Lin (East China Normal University) · Rongrong Ji (Xiamen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DanceCamera3D: 3D Camera Movement Synthesis with Music and Dance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zixuan Wang (None) · Jia Jia (Department of Computer Science and Technology, Tsinghua University) · Shikun Sun (Tsinghua University, Tsinghua University) · Haozhe Wu (Tsinghua University, Tsinghua University) · Rong Han (Tsinghua University, Tsinghua University) · Zhenyu Li (Tsinghua University, Tsinghua University) · Di Tang (ByteDance) · Jiaqing Zhou (bytedance) · Jiebo Luo (University of Rochester)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OmniGlue: Generalizable Feature Matching with Foundation Model Guidance</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hanwen Jiang (University of Texas at Austin) · Arjun Karpur (Google Research) · Bingyi Cao (Google Research) · Qixing Huang (University of Texas at Austin) · André Araujo (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linfeng Yuan (Nanjing University of Science and Technology) · Miaojing Shi (King's College London) · Zijie Yue (Tongji University) · Qijun Chen (Tongji University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Diffusion-FOF: Single-view Clothed Human Reconstruction via Diffusion-based Fourier Occupancy Field</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuanzhen Li (Wuhan University) · Fei LUO (Wuhan University) · Chunxia Xiao (Wuhan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Leveraging Frame Affinity for sRGB-to-RAW Video De-rendering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chen Zhang (Sensetime) · Wencheng Han (University of Macau) · Yang Zhou (Sensetime Group) · Jianbing Shen (University of Macau) · Cheng-Zhong Xu (University of Macau) · Wentao Liu (Sensetime)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Joint2Human: High-quality 3D Human Generation via Compact Spherical Embedding of 3D Joints</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Muxin Zhang (Tianjin University) · Qiao Feng (None) · Zhuo Su (ByteDance) · Chao Wen (ByteDance) · Zhou Xue (Li Auto) · Kun Li (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Investigating Compositional Challenges in Vision-Language Models for Visual Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunan Zeng (None) · Yan Huang (Institute of automation, Chinese academy of science, Chinese Academy of Sciences) · Jinjin Zhang (Beihang University) · Zequn Jie (Meituan) · Zhenhua Chai (Meituan) · Liang Wang (CASIA)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Relightful Harmonization: Lighting-aware Portrait Background Replacement</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengwei Ren () · Wei Xiong (Adobe Systems) · Jae Shin Yoon (Adobe Systems) · Zhixin Shu (Adobe Systems) · Jianming Zhang (Adobe Systems) · HyunJoon Jung (Adobe Systems) · Guido Gerig (New York University) · He Zhang (Adobe Systems)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>eTraM: Event-based Traffic Monitoring Dataset</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Aayush Atul Verma (Arizona State University) · Bharatesh Chakravarthi (Arizona State University) · Arpitsinh Vaghela (None) · Hua Wei (Arizona State University) · 'YZ' Yezhou Yang (Arizona State University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Separating the "Chirp" from the "Chat": Self-supervised Visual Grounding of Sound and Language</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mark Hamilton (Massachusetts Institute of Technology) · Andrew Zisserman (University of Oxford) · John Hershey (Google) · William Freeman (MIT and Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PTT: Point-Trajectory Transformer for Efficient Temporal 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kuan-Chih Huang (University of California, Merced) · Weijie Lyu (University of California, Merced) · Ming-Hsuan Yang (University of California at Merced) · Yi-Hsuan Tsai (Google)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FakeInversion: Learning to Detect Images from Unseen Text-to-Image Models by Inverting Stable Diffusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            George Cazenavette (Massachusetts Institute of Technology) · Avneesh Sud (Google) · Thomas Leung (Google Inc) · Ben Usman (Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Overcoming Data Limitations for High-Quality Video Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haoxin Chen (Tencent AI Lab) · Yong Zhang (Tencent AI Lab) · Xiaodong Cun (Tencent AI Lab) · Menghan Xia (Tencent AI Lab) · Xintao Wang (Tencent) · CHAO WENG (Tencent AI Lab) · Ying Shan (Tencent)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>TextNeRF: A Novel Scene-Text Image Synthesis Method based on Neural Radiance Fields</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jialei Cui (Peking University) · Jianwei Du (Southeast University) · Wenzhuo Liu (China University of Mining Technology - Beijing) · Zhouhui Lian (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Accept the Modality Gap: An Exploration in the Hyperbolic Space</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sameera Ramasinghe (Amazon) · Violetta Shevchenko (Amazon) · Gil Avraham (Amazon) · Thalaiyasingam Ajanthan (Amazon)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MirageRoom: 3D Scene Segmentation with 2D Pre-trained Models by Mirage Projection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haowen Sun (Tsinghua University, Tsinghua University) · Yueqi Duan (None) · Juncheng Yan (None) · Yifan Liu (Tsinghua University, Tsinghua University) · Jiwen Lu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>GigaPose: Fast and Robust Novel Object Pose Estimation via One Correspondence</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Van Nguyen Nguyen (Ecole des Ponts ParisTech) · Thibault Groueix (Adobe Systems) · Mathieu Salzmann (EPFL) · Vincent Lepetit (Ecole des Ponts ParisTech)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>6D-Diff: A Keypoint Diffusion Framework for 6D Object Pose Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Xu (Singapore University of Technology and Design) · Haoxuan Qu (Singapore University of Technology and Design) · Yujun Cai (Meta) · Jun Liu ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/xiuqhou/Salience-DETR" target="_blank">Salience DETR: Enhancing Detection Transformer with Hierarchical Salience Filtering Refinement</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiuquan Hou (Xi'an Jiaotong University) · Meiqin Liu (None) · Senlin Zhang (Zhejiang University) · Ping Wei (None) · Badong Chen (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multi-Session SLAM using Wide-Baseline Optical Flow</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lahav Lipson (Princeton University) · Jia Deng (Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Polarization Wavefront Lidar: Learning Large Scene Reconstruction from Polarized Wavefronts</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dominik Scheuble (Universität des Saarlandes) · Chenyang Lei (The Hong Kong University of Science and Technology) · Mario Bijelic (Princeton University) · Seung-Hwan Baek (POSTECH) · Felix Heide (Department of Computer Science, Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Multi-Attribute Interactions Matter for 3D Visual Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Can Xu (Nanjing University of Science and Technology) · Yuehui Han (Nanjing University of Science and Technology) · Rui Xu (Nanjing University Of Science And Technology) · Le Hui (Nanjing University Of Science And Technology) · Jin Xie (Department of Computer Science, Nanjing University of Science and Technology) · Jian Yang (Nanjing University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Bootstrapping Autonomous Radars with Self-Supervised Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yiduo Hao (University of Cambridge) · Sohrab Madani (UIUC) · Junfeng Guan (EPFL - EPF Lausanne) · Mo Alloulah (RadarEye) · Saurabh Gupta (University of Illinois, Urbana Champaign) · Haitham Al Hassanieh (University of Illinois at Urbana-Champaign)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>CAD: Photorealistic 3D Generation via Adversarial Distillation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziyu Wan (City University of Hong Kong) · Despoina Paschalidou (Stanford) · Ian Huang (Computer Science Department, Stanford University) · Hongyu Liu (Hong Kong University of Science and Technology) · Bokui Shen (Stanford University) · Xiaoyu Xiang (Meta) · Jing Liao (City University of Hong Kong) · Leonidas Guibas (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DiffusionTrack:  Point Set Diffusion Model for Visual Object Tracking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fei Xie (None) · Zhongdao Wang (Huawei Technologies Ltd.) · Chao Ma (Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Decoupled Pseudo-labeling in Semi-Supervised Monocular 3D Object Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiacheng Zhang (SUN YAT-SEN UNIVERSITY) · Jiaming Li (Baidu) · Xiangru Lin (Baidu) · Wei Zhang (Baidu) · Xiao Tan (Baidu) · Junyu Han (Baidu) · Errui Ding (Baidu Inc.) · Jingdong Wang (Baidu) · Guanbin Li (Sun Yat-sen University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SpiderMatch: 3D Shape Matching with Global Optimality and Geometric Consistency</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Paul Roetzer (University of Bonn) · Florian Bernard (University of Bonn)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Better Vision-Inspired Vision-Language Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yun-Hao Cao (Nanjing University) · Kaixiang Ji (Ant Group) · Ziyuan Huang (National University of Singapore) · Chuanyang Zheng (Ant Group) · Jiajia Liu (Alibaba Group) · Jian Wang (, Institute of automation, Chinese academy of science) · Jingdong Chen (Ant Group) · Ming Yang (Ant Group)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Gated Fields: Learning Scene Reconstruction from Gated Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Andrea Ramazzina (Saarland University, Universität des Saarlandes) · Stefanie Walz (Mercedes-Benz AG) · Pragyan Dahal (Polytechnic Institute of Milan) · Mario Bijelic (Princeton University) · Felix Heide (Department of Computer Science, Princeton University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/zyxia1009/CVPR2024-TSPNet" target="_blank">Realigning Confidence with Temporal Saliency Information for Point-Level Weakly-Supervised Temporal Action Localization</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ziying Xia () · Jian Cheng (University of Electronic Science and Technology of China) · Siyu Liu () · Yongxiang Hu (University of Electronic Science and Technology of China) · Shiguang Wang (None) · Zhang Yijie (University of Electronic Science and Technology of China) · Wanli Dang (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Generative Quanta Color Imaging</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Vishal Purohit (Purdue University) · Junjie Luo (Purdue University) · Yiheng Chi (Purdue University) · Qi Guo (Purdue University) · Stanley H. Chan (Purdue University, USA) · Qiang Qiu (Purdue University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ZeroNVS: Zero-Shot 360-Degree View Synthesis from a Single Image</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kyle Sargent (Computer Science Department, Stanford University) · Zizhang Li (Zhejiang University) · Tanmay Shah (Google) · Charles Herrmann (Google) · Hong-Xing Yu (Computer Science Department, Stanford University) · Yunzhi Zhang (Stanford University) · Eric Ryan Chan (Stanford University) · Dmitry Lagun (Google) · Li Fei-Fei (Stanford University) · Deqing Sun (Google) · Jiajun Wu (Stanford University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zijin Yang (None) · Kai Zeng (University of Science and Technology of China) · Kejiang Chen (University of Science and Technology of China) · Han Fang (National University of Singapore) · Weiming Zhang (University of Science and Technology of China) · Nenghai Yu (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Training Like a Medical Resident: Context-Prior Learning Toward Universal Medical Image Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yunhe Gao (Rutgers University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://www.infinitescript.com/project/city-dreamer" target="_blank">CityDreamer: Compositional Generative Model of Unbounded 3D Cities</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Haozhe Xie (Nanyang Technological University) · Zhaoxi Chen (Nanyang Technological University) · Fangzhou Hong (Nanyang Technological University) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Noisy-Correspondence Learning for Text-to-Image Person Re-identification</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yang Qin (Sichuan University) · Yingke Chen (Northumbria University) · Dezhong Peng (Sichuan University) · Xi Peng (Sichuan University) · Joey Tianyi Zhou (National University of Singapore ) · Peng Hu (Sichuan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ParamISP: Learned Forward and Inverse ISPs using Camera Parameters</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Woohyeok Kim (POSTECH) · Geonu Kim (POSTECH) · Junyong Lee (None) · Seungyong Lee (POSTECH) · Seung-Hwan Baek (POSTECH) · Sunghyun Cho (POSTECH)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Structured Model Probing: Empowering Efficient Transfer Learning by Structured Regularization</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhi-Fan Wu (Alibaba DAMO Academy) · Chaojie Mao (Alibaba Group) · Xue Wang (Pennsylvania State University) · Jianwen Jiang (Alibaba DAMO Academy) · Yiliang Lv (Gientech AIL) · Rong Jin (Twitter)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Instance-aware Contrastive Learning for Occluded Human Mesh Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mi-Gyeong Gwon (Konkuk University) · Gi-Mun Um (Electronics and Telecommucations Research Institute) · Won-Sik Cheong (Electronics and Telecommunications Research Institute) · Wonjun Kim (Konkuk University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>SurroundSDF: Implicit 3D Scene Understanding Based on Signed Distance Field</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Lizhe Liu (Xiaomi) · Bohua Wang (Xi'an Jiaotong University) · Hongwei Xie (Xiaom EV) · Daqi Liu (Xiaomi) · Li Liu (None) · Kuiyuan Yang (DeepMotion) · Bing Wang (Alibaba Group) · Zhiqiang Tian (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>WALT3D: Generating Realistic Training Data from Time-Lapse Imagery for Reconstructing Dynamic Objects under Occlusion</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Khiem Vuong (Carnegie Mellon University) · N. Dinesh Reddy (Carnegie Mellon University) · Robert Tamburo (Carnegie Mellon University) · Srinivasa G. Narasimhan (Carnegie Mellon University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://github.com/muz1lee/MOTdata/tree/main" target="_blank">Data Valuation and Detections in Federated Learning</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenqian Li (National University of Singapore) · Shuran Fu (National University of Singapore) · Fengrui Zhang (Rutgers University) · Yan Pang (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>UnO: Unsupervised Occupancy Fields for Perception and Forecasting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ben Agro (Waabi) · Quinlan Sykora (Waabi) · Sergio Casas (Waabi) · Thomas Gilles (Waabi) · Raquel Urtasun (Waabi)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DITTO: Dual and Integrated Latent Topologies for Implicit 3D Reconstruction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jaehyeok Shim (UNIST) · Kyungdon Joo (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Unveiling the Unknown: Unleashing the Power of Unknown to Known in Open-Set Source-Free Domain Adaptation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Fuli Wan (Xi'an University of Electronic Science and Technology) · Han Zhao (Xidian University) · Xu Yang (Xi'an University of Electronic Science and Technology) · Cheng Deng (Xidian University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AutoAD III: The Prequel -- Back to the Pixels</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tengda Han (University of Oxford, University of Oxford) · Max Bain (VGG, University of Oxford) · Arsha Nagrani (Google ) · Gül Varol (Ecole des Ponts ParisTech) · Weidi Xie (Shanghai Jiaotong University) · Andrew Zisserman (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Towards More Accurate Diffusion Model Acceleration with A Timestep Aligner</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Mengfei Xia (Tsinghua University, Tsinghua University) · Yujun Shen (The Chinese University of Hong Kong) · Changsong Lei (Tsinghua University, Tsinghua University) · Yu Zhou (Tsinghua University, Tsinghua University) · Deli Zhao (Alibaba Group) · Ran Yi (Shanghai Jiao Tong University) · Wenping Wang (Texas A&amp;M University - College Station) · Yong-Jin Liu (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sagnik Majumder (UT Austin &amp; Meta AI) · Ziad Al-Halah (University of Utah) · Kristen Grauman (University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Diversity-aware Channel Pruning for StyleGAN Compression</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jiwoo Chung (Sungkyunkwan University) · Sangeek Hyun (Sungkyunkwan University) · Sang-Heon Shim (Sungkyunkwan University) · Jae-Pil Heo (Sungkyunkwan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Linshan Wu (Hunan University) · Jia-Xin Zhuang (Department of Computer Science and Engineering, Hong Kong University of Science and Technology) · Hao Chen (The Hong Kong University of Science and Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>SimAC: A Simple Anti-Customization Method against Text-to-Image Synthesis of Diffusion Models</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feifei Wang (University of Science and Technology of China) · Zhentao Tan (Alibaba DAMO Academy; University of Science and Technology of China) · Tianyi Wei (None) · Yue Wu (Alibaba Group) · Qidong Huang (University of Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RobustSAM: Segment Anything Robustly on Degraded Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wei-Ting Chen (National Taiwan University) · Yu Jiet Vong (National Taiwan University) · Sy-Yen Kuo (National Taiwan University) · Sizhuo Ma (Snap Inc.) · Jian Wang (Snap Inc.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Learned Trajectory Embedding for Subspace Clustering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yaroslava Lochman (Chalmers University of Technology) · Christopher Zach (Chalmers University) · Carl Olsson (Lund University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xian Liu (The Chinese University of Hong Kong) · Xiaohang Zhan (Tencent) · Jiaxiang Tang (Baidu) · Ying Shan (Tencent) · Gang Zeng (Peking University) · Dahua Lin (The Chinese University of Hong Kong) · Xihui Liu (The University of Hong Kong) · Ziwei Liu (Nanyang Technological University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Rethinking Inductive Biases for Surface Normal Estimation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Gwangbin Bae (Imperial College London) · Andrew J. Davison (Imperial College London)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Total-Decom: Decomposed 3D Scene Reconstruction with Minimal Interaction</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaoyang Lyu (University of Hong Kong) · Chirui Chang (None) · Peng Dai (None) · Yangtian Sun (None) · Xiaojuan Qi (University of Oxford)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Dynamic Prompt Optimizing for Text-to-Image Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Wenyi Mo (Renmin University of China) · Tianyu Zhang (Du Xiaoman Financial) · Yalong Bai (JD AI Research) · Bing Su (None) · Ji-Rong Wen (Renmin University of China) · Qing Yang (Du Xiaoman Technology(BeiJing))
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Hallucination Augmented Contrastive Learning for Multimodal Large Language Model</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Chaoya Jiang (Peking University) · Haiyang Xu (Alibaba Group) · Mengfan Dong (Peking University) · Jiaxing Chen (Peking University) · Wei Ye (Peking University) · Ming Yan (Alibaba Group) · Qinghao Ye (Alibaba Group) · Ji Zhang (Alibaba Group) · Fei Huang (Alibaba Group) · Shikun Zhang (Peking University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <a href="https://github.com/Becomebright/GroundVQA" target="_blank">Grounded Question-Answering in Long Egocentric Videos</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shangzhe Di (Shanghai Jiao Tong University) · Weidi Xie (Shanghai Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <a href="https://ykdai.github.io/projects/InclusionMatching" target="_blank">Learning Inclusion Matching for Animation Paint Bucket Colorization</a>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuekun Dai (Nanyang Technological University) · Shangchen Zhou (Nanyang Technological University) · Blake Li (Nanyang Technological University) · Chongyi Li () · Chen Change Loy (NANYANG TECHNOLOGICAL UNIVERSITY)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yixuan Zhu (Tsinghua University) · Ao Li (Tsinghua University) · Yansong Tang (Tsinghua University) · Wenliang Zhao (Automation, Tsinghua University, Tsinghua University) · Jie Zhou (None) · Jiwen Lu (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Pose-Guided Self-Training with Two-Stage Clustering for Unsupervised Landmark Discovery</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Siddharth Tourani (MBZUAI) · Ahmed Alwheibi (Mohamed bin Zayed University of Artificial Intelligence) · Arif Mahmood (Information Technology University, Lahore) · Muhammad Haris Khan (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PromptAD: Learning Prompts with only Normal Samples for Few-Shot Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xiaofan Li (East China Normal University) · Zhizhong Zhang (East China Normal University) · Xin Tan (East China Normal University) · Yanyun Qu (Xiamen University) · Chengwei Chen (2nd Military Medical University) · Yuan Xie (East China Normal University) · Lizhuang Ma (Dept. of Computer Sci. &amp; Eng., Shanghai Jiao Tong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RepViT: Revisiting Mobile CNN From ViT Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ao Wang (Tsinghua University, Tsinghua University) · Hui Chen (Tsinghua University, Tsinghua University) · Zijia Lin (Kuaishou Technology) · Jungong Han (Aberystwyth University) · Guiguang Ding (Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Simple Semantic-Aided Few-Shot Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Hai Zhang (Sichuan University) · Junzhe Xu (None) · Shanlin Jiang (The University of Texas at Dallas) · Zhenan He (Sichuan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>OVMR: Open-Vocabulary Recognition with Multi-Modal References</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zehong Ma (Peking University) · Shiliang Zhang (Peking University) · Longhui Wei (Huawei Cloud Technologies Ltd.) · Qi Tian (Huawei Technologies Ltd.)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>An edit friendly ddpm noise space: inversion and manipulations</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Inbar Huberman-Spiegelglas (Technion - Israel Institute of Technology, Technion - Israel Institute of Technology) · Vladimir Kulikov (Technion - Israel Institute of Technology, Technion - Israel Institute of Technology) · Tomer Michaeli (Technion)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>AdaShift: Learning Discriminative Self-Gated Neural Feature Activation With an Adaptive Shift Factor</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Sudong Cai (Kyoto University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Improved Implicit Neural Representation with Fourier Reparameterized Training</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Kexuan Shi (None) · Xingyu Zhou (University of Electronic Science and Technology of China) · Shuhang Gu (University of Electronic Science and Technology of China)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>U-VAP: User-specified Visual Appearance Personalization via Decoupled Self Augmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            You Wu (Institute of Computing Technology, CAS) · Kean Liu (University of the Chinese Academy of Sciences) · Xiaoyue Mi (None) · Fan Tang (Institute of Computing Technology, CAS) · Juan Cao (Institute of Computing Technology, Chinese Academy of Sciences) · Jintao Li (Institute of Computing Technology, Chinese Academy of Sciences)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>DaReNeRF: Direction-aware Representation for Dynamic Scenes</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Ange Lou (Vanderbilt University) · Benjamin Planche (United Imaging Intelligence) · Zhongpai Gao (United Imaging Intelligence) · Yamin Li (Vanderbilt University) · Tianyu Luan (State University of New York at Buffalo) · Hao Ding (Johns Hopkins University) · Terrence Chen (United Imaging Intelligence) · Jack Noble (Vanderbilt University) · Ziyan Wu (United Imaging Intelligence)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>RoMa: Robust Dense Feature Matching</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Johan Edstedt (Computer Vision Laboratory, Linköping University) · Qiyu Sun (East China University of Science and Technology) · Georg Bökman (Chalmers University of Technology) · Mårten Wadenbäck (Linköping University) · Michael Felsberg (Linköping University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Federated Online Adaptation for Deep Stereo</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Matteo Poggi (Università di Bologna) · Fabio Tosi (University of Bologna)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Jianjian Cao (Fudan University) · Peng Ye (Fudan University) · Shengze Li (Fudan University) · Chong Yu (Fudan University    NVIDIA Corporation) · Yansong Tang (Tsinghua University) · Jiwen Lu (Tsinghua University) · Tao Chen (Fudan University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>COCONut: Modernizing COCO Segmentation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xueqing Deng (ByteDance Research) · Qihang Yu (Johns Hopkins University) · Peng Wang (Bytedance US AILab) · Xiaohui Shen (ByteDance) · Liang-Chieh Chen (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Towards Automated Movie Trailer Generation</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Dawit Argaw Argaw (None) · Mattia Soldan (None) · Alejandro Pardo (KAUST) · Chen Zhao (King Abdullah University of Science and Technology (KAUST)) · Fabian Caba Heilbron (Adobe Research) · Joon Chung (KAIST) · Bernard Ghanem (KAUST)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>How to Configure Good In-Context Sequence for Visual Question Answering</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Li Li (Southeast University) · Jiawei Peng (Southeast University) · huiyi chen (Southeast University - Monash University Joint Graduate School (Suzhou)) · Chongyang Gao (Northwestern University) · Xu Yang (Southeast University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Feng Liang (The University of Texas at Austin) · Bichen Wu (Facebook) · Jialiang Wang (Facebook) · Licheng Yu (None) · Kunpeng Li (Meta) · Yinan Zhao (Facebook) · Ishan Misra (Facebook) · Jia-Bin Huang (University of Maryland, College Park) · Peizhao Zhang (Facebook) · Peter Vajda (Facebook) · Diana Marculescu (The University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Capturing Closely Interacted Two-Person Motions with Reaction Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qi Fang (NetEase) · Yinghui Fan () · Yanjun Li (None) · Junting Dong (None) · Dingwei Wu (NetEase, Inc.) · Weidong Zhang (Netease Games AI Lab) · Kang Chen ()
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>ReconFusion: 3D Reconstruction with Diffusion Priors</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Rundi Wu (Columbia University) · Ben Mildenhall (Google) · Philipp Henzler (Google) · Ruiqi Gao (Google) · Keunhong Park (Google) · Daniel Watson (Google DeepMind) · Pratul P. Srinivasan (Google Research) · Dor Verbin (None) · Jonathan T. Barron (Google) · Ben Poole (Google) · Aleksander Holynski (UC Berkeley &amp; Google Research)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PredToken: Predicting Unknown Tokens and Beyond with Coarse-to-Fine Iterative Decoding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Xuesong Nie () · Haoyuan Jin (Zhejiang University) · Yunfeng Yan (Zhejiang University) · Xi Chen (the University of Hong Kong, University of Hong Kong) · Zhihang Zhu (Zhejiang University) · Donglian Qi (Zhejiang University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Learning Object State Changes in Videos: An Open-World Perspective</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zihui Xue (None) · Kumar Ashutosh (UT Austin &amp; FAIR, Meta) · Kristen Grauman (University of Texas at Austin)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame for 4D Medical Images</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            JungEun Kim (Korea Advanced Institute of Science and Technology) · Hangyul Yoon (Korea Advanced Institute of Science and Technology (KAIST)) · Geondo Park (Korea Advanced Institute of Science and Technology) · Kyungsu Kim (Harvard Medical School and Massachusetts General Hospital) · Eunho Yang (Korea Advanced Institute of Science &amp; Technology)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Step differences in instructional video</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Tushar Nagarajan (Meta) · Lorenzo Torresani (Facebook)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>PNeRV: Enhancing Spatial Consistency via Pyramidal Neural Representation for Videos</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Qi Zhao (None) · M. Salman Asif (University of California, Riverside) · Zhan Ma (Nanjing University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>G<span class="MathJax_Preview" style="color: inherit; display: none;"></span><span id="MathJax-Element-30-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" style="font-size: 120%; position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msup&gt;&lt;mi&gt;&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msup&gt;&lt;/math&gt;" role="presentation"><span id="MJXc-Node-174" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-175" class="mjx-mrow"><span id="MJXc-Node-176" class="mjx-msubsup"><span class="mjx-base"><span id="MJXc-Node-177" class="mjx-mi"><span class="mjx-char"></span></span></span><span class="mjx-sup" style="font-size: 70.7%; vertical-align: 0.513em; padding-left: 0px; padding-right: 0.071em;"><span id="MJXc-Node-178" class="mjx-mn" style=""><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.378em; padding-bottom: 0.378em;">3</span></span></span></span></span></span><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi></mi><mn>3</mn></msup></math></span></span><script type="math/tex" id="MathJax-Element-30">^3</script>-LQ: Marrying Hyperbolic Alignment with Explicit Semantic-Geometric Modeling for 3D Visual Grounding</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuan Wang (None) · Yali Li (Tsinghua University) · Shengjin Wang (Tsinghua University, Tsinghua University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>NightCC: Nighttime Color Constancy via  Adaptive Channel Masking</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Shuwei Li (national university of singaore, National University of Singapore) · Robby T. Tan (National University of Singapore)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>DYSON: Dynamic Feature Space Self-Organization for Online Task-Free Class Incremental Learning</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Yuhang He (Xi'an Jiaotong University) · YingJie Chen (Xi'an Jiaotong University) · Yuhan Jin (Xi'an Jiaotong University) · Songlin Dong (Xi'an Jiaotong University) · Xing Wei (None) · Yihong Gong (Xi'an Jiaotong University)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>Harnessing Large Language Models for Training-free Video Anomaly Detection</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Luca Zanella (University of Trento) · Willi Menapace (University of Trento) · Massimiliano Mancini (University of Trento) · Yiming Wang (Fondazione Bruno Kessler) · Elisa Ricci (University of Trento)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr style="background-color: #f3f3f3">

            <td>
                <strong>Total Selfie: Generating Full-Body Selfies</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Bowei Chen (University of Washington) · Brian Curless (University of Washington) · Ira Kemelmacher-Shlizerman (University of Washington) · Steve Seitz (University of Washington)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
        <tr>

            <td>
                <strong>ODCR: Orthogonal Decoupling Contrastive Regularization for Unpaired Image Dehazing</strong>
            
                <br>
                <div class="indented">
                    <i>
                        
                            Zhongze Wang (East China University of Science and Technology) · Haitao Zhao (East China University of Science and Technology) · Jingchao Peng (East China University of Science and Technology) · Lujian Yao (East China University of Science and Technology) · Kaijie Zhao (None)
                        
                    </i>
                </div>
            </td>

            <td>

                
            </td>
            <td>
                
            </td>
        </tr>
    
</tbody></table>
<p></p>

<p><br>
&nbsp;</p>
</div>
</div>
        
    </div>



    
        </div>
    

</main>
<!--END BLOCK CONTENT-->


<!--Footer for the edit button-->


<script>

    $(function () {
        if ($(".editable").length == 0) {
            $("#editFooter").hide();
        }
    })
</script>

<script src="./2024_files/fastclick.min.js.下载" type="text/javascript"></script>

<!--We don't know if there are editable tags on the page until after the django template engine has rendered the page. So,
test in javascript for "editable" tags and if present, load the ckeditor engine dynamically. -->

<script>
  if (document.getElementsByClassName('editable').length > 0) {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "/static/core/ckeditor/4.18/ckeditor.js";    // use this for linked script
    script.text = "alert('voila!');"               // use this for inline script
    document.body.appendChild(script);
  }

</script>


<script>
  function fetchContent() {
    $(".editable").each(function (index) {
    var myself = this;
    var docvID = this.getAttribute('documentversion');
    var blurbtext = this.getAttribute("blurbtext");
    $.ajax({
       url: "/Admin/RetrieveDocumentVersion",
       type:"POST",
       data:{
           docvID : docvID,
           blurbtext : blurbtext,
           csrfmiddlewaretoken: csrftoken,
       },
       success: function(data, textStatus, jqXHR) {
           myself.setAttribute("contenteditable", "true");
           myself.innerHTML = data;
           CKEDITOR.inline(myself.id);
       },
    });
  })
}

$("#nopageedit").hide();
function start_edit(){

  $(".editable").addClass("warning-ring");

  //At the beginning of an edit, we need to replace the content of the
  //editable div with it's databased content in order to preserve the
  //template tags. We want the tag, not the rendered tag.

  /* You must remove any countdown.js timers on the page before replacing the page with it's
  document version otherwise, Javascript will throw an exception.  */


  $("[class$='-countdown']").parent().remove();
  fetchContent();
  $(".editable").attr("onblur", "ckeditorsave(this)");
  window.status.bold();
  window.status = "Click outside the editable area to save. Changes are LIVE!! Refresh page to discard changes.";
  $("#editpage").hide();
  $("#noeditpage").show();
}


  function stop_edit() {
    ckeditorsave();
    $("#noeditpage").hide();
    $("#editpage").show();
    window.location.reload();
}
function ckeditorsave(event){
  for (var name in CKEDITOR.instances){
    if ( CKEDITOR.instances[name].checkDirty() ){
      editor = CKEDITOR.instances[name];
      saveEditable(editor);
    }
  }
}

function saveEditable(editor){
  var content = editor.getData();
  var contentId = editor.name;
  var pageId = window.location.pathname;
  var originalContent = "N/A";
  var documentversion = editor.container.getAttribute("documentversion");
  var blurbtext = editor.container.getAttribute("blurbtext");
  if ( contentId.match(/-aloha$/gi) ) {
    contentId = contentId.replace( /-aloha/gi, '' );
  }  /*I'm not sure what this does but it seems like it would matter*/
  var request = jQuery.ajax({
    url: "/Admin/SaveDocument",
    type: "POST",
    async: false,
    data: {
      content : content,
      originalContent: originalContent,
      contentId : contentId,
      pageId : pageId,
      documentversion:documentversion,
      blurbtext : blurbtext,
      csrfmiddlewaretoken: csrftoken
    },
    success: function(data){
        if (data['message']){
            alert(data['message']);
        }
    },
    error: function(xqXHR, textStatus){
        window.status = textStatus;
        debugger;
    }

  });

};




</script>

<script type="text/javascript">
       jQuery(document).ajaxSend(function(event, xhr, settings) {
           function getCookie(name) {
               var cookieValue = null;
               if (document.cookie && document.cookie != '') {
                   var cookies = document.cookie.split(';');
                   for (var i = 0; i < cookies.length; i++) {
                       var cookie = jQuery.trim(cookies[i]);
                       // Does this cookie string begin with the name we want?
                       if (cookie.substring(0, name.length + 1) == (name + '=')) {
                           cookieValue = decodeURIComponent(cookie.substring(name.length + 1));
                           break;
                       }
                   }
               }
               return cookieValue;
           }
           function sameOrigin(url) {
               // url could be relative or scheme relative or absolute
               var host = document.location.host; // host + port
               var protocol = document.location.protocol;
               var sr_origin = '//' + host;
               var origin = protocol + sr_origin;
               // Allow absolute or scheme relative URLs to same origin
               return (url == origin || url.slice(0, origin.length + 1) == origin + '/') ||
                   (url == sr_origin || url.slice(0, sr_origin.length + 1) == sr_origin + '/') ||
                   // or any other URL that isn't scheme relative or absolute i.e relative.
                   !(/^(\/\/|http:|https:).*/.test(url));
           }
           function safeMethod(method) {
               return (/^(GET|HEAD|OPTIONS|TRACE)$/.test(method));
           }

           if (!safeMethod(settings.type) && sameOrigin(settings.url)) {
               xhr.setRequestHeader("X-CSRFToken", getCookie('csrftoken'));
           }
       });
</script>





<div id="successful-page-load" class="hidden">Successful Page Load</div>




    <script src="./2024_files/settings.js.下载"></script>
    <link rel="stylesheet" type="text/css" href="./2024_files/cookieconsent.min.css">
    <script src="./2024_files/cookieconsent.min.js.下载"></script>
    <script>
        window.addEventListener("load", function () {
            window.cookieconsent.initialise(json)
        });
    </script>






<br>
<footer id="bootstrap-footer" class="text-center text-lg-start bg-light text-muted">

    <div class="text-center p-1 border-top border-dark">
    </div>
    <!-- Section: Links  -->
    <section class="pt-1">
        <div class="container text-center text-md-start mt-2">
            <!-- Grid row -->
            <div class="row mt-3">
                <!-- Grid column -->
                <div class="col-md-3 col-lg-3 col-xl-3 mx-auto mb-3">
                    <!-- Content -->
                    <h6 class="text-uppercase fw-bold mb-4">
                        <img src="./2024_files/CVPR-logo.svg" alt="CVPR logo" height="30px">
                    </h6>
                    <p>
                        The CVPR Logo above may be used on presentations. Right-click and choose
                        download. It is a vector graphic and may be used at any scale.
                    </p>

                </div>


                <!-- Grid column -->
                <div class="col-md-5 col-lg-4 col-xl-3 mx-auto mb-4" style="max-width: 300px;">
                    <!-- Links -->
                    <h6 class="text-uppercase fw-bold mb-4 text-center">
                        Useful links
                    </h6>
                    <div>
             <table class="table table-borderless">
	<tbody>
		<tr>
			<td><img alt="IEEE Logo" src="./2024_files/ieee_cs_.svg" style="height:20px; width:66px"></td>
			<td><a href="https://www.computer.org/">IEEE Computer Society</a></td>
		</tr>
		<tr>
			<td><img alt="" src="./2024_files/CVF-transparent-background.svg" style="height:20px; width:31px"></td>
			<td><a href="https://www.thecvf.com/">The Computer Vision Foundation</a></td>
		</tr>
	</tbody>
</table>

            </div>
                </div>
                <!-- Grid column -->

                <!-- Grid column -->
                
                <!-- Grid column -->
            </div>
            <!-- Grid row -->
        </div>
    </section>
    <!-- Section: Links  -->

    <!-- Copyright -->
    <div class="text-center p-4" style="background-color: rgba(0, 0, 0, 0.05);">
        <div>
             <p><a href="https://openaccess.thecvf.com/menu">CVF Proceedings</a></p>
            </div>
    </div>
    <!-- Copyright -->
</footer>
<!-- Footer -->

<!-- Footer -->



</body></html>