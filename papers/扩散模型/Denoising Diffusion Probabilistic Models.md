---
论文链接: https://arxiv.org/pdf/2006.11239.pdf
---
## 摘要

我们利用扩散概率模型提供了高质量的图像合成结果，这是一类受到非平衡热力学考虑启发的潜变量模型。我们的最佳结果是通过在根据扩散概率模型和带有 Langevin 动力学的去噪分数匹配之间的新颖联系设计的加权变分界进行训练而获得的，我们的模型自然地采用了一种渐进性的有损解压缩方案，可以解释为自回归解码的一种泛化。在无条件的 CIFAR10 数据集上，我们获得了 9.46 的 Inception 分数和 3.17 的最先进的 FID 分数。在 256x256 的 LSUN 数据集上，我们获得了与 ProgressiveGAN 类似的样本质量。我们的实现可在 https://github.com/hojonathanho/diffusion 找到。

## 1 Introduction

最近，各种类型的深度生成模型在各种数据模态中展示出了高质量的样本。生成对抗网络（GANs）、自回归模型、流模型和变分自动编码器（VAEs）已经合成了引人注目的图像和音频样本，而能量基模型和分数匹配方面的显著进展已经产生了与GAN相媲美的图像。

本文介绍了扩散概率模型的进展。扩散概率模型（简称为“扩散模型”）是使用变分推断训练的参数化马尔可夫链，经过有限时间产生与数据匹配的样本。该链的转移被学习为逆转扩散过程，即逐渐向数据添加噪声直到信号被破坏的马尔可夫链，其方向与采样相反。当扩散由小量高斯噪声组成时，将采样链的转移设置为条件高斯也就足够了，从而允许一种特别简单的神经网络参数化。

扩散模型易于定义且训练高效，但据我们所知，尚未证明它们能够生成高质量的样本。我们展示了扩散模型实际上能够生成高质量的样本，有时甚至优于其他类型生成模型的已发表结果。此外，我们展示了一种扩散模型参数化与训练期间多个噪声水平上的去噪分数匹配和退火 Langevin 动力学等价的关系。我们使用这种参数化获得了最佳的样本质量结果，因此我们认为这种等价性是我们的主要贡献之一。尽管样本质量很高，但与其他基于似然的模型相比，我们的模型的对数似然性并不具有竞争力（然而，我们的模型的对数似然性比已报道的基于能量模型和分数匹配的大估计要好）。我们发现，我们大多数模型的无损编码长度被用于描述不可察觉的图像细节。我们用无损压缩的语言对这一现象进行了更精细的分析，并展示了扩散模型的采样过程是一种类似自回归解码的渐进解码，其比特排序极大地推广了通常与自回归模型可能的范围。

## 2 Background

扩散模型是一种潜变量模型，形式为 pθ(x0) := ∫ pθ(x0:T) dx1:T ，其中 $\textbf{x}_1, \cdots, \textbf{x}_T$ 是与数据 $\textbf{x}_0 \sim q(\textbf{x}_0)$ 具有相同维度的潜变量。联合分布 $p_\theta(\textbf{x}_0, \cdots, \textbf{x}_T)$ 的求解被称为逆向过程，并被定义为具有从 $\textbf{X}_T \sim \mathcal{N}(\textbf{0}, \textbf{I})$ 开始的学习高斯转移的马尔可夫链：pθ(x0:T) := p(xT) ∏ᵀₜ₌₁ pθ(xₜ₋₁|xₜ)，pθ(xₜ₋₁|xₜ) := N(xₜ₋₁; µθ(xₜ, t), Σθ(xₜ, t))。

扩散模型与其他类型的潜变量模型的区别在于，称为正向过程或扩散过程的近似后验 q(x₁:T|x₀) 被固定为一个马尔可夫链，根据方差调度 β₁, . . . , βT 逐渐向数据添加高斯噪声：q(x₁:T|x₀) := ∏ᵀₜ₌₁ q(xₜ|xₜ₋₁)，q(xₜ|xₜ₋₁) := N(xₜ; p(1 − βt)xₜ₋₁, βtI)。

训练通过优化对数似然的通常变分界进行，前向过程的方差 βt 可以通过重参数化进行学习，也可以作为超参数保持不变，反向过程的表现力部分由于选择了 pθ(xₜ₋₁|xₜ) 中的高斯条件而得以确保，因为当 βt 较小时，两个过程具有相同的函数形式。前向过程的一个显著特性是它能够以封闭形式在任意时间步长 t 进行采样：使用符号 αt := 1 − βt 和 α¯t := ∏ᵗₛ₌₁ αs，我们有 q(xt|x₀) = N(xt; √α¯tx₀, (1 − α¯t)I)。

因此，可以通过使用随机梯度下降法优化损失函数 L 的随机项来实现高效训练。进一步的改进来自于方差减少，方法是将 L (3) 重写为：

等式  DKL(q(xT |x0) k p(xT )) | {z } LT + X t>1 DKL(q(xt−1|xt, x0) k pθ(xt−1|xt)) | {z } Lt−1 − log pθ(x0|x1) | {z } L0  (5)

（详情参见附录 A。术语将在第 3 节使用。）等式 (5) 使用 KL 散度直接比较 pθ(xt−1|xt) 和前向过程的后验分布，后者在给定 x0 时易于计算：

q(xt−1|xt, x0) = N (xt−1; µ˜t (xt, x0), β˜ tI), (6)

其中 µ˜t (xt, x0) := √ α¯t−1βt (1 − α¯t) x0 + √ αt(1 − α¯t−1) (1 − α¯t) xt 和 β˜ t := 1 − α¯t−1 (1 − α¯t) βt (7)

因此，等式 (5) 中的所有 KL 散度都是高斯分布之间的比较，因此可以用 Rao-Blackwellized 方式计算，并具有封闭形式的表达式，而不是方差较高的蒙特卡罗估计。

## 3 Diffusion models and denoising autoencoders

扩散模型看起来可能只是一类受限的潜在变量模型，但它们在实现上允许大量的自由度。我们需要选择前向过程的方差 βt、模型架构以及反向过程的高斯分布参数化。为了指导我们的选择，我们建立了扩散模型和去噪得分匹配之间一个新的显式联系（第 3.2 节），这导致了扩散模型简化后的加权变分界目标（第 3.4 节）。最终，我们的模型设计通过简单性和实证结果来证明 (第 4 节)。我们的讨论以等式 (5) 的术语为分类依据。

### 3.1 前向过程和 LT

我们忽略了可以通过重新参数化学习前向过程方差 βt 的事实，而是将它们固定为常数（详情见第 4 节）。因此，在我们的实现中，近似后验 q 没有可学习的参数，所以 LT 在训练过程中是一个常数，可以忽略。

### 3.2 反向过程和 L1:T-1

现在我们讨论我们在 pθ(xt-1|xt) = N (xt-1; µθ (xt, t), Σθ(xt, t)) 中的选择，其中 1 < t ≤ T。首先，我们将 Σθ(xt, t) 设置为 σ^2_t I，使其成为未经训练的与时间相关的常数。实验表明，σ^2_t = βt 和 σ^2_t = β˜_t = 1 - α¯_(t-1) / (1 - α¯_t) βt 都取得了类似的结果。第一个选择对于 x0 ∼ N(0, I) 是最优的，第二个选择对于确定性地设置为一个点的 x0 是最优的。对于具有单位方差的坐标数据的反向过程熵的上限和下限，这两个是对应的两个极端选择 [53]。

其次，为了表示均值 µθ(xt, t)，我们根据以下对 Lt 的分析提出了一种特定的参数化。对于 pθ(xt-1|xt) = N (xt-1; µθ (xt, t), σ^2_t I)，我们可以写成：

Lt-1 = Eq  1 / (2σ^2_t) * ||µ˜_t(xt, x0) - µθ(xt, t)||^2 + C (8)

其中 C 是不依赖于 θ 的常数。因此，我们看到 µθ 最直接的参数化是一个预测 µ˜_t（前向过程后验均值）的模型。但是，我们可以通过将等式 (4) 重新参数化为 xt(x0, ) = √(α¯_t * x0) + √(1 - α¯_t) *  并且应用前向过程后验公式 (7) 来进一步扩展等式 (8)：

Lt-1 - C = Ex0, " 1 / (2σ^2_t) * [ 1 / √α_t * ||xt(x0, ) - β_t * √(1 - α¯_t) *  ||^2 - µθ(xt(x0, ), t) ^2 ] (9)