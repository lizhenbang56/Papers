---
Publish: CVPR2024
Year: "2024"
Month: "03"
Url: https://arxiv.org/pdf/2403.12570.pdf
创新点: CLIP
---
```mermaid
graph LR;
	1张查询图像 --CLIP--> ImageFeature;
	K张正常参考图像 --CLIP--> MemoryBank;
	ImageFeature --> 正常异常二分类;
	MemoryBank --> 正常异常二分类;
```
- **自然图像**与医学图像有区别，所以限制了**自然图像**异常检测算法在**医学图像异常检测**中的应用。
	- **自然图像**与车底图像有区别，所以限制了**自然图像**异常检测算法在车底图像异常检测中的应用。
- 利用CLIP进行小样本/零样本**医学图像异常检测**
	- 本文的K-shot小样本指的是：
		- 查询图像：1张
		- 参考图像：K张
	- 本文的零样本指的是：
		- 查询图像：1张
		- 参考图像：0张
		- 参考问题：有
- 本文的目标是利用在**自然图像**上训练的视觉语言模型，适应于**医学图像异常检测**。
- **医学图像异常检测**数据集中有少量异常训练图像

## 摘要

最近，在大规模视觉语言预训练模型方面取得的进展在自然图像领域的零/少样本异常检测方面取得了显著进展。然而，自然图像与医学图像之间的实质性领域差异限制了这些方法在医学异常检测中的有效性。本文引入了一个新颖的轻量级多级适应和比较框架，以重新利用CLIP模型进行医学异常检测。我们的方法将多个残差适配器整合到预训练的视觉编码器中，实现了跨不同级别的视觉特征的逐步增强。这种多级适应是由多级、像素级的视觉语言特征对齐**损失**函数引导的，该**损失**函数重新校准了模型的焦点，将其从自然图像中的对象语义转移到医学图像中的异常识别。调整后的特征展现出对各种医学数据类型的改进泛化能力，即使在模型在训练过程中遇到未见过的医学模态和解剖区域的零样本情况下也是如此。我们在医学异常检测基准测试中的实验表明，我们的方法显著超越了当前最先进的模型，在零样本和少样本设置下，异常分类平均AUC提高了6.24%和7.33%，异常分割提高了2.03%和2.37%。源代码可在以下链接找到：https://github.com/MediaBrain-SJTU/MVFA-AD。

## 引言

医学异常检测（AD）专注于识别医学数据中的异常模式，对防止误诊和促进早期干预至关重要。医学图像的巨大变异性，无论是在模态还是解剖区域方面，都需要一种能够适用于各种数据类型的通用模型。

少样本AD方法致力于利用有限的训练数据实现模型泛化，体现了对通用AD模型的初步尝试，尽管对于每个新的AD任务需要轻量级的重新训练或分布调整。

当代大规模预训练的视觉语言模型（VLMs）最近为强大且具有泛化能力的异常检测铺平了道路。一个显著的初步努力是直接采用CLIP，这是一个代表性的自然图像开源VLM，用于AD，只需精心设计人工文本提示。通过进一步利用带注释的训练数据，Chen等人引入了额外的线性层，将图像特征映射到文本特征的联合嵌入空间，便于它们的比较。

尽管上述两种方法有希望，但它们在医学领域的扩展尚未探讨。本文尝试开发一种通用的医学图像AD模型，旨在适应先前未见的模态和解剖区域。创建这样一个模型具有重要的实际意义，但将CLIP模型定制为此目的面临三重挑战。

首先，重新将CLIP用于AD意味着任务需求的重大转变。CLIP中的视觉编码器主要捕捉图像语义，但通用AD模型必须在不同的语义上下文中分辨出异常。

其次，从在自然图像领域使用CLIP到医学图像领域的转变构成了一个重大的领域转换。

最后，在训练阶段将AD模型的适用性扩展到未遇到的成像模态和解剖区域的任务是特别具有挑战性的。

本文提出了一个轻量级的多级适应和比较框架，以重新利用CLIP进行医学图像AD，如图1所示。

设计了一个多级视觉特征适应架构，将CLIP的特征对齐到医学环境中的AD要求。视觉特征适应过程将适配器调整与多级考虑相结合。这是通过将多个残差适配器整合到预训练的视觉编码器中实现的。

通过多级、像素级的视觉语言特征对齐损失函数，这种逐级增强不同级别的视觉特征。

这些适配器重新校准了模型的焦点，将其从对象语义转移到识别图像中的异常，利用广泛分类图像为“正常”或“异常”的文本提示。

在测试期间，对比调整后的视觉特征与文本提示特征以及额外的参考图像特征（如果可用）进行比较，从而生成多级异常评分图。该方法在一个具有挑战性的医学AD基准测试中进行了评估，包括来自五个不同医学模态和解剖区域的数据集：脑MRI、肝CT、视网膜OCT、胸部X射线和数字组织病理学。

我们的方法优于最先进的方法，在零样本和少样本情况下，异常分类平均改善了6.24%和7.33%，异常分割分别提高了2.03%和2.37%。

主要贡献总结如下： 
• 提出了一个新颖的多级特征适应框架，据我们所知，这是首次尝试将预训练的视觉语言模型用于医学AD的零/少样本场景。 
• 在医学图像AD的一个具有挑战性的基准测试上进行了广泛实验，证明了其在各种数据模态和解剖区域上的出色泛化能力。

## 相关工作

传统异常检测。鉴于异常图像的有限可用性和高昂成本，当前部分关于AD的研究集中在仅依赖于正常图像的无监督方法上。方法如PatchCore创建了一个正常嵌入的内存库，并基于测试样本到最近的正常嵌入的距离来检测异常。另一种方法CflowAD使用正态流将正常样本投影到高斯分布上。然而，仅依赖于正常样本可能导致模糊的决策边界和降低的可区分性。在实际场景中，通常只有少量异常样本可用，这些样本可用于增强检测效果。

【零/少样本异常检测】在训练过程中利用少量已知异常可能会带来挑战，可能会使模型产生偏见，并阻碍对未见异常的泛化。DRA和BGAD引入了缓解此问题的方法。除了简单地最大化异常特征与正常模式之间的分离外，DRA学习了解耦的异常表示，以实现可泛化的检测，考虑了未见异常。BGAD提出了一个边界引导的半推拉对比学习机制，进一步缓解了偏见问题。像[[WinCLIP]]这样的最新进展探索了使用基础模型进行零/少样本AD，利用语言来辅助AD。在WinCLIP的基础上，April-GAN将从CLIP提取的视觉特征映射到文本特征所在的线性空间，受到像素级标注数据的监督。本文专注于医学AD，这是一个比传统工业AD更具挑战性的领域，因为不同数据模态之间的差距更大。

医学异常检测。当前的医学AD方法通常将AD视为一类分类问题，依赖于训练中的正常图像[3,8,27,61,64-66]。这些方法通常将异常识别为与正态分布偏离的偏差，通常需要每类大量的正常样本，使其在实际诊断中不切实际。许多这些技术是为特定的解剖区域设计的，或者仅限于处理一个特定的数据类型。这些方法在跨多样化的数据模态和解剖区域方面通常表现不佳，而这正是我们的论文旨在解决的一个关键方面。

视觉-语言建模。最近，VLMs已经取得了重大进展，并应用于许多不同的场景中。在大量的图像文本数据上训练的CLIP在泛化能力和鲁棒性方面表现出色，尤其是在启发式驱动的零样本推断方面。为了拓宽VLMs的应用，诸如广泛的LAION-5B数据集和开放的OpenCLIP代码库等资源已经公开提供。随后的研究强调了CLIP在零/少样本转移到除了纯分类之外的下游任务上的潜力。

## 问题阐述

我们的目标是将一个最初在自然图像上训练的视觉-语言模型（表示为Mnat），适应于医学图像中的异常检测（AD），形成一个医学适应模型（Mmed）。这种适应利用医学训练数据集Dmed，其中包含来自医学领域的带标注样本，从而实现将Mnat转换为Mmed。具体而言，Dmed被定义为一个元组集合{(xi，ci，Si)}，其中K是数据集中图像样本的总数。每个元组包括一个训练图像xi，其对应的图像级别异常分类（AC）标签ci ∈ {−，+}，以及像素级别异常分割（AS）注释Si ∈ {−，+}，其中图像大小为h×w。标签‘+’表示异常样本，‘−’表示正常样本。对于给定的测试图像xtest，模型旨在准确预测AC和AS的图像级别和像素级别异常。为了对来自未见成像模态和解剖区域的异常进行建模，我们在零样本学习的背景下处理问题。在这里，Dmed是一个预训练数据集，由不同于测试样本中的成像模态和解剖区域的医学数据组成，用于评估模型对未见场景的泛化能力。考虑到从目标场景获取有限数量样本的实际性，我们还将方法扩展到少样本学习的背景下。在这里，Dmed包括一小部分K个带标注的图像，其模态和解剖区域与测试样本相同，通常K代表一个小数值，例如本研究中的{2，4，8，16}。下面我们介绍我们提出的用于医学图像AD的多级适应和比较框架，包括(i) 多级特征适应（第4节），以及(ii) 多级特征比较（第5节）。

## 训练：多级特征适应 

为了将预训练的自然图像视觉-语言模型适应于医学成像中的异常检测（AD），我们引入了一个专门设计用于医学图像的AD的多级特征适应框架，利用最少的数据和轻量级的多级特征适配器。多级视觉特征适配器（MVFA）。为了解决由于参数数量过多和训练样本有限而导致的过拟合挑战，我们在多个特征级别上应用了CLIP适配器。这种方法在CLIP的视觉分支中附加了一小组可学习的瓶颈线性层，同时保持其原始的主干不变，从而使其能够在多个特征级别上进行适应。

具体来说，对于图像 $x \in \mathbb{R}^{h\times w\times 3}$，具有四个顺序阶段（S1到S4）的CLIP视觉编码器将图像 $x$ 转换为特征空间$\mathcal{F}_{vis} \in \mathbb{R}^{G\times d}$，其中，$G$ 是网格数量，$d$ 是特征维度。

前三个视觉编码器阶段（S1到S3）的输出，表示为 $\mathcal{F}_l \in \mathbb{R}^{G\times d}, l \in \{1, 2, 3\}$，表示三个中间阶段的特征。

visual feature adaptation 包括三个 feature adapter $A_l(\cdot), l \in \{1, 2, 3\}$，和一个 feature projector $P(\cdot)$，在不同级别上。

在每个 level $l \in \{1, 2, 3\}$，一个可学习的 feature adapter $A_l(\cdot)$ 被整合到特征 $\mathcal{F}_l$ 中，包括两个（最小数量的）线性变换层。这种整合将特征转换以进行适应，表示为：$$A_l(\mathcal{F}_l) = ReLU(\mathcal{F}_l^T  W_{l,1})W_{l,2}, \text{~where~} l \in \{1,2,3\}$$这里，Wl,1和Wl,2表示线性变换的可学习参数。与[15]一致，特征适配器中采用了残差连接以保留由预训练的CLIP编码的原始知识。具体来说，一个常数值γ作为残差比率，调整了保留原始知识的程度，以改善性能。因此，第l个特征级别的特征适配器可以表示为：F ∗ l = γAl(Fl) T + (1 − γ)Fl，其中l ∈ {1, 2, 3}，F ∗ l作为下一个编码器阶段Sl+1的输入。

默认情况下，我们设置γ = 0.1。此外，如图2（b）所示，为了同时处理AC和AS的全局和局部特征，一个双适配器架构替换了Eq.（1）中使用的单适配器，产生每个级别的两组并行特征，Fcls,l和Fseg,l。对于由CLIP视觉编码器生成的最终视觉特征Fvis，特征投影器P(·)使用参数Wcls和Wseg进行投影，得到全局和局部特征为Fcls,4 = F T visWcls和Fseg,4 = F T visWseg。利用多级适应特征，模型能够有效地区分全局异常以进行分类，以及局部异常以进行分割，通过以下视觉-语言特征对齐。

【语言特征格式化】为了开发一个有效的异常分类和分割框架，我们采用了受[9, 26]方法启发的文本提示的两层方法。这些方法利用了正常和异常对象的描述。在状态级别，我们的策略涉及使用简单直接、通用的文本描述，专注于清晰度并避免复杂的细节。在模板级别，我们对[11]中引用的35个模板进行了彻底的检查（详见附录B）。通过分别计算文本编码器提取的正常和异常状态的文本特征的平均值，我们获得一个表示为Ftext ∈ R 2×d的文本特征，其中d是特征维度。

【视觉-语言特征对齐】对于图像级别的anomaly annotation $c \in \{−, +\}$ 和相应的像素级别异常图S ∈ {−，+} h×w，我们在每个特征级别l ∈ {1, 2, 3, 4}上通过对齐MVFA给出的适应视觉特征和文本特征来优化模型。这是通过结合不同的组件来实现的**损失**函数：Ll =λ1Dice(softmax(Fseg,lF T text), S)+ λ2Focal(softmax(Fseg,lF T text), S)+ λ3BCE(max h×w (softmax(Fcls,lF T text)), c)，其中Dice(·, ·)、Focal(·, ·)和BCE(·, ·)分别是dice损失[36]、focal损失[33]和二元交叉熵**损失**。λ1、λ2和λ3是各个损失权重，我们设置λ1 = λ2 = λ3 = 1.0作为默认值。然后，总的适应损失Ladapt被计算为每个特征级别的损失之和，表示为Ladapt = P4 l=1 Ll。讨论。WinCLIP [26]依赖于来自预训练VLMs的类标记进行自然图像AD，而不进行适应。相比之下，MVFA引入了多级适应，冻结了主干同时通过适配器在相应的视觉-语言对齐中的每个级别上适应特征。由此产生的适应特征通过残余集成到后续的编码器块中，修改了这些块的输入特征。这种独特的方法通过梯度传播实现了不同级别适配器的协同训练，增强了主干模型的整体适应能力。因此，与APRIL-GAN [9]利用不适应主干的孤立特征投影相比，MVFA导致了在医学AD中的强大泛化。MVFA与[9]中的特征投影之间的差异也在第6.3节中进行了评估。

## 测试：多级特征比较 

在测试过程中，为了准确预测图像级别（AC）和像素级别（AS）的异常，我们的方法采用了一个两分支多级特征比较架构，包括零样本分支和少样本分支，如图2（c）所示。 零样本分支。测试图像xtest经过MVFA处理，生成多级适应特征。然后，将这些特征与文本特征Ftext进行比较。零样本AC和AS结果，表示为czero和Szero，通过四个级别的平均softmax得分计算而得，czero = 1 4 P4 l=1 max G (softmax(Fcls,lF T text)), Szero = 1 4 P4 l=1BI(softmax(Fseg,lF T text))。这里，BI(·)将异常地图重塑为√ G× √ G并使用双线性插值将其恢复到原始输入图像分辨率，其中G表示网格数。 少样本分支。

$\mathcal{D}_{med}$ 中少数标记的正常图像的多级视觉特征有助于构建一个 feature memory bank $\mathcal{G}$，从而促成特征比较。

少样本AC和AS得分，表示为cfew和Sfew，是通过在每个级别上测试特征与存储库特征之间的最小距离，通过最近邻搜索过程得出的，cfew = 1 4 P4 l=1 max G (min m∈G Dist(Fcls,l, m)), Sfew = 1 4 P4 l=1BI(min m∈G Dist(Fseg,l, m))。这里，Dist(·, ·)表示余弦距离，计算方法为1−cosine(·, ·)。最终预测的AC和AS结果结合了两个分支的结果：cpred = β1czero + β2cfew，Spred = β1Szero + β2Sfew。这里，β1和β2是零样本分支和少样本分支的权重因子，分别默认设置为0.5。

## 6. 实验 

### 6.1. 实验设置 

【数据集】我们考虑基于BMAD [3]的医学异常检测（AD）基准，涵盖了五个不同的医学领域，得到了六个数据集。其中包括脑MRI [1, 2, 35]、肝CT [7, 29]、视网膜OCT [21, 28]、胸部X光 [51] 和数字组织病理学 [4]。其中，BrainMRI [1, 2, 35]、LiverCT [7, 29] 和RESC [21] 数据集用于异常分类（AC）和分割（AS），而OCT17 [28]、ChestXray [51] 和HIS [4] 仅用于AC。这些数据集的详细描述见附录A。 

【竞争方法和基线】在本研究中，我们考虑了各种最先进的AD方法，涵盖了不同的训练设置作为竞争方法。这些设置包括
（i）使用所有正常数据的基线方法（CFlowAD [19]、RD4AD [10]、PatchCore [41] 和MKD [44]）、
（ii）少正常样本方法（CLIP [25]、MedCLIP [52]、WinCLIP [26]）、以及
（iii）少样本方法（DRA [12]、BGAD [57] 和April-GAN [9]）。

我们对这些方法进行了AC和AS的评估，但BGAD由于在训练期间需要像素级别的 **annotation**，所以仅用于分割。 评估协议。使用接收器操作特征曲线下的面积指标（AUC）来量化性能。这个指标在AD评估中是标准的，AC中有图像级别的AUC，而AS中有像素级别的AUC。 

【模型配置和训练细节】我们使用分辨率为240的输入图像，使用了CLIP与ViT-L/14架构。该模型共包含24层，分为4个阶段，每个阶段包含6层。

我们使用Adam优化器，学习率恒定为1e-3，批大小为16，在一块NVIDIA GeForce RTX 3090 GPU上进行50个epoch的训练。 

### 6.2. 与最先进方法的比较 

【少样本设置】在表1中，我们将K = 4的少样本设置下的MVFA性能与其他最先进的AD方法进行比较。对于MVFA在各种少样本情景（K ∈ {2, 4, 8, 16}）下的性能进行深入分析，请参阅图3。 MVFA展示了优越的性能，超过了竞争方法如DRA [12]、BGAD [57] 和April-GAN [9]。值得注意的是，MVFA在所有数据集上的AC的AUC平均提高了7.33%，AS的AUC平均提高了2.37%，超过了April-GAN，后者是CVPR 2023年VAND研讨会的获胜者 [68]。与具有像素级别 $\textbf{annotation}$ 的数据集相比，MVFA比BGAD [57]表现出了AC的AUC平均提高了9.18%，AS的AUC平均提高了3.53%。

MVFA优于少量正常样本的基于CLIP的方法，如CLIP [25]和WinCLIP [26]，这些方法也使用视觉-语言预训练的主干并利用特征比较进行AD。MVFA的优势在于其能够有效利用少量异常样本，从而显著超越了这些方法。