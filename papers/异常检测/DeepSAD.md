---
Url: https://arxiv.org/pdf/1906.02694.pdf
Publish: ICLR2020
Year: "2019"
创新点: 提出半监督异常检测任务
Title: Deep semi-supervised anomaly detection
Month: "06"
---
## 摘要 

近年来，深度方法在大型和复杂数据集上显示出比浅层方法更有前景的异常检测结果。通常，异常检测被视为无监督学习问题。然而，在实践中，除了一大批未标记样本外，还可以访问一小部分标记样本，例如，由领域专家验证为正常或异常的子集。半监督异常检测方法旨在利用这些标记样本，但大多数提出的方法仅限于包括标记的正常样本。只有少数方法利用了标记的异常，现有的深度方法是特定于领域的。在这项工作中，我们提出了Deep SAD，一种用于一般半监督异常检测的端到端深度方法。我们进一步引入了一个基于信息论的框架，用于深度异常检测，该框架的思想是正常数据的潜在分布的熵应低于异常分布的熵，这可以作为我们方法的理论解释。在MNIST、Fashion-MNIST和CIFAR-10等大量实验中，以及其他异常检测基准数据集上，我们展示了我们的方法与浅层、混合和深度竞争对手的水平相当或优于它们，即使只提供少量标记数据也能获得可观的性能改进。

## 引言

异常检测（AD）（Chandola等，2009年；Pimentel等，2014年）是识别数据中异常样本的任务。通常，AD方法试图以无监督的方式学习数据的“紧凑”描述，假设大多数样本是正常的（即，不是异常的）。例如，在单类分类中（Moya等，1993年；Schölkopf等，2001年），目标是找到一个包含大多数数据的小量集合，不包含在该集合中的样本被视为异常。浅层无监督AD方法，如一类支持向量机（One-Class SVM）（Schölkopf等，2001年；Tax＆Duin，2004年），核密度估计（Parzen，1962年；Kim＆Scott，2012年；Vandermeulen＆Scott，2013年）或孤立森林（Isolation Forest）（Liu等，2008年），通常需要手动进行特征工程才能在高维数据上发挥有效作用，并且在处理大型数据集时受到限制。这些限制引发了对开发新的深度AD方法的极大兴趣（Erfani等，2016年；Zhai等，2016年；Chen等，2017年；Ruff等，2018年；Deecke等，2018年；Ruff等，2019年；Golan＆El-Yaniv，2018年；Pang等，2019年；Hendrycks等，2019a;b）。

半监督异常检测的需求：训练数据（如图（a）所示）由大部分未标记数据（灰色）以及少量标记正常样本（蓝色）和标记异常样本（橙色）组成。图（b）–（f）显示了测试时各种学习范式的决策边界，以及发生的新异常（每个图的左下角）。我们的半监督AD方法利用了所有的训练数据：未标记样本、标记正常样本以及标记异常样本。这在单类学习和分类之间取得了平衡。与标准的无监督AD设置不同，许多实际应用中可能还可以获得一些经过验证的（即标记的）正常或异常样本，除了未标记数据。例如，这些样本可以由领域专家手动标记。这导致了半监督AD问题：给定n个（大多数是正常但可能包含一些异常污染的）未标记样本x1，...，xn和m个标记样本（x˜1，y˜1），...，（x˜m，y˜m），其中y˜= +1和y˜=−1分别表示正常和异常样本，则任务是学习一个能够紧凑地描述“正常类”的模型。

术语半监督异常检测已用于描述两种不同的AD设置。大多数现有的“半监督”AD方法，无论是浅层（Muñoz-Marí等，2010年；Blanchard等，2010年；Chandola等，2009年）还是深层（Song等，2017年；Akcay等，2018年；Chalapathy＆Chawla，2019年），仅利用标记的正常样本而不是标记的异常，即它们更确切地说是从正（即正常）和未标记示例（LPUE）中学习的实例（Denis，1998年；Zhang＆Zuo，2008年）。一些研究（Wang等，2005年；Liu＆Zheng，2006年；Görnitz等，2013年）已经调查了使用标记的异常的一般半监督AD设置，但现有的深度方法是特定于领域或数据类型的（Ergen等，2017年；Kiran等，2018年；Min等，2018年）。

对深度半监督学习的研究几乎完全集中在分类作为下游任务上（Kingma等，2014年；Rasmus等，2015年；Odena，2016年；Dai等，2017年；Oliver等，2018年）。这种半监督分类器通常假设相似的点可能属于同一类，这被称为簇假设（Zhu，2005年；Chapelle等，2009年）。然而，这种假设仅适用于AD中的“正常类”，但对于“异常类”却是关键无效的，因为异常不一定相似。相反，半监督AD方法必须找到正常类的紧凑描述，同时正确地区分标记的异常（Görnitz等，2013年）。图1说明了应用于AD的各种学习范式之间的差异，以及在玩具示例中发生的新异常。

我们在本文中介绍了Deep SAD（深度半监督异常检测），这是一种端到端的深度方法，用于一般的半监督AD。我们的主要贡献如下：

  
• 我们引入了Deep SAD，将无监督的Deep SVDD方法（Ruff等，2018年）推广到半监督AD设置中。

• 我们提出了一个信息论框架，用于深度AD，可以作为我们的Deep SAD方法及类似方法的解释。

• 我们进行了大量实验，建立了一般半监督AD问题的实验场景，并引入了新的基准。

## 信息论视角下的深度异常检测 

对深度学习的理论基础的研究是一个活跃且持续的研究工作。一个重要的研究方向源于信息论。在监督分类设置中，其中有输入变量X、潜变量Z（例如，深度网络的最终层）和输出变量Y（即标签）的情况下，著名的信息瓶颈原理提供了表示学习的解释，作为在找到输入X的最小压缩Z之间的权衡，同时保留Z对于预测标签Y的信息性。形式化地说，监督深度学习试图最小化输入X和潜在表示Z之间的互信息I（X；Z），同时最大化Z与分类任务Y之间的互信息I（Z；Y），即 min ⁡  (  ∣  )  (  ;  ) −   (  ;  ) p(z∣x) min ​ I(X;Z)−αI(Z;Y) 其中p（z|x）由深度网络建模，超参数α > 0控制压缩（即复杂性）和分类准确度之间的权衡。 对于无监督深度学习，由于缺乏标签Y及任务的明确，其他信息论学习原则已被制定。其中，Infomax原理是最为普遍和广泛使用的原则之一。与（1）不同，Infomax的目标是最大化数据X与其潜在表示Z之间的互信息I（X；Z）： max ⁡  (  ∣  )  (  ;  ) +   (  ) p(z∣x) max ​ I(X;Z)+βR(Z) 这通常是在表示Z上施加一些额外约束或正则化R(Z)，其中超参数β > 0以获得一些特定下游任务所需的统计特性。已经应用Infomax原理的例子包括独立成分分析、聚类、生成建模和一般无监督表示学习。 我们观察到，Infomax原理也已应用于先前的AD深度表示中。最显着的是自动编码器，它是深度AD的主要方法之一。它可以被理解为通过重建目标隐式地最大化输入X与潜在表示Z之间的互信息I（X；Z），在一些对Z的正则化下。对于AD的表示选择，正则化的选择包括稀疏性、与一些潜在先验分布的距离（例如，通过KL散度测量）、对抗损失或者简单的维度约束。这些对AD的约束共享的思想是正常数据的潜在表示在某种意义上应该是“紧凑”的。 

正如图1所示，对AD采用监督（或半监督）分类方法只学习识别与训练时看到的异常相似的异常，由于类集群假设。然而，任何不正常的都可以被定义为异常，因此异常不一定相似。这使得基于监督（或半监督）分类学习原则如（1）对于AD来说是不明确的。相反，我们建立在原则（2）之上，以激励一种深度方法，用于一般的半监督AD，其中我们通过基于熵的新型表示学习正则化目标R（Z）= R（Z；Y）来包含标签信息Y。

## 深度半监督异常检测 

在接下来的内容中，我们介绍了Deep SAD，这是一种用于一般半监督AD的深度方法。为了制定我们的目标，我们首先简要解释了无监督的Deep SVDD方法（Ruff等，2018年），然后将其推广到半监督AD设置中。 

### 无监督Deep SVDD和熵最小化 

对于输入空间  ⊆   X⊆R D 和输出空间  ⊆   Z⊆R d ，让  ( ⋅ ;  ) :  →  ϕ(⋅;W):X→Z 是一个具有L个隐藏层和相应的权重集  = {  1 , . . . ,   } W={W 1 ​ ,...,W L ​ } 的神经网络。Deep SVDD的目标是训练神经网络  ϕ 学习一个变换，最小化输出空间Z中以预定点c为中心的数据包围超球体的体积。给定n（未标记）训练样本  1 , . . . ,   ∈  x 1 ​ ,...,x n ​ ∈X，One-Class Deep SVDD目标是 min ⁡  1  ∑  = 1  ∥  (   ;  ) −  ∥ 2 +  2 ∑ ‘ = 1  ∥  ‘ ∥  2 W min ​ n 1 ​ i=1 ∑ n ​ ∥ϕ(x i ​ ;W)−c∥ 2 + 2 λ ​ ‘=1 ∑ L ​ ∥W‘∥ F 2 ​ 惩罚映射样本到超球体中心c的平均平方距离，迫使网络提取数据集中最稳定的公共变化因素。因此，正常数据点倾向于被映射到超球体中心附近，而异常值被映射到更远的地方。第二项是一个标准的权重衰减正则化项。Deep SVDD通过随机梯度下降（SGD）和反向传播进行优化。对于初始化，Ruff等人首先预训练自动编码器，然后将网络  ϕ 的权重W初始化为编码器的收敛权重。初始化后，超球体中心c被设置为从数据的初始前向传递获得的网络输出的均值。一旦网络训练完成，测试点x的异常得分由从  (  ;  ) ϕ(x;W) 到超球体中心的距离给出：  (  ) = ∥  (  ;  ) −  ∥ s(x)=∥ϕ(x;W)−c∥ 现在我们认为Deep SVDD不仅可以被解释为几何术语中的最小体积估计，而且可以被解释为概率术语中的对潜在分布的熵最小化。对于具有协方差Σ、概率密度函数p(z)和支持集  ⊆   Z⊆R d 的潜在随机变量Z，我们有熵的以下界限：  (  ) =  [ − log ⁡  (  ) ] = − ∫ ∫  (  ) log ⁡  (  )   ≤ 1 2 log ⁡ ( ( 2   )  det Σ ) H(Z)=E[−logp(Z)]=−∫∫p(z)logp(z)dz≤ 2 1 ​ log((2πe) d detΣ) 当且仅当Z是联合高斯分布时等式成立。假设潜在分布Z遵循各向同性高斯分布，  ∼  (  ,  2  ) Z∼N(μ,σ 2 I) 其中  > 0 σ>0，我们得到：  (  ) = 1 2 log ⁡ ( ( 2   )  det  2  ) = 1 2 log ⁡ ( ( 2    2 )  ⋅ 1 ) =  2 ( 1 + log ⁡ ( 2   2 ) ) ∝ log ⁡  2 H(Z)= 2 1 ​ log((2πe) d detσ 2 I)= 2 1 ​ log((2πeσ 2 ) d ⋅1)= 2 d ​ (1+log(2πσ 2 ))∝logσ 2 即对于固定维度d，Z的熵与其对数方差成正比。现在观察到Deep SVDD目标（3）（忽略权重衰减正则化）相当于最小化经验方差，从而最小化潜在高斯分布的熵的上界。由于Deep SVDD网络是基于隐含最大化互信息I(X;Z)的自动编码器目标进行预训练的，我们可以解释Deep SVDD遵循Infomax原理（2），并附加“紧凑性”目标，即潜在分布应具有最小熵。 Deep SAD 我们现在介绍我们的深度半监督异常检测方法：Deep SAD。假设除了  ⊆   X⊆R D 中的n个未标记样本  1 , . . . ,   x 1 ​ ,...,x n ​ 外，我们还可以访问m个带标签的样本 (  ˜ 1 ,  ˜ 1 ) , . . . , (  ˜  ,  ˜  ) (x˜ 1 ​ ,y˜ 1 ​ ),...,(x˜ m ​ ,y˜ m ​ ) ，其中  = { − 1 , + 1 } Y={−1,+1} ，其中  ˜ = + 1 y˜=+1 表示已知的正常样本，  ˜ = − 1 y˜=−1 表示已知的异常。我们定义我们的Deep SAD目标如下： min ⁡  1  +  ∑  = 1  ∥  (   ;  ) −  ∥ 2 +   +  ∑  = 1  ∥  (  ˜  ;  ) −  ∥ 2 W min ​ n+m 1 ​ i=1 ∑ n ​ ∥ϕ(x i ​ ;W)−c∥ 2 + n+m η ​ j=1 ∑ m ​ ∥ϕ(x˜ j ​ ;W)−c∥ 2