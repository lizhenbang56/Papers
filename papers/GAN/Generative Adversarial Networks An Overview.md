---
创新点: GAN综述
Url: https://arxiv.org/pdf/1710.07035.pdf
Year: "2017"
---
## 摘要

生成对抗网络（GANs）提供了一种学习深度表示的方式，而不需要大量注释的训练数据。它们通过在涉及一对网络的竞争过程中导出反向传播信号来实现这一点。GANs可以学习的表示可以用于各种应用，包括图像合成、语义图像编辑、风格转移、图像超分辨率和分类。本文的目的是为信号处理界提供GANs的概述，尽可能借鉴熟悉的类比和概念。除了确定训练和构建GANs的不同方法外，我们还指出了它们在理论和应用中的剩余挑战。

索引词-神经网络、无监督学习、半监督学习。

## I. 引言

生成对抗网络（GANs）是一种新兴的半监督和无监督学习技术。它们通过隐式建模数据的高维分布来实现这一点。2014年提出[1]，它们的特点是通过训练一对相互竞争的网络来进行的。一个常见的类比，适用于视觉数据，是将一个网络视为艺术伪造者，另一个网络视为艺术专家。在GAN文献中被称为生成器G的伪造者创造伪造品，目标是制作逼真的图像。专家，称为鉴别器D，接收伪造品和真实（真实）图像，并旨在将它们区分开（见图1）。两者同时训练，并相互竞争。

关键是，生成器没有直接访问真实图像-它学习的唯一方式是通过与鉴别器的交互。鉴别器可以访问合成样本和从真实图像堆栈中抽取的样本。鉴别器的错误信号通过简单的基本事实提供给鉴别器，即知道图像是来自真实堆栈还是生成器。通过鉴别器，相同的错误信号可以用于训练生成器，使其能够生成更高质量的伪造品。

表示生成器和鉴别器的网络通常由包含卷积和/或全连接层的多层网络实现。生成器和鉴别器网络必须是可微分的，尽管它们不必直接可逆。如果考虑生成器网络作为从某个表示空间（称为潜在空间）到数据空间的映射（我们将专注于图像），那么我们可以更正式地表示为G：G(z) → R|x|，其中z ∈ R|z|是潜在空间的样本，x ∈ R|x|是图像，| · |表示维数。

在基本GAN中，鉴别器网络D可以类似地被表征为一个将图像数据映射到图像来自真实数据分布的概率的函数，而不是生成器分布：D：D(x) → (0, 1)。对于固定的生成器G，鉴别器D可以被训练为将图像分类为来自训练数据（真实，接近1）或来自固定生成器（伪造，接近0）。当鉴别器是最佳的时，它可以被冻结，生成器G可以继续被训练以降低鉴别器的准确性。如果生成器分布能够完美地匹配真实数据分布，那么鉴别器将被最大程度地困惑，对所有输入预测0.5。在实践中，鉴别器可能不会被训练到最佳状态；我们将在第四节中更深入地探讨训练过程。

除了与训练和构建GANs相关的有趣的学术问题之外，训练GANs的动机不一定是生成器或鉴别器本身：由一对网络中的任一网络体现的表示可以用于各种后续任务。我们将在第六节中探讨这些表示的应用。

  
## II. 初步

### A. 术语

生成模型学习**捕获训练数据的统计分布**，使我们能够从学习到的分布中合成样本。除了合成新数据样本，这些模型学习的表示还可以用于分类和图像检索等任务。

我们偶尔提到深度网络的全连接和卷积层；这些是感知器或带有非线性后处理的空间滤波器组的泛化。在所有情况下，网络权重通过反向传播学习。

### B. 符号表示

GAN文献通常涉及多维向量，并经常通过斜体表示概率空间中的向量（例如，潜在空间为z）。在信号处理领域，常用粗体小写符号表示向量，我们采用这种约定以强调变量的多维性。因此，我们通常将pdata(x)表示为在位于R|x|的随机向量x上的概率密度函数。我们使用pg(x)来表示生成器网络产生的向量的分布。我们使用G和D的花体符号来表示生成器和鉴别器网络。这两个网络都有通过训练优化学习的参数（权重），ΘD和ΘG。与所有深度学习系统一样，训练需要有清晰的目标函数。按照惯例的表示法，我们使用JG(ΘG;ΘD)和JD(ΘD;ΘG)来指代生成器和鉴别器的目标函数。符号的选择提醒我们，这两个目标函数在某种程度上依赖于不断更新的网络参数ΘG和ΘD。我们将在第四节中进一步探讨这一点。最后，注意到在更新中使用了多维梯度；我们使用∇ΘG表示对生成器参数的权重的梯度运算符，∇ΘD表示对鉴别器的权重的梯度运算符。期望的梯度由E∇•表示。

### C. 捕获数据分布

信号处理和统计的一个核心问题是密度估计：获得数据在真实世界中的表示 - 隐式或显式、参数化或非参数化。这是GAN的关键动机。在GAN文献中，数据生成分布这个术语通常用于指观察数据的底层概率密度或概率质量函数。GAN通过隐式计算候选模型的分布与对应于真实数据的分布之间的某种相似性来学习。

为什么要费力进行密度估计？答案在于，这涉及到视觉推断的核心 - 可能是许多计算机视觉问题的核心，包括图像分类、视觉对象检测和识别、对象跟踪和对象配准。原则上，通过贝叶斯定理，计算机视觉的所有推断问题都可以通过估计条件密度函数来解决，可能间接地以模型的形式学习感兴趣的变量和观测数据的联合分布。我们面临的困难是，高维度、真实世界的图像数据的似然函数很难构建。虽然GANs并不明确提供评估密度函数的方法，但对于具有合适容量的生成器-鉴别器对，生成器隐式捕获数据的分布。

### D. 相关工作

可以通过将生成模型的原理与信号处理和数据分析中的标准技术进行比较来看待生成模型的原则。例如，信号处理广泛使用将信号表示为基函数的加权组合的思想。固定的基函数是标准技术的基础，例如基于傅立叶和小波的表示。构建基函数的数据驱动方法可以追溯到Hotelling [8]变换，根据Pearson的观察，主成分最小化了根据最小平方误差标准的重建误差。尽管PCA被广泛使用，但它本身没有观察数据的明显统计模型，尽管已经证明PCA的基础可以导出为最大似然参数估计问题。

尽管PCA本身被广泛采用，但它也有局限性-基函数出现为输入数据观察的协方差矩阵的特征向量，并且从表示空间映射回信号或图像空间的映射是线性的。因此，我们既有一个浅层次又有一个线性的映射，限制了模型和数据的复杂性。

独立成分分析（ICA）提供了更高水平的复杂性，其中信号分量不再需要正交；用于将组件混合在一起以构建数据示例的混合系数仅被视为统计独立。ICA有各种不同的表述，它们在估计信号组件期间使用的目标函数，或者在表达信号或图像如何从这些组件生成时使用的生成模型方面存在差异。通过ICA探索的一个最新创新是噪声对比估计（NCE）；这可以被看作是接近GAN的精神[9]：用于学习独立分量的目标函数将噪声应用于统计学上与由候选生成模型产生的统计量进行比较的方法[10]。原始NCE方法不包括对生成器的更新。

GANs和标准信号处理工具之间还可以进行哪些比较？对于PCA、ICA、傅里叶和小波表示，GANs的潜在空间在类比中类似于我们通常所说的变换空间的系数空间。将GANs与标准信号处理工具区分开来的是，生成器网络包含非线性，而且几乎可以是任意深度的，这种映射-与许多其他深度学习方法一样-可以非常复杂。

关于深度图像模型，现代生成图像建模方法可以分为显式密度模型和隐式密度模型。显式密度模型是可计算的（变量变换模型、自回归模型）或不可计算的（使用变分推理训练的有向模型，使用马尔可夫链训练的无向模型）。隐式密度模型通过生成过程捕获数据的统计分布，该生成过程利用祖先采样[11]或基于马尔可夫链的采样。GANs属于有向隐式模型类别。Ian Goodfellow在NIPS 2016教程[12]中提供了更详细的概述和相关论文。

## III. GAN架构

### A. 全连接GANs

最初的GAN架构使用全连接神经网络作为生成器和鉴别器[1]。这种类型的架构应用于相对简单的图像数据集，即MNIST（手写数字）、CIFAR-10（自然图像）和多伦多人脸数据集（TFD）。

### B. 卷积GANs

从全连接到卷积神经网络的转变是一种自然的延伸，因为CNN非常适合图像数据。早期在CIFAR-10上进行的实验表明，使用与监督学习相同容量和表示能力的CNN训练生成器和鉴别器网络更加困难。

对抗网络的拉普拉斯金字塔（LAPGAN）[13]提供了解决这个问题的一种方法，通过使用多个尺度来分解生成过程：地面实况图像本身被分解成拉普拉斯金字塔，条件卷积GAN被训练以产生每一层，给定上面的一层。

此外，Radford等人提出了一系列网络体系结构，称为DCGAN（“深度卷积GAN”），允许训练一对深度卷积生成器和鉴别器网络[5]。DCGAN利用了步进和分数步进卷积，允许在训练期间学习空间下采样和上采样运算符。这些运算符处理采样率和位置的变化，这是从图像空间到可能较低维度的潜在空间的映射以及从图像空间到鉴别器的映射的关键要求。DCGAN架构和训练的进一步细节将在第四节-B中介绍。

作为在2D中合成图像的扩展，Wu等人提出了能够使用体积卷积合成3D数据样本的GANs。Wu等人合成了包括椅子、桌子和汽车在内的新对象；此外，他们还提出了一种方法，将2D图像映射到这些图像中所描绘的对象的3D版本。

### C. 条件GANs

Mirza等人将（2D）GAN框架扩展到条件设置，通过使生成器和鉴别器网络类别条件化来实现[15]。条件GANs具有提供更好表示多模态数据生成的优势。可以在条件GANs和InfoGAN之间绘制并行，InfoGAN将噪声源分解为一个不可压缩的源和一个“潜在代码”，试图通过最大化潜在代码和生成器输出之间的互信息来发现变化的潜在因素。可以使用这个潜在代码以纯粹无监督的方式发现对象类别，尽管潜在代码不一定需要是分类的。InfoGAN学习到的表示似乎是有意义的，处理图像外观中复杂的纠缠因素，包括面部图像的姿势、光照和情感内容的变化。

D. 具有推理模型的GANs

在它们的原始制定中，GANs缺乏一种将给定观察x映射到潜在空间向量的方法 - 在GAN文献中，这通常被称为推理机制。已经提出了几种技术来反转预先训练的GAN的生成器。独立提出的对抗学习推理（ALI）[19]和双向GANs[20]提供了简单但有效的扩展，引入一个推理网络，其中鉴别器检查联合（数据、潜在）对。在这种形式中，生成器由两个网络组成：“编码器”（推理网络）和“解码器”。它们联合训练以愚弄鉴别器。鉴别器本身接收（x，z）向量对（见图4），并且必须确定哪个对组成了真实图像样本及其编码的真实元组，或者是假图像样本及其生成器的对应潜在空间输入。

理想情况下，在编码-解码模型中，输出，称为重构，应与输入类似。通常，使用ALI/BiGAN合成的样本的保真度较低。通过对数据样本及其重构的分布施加额外的对抗成本，可以改进样本的保真度。

E. 对抗自动编码器（AAE）

自动编码器是由“编码器”和“解码器”组成的网络，它们学习将数据映射到内部潜在表示再映射回来。也就是说，它们学习一个确定性映射（通过编码器）从数据空间 - 例如，图像 - 到潜在或表示空间，并且一个映射（通过解码器）从潜在空间返回到数据空间。这两个映射的组合导致一个“重构”，并且两个映射都被训练，以便重构图像尽可能接近原始图像。

自动编码器让人想起在图像和信号处理中广泛使用的完美重构滤波器组。然而，自动编码器通常在两个方向上学习非线性映射。此外，当使用深度网络实现时，用于实现自动编码器的可能架构非常灵活。训练可以是无监督的，通过在重建图像和原始图像之间应用反向传播来学习编码器和解码器的参数。

正如前面建议的，人们经常希望潜在空间具有有用的组织。此外，人们可能希望从自动编码器进行前馈、祖先采样[11]。对抗训练提供了实现这两个目标的途径。具体来说，可以在潜在空间和潜在空间上的预期先验分布之间应用对抗训练（潜在空间GAN）。这导致了一个综合的损失函数[22]，反映了重构误差和由候选编码网络产生的先验分布有多不同的度量。这种方法类似于变分自动编码器（VAE）[23]，其中潜在空间GAN扮演了损失函数的KL散度项的角色。

Mescheder等人在Adversarial Variational Bayes（AVB）框架中将变分自动编码器与对抗训练统一起来。Ian Goodfellow在NIPS 2016教程[12]中对自动编码器进行了更详细的总结。

IV. 训练GANs

在基于梯度的训练中，生成器和鉴别器共同构成了对抗网络的两个主要部分。这种训练可以通过多种方式进行，下面我们讨论其中的几种方法。

A. Mini-Max Game

在最初的GAN论文中，Goodfellow等人将训练过程描述为一个mini-max游戏，其中生成器试图最大化鉴别器对于其生成的样本的错误分类的概率，而鉴别器试图最小化这种概率。这产生了如下的minimax目标函数：

min⁡�max⁡��(�,�)=��∼�data(�)[log⁡�(�)]+��∼��(�)[log⁡(1−�(�(�)))]minG​maxD​V(D,G)=Ex∼pdata​(x)​[logD(x)]+Ez∼pz​(z)​[log(1−D(G(z)))]

其中，�(�)D(x) 是鉴别器将真实数据 �x 分类为真实数据的概率，�(�)G(z) 是生成器从潜在空间采样 �z 生成的数据，�(�(�))D(G(z)) 是鉴别器将生成的数据分类为真实数据的概率。

在训练过程中，生成器和鉴别器交替地更新参数，通过反向传播算法和随机梯度下降算法来优化目标函数。在每次更新时，生成器的参数会朝着使鉴别器更难将生成的样本与真实样本区分开的方向移动，而鉴别器的参数会朝着使其能够更好地区分生成的样本和真实样本的方向移动。

B. 改进的目标函数

尽管最初的GAN目标函数在理论上是可行的，但在实践中会出现一些问题，比如训练不稳定性和模式崩溃问题。因此，研究人员提出了许多改进的目标函数和训练技巧，以提高GAN的性能和稳定性。

其中一个常见的改进是使用非饱和的鉴别器损失，即使用对数损失代替最大化的对数损失，这可以减轻模式崩溃问题。此外，一些研究人员提出使用额外的正则化项或损失项来进一步稳定训练过程。

C. 评价GAN

评价GAN的性能是一个具有挑战性的问题，因为缺乏标准的客观评价标准。一种常见的评价方法是使用人类评分，即请人类评价生成的样本的质量和多样性。此外，还可以使用一些基于统计的度量标准，比如Fréchet Inception Distance（FID）和Inception Score（IS），这些度量标准尝试从图像的内容和统计分布的角度评估生成样本的质量。