---
Title: A Style-Based Generator Architecture for Generative Adversarial Networks
Url: https://arxiv.org/pdf/1812.04948.pdf
Publish: CVPR2019
Year: "2018"
---
## 摘要 

我们提出了一种为生成对抗网络（GAN）设计的替代生成器架构，该架构借鉴了风格迁移文献中的方法。新的架构能够自动学习并实现对高级属性（例如，在训练人脸时的姿态和身份）和生成图像中的随机变化（例如，雀斑、头发）的无监督分离，并且它能够直观地、按比例特定地控制合成过程。新的生成器在传统分布质量指标方面提高了最新水平，带来了明显更好的插值属性，并且更好地解耦了变化的潜在因素。为了量化插值质量和解耦，我们提出了两种新的自动化方法，适用于任何生成器架构。最后，我们介绍了一个新的、多样化且高质量的人脸数据集。

## 1. 引言 

近年来，特别是生成对抗网络（GAN）所产生的图像的分辨率和质量有了快速的提升。然而，生成器仍然像黑盒子一样运作，尽管最近有了一些努力，但对于图像合成过程（image synthesis）中的各个方面，例如随机特征的起源，仍然缺乏理解。潜在空间（latent space）的特性也未被充分理解，而通常展示的潜在空间插值没有提供量化的方式来比较不同的生成器。

受风格迁移文献的启发，我们重新设计了生成器架构，揭示了控制图像合成过程的新方法。我们的生成器从一个学习的常数输入开始，并根据潜在代码在每个卷积层调整图像的“风格”，从而直接控制不同尺度上图像特征的强度。结合直接注入网络的噪声，这种架构变化导致了在生成图像中高级属性（例如，姿态、身份）与随机变化（例如，雀斑、头发）的自动、无监督分离，并使得直观的比例特定混合和插值操作成为可能。**我们没有以任何方式修改鉴别器或损失函数**，因此我们的工作与当前关于GAN损失函数、正则化和超参数的讨论是正交的。

我们的生成器将 input latent code 嵌入到一个 intermediate latent space 中，这对网络中变化因素的表示方式产生了深远的影响。input latent code 必须遵循训练数据的概率密度，我们认为这导致了一定程度的不可避免的纠缠。我们的 intermediate latent space 不受此限制，因此被允许解耦。由于以前用于估计 latent space 解耦程度的方法在我们的案例中不直接适用，我们提出了两种新的自动化指标——感知路径长度和线性可分性——用于量化生成器的这些方面。使用这些指标，我们展示了与传统生成器架构相比，我们的生成器允许更线性、更少纠缠的不同变化因素的表示。

最后，我们提出了一个新的人脸数据集（Flickr-Faces-HQ，FFHQ），它提供了比现有高分辨率数据集更高的质量和更广泛的变异范围。我们已经公开了这个数据集，以及我们的源代码和预训练网络。随附的视频可以在同一个链接下找到。

## 2. 基于风格的生成器 

传统上，latent code 通过输入层提供给生成器，即前馈网络的第一层。我们通过完全省略输入层，而是从一个学习的常数开始来偏离这个设计。给定 input latent space $\mathcal{Z}$ 中的 latent code $\textbf{z}$，一个非线性映射网络 $f : \mathcal{Z} \to \mathcal{W}$ 首先产生 $\textbf{w} \in \mathcal{W}$。其中，$\mathbf{z}\in \mathbb{R}^{512}$，$\mathbf{w} \in \mathbb{R}^{512}$，并且使用一个 8 层的 MLP 来实现映射 $f$，我们将在第4.1节分析这个决定。

学习过的仿射变换然后将 $\mathbf{w}$ 特化为 styles $\textbf{y} = (\textbf{y}_s, \textbf{y}_b)$，这些 styles 控制合成网络 $g$ 中每个卷积层之后的自适应实例归一化（AdaIN）操作。AdaIN操作定义为AdaIN(xi , y) = ys,i xi − µ(xi) σ(xi) + yb,i，其中每个特征图xi都是分别归一化的，然后使用来自风格y的相应标量分量进行缩放和偏置。因此，y的维度是该层上特征图数量的两倍。

与我们的风格迁移方法相比，我们从向量 $\textbf{w}$ 而不是示例图像计算空间不变的 style $\textbf{y}$。我们选择重用“风格”这个词来表示y，因为类似的网络架构已经用于前馈风格迁移、无监督的图像到图像的翻译和领域混合。与更一般的特征变换相比，由于其效率和紧凑的表示，AdaIN特别适合我们的目的。

最后，我们通过引入明确的噪声输入为我们的生成器提供了直接生成随机细节的手段。这些是由不相关的高斯噪声组成的单通道图像，我们为合成网络的每层提供了一个专用的噪声图像。噪声图像使用学习过的每个特征的缩放因子广播到所有特征图，然后加到相应卷积的输出上，如图1b所示。添加噪声输入的影响在第3.2节和第3.3节中讨论。

### 2.1. 生成图像的质量 

在研究我们的生成器属性之前，我们通过实验证明重新设计并没有损害图像质量，实际上，它显著提高了图像质量。表1给出了CELEBA-HQ和我们新的FFHQ数据集中各种生成器架构的Frechet inception distances（FID）。其他数据集的结果在附录E中给出。我们的基础配置（A）是Karras等人的渐进式GAN设置，我们继承了网络和所有超参数，除非另有说明。我们首先通过使用双线性上/下采样操作、更长的训练时间和调整的超参数切换到改进的基础（B）。我们在附录C中包含了训练设置和超参数的详细描述。然后我们通过添加映射网络和AdaIN操作进一步改进这个新的基线（C），并惊讶地发现网络不再从将潜在代码输入到第一个卷积层中获益。因此，我们通过移除传统输入层并从学习到的4×4×512常数张量开始图像合成来简化架构（D）。我们发现合成网络即使只通过控制AdaIN操作的风格接收输入，也能产生有意义的结果，这相当了不起。最后，我们引入了噪声输入（E），进一步提高了结果，以及新颖的混合正则化（F），它解除了相邻风格的相关性，并使得对生成图像的控制更加精细（第3.1节）。我们使用两种不同的损失函数评估我们的方法：对于CELEBA-HQ，我们依赖于WGAN-GP，而对于FFHQ，我们为配置A使用WGAN-GP，对于配置B–F使用非饱和损失与R1正则化。我们发现这些选择给出了最好的结果。我们的贡献没有修改损失函数。我们观察到基于风格的生成器（E）相对于传统生成器（B）显著提高了FID，几乎提高了20%，这证实了在并行工作中对大规模ImageNet测量的结果。图2展示了使用我们的生成器从FFHQ数据集中生成的一组未筛选的新颖图像。正如FID所确认的，平均质量很高，甚至像眼镜和帽子这样的配件也成功合成了。对于这个图，我们避免使用所谓的截断技巧从W的极端区域采样——附录B详细介绍了如何在W而不是Z中执行这个技巧。请注意，我们的生成器允许仅选择性地应用于低分辨率的截断，以便不影响高分辨率细节。本文中的所有FID都是不使用截断技巧计算的，我们只在图2和视频中出于说明目的使用它。所有图像都是在10242分辨率下生成的。

### 2.2. 先前的工作 

关于GAN架构的大部分工作都集中在通过例如使用多个鉴别器、多分辨率鉴别或自我关注来改进鉴别器。关于生成器方面的工作主要集中在输入潜在空间中的确切分布，或通过高斯混合模型、聚类或鼓励凸性来塑造输入潜在空间。最近的有条件生成器通过一个单独的嵌入网络将类标识符馈送到生成器中的大量层，而潜在代码仍然通过输入层提供。一些作者考虑过将潜在代码的部分喂给多个生成器层。在并行工作中，Chen等人使用AdaINs“自我调节”生成器，与我们的工作类似，但他们没有考虑中间潜在空间或噪声输入。

## 3. 基于风格的生成器的属性 
   
我们的生成器架构使得通过按比例特定修改风格来控制图像合成成为可能。我们可以将映射网络和仿射变换视为从学习到的分布中为每种风格抽取样本的方式，将合成网络视为基于一系列风格的新颖图像生成的方式。每种风格的效果在网络中是局部化的，即，修改特定子集的风格只会影响图像的某些方面。

为了看这种局部化的原因，让我们考虑AdaIN操作（方程1）首先将每个通道归一化到零均值和单位方差，然后基于风格应用缩放和偏置。由风格决定的新通道统计数据修改了后续卷积操作中特征的相对重要性，但由于归一化，它们不依赖于原始统计数据。因此，每种风格只控制一个卷积，然后被下一个AdaIN操作覆盖。

### 3.1. 风格混合 

为了进一步鼓励风格的局部化，我们采用了混合正则化，在训练期间，一定比例的图像使用两个随机潜在代码而不是一个生成。在生成这样的图像时，我们简单地从一个潜在代码切换到另一个——我们称之为风格混合——在合成网络中的一个随机选择的点。具体来说，我们让两个潜在代码z1, z2通过映射网络运行，并让相应的w1, w2控制风格，以便w1在交叉点之前应用，w2在之后应用。这种正则化技术防止了网络假设相邻风格是相关的。

表2显示了启用混合正则化时FFHQ中的FID，用于训练不同百分比的训练样本。在这里，我们通过随机化1到4个潜在代码及其之间的交叉点来对训练过的网络进行压力测试。混合正则化显著提高了对这些不利操作的容忍度。标签E和F指的是表1中的配置。

## C. 超参数和训练细节 

我们在Karras等人的Progressive GANs的官方TensorFlow[1]实现的基础上进行构建，从中继承了大部分的训练细节。原始设置对应于表1中的配置A。特别是，我们使用相同的鉴别器架构、分辨率相关的小批量大小、Adam[33]的超参数以及生成器的指数移动平均值。我们为CelebA-HQ和FFHQ启用了镜像增强，但对于LSUN则禁用了。我们的训练时间大约为一周，使用了一台装有8个Tesla V100 GPU的NVIDIA DGX-1。 对于我们改进的基线（表1中的B），我们进行了几项修改以提高整体结果质量。我们将两个网络中的最近邻上/下采样替换为双线性采样，我们通过在每个上采样层之后和每个下采样层之前使用可分离的二阶二项式滤波器低通滤波来实现。我们以与Karras等人[30]相同的方式实现渐进式增长，但我们从8x8的图像开始，而不是4x4的图像。对于FFHQ数据集，我们从WGAN-GP切换到了非饱和损失[22]与R1正则化[44]，使用γ = 10。通过R1，我们发现FID分数的下降时间比WGAN-GP要长得多，因此我们将训练时间从1200万张图像增加到2500万张图像。对于FFHQ，我们使用与Karras等人[30]相同的学习率，但我们发现将512x512和1024x1024的学习率设置为0.002而不是0.003会使CelebA-HQ的稳定性更好。 对于我们的基于样式的生成器（表1中的F），我们使用泄漏ReLU[41]，α = 0.2，并为所有层使用均匀化学习率[30]。我们在卷积层中使用与Karras等人[30]相同的特征图数量。我们的映射网络由8个全连接层组成，所有输入和输出激活的维数——包括z和w——都是512。我们发现增加映射网络的深度往往会使训练不稳定，因此我们将映射网络的学习率降低了两个数量级，即λ0 = 0.01·λ。我们使用N(0, 1)初始化所有卷积、全连接和仿射变换层的权重。合成网络中的常量输入初始化为1。偏差和噪声缩放因子初始化为零，除了与ys关联的偏差，我们将其初始化为1。 我们的可分离性度量（第4.2节）所使用的分类器与我们的鉴别器具有相同的架构，只是禁用了小批量标准差[30]。我们使用学习率为10^-3，小批量大小为8，Adam优化器，训练长度为150,000张图像。