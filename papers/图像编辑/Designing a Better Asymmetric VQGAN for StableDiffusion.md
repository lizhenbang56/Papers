---
Url: https://arxiv.org/pdf/2306.04632.pdf
Year: "2023"
Month: "06"
---
朱子鑫1∗ 冯雪露1∗ 陈冬冬2 包建民2 王乐1 陈银鹏2 袁路2 华刚 1,3

## 摘要

StableDiffusion 是一种革命性的文本到图像生成器，它正在图像生成和编辑领域引起轰动。与传统方法在像素空间中学习扩散模型不同，StableDiffusion 通过 VQGAN 在潜在空间中学习扩散模型，确保了效率和质量。它不仅支持图像生成任务，还支持对真实图像进行图像编辑，例如图像修复和局部编辑。然而，我们观察到 StableDiffusion 中使用的默认 VQGAN 导致了严重的细节丢失，即使在未编辑的图像区域也会产生失真伪影。为此，我们提出了一个具有两个简单设计的新的非对称 VQGAN。首先，除了来自编码器的输入外，解码器还包含一个条件分支，该分支结合了来自特定任务先验的信息，例如修复中的未掩码图像区域。其次，解码器比编码器复杂得多，允许更详细的恢复，同时仅略微增加总推理成本。我们非对称 VQGAN 的训练成本很低，我们只需要重新训练一个新的非对称解码器，而保持默认 VQGAN 编码器和 StableDiffusion 不变。我们的非对称 VQGAN 可以广泛应用于基于 StableDiffusion 的修复和局部编辑方法。大量实验表明，它可以显着提高修复和编辑性能，同时保持原有的文本到图像生成能力。代码位于 https://github.com/buxiangzhiren/Asymmetric_VQGAN 。

## 1. 引言

扩散模型已经成为最流行的生成模型，在图像合成方面取得了显著成果。早期的扩散模型需要大量的计算资源，因为它们在 RGB 图像的高维像素空间中执行扩散过程。为了降低训练成本，同时保持生成质量，潜在扩散模型 (LDM) [31] 使用 VQ-GAN [10] 将扩散步骤转移到低维潜在空间。在随后的发展中，StableDiffusion 通过更大的模型和数据规模进一步扩展了 LDM，从而产生了一个功能强大的通用文本到图像生成器。自公开发布以来，它在生成式人工智能领域引起了极大的关注。

StableDiffusion 不仅具有文本到图像生成能力，还支持各种编辑相关任务，例如图像修复 [31] 和局部编辑任务 [5, 24, 40]。对于这些编辑任务，StableDiffusion 可以根据用户提供的输入条件为选定区域生成新内容，同时旨在保留其他区域。然而，我们观察到，所有现有方法 [31, 40] 中基于 StableDiffusion 的编辑结果在未编辑的图像区域都存在失真伪影，特别是对于具有精细结构（例如，文本）的区域。例如，如图 1 所示，尽管我们只打算修复黑色蒙版区域或将参考图像中提供的对象合成到蒙版区域中，但我们观察到非蒙版/未编辑区域存在严重的失真。

经过广泛的分析，我们发现这些问题是由 StableDiffusion 使用的默认 VQGAN 中存在的量化误差引起的。具体来说，VQ-GAN 利用编码器将图像多次下采样到潜在空间，然后根据码本对下采样的图像向量进行量化。因此，如果我们只将编码器的输出输入到解码器中，则即使对于未编辑区域也会出现量化误差，因为默认情况下默认 VQGAN 会执行此操作。此外，在推理过程中，VQGAN 编码器中使用的卷积层会由于蒙版区域而影响非蒙版区域的特征向量。

为此，我们在解码器部分设计了一个具有两个简单但有效设计的新型非对称 VQGAN。首先，我们将 VQGAN 解码器重新制定为条件解码器，以更好地支持局部编辑任务。这是通过引入一个额外的分支来实现的，该分支可以集成来自特定任务先验的信息。例如，对于修复或局部编辑任务，我们将未编辑的区域输入到这个分支中，以便解码器可以同时使用编码器的输出和原始的未编辑区域作为输入。相比之下，默认 VQGAN 解码器仅将 VQGAN 编码器的输出作为输入。其次，我们通过使用更深或更宽的解码器而不是与编码器类似的复杂性来增强解码器的功能。这种更强大的解码器可以更好地保留未编辑的区域并从编码器的量化输出中恢复更多细节。考虑到 StableDiffusion 推理中最耗时的部分是迭代扩散过程，这个更大的解码器只会稍微增加总推理成本。

除了推理成本之外，我们的非对称 VQGAN 的训练成本仍然很低。我们只需要重新训练一个新的非对称解码器，而保持 StableDiffusion 和原始默认 VQGAN 编码器不变。此外，通过交替将/不将特定任务先验输入到解码器中，我们的非对称 VQGAN 可以自然地支持需要特定任务先验的编辑任务和不需要此类先验输入的纯生成任务（例如文本到图像生成）。

为了证明我们非对称 VQGAN 的有效性，我们在三个不同的任务上进行了实验。在具有蒙版的修复和局部编辑任务（按示例绘画 [40]）中，我们的非对称 VQGAN 实现了最先进的性能（在 Place 数据集 [44] 上为 1.03 FID，在 COCOEE 数据集 [40] 上为 86.35% CLIP 分数）。在纯文本到图像任务中，与原始 StableDiffusion 相比，我们的模型还可以获得相当甚至更好的结果。我们的贡献可以总结为以下三点：

- 据我们所知，我们是第一个明确指出并研究基于 StableDiffusion 的编辑方法中的失真问题的。
    
- 我们设计了一个新的非对称 VQGAN 来解决上述失真问题，该 VQGAN 具有两个简单但有效的设计。与典型的对称 VQGAN 设计相比，这种新设计可以更好地保留未编辑区域并恢复细节，同时保持低训练和推理成本。
    
- 我们的非对称 VQGAN 在两个代表性任务上实现了最先进的性能：在 Place 数据集 [44] 上的修复任务和在 COCOEE 数据集上的局部编辑任务（即按示例绘画 [40]）。
    

## 2. 相关工作

### 2.1. 扩散模型

扩散模型是一系列强大的生成模型，由于其在各种任务上的出色表现，它们最近得到了发展并引起了广泛关注。最近的研究 [8, 35] 表明，扩散模型可以在高保真图像生成方面取得惊人的结果，甚至超过了生成对抗网络。扩散模型自然非常适合从复杂和多样化的数据中学习模型，并且最近提出了许多变体。例如，去噪扩散概率模型 (DDPM) [13] 是最流行的扩散模型，它学习对潜在变量的马尔可夫链执行扩散过程。去噪扩散隐式模型 (DDIM) [35] 被进一步提出以加速去噪过程。为了提高效率，同时保持高生成质量，潜在扩散模型 (LDM) [31] 提出在潜在空间而不是像素空间中学习扩散模型。

扩散模型已被证明在各种应用中非常有效，包括图像生成 [29, 26, 14, 4]、图像到图像的转换 [6, 37, 38]、超分辨率 [31] 和图像编辑 [26, 1]。特别是，最近扩散模型 [34] 的进步导致了最先进的图像合成 [8, 10, 9, 26, 32, 11, 2, 31] 以及其他模态（例如文本 [19]、音频 [18] 和视频 [15]）的生成模型。在本文中，我们专注于设计一种新的 VQGAN 架构，该架构可以提高 StableDiffusion 等在潜在空间中运行的扩散模型的性能，以用于图像生成和编辑任务。

### 2.2. VQGAN

矢量量化变分自动编码器 (VQ-VAE) [27, 30] 是一种广泛用于学习离散图像表示的方法。基于 VQ 的技术已成功应用于图像生成和完成，利用特征域中的学习码本。虽然 [27] 扩展了这种方法以使用学习表示的层次结构，但这些方法仍然依赖于来自卷积运算的密度估计，这使得捕获高分辨率图像中的远程交互变得具有挑战性。为了解决上述问题，提出了矢量量化生成对抗网络 (VQGAN) [10]，它确定强大的潜在空间自动编码器对于捕获图像细节以用于以下生成阶段至关重要。因此，VQGAN 使用 VQVAE 的量化过程并改进了 VQVAE 学习码本的丰富性。 VQGAN 使用 VQ-VAE 的量化过程并改进了其学习码本的丰富性。具体来说，它在第一阶段使用对抗性损失和感知损失来训练更好的自动编码器，以合成更大的图像细节。基于 VQ 的生成模型已应用于许多任务，包括文本 [19]、音频 [18]、图像修复 [22] 和视频 [15] 生成。虽然 VQGAN 有很多好处，但它引入的量化误差会导致图像细节丢失并造成严重的失真。受此观察的启发，与传统的对称 VQGAN 设计不同，我们探索了一种新的非对称 VQGAN 来保留更多细节，这可以使图像生成和编辑任务受益。

## 3. 方法

VQGAN 在 StableDiffusion 中起着重要作用，它将原始的高维像素空间映射到低维潜在空间。然而，这种映射过程会导致图像条件任务中的信息丢失，从而导致细节不足，从而损害生成结果的质量。在本节中，我们将首先讨论 VQGAN 中的信息丢失问题，然后介绍我们的解决方案，即非对称 VQGAN，它旨在解决这一挑战。

### 3.1. VQGAN 中的信息丢失

VQGAN 旨在将像素空间压缩成离散的潜在空间。假设 $X \in \mathbb{R}^{H\times W\times 3}$ 是输入图像。VQGAN 首先利用基于 CNN 的编码器来获得其特征变量 $\hat{z} \in \mathbb{R}^{h\times w\times n_z}$，其中 $h\times w$ 是空间分辨率，$n_z$ 是潜在向量的通道数。然后 VQGAN 旨在能够用离散码本 ${z_k}{k=1}^K$ 表示它，其中每个空间代码 $\hat{z}{ij}$ 在码本中找到其最近的码本条目 $z_k$，该过程可以表示如下：

$$  
z_{ij} = q(\hat{z}{ij}) := \underset{k\in 1,2,..,K}{\operatorname{argmin}} |\hat{z}{ij} - z_k| \quad (1)  
$$

其中 $q$ 是将向量映射到码本索引的量化编码器。基于量化的码字 $z$，VQGAN 然后采用解码器来重建输入图像 $x$。假设重建结果为 $\hat{x} = \operatorname{Dec}(q(\operatorname{Enc}(x))$。

然后可以通过损失函数端到端地训练模型和码本：

$$  
\begin{aligned}  
L_{VQ}(\operatorname{Enc}, \operatorname{Dec}, {z_k}{k=1}^K) &= L{pixel} + \lambda L_{percep} \  
&+ |\operatorname{sg}[\operatorname{Enc}(x)] - z|_2^2 + \beta |\operatorname{sg}[z] - \operatorname{Enc}(X)|_2^2.  
\end{aligned}\quad (2)  
$$

其中 $L_{pixel} = |x - \hat{x}|^2$ 是像素级损失，$L_{percep}$ 是使用 VGG16 [33] 网络计算的感知损失。$\operatorname{sg}[\cdot]$ 是停止梯度运算符，$\beta$ 是损失权重的超参数。为了进一步提高生成样本的质量，使用判别器与编码器和解码器执行对抗性训练过程。由于连续的像素空间被映射到有限的离散空间，因此在这个过程中存在信息丢失现象。

我们还注意到，在某些版本的 StableDiffusion 中，从像素空间到潜在空间的压缩是通过 KL-reg 训练的。 KL-reg 与 VQ-GAN 具有相似的目的，因为它们都试图避免任意高方差的潜在空间，并且它们都存在类似的问题：从像素空间到潜在空间的信息丢失。

### 3.2. 非对称 VQGAN

由于 StableDiffusion 在 text2image 生成方面的出色表现，它已被广泛应用于各种条件图像生成任务。这些条件中最重要和最典型的一个是使用图像输入。然而，根据我们的分析，这些图像条件必须映射到潜在空间才能满足 StableDiffusion 的扩散过程。结果，这些条件图像在操作过程中可能会丢失一些原始信息。我们在本文中的重点是保留条件图像输入的信息，同时保持 StableDiffusion 预训练权重不变。

为此，我们提出了非对称 VQGAN，以保留条件图像输入的信息。如图 2 所示，与原始 VQGAN 相比，非对称 VQGAN 涉及两个核心设计。首先，我们在 VQ-GAN 的解码器中引入一个条件分支，旨在处理条件输入以进行图像操作任务。其次，我们为 VQGAN 设计了一个更大的解码器，以便更好地恢复量化代码丢失的细节。在下一节中，我们将介绍非对称 VQGAN 的详细结构和训练策略。

**条件解码器。** 我们设计了一个条件解码器，旨在保留条件输入的细节。如图 3 所示，假设条件图像是一个带有掩码 $m$ 的掩码输入 $Y$，我们建议将条件图像输入表示为多级特征图，而不是将其压缩成单层特征。具体来说，我们将条件输入 $Y$ 输入到一个轻量级编码器 $E$ 中，并提取不同层的特征图作为条件输入表示。更正式地说，我们可以定义

$$
f_E(Y) = {f_1^E(Y), f_2^E(Y), \cdots f_n^E(Y)}, \quad (3)
$$

其中 $f_k^E(Y)$ 表示来自编码器 $E$ 的第 $k$ 级特征图，$n$ 是特征级别的数量。然后，这些特征将通过掩码引导混合 (MGB) 模块集成到解码器中。 MGB 旨在保留解码器解码潜在代码的能力，同时充分利用来自编码器 $E$ 的特征。它利用掩码直接复制解码器特征的掩码区域，同时组合来自编码器 $E$ 的未掩码区域特征。具体来说，假设解码器的第 $k$ 级特征是 $f_k^{\operatorname{Dec}}(z)$。所以混合过程可以表述为：

$$  
f_k^{\operatorname{Dec}}(z) = f_k^{\operatorname{Dec}}(z)\otimes m + f_k^E(Y)\otimes \bar{m}, \quad (4)  
$$

其中 $\otimes$ 是按元素相乘，$\bar{m} = 1 - m$。使用这个设计的掩码引导混合模块，我们不需要对解码器进行任何修改，只需将几个 MGB 模块插入解码器网络中，同时保持结构不变。

**更大的解码器。** 为了进一步增强解码器从给定潜在代码中恢复细节的能力，我们扩大了原始 VQGAN 的解码器模型大小。增加 VQGAN 的模型大小是有效的，因为在 StableDiffusion 用于条件图像输入任务的推理阶段，解码器只需要转发一次，而 StableDiffusion 模型需要转发多次，模型大小更大。

**训练策略。** 在训练期间，我们在非对称 VQGAN 中使用来自原始 VQGAN 的相同权重和码本，并且仅训练新的解码器。为了避免解码器开发仅从条件掩码输入中恢复信息的简单解决方案，我们考虑两种情况：一种是随机生成掩码，另一种是掩码完全填充为 1，这意味着解码器需要依赖潜在代码来恢复图像。我们交替使用这两种情况进行 50% 的训练过程。对于训练目标，我们使用 3.1 节中描述的像素级损失、感知损失和对抗性损失来仅更新解码器的权重。

非对称 VQGAN 轻巧且灵活，它保持编码和潜在空间扩散过程不变，其中我们可以利用编码器和 StableDiffusion 的所有预训练权重。我们只需要更改解码器部分，同时享受 StableDiffusion 的强大功能来完成各种任务。

## 4. 实验

为了证明我们模型的出色应用潜力，我们基于我们的基础模型和大型模型在三个不同的任务上进行了充分的实验。

**实现细节。** 根据 StableDiffusion [31] 中 VQGAN [10] 使用的训练设置，我们在 ImageNet [7] 数据集上训练我们的非对称 VQGAN。在训练期间，我们将图像分辨率预处理为 $256\times 256$，并训练我们的基础模型 12 个 epoch。这个过程在 8 个 NVIDIA V100 GPU 上大约需要 5 天，每个 GPU 的批量大小为 10，学习率在 처음 5,000 次迭代中预热到 3.6e-4。然后使用余弦调度器衰减学习率。至于我们的大型模型，我们使用 64 个 NVIDIA V100 GPU，每个 GPU 的批量大小为 5，学习率在 처음 5,000 次迭代中预热到 7.2e-4。大型模型的训练也大约需要 5 天，学习率也使用余弦调度器衰减。

**评估基准。** 对于我们的修复任务，我们使用与最近的修复模型 LaMa [36] 相同的协议来评估我们的模型，以生成图像掩码。我们的实验是在两个流行的数据集上进行的：Places [44] 和 ImageNet [7]。由于 Places 数据集中没有高分辨率图像，我们将 $256\times 256$ 的图像调整为 $512\times 512$。对于 ImageNet 数据集，我们尝试从 ImageNet 验证数据集中的每个类别中随机选择 3 张图像。但是，由于某些类别没有 3 张图像可用，我们最终选择了总共 2,968 张图像用于我们的实验。

对于按示例绘画任务，我们使用 COCOEE [40] 数据集评估我们的模型。 COCOEE 是一个基于 COCO 示例的图像编辑基准，包含来自 MSCOCO [20] 验证集的 3,500 张源图像。每张图像只包含一个边界框，并且掩码区域不超过整个图像的一半。相应的参考图像块选自 MSCOCO 训练集。

在文本到图像任务中，我们使用 MSCOCO [20] 验证集评估我们的模型。遵循广泛使用的“Karpathy”拆分 [17]，我们使用了 5,000 张图像进行验证，每张图像大约有 5 个标题。因此，我们根据标题生成了总共 25,000 张图像。所有图像都被调整为 $512\times 512$。

**评估指标。** 在修复任务中，我们使用 FID [12] 和 LPIPS [42] 作为指标来评估我们模型预测的质量。此外，为了展示模型保留图像未编辑区域的能力，我们报告了我们对这些未编辑区域的预测与真实值之间的均方误差 (MSE)。在按示例绘画任务中，我们使用未编辑图像区域的均方误差 (MSE) 和 CLIP 分数 [28] 来衡量我们模型输出的质量。 CLIP 分数评估编辑区域与参考图像之间的相似性。具体来说，我们将两张图像的大小调整为 $224\times 224$，通过 CLIP 图像编码器提取它们的特征，并计算它们的余弦相似度。较高的 CLIP 分数表示编辑区域与参考图像更相似。在文本到图像任务中，我们使用 FID 和 IS [3] 作为指标来评估我们模型的性能。

### 4.1. 修复任务评估

**与最先进方法的比较。** 表 1 显示了我们的修复方法与其他最先进方法的比较。我们的结果表明，将我们的非对称 VQGAN 应用于 StableDiffusion 可以将 FID 提高 1.24。此外，我们的方法在 40-50% 的图像区域需要修复的困难示例上优于其他方法，进一步突出了我们的条件分支和更大的解码器的有效性。

**我们模块的有效性。** 这项消融研究旨在支持我们的条件分支和更大的解码器的有效性。表 2 展示了将我们的条件解码器与 StableDiffusion [31] 中的原始解码器进行比较的修复结果。具体来说，我们用我们的条件解码器替换原始解码器，以解码从 StableDiffusion 获得的量化向量。值得注意的是，发送到不同解码器的示例结果是相同的，因为扩散过程会生成各种示例结果。

我们的基础模型和大型模型在 FID 上都显示出大约 2.00 的改进，在 LPIPS 上显示出 50% 的改进。此外，未编辑图像区域的错误显着减少。这些结果表明，在条件分支的帮助下，VQGAN 保留细节的能力得到了很大的提高。如果我们不使用条件分支，我们的基础模型仍然可以执行与原始解码器相同的功能。而我们更大的模型可以进一步超越原始解码器。这些结果表明，我们的模型兼容各种应用，无论它们是否有掩码。

**加法或级联。** 我们的模型提供两种不同的混合方法：掩码引导加法和掩码引导级联。这项消融研究旨在探索它们如何改进 VQGAN。由于掩码是硬掩码，这意味着其值要么为 0 要么为 255，因此当混合方式为掩码引导加法时，我们的条件分支不使用部分卷积层 [21]。这意味着主分支负责编辑区域，而条件分支处理未编辑的图像区域。因此，推断编辑区域信息的偏卷积层不能对条件分支做出贡献。详细结果如表 3 所示。级联的整体性能与加法相似。最后，我们选择加法作为我们的混合方式。

**视觉比较。** 为了保留未编辑图像区域的细节，一种常见的方法是通过将输入的未编辑区域添加到结果的编辑区域进行后处理。然而，这种朴素的解决方案会导致不协调问题，如第三栏图 4 所示。相比之下，我们的非对称 VQGAN 生成的图像与基线（原始 StableDiffusion）具有相同级别的协调性，同时保留了未编辑区域的许多细节。这表明我们的方法有效地保留了细节，而不会损害图像的协调性。总的来说，我们的结果表明，我们的模型具有非对称解码器和掩码引导加法混合方式，可以显着提高修复任务的性能，同时保持协调性并保留未编辑图像区域的细节。

### 4.2. 按示例绘画

按示例绘画 [40] 是一种新颖的图像编辑场景，它根据示例图像语义地改变图像内容。他们的方法依赖于 StableDiffusion 作为强先验，使得我们的模型可以轻松地应用于他们的方法。

**与最先进方法的比较。** 比较结果如表 4 所示，其中“CLIP 分数”表示编辑图像的编辑区域与参考图像之间的相似性，而“Pre error”表示编辑图像的未编辑图像区域与源图像之间的 MSE。我们的模型可以在掩码区域和未编辑图像区域中实现最佳性能。可以看出，我们的模型可以在掩码区域和未编辑图像区域中实现最佳性能。

**我们模块的有效性。** 这项消融研究的目的是证明我们条件分支和更大的解码器的有效性。结果如表 5 所示。与修复任务相比，按示例绘画任务涉及组合两个不同的图像，导致未编辑图像区域的细节丢失更严重。图像的未编辑区域会受到另一个图像的影响，这使得任务更加复杂。我们的基础模型将保留误差从 588.86 降低到 1.37，而我们更大的模型进一步将其降低到 0.76。令人惊讶的是，我们发现条件分支不仅改善了未编辑图像区域的保留，而且还提高了编辑区域的生成质量，如“CLIP 分数”所衡量的那样

**视觉结果。** 我们在图 5 和图 6 中提供了大量可视化结果，以展示我们非对称 VQGAN 的保留能力和协调性。这些可视化结果表明，我们的模型可以毫不费力地应用于不同数据集上的各种任务，同时始终如一地提高性能。我们坚信我们的模型在广泛的应用中具有巨大的潜力。

### 4.3. 文本到图像

这些实验旨在证明我们的非对称 VQGAN 可以处理没有掩码或特定任务先验的任务，以及有掩码（特定任务先验）的任务。结果如表 6 所示。我们可以观察到，当我们的基础模型不使用条件分支时，其性能与基线相当，这表明用全掩码替换一些掩码的训练策略是成功的。此外，当我们的大型模型不使用条件分支时，其性能与基线相当，这表明即使在没有条件分支的帮助下，更大的解码器也可以恢复更多细节。

**视觉结果。** 为了支持我们关于我们的非对称 VQGAN 即使没有条件分支也能表现良好的说法，我们在图 7 中展示了可视化结果。在第一行中，很明显我们的模型可以在没有掩码的情况下有效工作，并且不会产生劣质结果。在第二行和第三行中，我们观察到我们更大的解码器可以在一定程度上恢复更多复杂的细节。

**我们模块的有效性。** 这项消融研究的目的是证明我们条件分支和更大的解码器的有效性。结果如表 5 所示。与修复任务相比，按示例绘画任务涉及组合两个不同的图像，导致未编辑图像区域的细节丢失更严重。图像的未编辑区域会受到另一个图像的影响，这使得任务更加复杂。我们的基础模型将保留误差从 588.86 降低到 1.37，而我们更大的模型进一步将其降低到 0.76。令人惊讶的是，我们发现条件分支不仅改善了未编辑图像区域的保留，而且还提高了编辑区域的生成质量，如“CLIP 分数”所衡量的那样。

**视觉结果。** 我们在图 5 和图 6 中提供了大量可视化结果，以展示我们非对称 VQGAN 的保留能力和协调性。这些可视化结果表明，我们的模型可以毫不费力地应用于不同数据集上的各种任务，同时始终如一地提高性能。我们坚信我们的模型在广泛的应用中具有巨大的潜力。

### 4.4. 文本到图像

这些实验旨在证明我们的非对称 VQGAN 可以处理没有掩码或特定任务先验的任务，以及有掩码（特定任务先验）的任务。结果如表 6 所示。我们可以观察到，当我们的基础模型不使用条件分支时，其性能与基线相当，这表明用全掩码替换一些掩码的训练策略是成功的。此外，当我们的大型模型不使用条件分支时，其性能与基线相当，这表明即使在没有条件分支的帮助下，更大的解码器也可以恢复更多细节。

**视觉结果。** 为了支持我们关于我们的非对称 VQGAN 即使没有条件分支也能表现良好的说法，我们在图 7 中展示了可视化结果。在第一行中，很明显我们的模型可以在没有掩码的情况下有效工作，并且不会产生劣质结果。在第二行和第三行中，我们观察到我们更大的解码器可以在一定程度上恢复更多复杂的细节。

## 5. 结论

在本文中，我们提出了一种用于 StableDiffusion 的新型非对称 VQGAN，它具有两个新的设计特征。首先，我们的解码器包含一个额外的条件分支，允许它接受 VQGAN 编码器的输出和特定任务先验作为输入。其次，我们的解码器被设计成比编码器更复杂（例如更深更宽），使其能够更好地保留未编辑区域的局部细节并从编码器的量化输出中恢复细节。我们的非对称 VQGAN 架构对于训练和推理都非常有效，并且可以用于局部编辑任务和纯文本到图像生成任务。通过在两个代表性任务上的大量实验，我们证明了我们的非对称 VQGAN 设计的有效性。展望未来，我们计划探索扩大解码器规模是否可以进一步提高我们结果的质量。

## 参考文献

（省略）