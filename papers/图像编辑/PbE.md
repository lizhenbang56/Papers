- [[2211.13227 Paint by Example.pdf]]
- [Fantasy-Studio/Paint-by-Example: Paint by Example: Exemplar-based Image Editing with Diffusion Models (github.com)](https://github.com/Fantasy-Studio/Paint-by-Example)
	- 945 stars
- 输入
	- 源图像+指定的要替换的区域
	- 参考图像，用于填充到上述区域中
- 输出
	- 目标图像，指定区域中的内容是源图像

**作者：*杨斌鑫1、顾树阳2、张博2、张婷2、陈雪锦1、孙晓燕1、陈栋2、文方2

**机构：**1 中国科学技术大学；2 微软亚洲研究院

**摘要**

语言引导的图像编辑最近取得了巨大成功。在本文中，我们首次研究了基于示例图像的图像编辑，以实现更精确的控制。我们利用自监督训练来解耦和重组源图像和示例图像，从而实现这一目标。然而，这种简单的方法会导致明显的融合伪影。我们仔细分析了这一问题，并提出了信息瓶颈和强增强方法，以避免直接复制和粘贴示例图像的简单解决方案。同时，为了确保编辑过程的可控性，我们为示例图像设计了一个任意形状的掩模，并利用无分类器引导来增加与示例图像的相似性。整个框架只需进行一次扩散模型的前向传播，无需任何迭代优化。我们证明了我们的方法取得了令人印象深刻的性能，并能够在具有高保真度的野外图像上进行可控编辑。代码和预训练模型可在 [https://github.com/FantasyStudio/Paint-by-Example](https://github.com/FantasyStudio/Paint-by-Example) 获取。

**1. 简介**

随着众多社交媒体平台的进步，对照片进行创意编辑已成为一种普遍需求。基于人工智能的技术显著降低了传统上需要专业软件和劳动密集型手动操作的图像编辑门槛。深度神经网络现在可以通过学习丰富可用的配对数据，为各种低级图像编辑任务产生令人信服的结果，例如图像修复、合成、着色和美学增强。另一方面，更具挑战性的场景是语义图像编辑，它旨在操纵图像内容的高级语义，同时保持图像的真实感。沿着这条道路，人们做出了巨大的努力，主要依靠生成模型（如 GAN）的语义潜在空间，但现有的大多数工作都仅限于特定的图像类型。

最近，基于自回归模型或扩散模型的大规模语言图像（LLI）模型在建模复杂图像方面显示出前所未有的生成能力。这些模型能够完成以前无法实现的各种图像操作任务，允许在文本提示的指导下对一般图像进行编辑。然而，即使是详细的文本描述也不可避免地会引入歧义，并且可能无法准确反映用户期望的效果；事实上，许多细粒度的物体外观很难用简单的语言来指定。因此，开发一种更直观的方法来简化新手和非母语人士的细粒度图像编辑至关重要。

在本文中，我们提出了一种基于示例的图像编辑方法，该方法允许根据用户提供或从数据库中检索的示例图像对图像内容进行精确的语义操作。俗话说，“一图胜千言”。我们相信图像比文字能以更细致的方式更好地传达用户期望的图像定制。这项任务与图像协调完全不同，图像协调主要关注合成前景对象时的颜色和光照校正，而我们的目标是更复杂的工作：语义转换示例（例如，产生不同的姿势、变形或视点），以便可以根据图像上下文无缝地植入编辑后的内容。实际上，我们的方法自动化了传统的图像编辑工作流程，在该工作流程中，艺术家对图像资产执行繁琐的转换以实现图像的连贯融合。

为了实现我们的目标，我们训练了一个以示例图像为条件的扩散模型。与文本引导模型不同，核心挑战在于无法收集到足够的三元组训练对，包括源图像、示例图像和相应的编辑真值。一种解决方法是从输入图像中随机裁剪对象，作为训练修复模型时的参考。然而，从这种自我参考设置训练的模型无法泛化到真实的示例，因为模型只是学习将参考对象复制并粘贴到最终输出中。我们确定了几个关键因素来规避这个问题。首先是利用生成先验。具体来说，预训练的文本到图像模型能够生成高质量的预期结果，我们将其用作初始化，以避免陷入复制粘贴的简单解决方案。然而，长时间的微调仍然会导致模型偏离先验知识，并最终再次退化。因此，我们引入了自我参考条件的信息瓶颈，其中我们删除了空间标记，只将全局图像嵌入作为条件。通过这种方式，我们强制网络理解示例图像和来自源图像的上下文的高级语义，从而防止在自监督训练期间出现简单结果。此外，我们对自我参考图像应用了积极的增强，可以有效地减少训练-测试差距。

我们从两个方面进一步提高了我们方法的可编辑性。一个是我们的训练使用不规则随机掩模，以便模拟实际编辑中使用的随意用户笔刷。我们还证明，无分类器引导有助于提高图像质量和与参考图像的风格相似度。

据我们所知，我们是第一个解决语义图像合成问题的，其中参考图像在融合到另一个图像之前经过语义转换和协调，如图1和图2所示。与类似设置下的先前工作相比，我们的方法显示出显著的质量优势。值得注意的是，我们的编辑只需要进行一次扩散模型的前向传播，无需任何特定于图像的优化，这对于许多实际应用来说是必要的。总之，我们的贡献如下：

- 我们提出了一种新的图像编辑场景，它基于示例图像对图像内容进行语义更改。这种方法提供了细粒度的控制，同时方便使用。
    
- 我们使用以自监督方式训练的图像条件扩散模型来解决问题。我们提出了一组技术来解决退化挑战。
    
- 根据定量指标和主观评估，我们的方法在野外图像编辑方面优于先前的技术。
## 2. 相关工作

**图像合成** 从一幅图像中剪切前景并将其粘贴到另一幅图像上，形成逼真的合成图，这是照片编辑中常见且广泛使用的操作。许多方法被提出，重点关注图像协调，以使合成图看起来更逼真。传统方法倾向于提取手工制作的特征来匹配颜色分布。最近的工作利用深度语义特征来提高鲁棒性。最近的一项工作 DCCF 提出了四种人类可理解的神经滤波器，并以金字塔的方式实现了最先进的色彩协调结果。然而，它们都假设前景和背景在语义上是和谐的，并且只调整低级色彩空间中的合成，而保持结构不变。在本文中，我们的目标是语义图像合成，并将挑战性的语义不和谐考虑在内。

**语义图像编辑** 语义图像编辑旨在编辑图像的高级语义，由于其潜在的应用，它在视觉和图形社区中引起了极大的兴趣。一项稳步发展的工作仔细剖析了 GAN 的潜在空间，旨在找到语义解耦的潜在因素。而其他研究工作则利用判别模型（如属性分类器或人脸识别模型）来帮助解耦和操作图像。另一个流行的研究方向是利用语义掩模来控制编辑。然而，现有的大多数方法都仅限于特定的图像类型，例如人脸、汽车、鸟类、猫等。在本文中，我们专注于介绍一种适用于一般和复杂图像的高精度模型。

**文本驱动的图像编辑** 在各种语义图像编辑中，文本引导的图像编辑近年来引起了越来越多的关注。早期的工作利用预训练的 GAN 生成器和文本编码器，根据文本提示逐步优化图像。然而，由于 GAN 建模能力的限制，这些基于 GAN 的操作方法难以编辑复杂场景或各种物体的图像。扩散模型的迅速兴起和发展显示出合成高质量和多样化图像的强大能力。许多工作利用扩散模型进行文本驱动的图像编辑。例如，DiffusionCLIP、dreambooth 和 Imagic 对扩散模型进行了针对不同文本提示的特定情况微调。Blended Diffusion 提出了一种多步骤混合过程，使用用户提供的掩模执行局部操作。虽然这些方法取得了非常令人印象深刻的结果，但我们认为语言引导仍然缺乏精确的控制，而图像可以更好地表达人们的具体想法。因此，在这项工作中，我们对基于示例的图像编辑感兴趣。

**3. 方法**

**4. 实验**

**5. 结论**

**参考文献**

**附录 A. 附加结果**

**附录 B. 实现细节**

**附录 C. 限制**