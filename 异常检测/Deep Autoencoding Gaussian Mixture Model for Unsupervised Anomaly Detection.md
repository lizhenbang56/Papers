- [pdf (openreview.net)](https://openreview.net/pdf?id=BJJLHbb0-)
- ICLR2018

# 摘要

对多维或高维数据进行无监督异常检测在基础机器学习研究和工业应用中都非常重要，其中密度估计是其核心。尽管之前基于降维和密度估计的方法取得了丰硕的进展，但它们主要面临模型学习解耦、优化目标不一致以及无法保留低维空间中的基本信息的问题。在本文中，我们提出了一种用于无监督异常检测的深度自编码高斯混合模型（DAGMM）。我们的模型利用深度自动编码器为每个输入数据点生成低维表示和重建误差，并将其进一步馈送到高斯混合模型（GMM）中。 DAGMM 没有使用解耦的两阶段训练和标准期望最大化（EM）算法，而是以端到端的方式同时联合优化深度自动编码器和混合模型的参数，利用单独的估计网络来促进混合模型的参数学习。联合优化很好地平衡了自动编码重建、潜在表示的密度估计和正则化，帮助自动编码器摆脱不太有吸引力的局部最优，并进一步减少重建误差，避免了预训练的需要。在多个公共基准数据集上的实验结果表明，DAGMM 显着优于最先进的异常检测技术，并在标准 F1 分数的基础上实现了高达 14% 的改进。 

# 1 引言 

**无监督异常检测**是机器学习中的一个基本问题，在许多领域都有关键应用，例如网络安全（Tan 等人（2011））、复杂系统管理（Liu 等人（2008））、医疗保健（凯勒等人（2012）），等等。异常检测的核心是密度估计：给定大量输入样本，异常是那些位于低概率密度区域的异常。尽管过去几年取得了卓有成效的进展，但在没有人工监督的情况下对多维或高维数据进行稳健的异常检测仍然是一项具有挑战性的任务。特别是，当输入数据的维度变得更高时，在原始特征空间中执行密度估计变得更加困难，因为任何输入样本都可能是观察概率较低的罕见事件（Chandola et al.（2009））。为了解决维数灾难引起的这个问题，广泛采用两步方法（Candès et al. (2011)），首先进行降维，然后在潜在低维空间中进行密度估计。然而，这些方法很容易导致性能不佳，因为第一步中的降维不知道后续的密度估计任务，并且异常检测的关键信息可能会首先被删除。因此，需要将降维和密度估计的力量结合起来，尽管考虑这两个组成部分的联合优化通常在计算上很困难。最近的几篇论文（Zhai et al. (2016); Yang et al. (2017a); Xie et al. (2016)）利用深度网络强大的建模能力探索了这个方向，但最终的性能受到限制，要么是由于低维空间减少而无法保留输入样本的基本信息，要么是没有足够容量的过度简化的密度估计模型，或者是不适合密度估计任务的训练策略。

图 1：私有网络安全数据集中样本的低维表示：(1) 每个样本表示最初具有 20 个维度的网络流，(2) 红/蓝点是异常/正常样本，(3) 横轴表示通过深度自动编码器学习的缩减一维空间，(4) 纵轴表示由一维表示引起的重建误差。

在本文中，我们提出了深度自编码高斯混合模型（DAGMM），这是一种深度学习框架，从多个方面解决了无监督异常检测中的上述挑战。首先，DAGMM 将输入样本的关键信息保留在低维空间中，其中包括通过降维发现的降维特征和引入的重建误差。从图1所示的例子中，我们可以看到异常样本与正常样本有两个不同之处：

- (1) 异常样本在降维中可能存在显着偏差，其特征相关性不同； 
- (2)与正常样本相比，异常样本更难重建。

与仅涉及性能次优的某一方面的现有方法（Zimek 等人（2012）；Zhai 等人（2016））不同，DAGMM 利用称为压缩网络的子网络通过自动编码器执行降维，它通过连接编码中减少的低维特征和解码中的重建误差来为输入样本准备低维表示。其次，DAGMM 在学习的低维空间上利用高斯混合模型（GMM）来处理具有复杂结构的输入数据的密度估计任务，这对于现有工作中使用的简单模型来说相当困难（Zhai 等人（2016） ））。 GMM能力强大的同时，也给模型学习带来了新的挑战。由于 GMM 通常是通过期望最大化（EM）（Huber（2011））等交替算法来学习的，因此很难执行有利于 GMM 学习的降维和密度估计的联合优化，这通常会退化为传统的两步方法。为了解决这一训练挑战，DAGMM 使用称为估计网络的子网络，该子网络从压缩网络获取低维输入，并输出每个样本的混合隶属度预测。通过预测的样本隶属度，我们可以直接估计GMM的参数，便于评估输入样本的能量/似然度。通过同时最小化压缩网络的重建误差和估计网络的样本能量，我们可以联合训练直接帮助目标密度估计任务的降维组件。

最后，DAGMM 对端到端训练很友好。通常，通过端到端训练来学习深度自编码器是很困难的，因为它们很容易陷入吸引力较小的局部最优，因此预训练被广泛采用（Vincent et al. (2010); Yang et al.（ 2017a)；Xie 等人 (2016))。然而，预训练限制了调整降维行为的潜力，因为很难通过微调对训练有素的自动编码器做出任何重大改变。我们的实证研究表明，DAGMM 通过端到端训练得到了很好的学习，因为估计网络引入的正则化极大地帮助压缩网络中的自动编码器摆脱了不太有吸引力的局部最优。在多个公共基准数据集上的实验表明，DAGMM 具有优于最先进技术的性能，异常检测的 F1 分数提高了高达 14%。此外，我们观察到，通过端到端训练，DAGMM 中的自动编码器的重建误差与预训练对应物的重建误差一样低，而未经估计网络正则化的自动编码器的重建误差保持不变高的。此外，端到端训练的 DAGMM 显着优于所有依赖预训练自动编码器的基线方法。 

# 2 相关工作 

人们在无监督异常检测方面付出了巨大的努力（Chandola 等人（2009）），**现有的方法可以分为三类**。基于重建的方法假设异常是不可压缩的，因此不能从低维投影有效地重建。此类中的传统方法包括具有显式线性投影的主成分分析 (PCA) (Jolliffe (1986))、具有由特定核引发的隐式非线性投影的核 PCA (Gu ̈ nter 等人) 和鲁棒 PCA (RPCA) （Huber (2011)；Candès et al. (2011)）通过强制稀疏结构降低 PCA 对噪声的敏感度。此外，最近的多项工作提出分析深度自动编码器引起的重建误差，并展示了有希望的结果（Zhou & Paffenroth (2017)；Zhai et al. (2016)）。然而，基于重建的方法的性能受到限制，因为它们仅从单一方面（即重建误差）进行异常分析。尽管异常样本的压缩可能与正常样本的压缩不同，并且其中一些样本确实表现出异常高的重建误差，但大量异常样本也可能潜伏着正常水平的误差，这通常发生在底层降维时方法具有较高的模型复杂度或感兴趣的样本具有复杂结构的噪声。即使在这些情况下，我们仍然有希望检测到这种“潜伏”的异常，因为它们仍然存在于缩小的低维空间中的低密度区域。与现有的基于重建的方法不同，DAGMM 考虑了这两方面，并根据减少的表示和降维引起的重建误差在低维空间中进行密度估计，以获得全面的视图。

聚类分析是用于密度估计和异常检测的另一类流行方法，例如多元高斯模型、高斯混合模型、k 均值等（Barnett & Lewis (1984)；Zimek et al. (2012)；Kim & Scott (2011)；Xiong 等人 (2011))。由于维数灾难，很难将此类方法直接应用于多维或高维数据。传统技术采用两步方法（Chandola et al. (2009)），首先进行降维，然后进行聚类分析，两个步骤分别学习。两步方法的缺点之一是降维训练没有后续聚类分析的指导，因此在降维过程中可能会丢失聚类分析的关键信息。为了解决这个问题，最近的工作提出了基于深度自动编码器的方法，以便共同学习降维和聚类组件。然而，最先进方法的性能受到过度简化的聚类模型的限制，这些模型无法处理复杂结构数据的聚类或密度估计任务，或者预训练的降维组件（即自动编码器） ）几乎没有潜力通过随后的异常检测微调来进行进一步调整。 DAGMM 通过称为估计网络的子网络明确解决这些问题，该子网络评估其压缩网络产生的低维空间中的样本密度。通过预测样本混合隶属度，我们能够估计 GMM 的参数，而无需类似 EM 的交替过程。此外，DAGMM对端到端训练友好，使我们可以充分发挥调整降维组件的潜力，共同提高聚类分析/密度估计的质量。此外，一类分类方法也广泛用于异常检测。在此框架下，通过算法学习围绕正常实例的判别边界，3 作为单类 SVM 等会议论文发表在 ICLR 2018（Chen 等人（2001）；Song 等人（2002）；Williams 等人等（2002））。当维度数量增加时，此类技术通常会因维度灾难而导致性能不佳。与这些方法不同，DAGMM 估计联合学习的低维空间中的数据密度，以实现更稳健的异常检测。人们对降维（特征选择）和高斯混合建模的联合学习越来越感兴趣。杨等人。 (2014;2017b)提出了一种联合学习线性降维和 GMM 的方法。 Paulik (2013) 研究了如何使用预训练的 GMM 作为正则化器来执行更好的特征选择。瓦里安尼等人。 (2015) 和Zhang & Woodland (2017) 提出了联合学习框架，其中GMM 的参数是通过语音识别应用中的监督信息直接估计的。 Tu ̈ ske 等人。 (2015a;b) 研究如何在给定类/混合先验分布且协方差矩阵全局共享的条件下使用对数线性混合模型来近似 GMM 后验。与现有的工作不同，我们专注于无监督设置：DAGMM 通过深度自动编码器实现的非线性降维来提取用于异常检测的有用特征，并通过混合隶属度估计在 GMM 框架下共同学习它们的密度，其中 DAGMM 可以查看作为与深度自动编码器相结合的更强大的深度无监督版本的自适应专家混合（Jacobs 等人（1991））。更重要的是，DAGMM 结合了诱导重建误差和学习的潜在表示来进行无监督异常检测。 3 3.1 深度自编码高斯混合模型概述 图2：深度自编码高斯混合模型概述 深度自编码高斯混合模型（DAGMM）由两个主要部分组成：压缩网络和估计网络。如图 2 所示，DAGMM 的工作原理如下：（1）压缩网络通过深度自动编码器对输入样本进行降维，从降维空间和重构误差特征中准备其低维表示，并将表示馈送到后续估计网络； (2)估计网络获取馈送，并在高斯混合模型（GMM）的框架中预测它们的可能性/能量。 

## 3.2 压缩网络 

压缩网络提供的低维表示包含两个特征源：（1）深度自编码器学习的简化低维表示； (2)从重建误差导出的特征。给定样本 x，压缩网络计算其低维表示 z 如下。 x0 = g(zc ; θd ), zc = h(x; θe ), 0 zr = f (x, x ), z = [zc , zr ], 4 (1) (2) (3) 作为会议发布ICLR 2018 的论文，其中 zc 是深度自动编码器学习的简化低维表示，zr 包括从重建误差导出的特征，θe 和 θd 是深度自动编码器的参数，x0 是 x, h( ·)表示编码函数，g(·)表示解码函数，f(·)表示计算重构误差特征的函数。特别地，zr可以是多维的，考虑多个距离度量，例如绝对欧几里德距离、相对欧几里德距离、余弦相似度等。最后，压缩网络将z馈送到后续的估计网络。3.3 估计网络 给定输入样本的低维表示，估计网络在 GMM 框架下进行密度估计。在未知混合成分分布 φ、混合平均值 µ 和混合协方差 Σ 的训练阶段，估计网络估计 GMM 参数并评估样本的似然/能量，而无需使用 EM 等交替程序（Zimek 等人（2012 年） ））。估计网络通过利用多层神经网络来预测每个样本的混合隶属度来实现这一点。给定低维表示 z 和整数 K 作为混合分量的数量，估计网络进行如下的隶属度预测。 p = M LN (z; θm ), γ ̂ = softmax(p), (4) 其中γ ̂是用于软混合分量隶属度预测的 K 维向量，p 是多层网络的输出由θ m 参数化。给定一批 N 个样本及其隶属度预测， ∀ 1 ≤ k ≤ K，我们可以进一步估计 GMM 中的参数，如下所示。 PN PN NTX γ ̂ ik zi γ ̂ ik i=1 γ ̂ ik (zi − µ ̂ k )(zi − µ ̂ k ) φ ̂ k = , µ ̂ k = Pi=1 , Σ ̂ = . (5) PN k NN i=1 γ ̂ ik i=1 γ ̂ ik i=1 其中γ ̂ i 是低维表示 zi 的隶属度预测， φ ̂ k 、 µ ̂ k 、 Σ ̂ k 分别是分别是 GMM 中分量 k 的混合概率、均值和协方差。利用估计的参数，可以通过 XK exp − 21 (z − µ ̂ k )T Σ ̂− 1 k (z − µ ̂ k ) q E(z) = − log φ ̂ k 。 k=1 |2 π Σ ̂ k | (6) 其中 | · |表示矩阵的行列式。此外，在使用学习到的 GMM 参数的测试阶段，可以直接估计样本能量，并通过预先选择的阈值将高能量样本预测为异常。 

## 3.4 目标函数

给定一个包含 N 个样本的数据集，指导 DAGMM 训练的目标函数构建如下。 NN 1 X λ 1 XJ( θ e , θ d , θ m ) = L(xi , x0i ) + E(zi ) + λ 2 P ( Σ ̂ )。 (7) Ni=1 Ni=1 该目标函数包括三个分量。 L(xi , x0i ) 是表征压缩网络中深度自编码器引起的重建误差的损失函数。直观上，如果压缩网络能够使重建误差较低，则低维表示可以更好地保留输入样本的关键信息。因此，总是需要一种具有较低重构误差的压缩网络。在实践中，L2-范数通常会给出理想的结果，2 为 L(xi , x0i ) = kxi − x0i k2 。 E(zi ) 对我们可以观察输入样本的概率进行建模。通过最小化样本能量，我们寻找压缩和估计网络的最佳组合，以最大化观察输入样本的可能性。 5 作为会议论文发表于 ICLR 2018 DAGMM 也存在与 GMM 一样的奇点问题：当协方差矩阵中的对角线条目退化为 0 时，会触发平凡解。为了避免这个问题，我们通过 P (Σ ̂ ) = k=1来惩罚对角线条目上的 PK Pd 小值j=1 Σ ̂ 1 ，其中 d 是压缩网络提供的低维表示中的维数 kjj 。  λ 1 和λ 2 是DAGMM 中的元参数。实际上， λ 1 = 0.1 和λ 2 = 0.005 通常会产生理想的结果。 

## 3.5 与变分推理的关系 

在 DAGMM 中，我们利用估计网络对每个样本进行隶属度预测。从概率图模型的角度来看，估计网络起着类似于潜在变量（即样本隶属度）推断的作用。最近，神经变分推理（Mnih & Gregor (2014)）被提出，利用深度神经网络来解决困难的潜变量推理问题，其中精确的模型推理很棘手，传统的近似方法无法很好地扩展。理论上，我们也可以将DAGMM的隶属度预测任务应用到神经变分推理的框架中。对于样本 xi ，其压缩表示 zi 对能量函数的贡献可以是上限如下（Jordan et al. (1999)）， XE(zi ) = − log p(zi ) = − log p(zi , k) k = − log Q θ m (k | zi ) p(zi , k) Q θ m (k | zi ) Q θ m (k | zi ) log p(zi , k) Q θ m (k | zi ) ) X k ≤ − X k = − EQ θ m [log p(zi , k) − log Q θ m (k | zi )] (8) = − EQ θ m [log p(zi | k)] + KL (Q θ m (k | zi )||p(k)) (9) = − log p(zi ) + KL(Q θ m (k | zi )||p(k | zi )) = E(zi ) + KL(Q θ m (k | zi )||p(k | zi )) (10) 其中 Q θ m (k | zi ) 是预测 zi 隶属度的估计网络，KL( · || · ) 是两个输入分布之间的 Kullback-Leibler 散度，p(k) = φk 是要估计的混合系数，p(k | zi ) 是给定 zi 的混合分量 k 的后验概率分布。通过最小化等式（8）中的负证据下界，我们可以使估计网络逼近真实后验并收紧能量函数的界限。在DAGMM中，我们使用方程（6）作为目标函数的一部分，而不是方程（10）中的上限，仅仅是因为DAGMM的能量函数易于处理且评估高效。与如上所述使用深度估计网络来定义变分后验分布的神经变分推理不同，DAGMM 显式地使用深度估计网络来参数化依赖于样本的先验分布。在机器学习研究的历史中，有一些研究致力于利用神经网络来计算混合模型中的样本成员资格，例如自适应专家混合（Jacobs et al. (1991)）。从这个角度来看，DAGMM 可以被视为与深度自动编码器相结合的自适应专家混合的强大深度无监督版本。 3.6 训练策略 与依赖于预训练的现有基于深度自动编码器的方法（Yang 等人（2017a）；Xie 等人（2016））不同，DAGMM 采用端到端训练。首先，在我们的研究中，我们发现预训练的压缩网络的异常检测性能有限，因为很难对训练有素的深度自动编码器进行重大改变以有利于后续的密度估计任务。其次，我们还发现压缩网络和估计网络可以相互提高彼此的性能。一方面，通过估计网络引入的正则化，通过端到端训练学习的压缩网络中的深度自编码器可以将重构误差降低到与预训练对应物的误差一样低，而这是通过只需仅使用深度自动编码器执行端到端训练。另一方面，利用压缩网络中充分学习的低维表示，估计网络能够做出有意义的密度估计。 6 作为 ICLR 2018 会议论文发表 在第 4.5 节中，我们采用公共基准数据集中的示例来讨论 DAGMM 中预训练和端到端训练之间的选择。 

# 4 实验结果 

在本节中，我们使用公共基准数据集来证明 DAGMM 在无监督异常检测中的有效性。 

## 4.1 数据集 

KDDCUP 甲状腺心律失常 KDDCUP-Rev # 维度 # 实例 异常比率 (ρ) 120 6 274 120 494,021 3,772 452 121,597 0.2 0.025 0.15 0.2 表 1：公共基准数据集的统计 我们采用四个基准数据集：KDDCUP、T甲状腺、心律失常、和 KDDCUP-Rev。 KDDCUP。来自 UCI 存储库的 KDDCUP99 10% 数据集（Lichman (2013)）最初包含 41 个维度的样本，其中 34 个是连续的，7 个是分类的。对于分类特征，我们进一步使用one-hot表示对其进行编码，最终获得120维的数据集。由于20%的数据样本被标记为“正常”，其余的被标记为“攻击”，因此“正常”样本属于少数；因此，在此任务中，“正常”的被视为异常。甲状腺。甲状腺（Lichman (2013)）数据集是从 ODDS 存储库 1 获得的。原始数据集中有 3 个类。在此任务中，机能亢进类被视为异常类，而其他两个类被视为正常类，因为机能亢进是明显的少数类。心律失常。心律失常 (Lichman (2013)) 数据集也从 ODDS 存储库获得。最小的类，包括 3、4、5、7、8、9、14 和 15，组合起来形成异常类，其余的类组合起来形成正常类。 KDDCUP-修订版。该数据集源自 KDDCUP。我们保留所有标记为“正常”的数据样本，并随机抽取标记为“攻击”的样本，使得“正常”和“攻击”之间的比例为4：1。这样，我们得到了异常比率为0.2的数据集，其中“攻击”样本属于少数群体并被视为异常。请注意，“攻击”样本不是固定的，我们在每次运行中随机抽取“攻击”样本。有关数据集的详细信息如表 1 所示。

## 4.2 基线方法 

我们将传统和最先进的深度学习方法视为基线。 OC-SVM。一类支持向量机（Chen et al. (2001)）是异常检测中使用的一种流行的基于内核的方法。在实验中，我们在所有任务中都采用了广泛采用的径向基函数（RBF）内核。 DSEBM-e。基于深度结构化能量的模型（DSEBM）（Zhai 等人（2016））是一种用于无监督异常检测的最先进的深度学习方法。在 DSEBM-e 中，利用样本能量作为检测异常的标准。 DSEBM-r。 DSEBM-e 和 DSEBM-r (Zhai et al. (2016)) 共享相同的核心技术，但 DSEBM-r 使用重构误差作为异常检测的标准。 1 http://odds.cs.stonybrook.edu/ 7 作为会议论文发表于 ICLR 2018 DCN。深度聚类网络（DCN）（Yang et al. (2017a)）是一种最先进的聚类算法，通过 k-means 调节自动编码器性能。我们将该技术应用于异常检测任务。特别地，将样本与其聚类中心之间的距离作为异常检测的标准：距离聚类中心越远的样本更有可能是异常的。此外，我们还包括以下 DAGMM 变体作为基线，以证明 DAGMM 中各个组件的重要性。 GMM-EN。在此变体中，我们从 DAGMM 的目标函数中删除了重建误差分量。换句话说，DAGMM 中的估计网络在不受压缩网络约束的情况下执行隶属度估计。通过学习到的隶属度估计，我们在 GMM 框架下通过方程（5）和（6）推断样本能量。样本能量用作异常检测的标准。 PAE。我们通过从 DAGMM 的目标函数中去除能量函数来获得这个变体，这个 DAGMM 变体相当于一个深度自编码器。为了确保压缩网络得到良好的训练，我们采用预训练策略（Vincent et al. (2010)）。样本重建误差是异常检测的标准。 E2E-AE。该变体与 PAE 具有相同的设置，但深度自动编码器是通过端到端训练学习的。样本重构误差是PAE-GMM-EM异常检测的标准。该变体采用两步方法。第一步，我们通过预训练深度自动编码器来学习压缩网络。第二步，我们使用压缩网络的输出通过传统的 EM 算法来训练 GMM。两个步骤中的训练过程是分开的。样本能量用作异常检测的标准。 PAE-GMM。该变体也采用了两步方法。第一步，我们通过预训练深度自动编码器来学习压缩网络。第二步，我们使用压缩网络的输出来训练估计网络。两个步骤中的训练过程是分开的。样本能量用作异常检测的标准。 DAGMM-p。该变体是 DAGMM 和 PAE-GMM 之间的折衷方案：我们首先通过预训练来训练压缩网络，然后通过端到端训练来微调 DAGMM。样本能量是异常检测的标准。 DAGMM-NVI。该变体与 DAGMM 的唯一区别在于，该变体采用神经变分推理框架（Mnih & Gregor (2014)），并将方程（6）替换为方程（10）中的上界作为目标函数的一部分。4.3 DAGMM 配置在所有实验中，我们考虑压缩网络的两个重建特征：相对欧几里得距离和余弦相似度。给定样本 x 及其重构对应物 x0 ，它们的 0 kx − x0 k 相对欧几里德距离定义为 kxk 2 ，余弦相似度由 kxkx·x 导出。 0 2 2 kx k2 在附录D中，对于感兴趣的读者，我们讨论了为什么重建特征对于DAGMM很重要以及在实践中如何选择重建特征。在各个数据集上使用的 DAGMM 的网络结构总结如下。 KDDCUP。对于该数据集，其压缩网络向估计网络提供 3 维输入，其中一个是降维，另外两个来自重建误差。估计网络考虑具有 4 个混合分量的 GMM 以获得最佳性能。特别地，压缩网络以 FC(120, 60, tanh)-FC(60, 30, tanh)-FC(30, 10, tanh)-FC(10, 1, none)-FC(1, 10, tanh)-FC(10, 30, tanh)-FC(30, 60, tanh)-FC(60, 120, none)，估计网络执行 FC(3, 10, tanh)-Drop(0.5)- FC(10, 4, softmax)。甲状腺。该数据集的压缩网络还为估计网络提供 3 维输入，并且估计网络采用 2 个混合组件以获得最佳性能。特别是，压缩网络运行 FC(6, 12, tanh)-FC(12, 4, 8 作为 ICLR 2018 会议论文发表 tanh)-FC(4, 1, none)-FC(1, 4, tanh)-FC(4, 12, tanh)-FC(12, 6, none)，估计网络以 FC(3, 10, tanh)-Drop(0.5)-FC(10, 2, softmax) 执行。心律失常。该数据集的压缩网络提供 4 维输入，其中两个是降维，估计网络采用 2 个混合组件的设置以获得最佳性能。特别地，压缩网络以 FC(274, 10, tanh)-FC(10, 2, none)-FC(2, 10, tanh)-FC(10, 274, none) 运行，估计网络以FC(4, 10, tanh)-Drop(0.5)-FC(10, 2, softmax)。 KDDCUP-修订版。对于该数据集，其压缩网络向估计网络提供 3 维输入，其中一个是降维，另外两个来自重建误差。估计网络考虑具有 2 个混合分量的 GMM 以获得最佳性能。特别地，压缩网络以 FC(120, 60, tanh)-FC(60, 30, tanh)-FC(30, 10, tanh)-FC(10, 1, none)-FC(1, 10, tanh)-FC(10, 30, tanh)-FC(30, 60, tanh)-FC(60, 120, none)，估计网络执行 FC(3, 10, tanh)-Drop(0.5)- FC(10, 2, softmax)。其中 FC(a, b, f ) 表示由函数 f 激活的具有 a 个输入神经元和 b 个输出神经元的全连接层（无表示不使用激活函数），Drop(p) 表示保持概率为 p 的 dropout 层在训练中。所有 DAGMM 实例均由张量流（Abadi 等人（2016））实现，并由 Adam（Kingma & Ba（2015））算法以学习率 0.0001 进行训练。对于 KDDCUP、甲状腺、心律失常和 KDDCUP-Rev，训练 epoch 数分别为 200、20000、10000 和 400。对于小批量的大小，分别设置为 1024、1024、128 和 1024。此外，在所有 DAGMM 实例中，我们将 λ1 设置为 0.1，将 λ2 设置为 0.005。对于感兴趣的读者，我们在附录 F 中讨论 λ1 和 λ2 如何影响 DAGMM。对于基线方法，我们进行详尽的搜索以找到它们的最佳元参数，以实现最佳性能。我们在附录 A 中详细介绍了它们的具体配置。 

## 4.4 精度指标

我们将平均精度、召回率和 F1 分数视为比较异常检测性能的直观方法。特别是，根据表1中建议的异常比率，我们选择识别异常样本的阈值。例如，当DAGMM在KDDCUP上执行时，最高能量的前20%样本将被标记为异常。我们将异常类视为正类，并相应地定义精度、召回率和 F1 分数。在第一组实验中，我们遵循（Zhai et al. (2016)）中的设置，使用完全干净的训练数据：在每次运行中，我们随机采样 50% 的数据进行训练，其余 50% 保留用于测试，并且仅使用正常类的数据样本来训练模型。表 2 报告了 DAGMM 及其基线运行 20 次后的平均精度、召回率和 F1 分数。一般来说，DAGMM 在所有数据集上的 F1 分数方面都表现出了优于基线方法的性能。特别是在 KDDCUP 和 KDDCUP-Rev 上，与现有方法相比，DAGMM 在 F1 分数上实现了 14% 和 10% 的提高。对于 OC-SVM 来说，维数灾难可能是限制其性能的主要原因。对于 DSEBM，虽然它在多个数据集上工作得相当好，但 DAGMM 的表现优于，因为在能量建模中同时考虑了潜在表示和重建误差。对于 DCN、PAE-GMM 和 DAGMM-p，它们的性能可能受到预训练深度自动编码器的限制。当深度自动编码器经过良好训练时，很难在减少的维度上做出任何重大改变并有利于后续的密度估计任务。对于GMM-EN，如果没有重构约束，似乎很难进行合理的密度估计。就 PAE 而言，重建误差的单一视图可能不足以完成异常检测任务。对于 E2E-AE，我们观察到它无法像 PAE 和 DAGMM 在 KDDCUP、KDDCUP-Rev 和 Thyroid 上那样将重建误差降低得那么低。由于降维过程中数据的关键信息可能会丢失，E2E-AE在KDDCUP和Thyroid上的性能较差。此外，DAGMM和DAGMM-NVI的性能非常相似。由于 GMM 是一个相当简单的图模型，我们无法发现 DAGMM 中神经变分推理带来的显着改进。在附录B中，为了方便读者感兴趣，我们展示了在干净的训练数据设置下，DAGMM对于所有数据集学习到的能量函数的累积分布函数。 9 作为会议论文发表在 ICLR 2018 方法精度 OC-SVM DSEBM-r DSEBM-e DCN GMM-EN PAE E2E-AE PAE-GMM-EM PAE-GMM DAGMM-p DAGMM-NVI DAGMM 0.7457 0.1972 0.7369 0.7696 0.1932 0.7276 0.0024 0.7183 0.7251 0.7579 0.9290 0.9297 KDDCUP 召回 0.8523 0.2001 0.7477 0.7829 0.1967 0.7397 0.0025 0.7311 0.7384 0.7710 0.9447 0.944 2 F1 精度 0.7954 0.1987 0.7423 0.7762 0.1949 0.7336 0.0024 0.7246 0.7317 0.7644 0.9368 0.9369 0.3639 0.0404 0.1319 0.3319 0.0213 0 .1894 0.1064 0.4745 0.4532 0.4723 0.4383 0.4766 心律失常精确回忆法 OC -SVM DSEBM-r DSEBM-e DCN GMM-EN PAE E2E-AE PAE-GMM-EM PAE-GMM DAGMM-p DAGMM-NVI DAGMM 0.5397 0.1515 0.4667 0.3758 0.3000 0.4393 0.4667 0.3970 0.4575 0.4909 0 .5091 0.4909 0.4082 0.1513 0.4565 0.3907 0.2792 0.4437 0.4538 0.4168 0.4823 0.4679 0.4892 0.5078 甲状腺召回 F1 0.4239 0.0403 0.1319 0.3196 0.0227 0.2062 0.1316 0.4538 0.4881 0.4725 0.4587 0.48 34 0.3887 0.0403 0.1319 0.3251 0.0220 0.1971 0.1176 0.4635 0.4688 0.4713 0.4470 0.4782 KDDCUP-Rev 精确召回 F1 F1 0.4581 0.1510 0.4601 0.3 815 0.2886 0.4403 0.4591 0.4056 0.4684 0.4787 0.4981 0.4983 0.7148 0.2036 0.2212 0.2875 0.1846 0.7835 0.7434 0.2822 0.6307 0.2750 0.9211 0.9370 0.9940 0.2036 0.2213 0.2895 0.17 46 0.7817 0.7463 0.2847 0.6278 0.2810 0.9211 0.9390 0.8316 0.2036 0.2213 0.2885 0.1795 0.7826 0.7448 0.2835 0.6292 0.2780 0.9 211 0.9380 表 2：DAGMM 的平均精度、召回率和 F1基线方法。对于每个指标，最佳结果以粗体显示。在第二组实验中，我们研究了 DAGMM 如何响应受污染的训练数据。在每次运行中，我们通过随机抽样保留 50% 的数据进行测试。对于剩下的 50%，我们将正常类的所有样本与异常类的 c% 样本混合进行模型训练。比率 c 1% 2% 3% 4% 5% 比率 c 1% 2% 3% 4% 5% 精度 0.9201 0.9186 0.9132 0.8837 0.8504 DAGMM 召回率 0.9337 0.9340 0.9272 0.8989 0.8643 DSEBM-e 精度召回率 0.699 5 0.6780 0.6213 0.5704 0.5345 0.7135 0.6876 0.6367 0.5813 0.5375 F1 精度 DCN 召回 F1 0.9268 0.9262 0.9201 0.8912 0.8573 0.7585 0.7380 0.7163 0.6971 0.6763 0.7611 0.7424 0.7293 0.7106 0. 6893 0.7598 0.7402 0.7228 0.7037 0.6827 F1 精度 0.7065 0.6827 0.6289 0.5758 0.5360 0.7129 0.6668 0.6393 0.5991 0.1155 OC-SVM 召回率 0.678 5 0.5207 0.4470 0.3719 0.3369 F1 0.6953 0.5847 0.5261 0.4589 0.1720 表 3：KDDCUP 污染训练数据的异常检测结果 表 3 分别报告了在 KDDCUP 数据集上运行 20 次 DAGMM、DCN、DSEBMe 和 OC-SVM 后的平均精度、召回率和 F1 分数。正如预期的那样，受污染的训练数据会对检测准确性产生负面影响。当污染率 c 从 1% 增加到 5% 时，所有方法的平均精度、召回率和 F1 分数都会下降。同时，我们注意到DAGMM能够在5%的污染数据下保持良好的检测精度。对于 OC-SVM，我们采用与实验中使用的相同的参数设置，并使用干净的训练数据，并观察到 OC-SVM 对污染率比 10 更敏感。为了获得更好的检测精度，使用高质量数据（即干净或保持污染率尽可能低）训练模型非常重要。总之，通过端到端训练学习到的 DAGMM 在公共基准数据集上实现了最先进的准确性，并为无监督异常检测提供了一种有前途的替代方案。 4.5 学习到的低维表示的可视化 在本节中，我们使用一个示例来演示通过端到端训练学习到的 DAGMM 与依赖于预训练深度自动编码器的基线相比的优势。 (a) KDDCUP@DAGMM (b) KDDCUP@PAE (c) KDDCUP@DAGMM-p (d) KDDCUP@DCN 图 3：通过 DAGMM、PAE、DAGMM-p 和 DCN 学习的 3 维空间中的 KDDCUP 样本，其中红点是来自异常类的样本，蓝色点是来自正常类的样本。图 3 显示了 DAGMM、PAE、DAGMM-p 和 DCN 从 KDDCUP 数据集上运行的一项实验中学习到的低维表示。首先，从图3a中我们可以看到，DAGMM可以更好地将所学习的低维空间中的异常样本与正常样本分开，而PAE、DAGMM-p或DCN所学习的低维空间中异常与正常样本重叠更多。其次，即使 DAGMM-p 和 DCN 努力通过其估计网络或 k 均值正则化来微调预训练的深度自动编码器，人们也几乎看不到图 3b、图 3c 和图 3d 之间的显着变化，其中许多异常样本仍然与正常样本混合在一起。事实上，当深度自动编码器经过预训练时，它往往会陷入良好的局部最优状态，仅用于重建目的，但对于后续的密度估计任务来说，它可能不是最优的。此外，在我们的研究中，我们发现经过训练的 DAGMM 中的重建误差与从预训练的深度自动编码器接收到的误差一样低（例如，KDDCUP 的每样本重建误差约为 0.26）。同时，我们还观察到，通过端到端训练很难降低相同结构的深度自编码器的重建误差（例如，KDDCUP 的每样本重建误差约为 1.13）。换句话说，压缩网络和估计网络在端到端训练期间相互提高彼此的性能：估计网络引入的正则化帮助深度自动编码器摆脱不太有吸引力的局部最优以获得更好的压缩，而压缩网络则提供反馈为估计网络提供更有意义的低维表示，以实现鲁棒性 11 作为 ICLR 2018 密度估计的会议论文发表。在附录 C 中，对于感兴趣的读者，我们展示了 DSEBM 学习到的潜在表示的可视化。总之，我们的实验结果表明，DAGMM 为密度估计和异常检测提出了一个有前途的方向，其中可以通过端到端训练将降维和密度估计的力量结合起来。在附录 E 中，我们提供了另一个案例研究，为感兴趣的读者讨论哪种样本更能从 DAGMM 联合训练中受益。 5 结论 在本文中，我们提出了用于无监督异常检测的深度自编码高斯混合模型（DAGMM）。 DAGMM由压缩网络和估计网络两个主要组成部分组成，其中压缩网络将样本投影到保留异常检测关键信息的低维空间，估计网络在框架下评估低维空间中的样本能量高斯混合模型。 DAGMM 对端到端训练友好：（1）估计网络预测样本混合隶属度，从而无需交替过程即可估计 GMM 中的参数； （2）估计网络引入的正则化帮助压缩网络摆脱吸引力较小的局部最优，并通过端到端训练实现低重建误差。与预训练策略相比，端到端训练可能更有利于密度估计任务，因为我们可以更自由地调整降维过程以有利于后续的密度估计任务。在实验研究中，DAGMM 在公共基准数据集上展示了优于最先进技术的性能，在标准 F1 分数上提高了 14%，并为多维或高维数据的无监督异常检测提供了一个有前途的方向。 12 作为会议论文发表于 ICLR 2018 

# 参考资料

Mart ı́ n Abadi, Paul Barham, Jiangmin Chen,zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow：大规模机器学习系统。 OSDI，第 16 卷，第 265-283 页，2016 年。V. Barnett 和 T. Lewis。统计数据中的异常值。 Wiley，1984。Emmanuel J. Candès、Xiaodong Li、Yi Ma 和 John Wright。稳健的主成分分析？ J. ACM，58(3):11:1–11:37，2011。ISSN 0004-5411。瓦伦·昌多拉、阿林达姆·班纳吉和维平·库马尔。异常检测：一项调查。 ACM 计算。 Surv, 41:15:1–15:58, 2009。陈云强、周翔、Thomas S Huang。用于图像检索学习的一类支持向量机。国际图像处理会议，第 1 卷，第 34-37 页，2001 年。Simon Günter、Nicol N. Schraudolph 和 SVN Vishwanathan。快速迭代核主成分分析。 JMLR，8：1893-1918。彼得·J·胡贝尔.稳健的统计数据。国际统计科学百科全书，第 1248–1251 页。施普林格，2011。罗伯特·A·雅各布斯、迈克尔·乔丹、史蒂文·J·诺兰和杰弗里·E·辛顿。当地专家的适应性组合。神经计算，3(1):79–87, 1991。IT Jolliffe。主成分分析。在主成分分析中。 Springer Verlag，纽约，1986 年。Michael I Jordan、Zoubin Ghahramani、Tommi S Jaakkola 和 Lawrence K Saul。图模型变分方法简介。机器学习，37(2):183–233, 1999。Fabian Keller、Emmanuel Muller 和 Klemens Bohm。 Hics：用于基于密度的异常值排名的高对比度子空间。国际数据工程会议，第 1037-1048 页。 IEEE，2012。JooSeuk Kim 和 Clayton D. Scott。鲁棒核密度估计。 CoRR，abs/1107.3133，2011。Diederik Kingma 和 Jimmy Ba。 Adam：一种随机优化方法。国际学习表征会议，2015 年。M. Lichman。 UCI 机器学习存储库，2013 年。URL http://archive.ics.uci.edu/ml。刘飞、丁凯明和周志华。隔离森林。国际数据挖掘会议，第 413-422 页。 IEEE，2008。Andriy Mnih 和 Karol Gregor。信念网络中的神经变分推理和学习。 ICML，第 1791–1799 页，2014 年。Matthias Paulik。瓶颈特征提取神经网络的基于格的训练。 In Interspeech，第 89-93 页，2013 年。宋，WJ Hu，WF Xie。具有弹孔图像分类功能的鲁棒支持向量机。 IEEE 传输。系统、人与控制论，32:440–448，2002。Swee Chuan Tan、Kai Ming Ting 和 Tony Fei Liu。流数据的快速异常检测。 In IJCAI Proceedings-International Joint Conference on Artificial Intelligence，第 22 卷，第 1511 页，2011 年。Zoltán Tu ̈ ske、Pavel Golik、Ralf Schlu ̈ ter 和 Hermann Ney。高斯混合模型和瓶颈特征的说话人自适应联合训练。载于 IEEE 自动语音识别和理解 (ASRU) 研讨会，第 596 – 603 页，2015a。 Zoltán Tu ske 、穆罕默德·阿里·塔希尔、拉尔夫·施鲁特和赫尔曼·内伊。将高斯混合集成到深度神经网络中：带有隐藏变量的 softmax 层。载于 ICASSP，第 4285–4289 页，2015b。 13 Ehsan Variani、Erik McDermott 和 Georg Heigold 在 ICLR 2018 上作为会议论文发表。高斯混合模型层与深度神经网络架构中的判别特征联合优化。 ICASSP，第 4270–4274 页，2015 年。Pascal Vincent、Hugo Larochelle、Isabelle Lajoie、Yoshua Bengio 和 Pierre-Antoine Manzagol。堆叠式去噪自动编码器：使用局部去噪标准在深度网络中学习有用的表示。 《机器学习研究杂志》，11（12 月）：3371–3408，2010。Graham Williams、Rohan Baxter、何红星和 Simon Hawkins。 RNN 在数据挖掘中异常值检测的比较研究。 ICDM02 会议记录，第 709-712 页，2002 年。谢俊元、Ross Girshick 和 Ali Farhadi。用于聚类分析的无监督深度嵌入。国际机器学习会议，第 478–487 页，2016 年。Liang Xiong、Barnabás Póczos 和 Jeff G. Schneider。使用灵活的流派模型进行群体异常检测。 《神经信息处理系统进展》，第 1071–1079 页，2011 年。Bo Yang、Xiao Fu、Nicholas D Sidiropoulos 和 Mingyi Hong。迈向 k 均值友好空间：同时进行深度学习和聚类。国际机器学习会议，2017a。杨希，黄凯柱，张锐。高斯混合模型的无监督降维。神经信息处理国际会议，第 84-92 页。 Springer，2014。杨希、黄凯柱、约翰·雅尼斯·古勒马斯和张睿。无监督降维和高斯混合模型的联合学习。神经处理快报，45(3)：791–806，2017b。翟双飞，程宇，卢伟宁，张忠飞。用于异常检测的基于深度结构化能量的模型。国际机器学习会议，第 1100–1109 页，2016 年。C. 张和 PC Woodland。使用高斯混合密度神经网络判别序列训练的串联系统联合优化。 ICASSP，第 5015–5019 页，2017 年。Chong Zhou 和 Randy C. Paffenroth。使用强大的深度自动编码器进行异常检测。第 23 届 ACM SIGKDD 国际知识发现和数据挖掘会议论文集，第 665-674 页，2017 年。Arthur Zimek、Erich Schubert 和 Hans-Peter Kriegel。高维数值数据中无监督异常值检测的调查。统计分析和数据挖掘，5:363–387，2012。 14 作为 ICLR 2018 基线配置 OC-SVM 会议论文发表。与其他基线在测试阶段只需要决策阈值不同，OC-SVM 需要在训练阶段设置参数 ν。虽然ν直观上意味着训练数据中的异常率，但在训练数据都是正常样本并且测试阶段的异常率可以是任意的情况下，设置合理的ν并非易事。在本研究中，我们只是执行详尽的搜索来找到在各个数据集上呈现最高 F1 分数的最佳 ν。特别地，对于KDDCUP、甲状腺、心律失常和KDDCUP-Rev，ν分别设置为0.1、0.02、0.04和0.1。 DSEBM。我们使用 DAGMM 中编码的网络结构作为设置 DSEBM 实例的指南。对于 KDDCUP 和 KDDCUP-Rev，配置为 FC(120, 60, softplus)-FC(60, 30, softplus)-FC(30, 10, softplus)-FC(10, 1, softplus)。对于甲状腺，它是 FC(6, 12, softplus)-FC(12, 4, softplus)-FC(4, 1, softplus)。对于心律失常，它是 FC(274, 10, softplus)-FC(10, 2, softplus)。此外，对于 KDDCUP、甲状腺、心律失常和 KDDCUP-Rev，epoch 的数量分别为 200、20000、10000 和 400，小批量的大小分别为 1024、1024、128 和 1024。 DCN。我们使用 DAGMM 中自动编码器的网络配置作为在 DCN 中设置自动编码器的指南。对于 KDDCUP 和 KDDCUP-Rev，结构为 FC(120, 60, tanh)-FC(60, 30, tanh)-FC(30, 10, tanh)-FC(10, 1, none)-FC(1, 10, tanh)-FC(10, 30, tanh)-FC(30, 60, tanh)FC(60, 120, 无)。对于甲状腺，它是 FC(6, 12, tanh)-FC(12, 4, tanh)-FC(4, 1, none)-FC(1, 4, tanh)-FC(4, 12, tanh)- FC（12、6、无）。对于心律失常，其为 FC(274, 10, tanh)-FC(10, 2, none)-FC(2, 10, tanh)-FC(10, 274, none)。此外，对于KDDCUP、Thyroid、Arrythmia和KDDCUP-Rev，每层预训练的epoch数分别为200、20000、10000和400，微调的epoch数分别为200、20000、10000 、 和 400，所有训练阶段的小批量大小分别为 1024、1024、128 和 1024。 GMM-EN。 GMM-EN 还借鉴了 DAGMM 中网络配置的智慧。对于 KDDCUP，为 FC(120, 60, tanh)-FC(60, 30, tanh)-FC(30, 10, tanh)-FC(10, 1, none)-FC(1, 10, tanh)-下降 (0.5)-FC(10, 4, softmax)。对于甲状腺，它是 FC(6, 12, tanh)-FC(12, 4, tanh)-FC(4, 1, none)-FC(1, 10, tanh)-Drop(0.5)-FC(10, 2、softmax）。对于心律失常，它是 FC(274, 10, tanh)FC(10, 2, none)-FC(2, 10, tanh)-Drop(0.5)-FC(10, 2, softmax)。对于 KDDCUP-Rev，为 FC(120, 60, tanh)-FC(60, 30, tanh)-FC(30, 10, tanh)-FC(10, 1, none)-FC(1, 10, tanh) )-Drop(0.5)-FC(10, 2, softmax)。对于 KDDCUP、甲状腺、心律失常和 KDDCUP-Rev，训练的 epoch 数分别为 200、20000、10000 和 400，小批量的大小分别为 1024、1024、128 和 1024。 PAE。 PAE 与 DAGMM 中的自动编码器共享相同的网络结构。对于KDDCUP、Thyroid、Arrythmia和KDDCUP-Rev，每层预训练的epoch数分别为200、20000、10000和400，微调的epoch数分别为200、20000、10000和分别为 400，所有训练阶段的小批量大小分别为 1024、1024、128 和 1024。 E2E-AE。 E2E-AE 与 DAGMM 中的自动编码器共享相同的网络结构。对于 KDDCUP、Thyroid、Arrythmia 和 KDDCUP-Rev，端到端训练的 epoch 数分别为 200、20000、10000 和 400，小批量的大小分别为 1024、1024、128 和分别为 1024。 PAE-GMM-EM。 PAE-GMM 和 DAGMM 共享相同的网络配置。对于KDDCUP、Thyroid、Arrythmia和KDDCUP-Rev，每层预训练的epoch数分别为200、20000、10000和400，微调的epoch数分别为200、20000、10000和分别为 400，所有训练阶段的小批量大小分别为 1024、1024、128 和 1024。对于 GMM 学习，当当前迭代与前一次迭代之间的参数最大差值小于 10 − 6 时，EM 算法停止。 15 作为会议论文发表在 ICLR 2018 PAE-GMM 上。 PAE-GMM 和 DAGMM 共享相同的网络配置。对于KDDCUP、Thyroid、Arrythmia和KDDCUP-Rev，每层预训练的epoch数分别为200、20000、10000和400，微调或GMM训练的epoch数分别为200、20000、分别为 10000 和 400，所有训练阶段的小批量大小分别为 1024、1024、128 和 1024。 DAGMM-p。 DAGMM-p 和 DAGMM 具有相同的网络配置，只是训练策略不同：DAGMM 采用端到端训练的策略，而 DAGMM-p 依赖于预训练压缩网络，然后联合微调。对于KDDCUP、Thyroid、Arrythmia和KDDCUP-Rev，每层预训练的epoch数分别为200、20000、10000和400，微调的epoch数分别为200、20000、10000和分别为 400，所有训练阶段的小批量大小分别为 1024、1024、128 和 1024。 DAGMM-NVI。 DAGMM 和 DAGMM-NVI 共享相同的网络配置和训练策略，如第 4 节中所述。 DAGMM 学习的能量函数的 BC 累积分布函数 (a) KDDCUP (b) 甲状腺 (c) 心律失常 (d) KDDCUP-Rev 图 4： DAGMM 分别在 KDDCUP、心律失常、甲状腺和 KDDCUP-Rev 上学习能量函数的累积分布函数。横轴表示能量空间，纵轴表示百分比。图 4 显示了 DAGMM 分别在 KDDCUP、心律失常、甲状腺和 KDDCUP-Rev 上学习的能量函数的累积分布函数 (cdf)。特别是，在 KDDCUP 和 KDDCUP-Rev 上，我们观察到能量以 80% 左右的速度快速增加，并且大多数能量超过 80% 的样本都是真正的异常样本。 16 作为会议论文发表于 ICLR 2018 CL OW- DIMENSIONAL REPRESENTATION LEARNED BY DSEBM (a) 所有样本 (b) 仅正态样本 图 5：DSEBM 简化一维空间中的 KDDCUP 样本，其中红点是来自异常类和蓝色类是来自正常类的样本。图 5 展示了 DSEBM 学习到的 KDDCUP 样本的简化一维表示，其中图 5a 包括所有样本，图 5b 仅包括正常样本。如上所示，正常样本和异常样本混合在[ −8 , −7.8 ]范围内。对于这个范围内的样本，很难使用从潜在表示中获得的能量来分离它们。 DAGMM 中的 DR 重建功能在本节中，我们详细讨论重建功能。为什么重建特征很重要？通过对私有网络安全数据集的调查，我们认识到重建特征的重要性。在该数据集中，正常样本是正常的网络流，异常是受到欺骗攻击的网络流。由于很难从 20 维的原始空间中分析样本，因此我们利用深度自动编码器来执行降维。在这种情况下，我们有点雄心勃勃，将维度从 20 减少到 1。在减少的一维空间中，对于一些异常，我们能够轻松地将它们与正常样本分开。然而，对于其余的，它们的潜在表示与正常样本的表示非常相似。而在原本的空间里，它们其实与正常的有着很大的不同。受这一观察的启发，我们研究了它们的 L2 重建误差，并获得了如图 1 所示的图。在图 1 中，右上角的红点是与缩小空间中的正常样本具有相似表示的异常。通过重建误差的附加视图，可以更容易地将这些异常与正常样本分开。在我们的研究中，这个具体的例子激励我们将重建特征纳入 DAGMM 中。重建特征选择的指导原则是什么？在实际应用中，可以按照以下规则选择重构特征。首先，对于用于导出重建特征的误差度量，其分析形式应该是连续且可微的。其次，误差度量的输出应该在相对较小的值范围内，以便于在 DAGMM 中训练估计网络。在本文的实验中，我们根据这两个规则选择余弦相似度和相对欧氏距离。对于余弦相似度来说，它是连续且可微的，其输出的范围是[ −1 , 1]。对于相对欧氏距离，它也是连续且可微的。理论上，其输出的范围是[0,+∞)。在实验中考虑的数据集上，我们观察到它的输出通常是一个小的正值；因此，我们将此指标作为重建特征之一。总之，只要误差度量满足上述两个规则，它就可以作为候选度量来导出 DAGMM 的重建特征。 17 作为会议论文发表在 ICLR 2018 EC ASE 研究：联合训练何时优于解耦训练？在本节中，我们进行了一个案例研究，以调查与解耦训练相比，哪种样本更能从 DAGMM 中应用的联合训练中受益。在评估中，我们采用 PAE-GMM 作为利用解耦训练的方法的代表，并且在 KDDCUP 数据集上运行一次生成以下结果。 DAGMM 检测 DAGMM 未命中 PAE-GMM 检测 PAE-GMM 未命中 34,285 1,640 12,038 926 表 4：DAGMM 与 PAE-GMM 异常检测结果比较 表 4 展示了 DAGMM 与 PAE-GMM 异常检测结果的比较。本次测试数据中，共有 48, 889 个异常，其中 34, 285 个异常均被两种技术检测到，926 个异常均未被两种技术检测到，1, 640 个异常只能被 PAE-GMM 检测到, 其中 12, 038 只能通过 DAGMM 检测到。接下来，我们更深入地钻研并研究这 12, 038 个只能由 DAGMM 检测到的异常的共性。图6说明了DAGMM和PAE-GMM学习到的低维空间中的样本分布，其中图6a（6b）包括所有正常样本和异常样本，图6c（6d）包括所有正常样本和两种技术检测到的异常样本，图 6e (6f) 包括所有正常样本和仅由 DAGMM 检测到的异常。从图 6c 和 6d 中，我们观察到低余弦相似度和高相对欧几里得距离的异常可能是两种技术最容易捕获的异常。对于图 6e 和 6f 中所示的困难问题，我们观察到它们通常具有中等水平的相对欧几里德距离（两种情况都在 [1.0, 1.2] 范围内），余弦相似度大于 0.6。对于此类异常样本，PAE-GMM 学习的模型很难将其与正常样本分开。此外，我们还观察到，与 PAE-GMM 相比，DAGMM 学习的模型倾向于为此类异常分配较低的余弦相似度，这也使得更容易区分异常和正常样本。目标函数中的超参数影响 DAGMM 如等式（7）所示，DAGMM 的目标函数包括三个组成部分：深度自编码器的损失函数、估计网络的能量函数以及协方差矩阵的惩罚函数。三个分量之间的系数比可以表征为1：λ1：λ2。就 λ1 而言，较大的值可能会使深度自动编码器的损失函数在优化中发挥很小的作用，从而无法获得输入样本的良好缩减表示，而较小的值可能会导致估计网络无效，从而导致 GMM 为没有受过良好的训练。对于较大值的 λ2，DAGMM 倾向于找到具有较大协方差的 GMM，但这是不太理想的，因为许多样本将具有作为稀有事件的高能量。对于较小值的 λ2，正则化可能不足以对抗奇点效应。在我们的探索中，我们发现比率 1:0.1:0.005 在实验中的所有数据集中始终提供预期结果。为了研究这个比率的敏感性，我们改变它的基数，看看不同的基数如何影响异常检测的准确性。例如，当基数设置为2时，λ1和λ2分别调整为0.2和0.01。表 5 显示了在 KDDCUP 数据集上运行 20 次 DAGMM 后的平均精度、召回率和 F1 分数。当我们通过步骤 2 将基数从 1 更改为 9 时，DAGMM 以一致的方式执行，并且 λ1 、 λ2 对基数的变化不敏感。18 作为 ICLR 2018 的会议论文发表 (a) All anomalies@DAGMM (b) 所有异常@PAE-GMM (c) 均检测@DAGMM (d) 均检测@PAE-GMM (e) 仅 DAGMM 检测@DAGMM (f) 仅 DAGMM 检测@PAE-GMM 图 6：通过DAGMM和PAE-GMM学习3维空间，其中红点是来自异常类的样本，蓝色点是来自正常类的样本Base Precision Recall F1 1 3 5 7 9 0.9298 0.9301 0.9296 0.9300 0.9300 0.9445 0.9442 0.9451 0.9453 0.9439 0。 9371 0.9371 0.9373 0.9376 0.9369 表 5：KDDCUP 19 上固定比率 1 : 0.1 : 0.005 的 λ1 和 λ2 的灵敏度